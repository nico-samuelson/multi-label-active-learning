{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98611964",
   "metadata": {
    "papermill": {
     "duration": 0.015086,
     "end_time": "2025-03-08T10:30:42.032117",
     "exception": false,
     "start_time": "2025-03-08T10:30:42.017031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346f89e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:30:42.061658Z",
     "iopub.status.busy": "2025-03-08T10:30:42.061322Z",
     "iopub.status.idle": "2025-03-08T10:31:16.783223Z",
     "shell.execute_reply": "2025-03-08T10:31:16.782104Z"
    },
    "papermill": {
     "duration": 34.738448,
     "end_time": "2025-03-08T10:31:16.785115",
     "exception": false,
     "start_time": "2025-03-08T10:30:42.046667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107015b80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nicost/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1afb9a",
   "metadata": {
    "papermill": {
     "duration": 0.013657,
     "end_time": "2025-03-08T10:31:16.813480",
     "exception": false,
     "start_time": "2025-03-08T10:31:16.799823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3aa53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:16.842667Z",
     "iopub.status.busy": "2025-03-08T10:31:16.841974Z",
     "iopub.status.idle": "2025-03-08T10:31:16.846303Z",
     "shell.execute_reply": "2025-03-08T10:31:16.845483Z"
    },
    "papermill": {
     "duration": 0.020487,
     "end_time": "2025-03-08T10:31:16.847812",
     "exception": false,
     "start_time": "2025-03-08T10:31:16.827325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292fa0da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:16.877102Z",
     "iopub.status.busy": "2025-03-08T10:31:16.876778Z",
     "iopub.status.idle": "2025-03-08T10:31:16.881370Z",
     "shell.execute_reply": "2025-03-08T10:31:16.880392Z"
    },
    "papermill": {
     "duration": 0.021281,
     "end_time": "2025-03-08T10:31:16.882846",
     "exception": false,
     "start_time": "2025-03-08T10:31:16.861565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3b6ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:16.912215Z",
     "iopub.status.busy": "2025-03-08T10:31:16.911829Z",
     "iopub.status.idle": "2025-03-08T10:31:16.925770Z",
     "shell.execute_reply": "2025-03-08T10:31:16.925031Z"
    },
    "papermill": {
     "duration": 0.030214,
     "end_time": "2025-03-08T10:31:16.927260",
     "exception": false,
     "start_time": "2025-03-08T10:31:16.897046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e67163",
   "metadata": {
    "papermill": {
     "duration": 0.013489,
     "end_time": "2025-03-08T10:31:16.954231",
     "exception": false,
     "start_time": "2025-03-08T10:31:16.940742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6fd043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:16.982624Z",
     "iopub.status.busy": "2025-03-08T10:31:16.982284Z",
     "iopub.status.idle": "2025-03-08T10:31:17.057672Z",
     "shell.execute_reply": "2025-03-08T10:31:17.055931Z"
    },
    "papermill": {
     "duration": 0.091945,
     "end_time": "2025-03-08T10:31:17.059719",
     "exception": false,
     "start_time": "2025-03-08T10:31:16.967774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "aspect_accuracies = manager.list()\n",
    "aspect_f1_micros = manager.list()\n",
    "aspect_f1_macros = manager.list()\n",
    "sentiment_accuracies = manager.list()\n",
    "sentiment_f1_micros = manager.list()\n",
    "sentiment_f1_macros = manager.list()\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'casa-besra'\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "sequence_length = 48\n",
    "\n",
    "aspect_list = ['fuel', 'machine', 'others', 'part', 'price', 'service']\n",
    "aspect_mapping = {'fuel': 0, 'machine': 1, 'others': 2, 'part': 3, 'price': 4, 'service': 5 }\n",
    "label_mapping = {\"negative\": 0, \"neutral\": 1, 'positive': 2}\n",
    "ignored_keys = ['labels', 'ori_text', 'ori_label', 'ori_indices', 'aspect']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f7191",
   "metadata": {
    "papermill": {
     "duration": 0.013331,
     "end_time": "2025-03-08T10:31:17.087096",
     "exception": false,
     "start_time": "2025-03-08T10:31:17.073765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70130859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:17.115873Z",
     "iopub.status.busy": "2025-03-08T10:31:17.115500Z",
     "iopub.status.idle": "2025-03-08T10:31:17.202346Z",
     "shell.execute_reply": "2025-03-08T10:31:17.201256Z"
    },
    "papermill": {
     "duration": 0.103332,
     "end_time": "2025-03-08T10:31:17.203961",
     "exception": false,
     "start_time": "2025-03-08T10:31:17.100629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>fuel</th>\n",
       "      <th>machine</th>\n",
       "      <th>others</th>\n",
       "      <th>part</th>\n",
       "      <th>price</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saya memakai Honda Jazz GK5 tahun 2014 ( perta...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avanza kenapa jadi boros bensin begini dah ah....</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saran ku dan pengalaman ku , mending beli mobi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dari segi harga juga pajero lebih mahal 30 jut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo menurut gw enak pajero si</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      fuel   machine  \\\n",
       "0  Saya memakai Honda Jazz GK5 tahun 2014 ( perta...   neutral   neutral   \n",
       "1  Avanza kenapa jadi boros bensin begini dah ah....  negative   neutral   \n",
       "2  saran ku dan pengalaman ku , mending beli mobi...  positive  positive   \n",
       "3  Dari segi harga juga pajero lebih mahal 30 jut...   neutral   neutral   \n",
       "4                     Kalo menurut gw enak pajero si   neutral   neutral   \n",
       "\n",
       "     others     part     price  service  \n",
       "0  positive  neutral   neutral  neutral  \n",
       "1   neutral  neutral   neutral  neutral  \n",
       "2   neutral  neutral   neutral  neutral  \n",
       "3   neutral  neutral  positive  neutral  \n",
       "4  positive  neutral   neutral  neutral  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/casa-dataset-2/train_preprocess.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/casa-dataset-2/valid_preprocess.csv', encoding='latin-1')\n",
    "test_data = pd.read_csv('/kaggle/input/casa-dataset-2/test_preprocess.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff78960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:17.232590Z",
     "iopub.status.busy": "2025-03-08T10:31:17.232263Z",
     "iopub.status.idle": "2025-03-08T10:31:17.243925Z",
     "shell.execute_reply": "2025-03-08T10:31:17.242954Z"
    },
    "papermill": {
     "duration": 0.027597,
     "end_time": "2025-03-08T10:31:17.245396",
     "exception": false,
     "start_time": "2025-03-08T10:31:17.217799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>fuel</th>\n",
       "      <th>machine</th>\n",
       "      <th>others</th>\n",
       "      <th>part</th>\n",
       "      <th>price</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saya memakai Honda Jazz GK5 tahun 2014 ( perta...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avanza kenapa jadi boros bensin begini dah ah....</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saran ku dan pengalaman ku , mending beli mobi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dari segi harga juga pajero lebih mahal 30 jut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo menurut gw enak pajero si</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      fuel   machine  \\\n",
       "0  Saya memakai Honda Jazz GK5 tahun 2014 ( perta...   neutral   neutral   \n",
       "1  Avanza kenapa jadi boros bensin begini dah ah....  negative   neutral   \n",
       "2  saran ku dan pengalaman ku , mending beli mobi...  positive  positive   \n",
       "3  Dari segi harga juga pajero lebih mahal 30 jut...   neutral   neutral   \n",
       "4                     Kalo menurut gw enak pajero si   neutral   neutral   \n",
       "\n",
       "     others     part     price  service  \n",
       "0  positive  neutral   neutral  neutral  \n",
       "1   neutral  neutral   neutral  neutral  \n",
       "2   neutral  neutral   neutral  neutral  \n",
       "3   neutral  neutral  positive  neutral  \n",
       "4  positive  neutral   neutral  neutral  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b2d96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:17.274495Z",
     "iopub.status.busy": "2025-03-08T10:31:17.274155Z",
     "iopub.status.idle": "2025-03-08T10:31:17.285281Z",
     "shell.execute_reply": "2025-03-08T10:31:17.284213Z"
    },
    "papermill": {
     "duration": 0.027444,
     "end_time": "2025-03-08T10:31:17.286929",
     "exception": false,
     "start_time": "2025-03-08T10:31:17.259485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d7013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:17.316107Z",
     "iopub.status.busy": "2025-03-08T10:31:17.315771Z",
     "iopub.status.idle": "2025-03-08T10:31:17.331210Z",
     "shell.execute_reply": "2025-03-08T10:31:17.330018Z"
    },
    "papermill": {
     "duration": 0.031719,
     "end_time": "2025-03-08T10:31:17.332734",
     "exception": false,
     "start_time": "2025-03-08T10:31:17.301015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864,) (864, 6)\n",
      "(216,) (216, 6)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_data.columns[1:]\n",
    "val_labels = val_data.columns[1:]\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['sentence'].values\n",
    "y_train = train_data[train_labels].values\n",
    "X_val = val_data['sentence'].values\n",
    "y_val = val_data[val_labels].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed2956",
   "metadata": {
    "papermill": {
     "duration": 0.013766,
     "end_time": "2025-03-08T10:31:17.360926",
     "exception": false,
     "start_time": "2025-03-08T10:31:17.347160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45e4fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:17.390801Z",
     "iopub.status.busy": "2025-03-08T10:31:17.390462Z",
     "iopub.status.idle": "2025-03-08T10:31:17.401445Z",
     "shell.execute_reply": "2025-03-08T10:31:17.400380Z"
    },
    "papermill": {
     "duration": 0.027802,
     "end_time": "2025-03-08T10:31:17.403000",
     "exception": false,
     "start_time": "2025-03-08T10:31:17.375198",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AspectDetectionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, label_mapping, tokenizer, max_length=sequence_length, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        \n",
    "        original_labels = [self.label_mapping[label] for label in self.labels[idx]]\n",
    "        encoded_labels = [1 if label == 1 else 0 for label in original_labels]\n",
    "        \n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['ori_indices'] = idx\n",
    "        item['ori_text'] = self.texts[idx]\n",
    "        item['ori_label'] = torch.tensor(original_labels, dtype=torch.float)\n",
    "        item['labels'] = torch.tensor(encoded_labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e44b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:17.440100Z",
     "iopub.status.busy": "2025-03-08T10:31:17.439728Z",
     "iopub.status.idle": "2025-03-08T10:31:17.452085Z",
     "shell.execute_reply": "2025-03-08T10:31:17.451224Z"
    },
    "papermill": {
     "duration": 0.036246,
     "end_time": "2025-03-08T10:31:17.453738",
     "exception": false,
     "start_time": "2025-03-08T10:31:17.417492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentAnalysisDataset(Dataset):\n",
    "    def __init__(self, texts, labels, aspects, indices, label_mapping, tokenizer, max_length=96, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.aspects = aspects\n",
    "        self.indices = indices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = aspect_token + ' ' + self.aspects[idx] + ' ' + review_token + ' ' + self.texts[idx] \n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        if isinstance(self.labels[idx], str):\n",
    "            self.labels[idx] = self.label_mapping[self.labels[idx]]\n",
    "        elif torch.is_tensor(self.labels[idx]):\n",
    "            self.labels[idx] = int(self.labels[idx].item())\n",
    "\n",
    "        encoded_label = 1 if self.labels[idx] == 2 else self.labels[idx]\n",
    "        one_hot_label = F.one_hot(torch.tensor(encoded_label, dtype=torch.long), num_classes=2).float()\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['aspect'] = self.aspects[idx]\n",
    "        item['labels'] = one_hot_label\n",
    "        item['ori_indices'] = self.indices[idx]\n",
    "        item['ori_text'] = self.texts[idx]\n",
    "        item['ori_label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8e728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:17.483535Z",
     "iopub.status.busy": "2025-03-08T10:31:17.483180Z",
     "iopub.status.idle": "2025-03-08T10:31:18.601581Z",
     "shell.execute_reply": "2025-03-08T10:31:18.600603Z"
    },
    "papermill": {
     "duration": 1.135573,
     "end_time": "2025-03-08T10:31:18.603449",
     "exception": false,
     "start_time": "2025-03-08T10:31:17.467876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651cb98179424b1585d3a65a47513166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e63685bf492404cba43e56966552ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbde207bbd34e299dfc81ea77d699f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f34109dcb54d258cee854f312324bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "review_token = '[REVIEW]'\n",
    "aspect_token = '[ASPECT]'\n",
    "special_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33a483f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:18.634588Z",
     "iopub.status.busy": "2025-03-08T10:31:18.634248Z",
     "iopub.status.idle": "2025-03-08T10:31:18.639214Z",
     "shell.execute_reply": "2025-03-08T10:31:18.638344Z"
    },
    "papermill": {
     "duration": 0.021817,
     "end_time": "2025-03-08T10:31:18.640634",
     "exception": false,
     "start_time": "2025-03-08T10:31:18.618817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_aspect_dataset(X_train, y_train, X_val, y_val, sequence_length, num_workers=4):\n",
    "    train_dataset = AspectDetectionDataset(X_train, y_train, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = AspectDetectionDataset(X_val, y_val, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers, \n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e718ca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:18.670211Z",
     "iopub.status.busy": "2025-03-08T10:31:18.669876Z",
     "iopub.status.idle": "2025-03-08T10:31:18.681992Z",
     "shell.execute_reply": "2025-03-08T10:31:18.681163Z"
    },
    "papermill": {
     "duration": 0.02819,
     "end_time": "2025-03-08T10:31:18.683564",
     "exception": false,
     "start_time": "2025-03-08T10:31:18.655374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_sentiment_dataset(device, train_dataset, val_dataset, aspect_detection_model, tokenizer, max_length=sequence_length):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "\n",
    "    aspect_detection_model.to(device)\n",
    "    aspect_detection_model.eval()\n",
    "\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    train_aspects = []\n",
    "    train_indices = []\n",
    "\n",
    "    val_data = []\n",
    "    val_labels = []\n",
    "    val_aspects = []\n",
    "    val_indices = []\n",
    "\n",
    "    # Transform train set\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = aspect_detection_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                for j in range(len(preds[i])):\n",
    "                    if int(preds[i][j]) != 1:\n",
    "                        train_aspects.append(aspect_list[j])\n",
    "                        train_data.append(batch['ori_text'][i])\n",
    "                        train_labels.append(batch['ori_label'][i][j])\n",
    "                        train_indices.append(batch['ori_indices'][i])\n",
    "            \n",
    "        # Transform validation set\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = aspect_detection_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                for j in range(len(preds[i])):\n",
    "                    if int(preds[i][j]) != 1:\n",
    "                        val_aspects.append(aspect_list[j])\n",
    "                        val_data.append(batch['ori_text'][i])\n",
    "                        val_labels.append(batch['ori_label'][i][j])\n",
    "                        val_indices.append(batch['ori_indices'][i])\n",
    "\n",
    "    # if len(train_data) > 0:\n",
    "    train_dataset = SentimentAnalysisDataset(train_data, train_labels, train_aspects, train_indices, label_mapping, tokenizer, max_length=max_length)\n",
    "    val_dataset = SentimentAnalysisDataset(val_data, val_labels, val_aspects, val_indices, label_mapping, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset\n",
    "\n",
    "    # return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2831ab6",
   "metadata": {
    "papermill": {
     "duration": 0.014397,
     "end_time": "2025-03-08T10:31:18.712601",
     "exception": false,
     "start_time": "2025-03-08T10:31:18.698204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c41a7065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:18.742479Z",
     "iopub.status.busy": "2025-03-08T10:31:18.742073Z",
     "iopub.status.idle": "2025-03-08T10:31:18.746291Z",
     "shell.execute_reply": "2025-03-08T10:31:18.745493Z"
    },
    "papermill": {
     "duration": 0.020729,
     "end_time": "2025-03-08T10:31:18.747806",
     "exception": false,
     "start_time": "2025-03-08T10:31:18.727077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7bd0ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:18.777399Z",
     "iopub.status.busy": "2025-03-08T10:31:18.777023Z",
     "iopub.status.idle": "2025-03-08T10:31:18.782677Z",
     "shell.execute_reply": "2025-03-08T10:31:18.781703Z"
    },
    "papermill": {
     "duration": 0.022141,
     "end_time": "2025-03-08T10:31:18.784180",
     "exception": false,
     "start_time": "2025-03-08T10:31:18.762039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p, label, classes):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    hamming_accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        labels=label,\n",
    "        target_names=classes,\n",
    "        zero_division=0\n",
    "    ) \n",
    "\n",
    "    return {\n",
    "        'accuracy': hamming_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dae23b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:18.815104Z",
     "iopub.status.busy": "2025-03-08T10:31:18.814766Z",
     "iopub.status.idle": "2025-03-08T10:31:18.822372Z",
     "shell.execute_reply": "2025-03-08T10:31:18.821545Z"
    },
    "papermill": {
     "duration": 0.024777,
     "end_time": "2025-03-08T10:31:18.823771",
     "exception": false,
     "start_time": "2025-03-08T10:31:18.798994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics_overall(p, classes):\n",
    "    preds = torch.tensor(p.predictions)\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Ensure it's in the correct shape\n",
    "    if preds.shape != labels.shape:\n",
    "        raise ValueError(\"Shape mismatch: predictions and labels must have the same shape.\")\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    hamming_accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Compute per-label (column-wise) precision, recall, F1\n",
    "    precision_list, recall_list, f1_micro_list, f1_macro_list = [], [], [], []\n",
    "    \n",
    "    for i in range(labels.shape[1]):  # Loop through each column (multi-output)\n",
    "        prec, rec, f1_micro, _ = precision_recall_fscore_support(\n",
    "            labels[:, i], preds[:, i], average='micro', zero_division=0\n",
    "        )\n",
    "        _, _, f1_macro, _ = precision_recall_fscore_support(\n",
    "            labels[:, i], preds[:, i], average='macro', zero_division=0\n",
    "        )\n",
    "\n",
    "        precision_list.append(prec)\n",
    "        recall_list.append(rec)\n",
    "        f1_micro_list.append(f1_micro)\n",
    "        f1_macro_list.append(f1_macro)\n",
    "\n",
    "    # Compute average metrics across all outputs\n",
    "    precision = sum(precision_list) / len(precision_list)\n",
    "    recall = sum(recall_list) / len(recall_list)\n",
    "    f1_micro = sum(f1_micro_list) / len(f1_micro_list)\n",
    "    f1_macro = sum(f1_macro_list) / len(f1_macro_list)\n",
    "\n",
    "    # Generate classification report per output\n",
    "    reports = [classification_report(labels[:, i], preds[:, i], target_names=classes, zero_division=0) for i in range(labels.shape[1])]\n",
    "\n",
    "    return {\n",
    "        'accuracy': hamming_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'reports': reports  # Returns list of reports, one for each output label\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5de1df1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:18.855177Z",
     "iopub.status.busy": "2025-03-08T10:31:18.854803Z",
     "iopub.status.idle": "2025-03-08T10:31:18.886088Z",
     "shell.execute_reply": "2025-03-08T10:31:18.885341Z"
    },
    "papermill": {
     "duration": 0.048892,
     "end_time": "2025-03-08T10:31:18.887599",
     "exception": false,
     "start_time": "2025-03-08T10:31:18.838707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, aspect_metrics, sentiment_metrics, metrics, trials, model_num):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Setup Aspect Model\n",
    "    aspect_model = BertForSequenceClassification.from_pretrained(\n",
    "        'indobenchmark/indobert-base-p1',\n",
    "        num_labels=len(train_labels),\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    ) \n",
    "    aspect_optimizer = torch.optim.AdamW(aspect_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    for name, param in aspect_model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Setup Sentiment Model\n",
    "    sentiment_model = BertForSequenceClassification.from_pretrained(\n",
    "        'indobenchmark/indobert-base-p1',\n",
    "        num_labels=2,\n",
    "    )\n",
    "    sentiment_optimizer = torch.optim.AdamW(sentiment_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    for name, param in sentiment_model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare models\n",
    "    aspect_model, aspect_optimizer = accelerator.prepare(aspect_model, aspect_optimizer)\n",
    "    sentiment_model, sentiment_optimizer = accelerator.prepare(sentiment_model, sentiment_optimizer)\n",
    "\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    aspect_train_loader, aspect_val_loader, aspect_train_dataset, aspect_val_dataset = build_aspect_dataset(current_X_train, current_y_train, X_val, y_val, sequence_length)\n",
    "\n",
    "    # Prepare train loaders\n",
    "    aspect_train_loader, aspect_val_loader = accelerator.prepare(\n",
    "        aspect_train_loader, aspect_val_loader\n",
    "    )\n",
    "\n",
    "    aspect_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # ASPECT DETECTION\n",
    "    accelerator.print(\"ASPECT DETECTION\")\n",
    "    for epoch in range(epochs):\n",
    "        aspect_model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in aspect_train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels']\n",
    "        \n",
    "            aspect_optimizer.zero_grad()\n",
    "            outputs = aspect_model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            aspect_optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        aspect_model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in aspect_val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = aspect_model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(\n",
    "            type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}),\n",
    "            None,\n",
    "            ['fuel', 'machine', 'others', 'part', 'price', 'service']\n",
    "        )\n",
    "\n",
    "        if aspect_result is None or result['f1_micro'] >= aspect_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(aspect_model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-aspect-{trials + 1}-model-{model_num+1}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            aspect_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(aspect_train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"\\nModel {model_num+1} of aspect detection, Accuracy: {round(aspect_result['accuracy'], 4)}, F1 Micro: {round(aspect_result['f1_micro'], 4)}, F1 Macro: {round(aspect_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(aspect_result['report'])\n",
    "\n",
    "    best_aspect_model = BertForSequenceClassification.from_pretrained(f'{filename}-aspect-{trials + 1}-model-{model_num+1}')\n",
    "    best_aspect_model = accelerator.prepare(best_aspect_model)\n",
    "\n",
    "    # SENTIMENT ANALYSIS ON NON NEUTRAL ASPECTS\n",
    "    accelerator.print(\"--------------------------------------------------\")\n",
    "    accelerator.print(\"SENTIMENT ANALYSIS\")\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    sentiment_train_loader, sentiment_val_loader, sentiment_train_dataset, sentiment_val_dataset = build_sentiment_dataset(\n",
    "        device, aspect_train_dataset, aspect_val_dataset, best_aspect_model, tokenizer, max_length=sequence_length\n",
    "    )\n",
    "    sentiment_model, sentiment_optimizer, sentiment_train_loader, sentiment_val_loader = accelerator.prepare(\n",
    "        sentiment_model, sentiment_optimizer, sentiment_train_loader, sentiment_val_loader\n",
    "    )\n",
    "    sentiment_result = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        sentiment_model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in sentiment_train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels']\n",
    "        \n",
    "            sentiment_optimizer.zero_grad()\n",
    "            outputs = sentiment_model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            sentiment_optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        sentiment_model.eval()\n",
    "        sentiment_val_outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in sentiment_val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "                \n",
    "                outputs = sentiment_model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                for i in range(len(preds)):\n",
    "                    val_output = {\n",
    "                        'label': batch['labels'][i],\n",
    "                        'aspect': batch['aspect'][i],\n",
    "                        'ori_indices': batch['ori_indices'][i],\n",
    "                        'pred': np.argmax(preds[i].cpu().numpy()),\n",
    "                    }\n",
    "                    sentiment_val_outputs.append(val_output)\n",
    "\n",
    "        sentiment_val_outputs = accelerator.gather_for_metrics(sentiment_val_outputs)\n",
    "        unique_val_outputs = {(x['ori_indices'].item(), x['aspect']): x for x in sentiment_val_outputs}\n",
    "        sentiment_val_outputs = list(unique_val_outputs.values())\n",
    "\n",
    "        result = compute_metrics(\n",
    "            type('EvalOutput', (object,), {'predictions': [item['pred'] for item in sentiment_val_outputs], 'label_ids': [np.argmax(item['label'].cpu().numpy()) for item in sentiment_val_outputs]}),\n",
    "            [0, 1],\n",
    "            ['negative', 'positive']\n",
    "        )\n",
    "\n",
    "        if sentiment_result is None or result['f1_micro'] >= sentiment_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            sentiment_result = result\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(sentiment_model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                 f'{filename}-sentiment-{trials + 1}-model-{model_num+1}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(sentiment_train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    accelerator.print(f\"\\nModel {model_num+1} of sentiment analysis, accuracy: {round(sentiment_result['accuracy'], 4)}, F1 Micro: {round(sentiment_result['f1_micro'], 4)}, F1 Macro: {round(sentiment_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(sentiment_result['report'])\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    best_sentiment_model = BertForSequenceClassification.from_pretrained( f'{filename}-sentiment-{trials + 1}-model-{model_num+1}')\n",
    "    best_sentiment_model = accelerator.prepare(best_sentiment_model)\n",
    "\n",
    "    # Compute overall metrics\n",
    "    aspect_labels = []\n",
    "    aspect_indices = []\n",
    "    aspect_preds = []\n",
    "\n",
    "    aspect_outputs = {}\n",
    "    sentiment_outputs = []\n",
    "    \n",
    "    best_aspect_model.eval()\n",
    "    best_sentiment_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in aspect_val_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            outputs = aspect_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            aspect_indices.append(accelerator.gather(batch['ori_indices']))\n",
    "            aspect_labels.append(accelerator.gather(batch['ori_label']))\n",
    "            aspect_preds.append(accelerator.gather(preds))\n",
    "\n",
    "        aspect_indices = torch.cat(aspect_indices).cpu().numpy()\n",
    "        aspect_labels = torch.cat(aspect_labels).cpu().numpy()\n",
    "        aspect_preds = torch.cat(aspect_preds).cpu().numpy()\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        aspect_outputs = [\n",
    "            {'ori_indices': aspect_indices[i], \n",
    "             'ori_labels': aspect_labels[i], \n",
    "             'pred': aspect_preds[i]}\n",
    "            for i in range(len(aspect_preds))\n",
    "        ]\n",
    "        aspect_outputs = {x['ori_indices'].item(): x for x in aspect_outputs}\n",
    "    \n",
    "        for batch in sentiment_val_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            outputs = sentiment_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "    \n",
    "            for i in range(len(preds)):\n",
    "                output = {\n",
    "                    'aspect': batch['aspect'][i],\n",
    "                    'ori_indices': batch['ori_indices'][i],\n",
    "                    'pred': np.argmax(preds[i].cpu().numpy()),\n",
    "                }\n",
    "                sentiment_outputs.append(output)\n",
    "\n",
    "        sentiment_outputs = accelerator.gather_for_metrics(sentiment_outputs)\n",
    "        sentiment_outputs = {(x['ori_indices'].item(), x['aspect']): x for x in sentiment_outputs}\n",
    "\n",
    "    # Replcae non neutral aspect to its predicted sentiment\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        i = -1\n",
    "        for (ori_index, aspect), value in sentiment_outputs.items():\n",
    "            aspect = aspect_mapping[aspect]\n",
    "            aspect_outputs[ori_index]['pred'][aspect] = 2 if value['pred'] == 1.0 else value['pred']\n",
    "\n",
    "        result = compute_metrics_overall(\n",
    "            type('EvalOutput', (object,), {'predictions': [output['pred'] for output in aspect_outputs.values()], 'label_ids': [output['ori_labels'] for output in aspect_outputs.values()]}),\n",
    "            ['negative', 'neutral', 'positive'],\n",
    "        )\n",
    "\n",
    "        accelerator.print(\"--------------------------------------------------\")\n",
    "        accelerator.print(f\"Model {model_num+1} - Iteration {current_train_size}: Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "        accelerator.print(\"--------------------------------------------------\")\n",
    "        for i in range(len(train_labels)):\n",
    "            accelerator.print(f\"Aspect {aspect_list[i]} report:\")\n",
    "            accelerator.print(result['reports'][i])\n",
    "       \n",
    "        aspect_metrics[0].append(aspect_result['accuracy'])\n",
    "        aspect_metrics[1].append(aspect_result['f1_micro'])\n",
    "        aspect_metrics[2].append(aspect_result['f1_macro'])\n",
    "        sentiment_metrics[0].append(sentiment_result['accuracy'])\n",
    "        sentiment_metrics[1].append(sentiment_result['f1_micro'])\n",
    "        sentiment_metrics[2].append(sentiment_result['f1_macro'])\n",
    "        metrics[0].append(result['accuracy'])\n",
    "        metrics[1].append(result['f1_micro'])\n",
    "        metrics[2].append(result['f1_macro'])\n",
    "        \n",
    "    accelerator.print(f\"Total train time: {duration} s\")\n",
    "    accelerator.end_training()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3fb32",
   "metadata": {
    "papermill": {
     "duration": 0.01511,
     "end_time": "2025-03-08T10:31:18.917757",
     "exception": false,
     "start_time": "2025-03-08T10:31:18.902647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1d50b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:18.948747Z",
     "iopub.status.busy": "2025-03-08T10:31:18.948420Z",
     "iopub.status.idle": "2025-03-08T10:31:18.954855Z",
     "shell.execute_reply": "2025-03-08T10:31:18.954023Z"
    },
    "papermill": {
     "duration": 0.023726,
     "end_time": "2025-03-08T10:31:18.956373",
     "exception": false,
     "start_time": "2025-03-08T10:31:18.932647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb095b1f",
   "metadata": {
    "papermill": {
     "duration": 0.01459,
     "end_time": "2025-03-08T10:31:18.985909",
     "exception": false,
     "start_time": "2025-03-08T10:31:18.971319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4840c7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:19.016767Z",
     "iopub.status.busy": "2025-03-08T10:31:19.016442Z",
     "iopub.status.idle": "2025-03-08T10:31:19.023837Z",
     "shell.execute_reply": "2025-03-08T10:31:19.023012Z"
    },
    "papermill": {
     "duration": 0.024577,
     "end_time": "2025-03-08T10:31:19.025324",
     "exception": false,
     "start_time": "2025-03-08T10:31:19.000747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(p, y, alpha=0.1, beta=3):\n",
    "    \"\"\"Calculates Beta score for a given probability p and label y.\"\"\"\n",
    "    \n",
    "    if y == 1:\n",
    "        return -betaln(alpha, beta + 1) + betaln(alpha + p, beta + 1 - p)\n",
    "    elif y == 0:\n",
    "        return -betaln(alpha + 1, beta) + betaln(alpha + 1 - p, beta + p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: y must be 0 or 1.\")\n",
    "\n",
    "def bayesian_update(prior, likelihood, evidence, alpha=0.1, beta_param=3):\n",
    "    \"\"\" \n",
    "    Bayes' Theorem: P(y'|x') = P(x'|y') * P(y') / P(x')\n",
    "    P(y'|x') or likelihood = model probs\n",
    "    p(y') or prior = class probabilities\n",
    "    p(x') or evidence = 1 / number of data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the Beta score to simulate the posterior\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    \n",
    "    # We calculate the posterior using the Beta distribution\n",
    "    return posterior\n",
    "\n",
    "def compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx):\n",
    "    scores_before = []\n",
    "    scores_after = []\n",
    "\n",
    "    # Before data addition: calculate Beta score for predicted prob\n",
    "    scores_before.append(beta_score(predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    scores_before.append(beta_score(1-predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    \n",
    "    for k in range(2):\n",
    "        prior = predicted_prob\n",
    "        likelihood = class_probs[class_idx][k]  # Likelihood is the true label (0 or 1)\n",
    "        posterior = bayesian_update(prior, likelihood, 1)\n",
    "        scores_after.append(beta_score(posterior, int(1 if posterior >= 0.5 else 0)))\n",
    "\n",
    "    score_diff_0 = scores_after[0] - scores_before[0]\n",
    "    score_diff_1 = scores_after[1] - scores_before[1]\n",
    "    \n",
    "    return label_probs['0'] * score_diff_0 + label_probs['1'] * score_diff_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a60c6874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:19.056278Z",
     "iopub.status.busy": "2025-03-08T10:31:19.055929Z",
     "iopub.status.idle": "2025-03-08T10:31:19.082837Z",
     "shell.execute_reply": "2025-03-08T10:31:19.082008Z"
    },
    "papermill": {
     "duration": 0.044567,
     "end_time": "2025-03-08T10:31:19.084497",
     "exception": false,
     "start_time": "2025-03-08T10:31:19.039930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def besra_sampling(aspect_models, sentiment_models, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, trials, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    for aspect_model in aspect_models:\n",
    "        aspect_model.to(device)\n",
    "        aspect_model.eval()\n",
    "\n",
    "    for sentiment_model in sentiment_models:\n",
    "        sentiment_model.to(device)\n",
    "        sentiment_model.eval()\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    aspect_dataset = AspectDetectionDataset(\n",
    "        X_pool, \n",
    "        [['neutral' for i in range(len(train_labels))] for x in range(len(X_pool))], \n",
    "        label_mapping, \n",
    "        tokenizer, \n",
    "        max_length=sequence_length\n",
    "    )\n",
    "    aspect_loader = DataLoader(\n",
    "        aspect_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    labeled_aspect_dataset = AspectDetectionDataset(current_X_train, current_y_train, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    label_aspect_probs = labeled_aspect_dataset.get_global_probs()\n",
    "    class_aspect_probs = labeled_aspect_dataset.get_per_class_probs()\n",
    "\n",
    "    _, _, labeled_sentiment_dataset, _ = build_sentiment_dataset(\n",
    "        aspect_models[0].device, labeled_aspect_dataset, labeled_aspect_dataset, aspect_models[0], tokenizer, max_length=sequence_length\n",
    "    )\n",
    "    label_sentiment_probs = labeled_sentiment_dataset.get_global_probs()\n",
    "    class_sentiment_probs = labeled_sentiment_dataset.get_per_class_probs()\n",
    "    \n",
    "    aspect_outputs = {}\n",
    "    sentiment_outputs = {}\n",
    "\n",
    "    aspects = []\n",
    "    data = []\n",
    "    labels = []\n",
    "    indices = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    aspect_uncertainties = []\n",
    "\n",
    "    # Pass through aspect detction model\n",
    "    for batch in aspect_loader:\n",
    "        model_probs = []\n",
    "        score_changes = []\n",
    "        \n",
    "        for model in aspect_models:\n",
    "            token_type_ids = batch['token_type_ids'].to(device, non_blocking=True)\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)  # Multi-label classification uses sigmoid\n",
    "                model_probs.append(probs.unsqueeze(0))  # Add batch dimension for averaging\n",
    "\n",
    "        # Stack all model predictions and compute the mean across models\n",
    "        model_probs = torch.cat(model_probs, dim=0)  # Concatenate predictions across models\n",
    "        mean_probs = model_probs.mean(dim=0)  # Take the mean along the model axis\n",
    "\n",
    "        # Calculate Beta scores before and after data addition\n",
    "        for i in range(len(mean_probs)):\n",
    "            score_diff = []\n",
    "            for class_idx in range(mean_probs.shape[1]):\n",
    "                predicted_prob = mean_probs[i, class_idx].item()\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_aspect_probs, label_aspect_probs, class_idx))\n",
    "\n",
    "                if int(mean_probs[i][class_idx].round()) != 1:\n",
    "                        aspects.append(aspect_list[class_idx])\n",
    "                        data.append(batch['ori_text'][i])\n",
    "                        labels.append(batch['ori_label'][i][class_idx])\n",
    "                        indices.append(batch['ori_indices'][i])\n",
    "\n",
    "            aspect_outputs[batch['ori_indices'][i].item()] = np.mean(score_diff)\n",
    "    \n",
    "    sentiment_dataset = SentimentAnalysisDataset(data, labels, aspects, indices, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    sentiment_loader = torch.utils.data.DataLoader(\n",
    "        sentiment_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4,\n",
    "    )\n",
    "\n",
    "    # Pass through sentiment analysis model\n",
    "    for batch in sentiment_loader:\n",
    "        token_type_ids = batch['token_type_ids'].to(device, non_blocking=True)\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "\n",
    "        batch_probs = []\n",
    "        for sentiment_model in sentiment_models:\n",
    "            with torch.no_grad():\n",
    "                outputs = sentiment_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                preds = torch.sigmoid(outputs.logits)\n",
    "\n",
    "                for j in range(len(preds)):\n",
    "                    ori_index = batch['ori_indices'][j].item()\n",
    "                    if ori_index in sentiment_outputs.keys():\n",
    "                        sentiment_outputs[ori_index].append(preds[j].cpu().numpy())\n",
    "                    else:\n",
    "                        sentiment_outputs[ori_index] = [preds[j].cpu().numpy()]\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    for indices, probs in sentiment_outputs.items():\n",
    "        sentiment_outputs[indices] = [[probs[i], probs[i+1], probs[i+2]] for i in range(int(len(probs) / 3))]\n",
    "        mean_probs = np.mean(sentiment_outputs[indices], axis=1)\n",
    "\n",
    "        score_changes = []\n",
    "        for prob in mean_probs:\n",
    "            score_diff = []\n",
    "            for class_idx in range(len(prob)):\n",
    "                predicted_prob = prob[class_idx]\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_sentiment_probs, label_sentiment_probs, class_idx))\n",
    "\n",
    "            score_changes.append(np.mean(score_diff))\n",
    "        sentiment_outputs[indices] = np.mean(score_changes)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        aspect_outputs = dict(sorted(aspect_outputs.items()))\n",
    "        if len(data) > 0:\n",
    "            for key, val in sentiment_outputs.items():\n",
    "                aspect_outputs[key] = (val + aspect_outputs[key]) / 2\n",
    "\n",
    "        score_changes = np.array(list(aspect_outputs.values())).reshape(-1, 1)\n",
    "\n",
    "        collected_indices = set()\n",
    "        thresholds = []\n",
    "\n",
    "        target_samples = math.ceil(0.1 * len(X_pool))\n",
    "\n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "\n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "\n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'fuel': [y_train[i][0] for i in temp],\n",
    "                'machine': [y_train[i][1] for i in temp],\n",
    "                'others': [y_train[i][2] for i in temp],\n",
    "                'part': [y_train[i][3] for i in temp],\n",
    "                'price': [y_train[i][4] for i in temp],\n",
    "                'service': [y_train[i][5] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "        else:\n",
    "            # Cluster the data based on its score changes\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(score_changes)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(score_changes[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 10)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances <= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "\n",
    "            # Handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'fuel': [y_train[i][0] for i in temp],\n",
    "                    'machine': [y_train[i][1] for i in temp],\n",
    "                    'others': [y_train[i][2] for i in temp],\n",
    "                    'part': [y_train[i][3] for i in temp],\n",
    "                    'price': [y_train[i][4] for i in temp],\n",
    "                    'service': [y_train[i][5] for i in temp],\n",
    "                })\n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "        \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "\n",
    "        threshold_data = pd.DataFrame({\n",
    "            'Threshold': thresholds\n",
    "        })\n",
    "        threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e451dc4",
   "metadata": {
    "papermill": {
     "duration": 0.014297,
     "end_time": "2025-03-08T10:31:19.113911",
     "exception": false,
     "start_time": "2025-03-08T10:31:19.099614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "112a4d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:19.144081Z",
     "iopub.status.busy": "2025-03-08T10:31:19.143722Z",
     "iopub.status.idle": "2025-03-08T10:31:19.160634Z",
     "shell.execute_reply": "2025-03-08T10:31:19.159593Z"
    },
    "papermill": {
     "duration": 0.033725,
     "end_time": "2025-03-08T10:31:19.162128",
     "exception": false,
     "start_time": "2025-03-08T10:31:19.128403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    aspect_accuracies = manager.list()\n",
    "    aspect_f1_micros = manager.list()\n",
    "    aspect_f1_macros = manager.list()\n",
    "    sentiment_accuracies = manager.list()\n",
    "    sentiment_f1_micros = manager.list()\n",
    "    sentiment_f1_macros = manager.list()\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"===============================================\")\n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_aspect_accuracies = manager.list()\n",
    "        model_aspect_f1_micros = manager.list()\n",
    "        model_aspect_f1_macros = manager.list()\n",
    "        model_sentiment_accuracies = manager.list()\n",
    "        model_sentiment_f1_micros = manager.list()\n",
    "        model_sentiment_f1_macros = manager.list()\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "\n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (\n",
    "                current_train_size, \n",
    "                train_indices, \n",
    "                (model_aspect_accuracies, model_aspect_f1_micros, model_aspect_f1_macros), \n",
    "                (model_sentiment_accuracies, model_sentiment_f1_micros, model_sentiment_f1_macros),\n",
    "                (model_accuracies, model_f1_micros, model_f1_macros), \n",
    "                i,\n",
    "                j\n",
    "            )\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        aspect_accuracies.append(np.mean(model_aspect_accuracies))\n",
    "        aspect_f1_micros.append(np.mean(model_aspect_f1_micros))\n",
    "        aspect_f1_macros.append(np.mean(model_aspect_f1_macros))\n",
    "        sentiment_accuracies.append(np.mean(model_sentiment_accuracies))\n",
    "        sentiment_f1_micros.append(np.mean(model_sentiment_f1_micros))\n",
    "        sentiment_f1_macros.append(np.mean(model_sentiment_f1_macros))\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "\n",
    "        aspect_models = []\n",
    "        sentiment_models = []\n",
    "        for j in range(3):\n",
    "            aspect_model = BertForSequenceClassification.from_pretrained(f'{filename}-aspect-{i+1}-model-{j+1}')\n",
    "            sentiment_model = BertForSequenceClassification.from_pretrained(f'{filename}-sentiment-{i+1}-model-{j+1}')\n",
    "            \n",
    "            aspect_models.append(aspect_model)\n",
    "            sentiment_models.append(sentiment_model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (\n",
    "            aspect_models, \n",
    "            sentiment_models, \n",
    "            [X_train[i] for i in remaining_indices], \n",
    "            train_indices, \n",
    "            remaining_indices, \n",
    "            tokenizer,\n",
    "            sampling_dur, \n",
    "            new_samples, \n",
    "            i\n",
    "        )\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (\n",
    "            current_train_size, \n",
    "            train_indices, \n",
    "            (model_aspect_accuracies, model_aspect_f1_micros, model_aspect_f1_macros), \n",
    "            (model_sentiment_accuracies, model_sentiment_f1_micros, model_sentiment_f1_macros),\n",
    "            (model_accuracies, model_f1_micros, model_f1_macros), \n",
    "            i,\n",
    "            j\n",
    "        )\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "    data_used.append(current_train_size)\n",
    "    aspect_accuracies.append(np.mean(model_aspect_accuracies))\n",
    "    aspect_f1_micros.append(np.mean(model_aspect_f1_micros))\n",
    "    aspect_f1_macros.append(np.mean(model_aspect_f1_macros))\n",
    "    sentiment_accuracies.append(np.mean(model_sentiment_accuracies))\n",
    "    sentiment_f1_micros.append(np.mean(model_sentiment_f1_micros))\n",
    "    sentiment_f1_macros.append(np.mean(model_sentiment_f1_macros))\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "\n",
    "    aspect_accuracies, aspect_f1_micros, aspect_f1_macros = list(aspect_accuracies), list(aspect_f1_micros), list(aspect_f1_macros)\n",
    "    sentiment_accuracies, sentiment_f1_micros, sentiment_f1_macros = list(sentiment_accuracies), list(sentiment_f1_micros), list(sentiment_f1_macros)\n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Aspect Accuracy': aspect_accuracies,\n",
    "        'Aspect F1 Micro': aspect_f1_micros,\n",
    "        'Aspect F1 Macro': aspect_f1_macros,\n",
    "        'Sentiment Accuracy': sentiment_accuracies,\n",
    "        'Sentiment F1 Micro': sentiment_f1_micros,\n",
    "        'Sentiment F1 Macro': sentiment_f1_macros,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001dcb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:19.193078Z",
     "iopub.status.busy": "2025-03-08T10:31:19.192707Z",
     "iopub.status.idle": "2025-03-08T10:31:19.197117Z",
     "shell.execute_reply": "2025-03-08T10:31:19.196187Z"
    },
    "papermill": {
     "duration": 0.021343,
     "end_time": "2025-03-08T10:31:19.198629",
     "exception": false,
     "start_time": "2025-03-08T10:31:19.177286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [[50, 67, 42], [81, 90, 11], [14, 61, 33], [3, 44, 85], [94, 21, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078af7fe",
   "metadata": {
    "papermill": {
     "duration": 0.014366,
     "end_time": "2025-03-08T10:31:19.227806",
     "exception": false,
     "start_time": "2025-03-08T10:31:19.213440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e009794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T10:31:19.258361Z",
     "iopub.status.busy": "2025-03-08T10:31:19.257977Z",
     "iopub.status.idle": "2025-03-08T13:17:53.042929Z",
     "shell.execute_reply": "2025-03-08T13:17:53.041771Z"
    },
    "papermill": {
     "duration": 9994.352498,
     "end_time": "2025-03-08T13:17:53.594905",
     "exception": false,
     "start_time": "2025-03-08T10:31:19.242407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36374a4b140e44f8a854f9e117afb229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6756, Accuracy: 0.7701, F1 Micro: 0.8685, F1 Macro: 0.8664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5949, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5663, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5413, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.485, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Epoch 6/10, Train Loss: 0.4934, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 7/10, Train Loss: 0.4861, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4478, Accuracy: 0.7924, F1 Micro: 0.8836, F1 Macro: 0.882\n",
      "Epoch 9/10, Train Loss: 0.4182, Accuracy: 0.7924, F1 Micro: 0.8826, F1 Macro: 0.8805\n",
      "Epoch 10/10, Train Loss: 0.3944, Accuracy: 0.7939, F1 Micro: 0.8833, F1 Macro: 0.881\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7924, F1 Micro: 0.8836, F1 Macro: 0.882\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.72      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7484, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5901, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4859, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.421, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3657, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2973, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2815, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2474, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2277, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2016, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         2\n",
      "    positive       0.75      1.00      0.86         6\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.38      0.50      0.43         8\n",
      "weighted avg       0.56      0.75      0.64         8\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7863, F1 Micro: 0.7863, F1 Macro: 0.3108\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.24      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.15      0.13      0.14        23\n",
      "     neutral       0.75      0.93      0.83       152\n",
      "    positive       0.50      0.10      0.16        41\n",
      "\n",
      "    accuracy                           0.69       216\n",
      "   macro avg       0.47      0.39      0.38       216\n",
      "weighted avg       0.64      0.69      0.63       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 66.18220615386963 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6474, Accuracy: 0.7091, F1 Micro: 0.8196, F1 Macro: 0.7917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.589, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5429, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5279, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Epoch 5/10, Train Loss: 0.4875, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Epoch 6/10, Train Loss: 0.4868, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Epoch 7/10, Train Loss: 0.4839, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4515, Accuracy: 0.7932, F1 Micro: 0.8838, F1 Macro: 0.8822\n",
      "Epoch 9/10, Train Loss: 0.4287, Accuracy: 0.7932, F1 Micro: 0.8834, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.397, Accuracy: 0.7976, F1 Micro: 0.8856, F1 Macro: 0.8839\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7976, F1 Micro: 0.8856, F1 Macro: 0.8839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.80      0.98      0.88       175\n",
      "      others       0.74      0.97      0.84       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7455, Accuracy: 0.5385, F1 Micro: 0.5385, F1 Macro: 0.35\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6946, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5674, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.579, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.5553, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5072, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4154, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4522, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.5831, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3683, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         5\n",
      "    positive       0.81      1.00      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.40      0.50      0.45        26\n",
      "weighted avg       0.65      0.81      0.72        26\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7948, F1 Micro: 0.7948, F1 Macro: 0.3184\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.79      0.98      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.76       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.61      0.76      0.68       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.74      0.97      0.84       152\n",
      "    positive       0.76      0.25      0.38        52\n",
      "\n",
      "    accuracy                           0.75       216\n",
      "   macro avg       0.50      0.41      0.41       216\n",
      "weighted avg       0.71      0.75      0.68       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.05        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.69      0.71      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 65.83495330810547 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6607, Accuracy: 0.7649, F1 Micro: 0.864, F1 Macro: 0.8569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5793, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5503, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.517, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4778, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4767, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4783, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4419, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4097, Accuracy: 0.7939, F1 Micro: 0.8842, F1 Macro: 0.8826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3844, Accuracy: 0.7976, F1 Micro: 0.8852, F1 Macro: 0.8831\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7976, F1 Micro: 0.8852, F1 Macro: 0.8831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.72      0.99      0.84       158\n",
      "        part       0.75      0.93      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6786, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.5733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5919, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4489, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4731, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4406, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5754, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.6467, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3345, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.353, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.359, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         6\n",
      "    positive       0.82      1.00      0.90        27\n",
      "\n",
      "    accuracy                           0.82        33\n",
      "   macro avg       0.41      0.50      0.45        33\n",
      "weighted avg       0.67      0.82      0.74        33\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7948, F1 Micro: 0.7948, F1 Macro: 0.3236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.67      0.08      0.14        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.46      0.36      0.32       216\n",
      "weighted avg       0.67      0.72      0.62       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.76      0.94      0.84       152\n",
      "    positive       0.48      0.32      0.38        41\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.41      0.42      0.41       216\n",
      "weighted avg       0.62      0.72      0.66       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 75.94754195213318 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7919, F1 Micro: 0.7919, F1 Macro: 0.3176\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 9.079970836639404 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.637, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5378, Accuracy: 0.7939, F1 Micro: 0.8844, F1 Macro: 0.8829\n",
      "Epoch 3/10, Train Loss: 0.4902, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Epoch 4/10, Train Loss: 0.4661, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4441, Accuracy: 0.8125, F1 Micro: 0.8937, F1 Macro: 0.8928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.395, Accuracy: 0.8147, F1 Micro: 0.895, F1 Macro: 0.8938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3825, Accuracy: 0.8415, F1 Micro: 0.9084, F1 Macro: 0.9078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3416, Accuracy: 0.8661, F1 Micro: 0.9213, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3066, Accuracy: 0.8862, F1 Micro: 0.9314, F1 Macro: 0.93\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2519, Accuracy: 0.9003, F1 Micro: 0.9386, F1 Macro: 0.9361\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9003, F1 Micro: 0.9386, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.92      1.00      0.96       187\n",
      "     machine       0.90      0.90      0.90       175\n",
      "      others       0.84      0.90      0.87       158\n",
      "        part       0.89      1.00      0.94       158\n",
      "       price       0.95      1.00      0.97       192\n",
      "     service       0.97      0.99      0.98       191\n",
      "\n",
      "   micro avg       0.91      0.97      0.94      1061\n",
      "   macro avg       0.91      0.96      0.94      1061\n",
      "weighted avg       0.91      0.97      0.94      1061\n",
      " samples avg       0.92      0.97      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6221, Accuracy: 0.7109, F1 Micro: 0.7109, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5929, Accuracy: 0.7109, F1 Micro: 0.7109, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5364, Accuracy: 0.7109, F1 Micro: 0.7109, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3624, Accuracy: 0.8531, F1 Micro: 0.8531, F1 Macro: 0.7954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2449, Accuracy: 0.8673, F1 Micro: 0.8673, F1 Macro: 0.8401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1711, Accuracy: 0.8768, F1 Micro: 0.8768, F1 Macro: 0.8501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1413, Accuracy: 0.891, F1 Micro: 0.891, F1 Macro: 0.8668\n",
      "Epoch 8/10, Train Loss: 0.112, Accuracy: 0.872, F1 Micro: 0.872, F1 Macro: 0.831\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.872, F1 Micro: 0.872, F1 Macro: 0.831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0978, Accuracy: 0.9005, F1 Micro: 0.9005, F1 Macro: 0.8745\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9005, F1 Micro: 0.9005, F1 Macro: 0.8745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.77      0.82        61\n",
      "    positive       0.91      0.95      0.93       150\n",
      "\n",
      "    accuracy                           0.90       211\n",
      "   macro avg       0.89      0.86      0.87       211\n",
      "weighted avg       0.90      0.90      0.90       211\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8873, F1 Micro: 0.8873, F1 Macro: 0.7416\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.09      0.17        11\n",
      "     neutral       0.92      1.00      0.96       181\n",
      "    positive       0.83      0.62      0.71        24\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.57      0.61       216\n",
      "weighted avg       0.91      0.91      0.89       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.69      0.71        16\n",
      "     neutral       0.89      0.90      0.90       167\n",
      "    positive       0.55      0.55      0.55        33\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.72      0.71      0.72       216\n",
      "weighted avg       0.83      0.83      0.83       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.33      0.44        12\n",
      "     neutral       0.84      0.90      0.87       152\n",
      "    positive       0.66      0.60      0.63        52\n",
      "\n",
      "    accuracy                           0.80       216\n",
      "   macro avg       0.72      0.61      0.65       216\n",
      "weighted avg       0.79      0.80      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.61      0.74        23\n",
      "     neutral       0.88      1.00      0.94       152\n",
      "    positive       0.86      0.61      0.71        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.89      0.74      0.80       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.46      0.63        13\n",
      "     neutral       0.94      1.00      0.97       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.96      0.72      0.80       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      0.99      0.98       185\n",
      "    positive       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 83.6701819896698 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6278, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5295, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4957, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4676, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4446, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3967, Accuracy: 0.808, F1 Micro: 0.8913, F1 Macro: 0.8897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3745, Accuracy: 0.8423, F1 Micro: 0.9086, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3207, Accuracy: 0.8705, F1 Micro: 0.9227, F1 Macro: 0.9209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2813, Accuracy: 0.8862, F1 Micro: 0.9312, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2381, Accuracy: 0.8966, F1 Micro: 0.9366, F1 Macro: 0.9341\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8966, F1 Micro: 0.9366, F1 Macro: 0.9341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.92      1.00      0.96       187\n",
      "     machine       0.90      0.90      0.90       175\n",
      "      others       0.86      0.90      0.88       158\n",
      "        part       0.88      0.99      0.93       158\n",
      "       price       0.93      1.00      0.96       192\n",
      "     service       0.95      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.91      0.97      0.94      1061\n",
      "   macro avg       0.90      0.97      0.93      1061\n",
      "weighted avg       0.91      0.97      0.94      1061\n",
      " samples avg       0.91      0.97      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6094, Accuracy: 0.6995, F1 Micro: 0.6995, F1 Macro: 0.4116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5728, Accuracy: 0.6995, F1 Micro: 0.6995, F1 Macro: 0.4116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4926, Accuracy: 0.7488, F1 Micro: 0.7488, F1 Macro: 0.6067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3607, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.8241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2325, Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.8425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.179, Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.8411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1508, Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.8362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1048, Accuracy: 0.8768, F1 Micro: 0.8768, F1 Macro: 0.8514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.8768, F1 Micro: 0.8768, F1 Macro: 0.8467\n",
      "Epoch 10/10, Train Loss: 0.059, Accuracy: 0.8424, F1 Micro: 0.8424, F1 Macro: 0.7957\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8768, F1 Micro: 0.8768, F1 Macro: 0.8467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.72      0.78        61\n",
      "    positive       0.89      0.94      0.91       142\n",
      "\n",
      "    accuracy                           0.88       203\n",
      "   macro avg       0.87      0.83      0.85       203\n",
      "weighted avg       0.88      0.88      0.87       203\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8781, F1 Micro: 0.8781, F1 Macro: 0.6927\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.09      0.17        11\n",
      "     neutral       0.92      1.00      0.96       181\n",
      "    positive       0.83      0.62      0.71        24\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.57      0.61       216\n",
      "weighted avg       0.91      0.91      0.89       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.50      0.57        16\n",
      "     neutral       0.89      0.90      0.90       167\n",
      "    positive       0.51      0.55      0.53        33\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.69      0.65      0.67       216\n",
      "weighted avg       0.82      0.82      0.82       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.33      0.17      0.22        12\n",
      "     neutral       0.86      0.90      0.88       152\n",
      "    positive       0.66      0.63      0.65        52\n",
      "\n",
      "    accuracy                           0.80       216\n",
      "   macro avg       0.62      0.57      0.58       216\n",
      "weighted avg       0.78      0.80      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.52      0.69        23\n",
      "     neutral       0.88      0.99      0.93       152\n",
      "    positive       0.81      0.63      0.71        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.90      0.72      0.78       216\n",
      "weighted avg       0.88      0.88      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.54      0.70        13\n",
      "     neutral       0.93      1.00      0.96       186\n",
      "    positive       0.78      0.41      0.54        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.65      0.73       216\n",
      "weighted avg       0.92      0.93      0.91       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        14\n",
      "     neutral       0.95      1.00      0.97       185\n",
      "    positive       0.79      0.65      0.71        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.72      0.78       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Total train time: 87.20022416114807 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6128, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5201, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.491, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4562, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4393, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3943, Accuracy: 0.8177, F1 Micro: 0.8965, F1 Macro: 0.8955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3724, Accuracy: 0.8475, F1 Micro: 0.9114, F1 Macro: 0.9106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3193, Accuracy: 0.8705, F1 Micro: 0.9231, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2902, Accuracy: 0.8958, F1 Micro: 0.9366, F1 Macro: 0.9346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2403, Accuracy: 0.9062, F1 Micro: 0.9422, F1 Macro: 0.9396\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9062, F1 Micro: 0.9422, F1 Macro: 0.9396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.94      1.00      0.97       187\n",
      "     machine       0.90      0.92      0.91       175\n",
      "      others       0.84      0.89      0.87       158\n",
      "        part       0.90      0.99      0.94       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.92      0.97      0.94      1061\n",
      "   macro avg       0.92      0.97      0.94      1061\n",
      "weighted avg       0.92      0.97      0.94      1061\n",
      " samples avg       0.92      0.97      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5943, Accuracy: 0.6866, F1 Micro: 0.6866, F1 Macro: 0.4071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5155, Accuracy: 0.6866, F1 Micro: 0.6866, F1 Macro: 0.4071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3721, Accuracy: 0.8111, F1 Micro: 0.8111, F1 Macro: 0.7355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2949, Accuracy: 0.8756, F1 Micro: 0.8756, F1 Macro: 0.8602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1955, Accuracy: 0.8848, F1 Micro: 0.8848, F1 Macro: 0.8633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.179, Accuracy: 0.9032, F1 Micro: 0.9032, F1 Macro: 0.8897\n",
      "Epoch 7/10, Train Loss: 0.1411, Accuracy: 0.8848, F1 Micro: 0.8848, F1 Macro: 0.8609\n",
      "Epoch 8/10, Train Loss: 0.1324, Accuracy: 0.894, F1 Micro: 0.894, F1 Macro: 0.8731\n",
      "Epoch 9/10, Train Loss: 0.1159, Accuracy: 0.8894, F1 Micro: 0.8894, F1 Macro: 0.867\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.8986, F1 Micro: 0.8986, F1 Macro: 0.8781\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9032, F1 Micro: 0.9032, F1 Macro: 0.8897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        68\n",
      "    positive       0.94      0.91      0.93       149\n",
      "\n",
      "    accuracy                           0.90       217\n",
      "   macro avg       0.88      0.90      0.89       217\n",
      "weighted avg       0.91      0.90      0.90       217\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8935, F1 Micro: 0.8935, F1 Macro: 0.7788\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.45      0.62        11\n",
      "     neutral       0.95      1.00      0.97       181\n",
      "    positive       0.90      0.75      0.82        24\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.95      0.73      0.81       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.69      0.73        16\n",
      "     neutral       0.89      0.92      0.91       167\n",
      "    positive       0.58      0.55      0.56        33\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.75      0.72      0.73       216\n",
      "weighted avg       0.84      0.84      0.84       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.33      0.44        12\n",
      "     neutral       0.84      0.89      0.87       152\n",
      "    positive       0.60      0.56      0.58        52\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.70      0.60      0.63       216\n",
      "weighted avg       0.77      0.78      0.77       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.65      0.77        23\n",
      "     neutral       0.90      0.99      0.94       152\n",
      "    positive       0.82      0.66      0.73        41\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.77      0.81       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.62      0.73        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.77      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.85      0.65      0.73        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.79      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Total train time: 81.0377938747406 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8391, F1 Micro: 0.8391, F1 Macro: 0.5276\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 15.885344505310059 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5958, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.512, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Epoch 3/10, Train Loss: 0.4934, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4457, Accuracy: 0.8103, F1 Micro: 0.8924, F1 Macro: 0.8914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4133, Accuracy: 0.8371, F1 Micro: 0.906, F1 Macro: 0.9055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3562, Accuracy: 0.8772, F1 Micro: 0.9273, F1 Macro: 0.9264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3023, Accuracy: 0.9055, F1 Micro: 0.9425, F1 Macro: 0.9402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2595, Accuracy: 0.9137, F1 Micro: 0.9461, F1 Macro: 0.9427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.218, Accuracy: 0.9249, F1 Micro: 0.9533, F1 Macro: 0.9502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1925, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.959\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.95      0.94       175\n",
      "      others       0.88      0.92      0.90       158\n",
      "        part       0.91      0.99      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6161, Accuracy: 0.678, F1 Micro: 0.678, F1 Macro: 0.404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.48, Accuracy: 0.678, F1 Micro: 0.678, F1 Macro: 0.404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3759, Accuracy: 0.8983, F1 Micro: 0.8983, F1 Macro: 0.8843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2315, Accuracy: 0.9068, F1 Micro: 0.9068, F1 Macro: 0.8953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1696, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9338\n",
      "Epoch 7/10, Train Loss: 0.0762, Accuracy: 0.9322, F1 Micro: 0.9322, F1 Macro: 0.9229\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9364, F1 Micro: 0.9364, F1 Macro: 0.9275\n",
      "Epoch 9/10, Train Loss: 0.0613, Accuracy: 0.9322, F1 Micro: 0.9322, F1 Macro: 0.9229\n",
      "Epoch 10/10, Train Loss: 0.0332, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9127\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.91        76\n",
      "    positive       0.98      0.93      0.96       160\n",
      "\n",
      "    accuracy                           0.94       236\n",
      "   macro avg       0.92      0.95      0.93       236\n",
      "weighted avg       0.94      0.94      0.94       236\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.8554\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.88      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.92      0.95      0.94       167\n",
      "    positive       0.70      0.64      0.67        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.78      0.80       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.89      0.93      0.91       152\n",
      "    positive       0.78      0.69      0.73        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.80      0.76      0.78       216\n",
      "weighted avg       0.85      0.86      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.78      0.84        23\n",
      "     neutral       0.90      0.99      0.94       152\n",
      "    positive       0.93      0.68      0.79        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 89.74562215805054 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.588, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.518, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Epoch 3/10, Train Loss: 0.4969, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4561, Accuracy: 0.7917, F1 Micro: 0.8829, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4144, Accuracy: 0.8289, F1 Micro: 0.9017, F1 Macro: 0.9001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3488, Accuracy: 0.8869, F1 Micro: 0.9314, F1 Macro: 0.9289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2845, Accuracy: 0.9122, F1 Micro: 0.9462, F1 Macro: 0.9435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2477, Accuracy: 0.9189, F1 Micro: 0.9496, F1 Macro: 0.947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2046, Accuracy: 0.9301, F1 Micro: 0.9565, F1 Macro: 0.9537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1794, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.9591\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.9591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.87      0.92      0.89       158\n",
      "        part       0.94      0.97      0.95       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.592, Accuracy: 0.6752, F1 Micro: 0.6752, F1 Macro: 0.4031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4844, Accuracy: 0.765, F1 Micro: 0.765, F1 Macro: 0.6668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3429, Accuracy: 0.8675, F1 Micro: 0.8675, F1 Macro: 0.8474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.209, Accuracy: 0.8974, F1 Micro: 0.8974, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1725, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0818, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Epoch 7/10, Train Loss: 0.0515, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8913\n",
      "Epoch 8/10, Train Loss: 0.0938, Accuracy: 0.9188, F1 Micro: 0.9188, F1 Macro: 0.9071\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9103, F1 Micro: 0.9103, F1 Macro: 0.9\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.9045\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.92      0.90        76\n",
      "    positive       0.96      0.94      0.95       158\n",
      "\n",
      "    accuracy                           0.93       234\n",
      "   macro avg       0.92      0.93      0.92       234\n",
      "weighted avg       0.93      0.93      0.93       234\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.8508\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.92      0.99      0.95       167\n",
      "    positive       0.88      0.64      0.74        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.79      0.84       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.58      0.61        12\n",
      "     neutral       0.87      0.92      0.89       152\n",
      "    positive       0.75      0.63      0.69        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.75      0.71      0.73       216\n",
      "weighted avg       0.83      0.83      0.83       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.87      0.82        23\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.85      0.68      0.76        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.84      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.80      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 88.86843204498291 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.581, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5032, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4906, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4399, Accuracy: 0.7999, F1 Micro: 0.8875, F1 Macro: 0.8861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4065, Accuracy: 0.8482, F1 Micro: 0.9119, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.346, Accuracy: 0.8936, F1 Micro: 0.9358, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2865, Accuracy: 0.9159, F1 Micro: 0.9486, F1 Macro: 0.9466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2526, Accuracy: 0.9278, F1 Micro: 0.955, F1 Macro: 0.9522\n",
      "Epoch 9/10, Train Loss: 0.2057, Accuracy: 0.9226, F1 Micro: 0.9522, F1 Macro: 0.9491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1799, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9613\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.86      0.91      0.88       158\n",
      "        part       0.95      0.98      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5615, Accuracy: 0.6723, F1 Micro: 0.6723, F1 Macro: 0.402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4869, Accuracy: 0.8085, F1 Micro: 0.8085, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3037, Accuracy: 0.8851, F1 Micro: 0.8851, F1 Macro: 0.8732\n",
      "Epoch 4/10, Train Loss: 0.1865, Accuracy: 0.8723, F1 Micro: 0.8723, F1 Macro: 0.8498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1329, Accuracy: 0.8979, F1 Micro: 0.8979, F1 Macro: 0.8863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0723, Accuracy: 0.8979, F1 Micro: 0.8979, F1 Macro: 0.8799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0631, Accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.8996\n",
      "Epoch 8/10, Train Loss: 0.0667, Accuracy: 0.8894, F1 Micro: 0.8894, F1 Macro: 0.8698\n",
      "Epoch 9/10, Train Loss: 0.0732, Accuracy: 0.9064, F1 Micro: 0.9064, F1 Macro: 0.8981\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.9021, F1 Micro: 0.9021, F1 Macro: 0.8878\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.8996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.88      0.87        77\n",
      "    positive       0.94      0.92      0.93       158\n",
      "\n",
      "    accuracy                           0.91       235\n",
      "   macro avg       0.90      0.90      0.90       235\n",
      "weighted avg       0.91      0.91      0.91       235\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.8521\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.62      0.74        16\n",
      "     neutral       0.92      0.99      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.76      0.82       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.86      0.91      0.88       152\n",
      "    positive       0.71      0.62      0.66        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.77      0.73      0.75       216\n",
      "weighted avg       0.82      0.82      0.82       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.74      0.76        23\n",
      "     neutral       0.94      0.98      0.96       152\n",
      "    positive       0.81      0.71      0.75        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.81      0.82       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 85.9886691570282 s\n",
      "Averaged - Iteration 208: Accuracy: 0.8683, F1 Micro: 0.8683, F1 Macro: 0.636\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 15.471297264099121 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5992, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Epoch 2/10, Train Loss: 0.5086, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4611, Accuracy: 0.8132, F1 Micro: 0.8941, F1 Macro: 0.8934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4069, Accuracy: 0.8289, F1 Micro: 0.902, F1 Macro: 0.9013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3698, Accuracy: 0.8929, F1 Micro: 0.9361, F1 Macro: 0.9352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.299, Accuracy: 0.9278, F1 Micro: 0.9556, F1 Macro: 0.9539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2628, Accuracy: 0.9323, F1 Micro: 0.9576, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2004, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9646\n",
      "Epoch 9/10, Train Loss: 0.1715, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1496, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.969\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.91      0.91       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6396, Accuracy: 0.6786, F1 Micro: 0.6786, F1 Macro: 0.4043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4382, Accuracy: 0.8532, F1 Micro: 0.8532, F1 Macro: 0.8162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2888, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1084, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.932\n",
      "Epoch 5/10, Train Loss: 0.0958, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "Epoch 7/10, Train Loss: 0.0966, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8861\n",
      "Epoch 8/10, Train Loss: 0.0517, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9248\n",
      "Epoch 9/10, Train Loss: 0.0332, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9219\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9256\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.92        81\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.94      0.94       252\n",
      "weighted avg       0.95      0.94      0.94       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9437, F1 Micro: 0.9437, F1 Macro: 0.8919\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.75      0.77      0.76        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.77      0.81      0.79       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.97      0.68      0.80        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.86      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 92.60332417488098 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.583, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.514, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.4706, Accuracy: 0.7902, F1 Micro: 0.8825, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4222, Accuracy: 0.8304, F1 Micro: 0.9023, F1 Macro: 0.9007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3625, Accuracy: 0.9033, F1 Micro: 0.941, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2887, Accuracy: 0.9286, F1 Micro: 0.9556, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2401, Accuracy: 0.9353, F1 Micro: 0.9596, F1 Macro: 0.9574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1861, Accuracy: 0.9368, F1 Micro: 0.9603, F1 Macro: 0.9568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1574, Accuracy: 0.9412, F1 Micro: 0.9631, F1 Macro: 0.9604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1382, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9701\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.89      0.94      0.92       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6126, Accuracy: 0.6762, F1 Micro: 0.6762, F1 Macro: 0.4034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4568, Accuracy: 0.8402, F1 Micro: 0.8402, F1 Macro: 0.7965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2345, Accuracy: 0.9057, F1 Micro: 0.9057, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9349\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9207\n",
      "Epoch 6/10, Train Loss: 0.088, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0564, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9493\n",
      "Epoch 8/10, Train Loss: 0.0843, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 10/10, Train Loss: 0.0552, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.93\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        79\n",
      "    positive       0.98      0.95      0.97       165\n",
      "\n",
      "    accuracy                           0.95       244\n",
      "   macro avg       0.94      0.96      0.95       244\n",
      "weighted avg       0.96      0.95      0.96       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.8927\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.89      0.94      0.92       152\n",
      "    positive       0.82      0.69      0.75        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.82      0.79      0.81       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.86      0.76      0.81        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 93.92071199417114 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5885, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5035, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4567, Accuracy: 0.7954, F1 Micro: 0.8853, F1 Macro: 0.8838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.402, Accuracy: 0.8363, F1 Micro: 0.9055, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3533, Accuracy: 0.9055, F1 Micro: 0.9428, F1 Macro: 0.9412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.286, Accuracy: 0.9286, F1 Micro: 0.956, F1 Macro: 0.9537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2442, Accuracy: 0.9353, F1 Micro: 0.9595, F1 Macro: 0.957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1887, Accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.156, Accuracy: 0.9442, F1 Micro: 0.9652, F1 Macro: 0.9628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1367, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9694\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.93      0.91       158\n",
      "        part       0.95      0.98      0.96       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.582, Accuracy: 0.6694, F1 Micro: 0.6694, F1 Macro: 0.401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4515, Accuracy: 0.8694, F1 Micro: 0.8694, F1 Macro: 0.8542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2238, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8913\n",
      "Epoch 4/10, Train Loss: 0.1153, Accuracy: 0.8939, F1 Micro: 0.8939, F1 Macro: 0.8858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1089, Accuracy: 0.9061, F1 Micro: 0.9061, F1 Macro: 0.8891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0552, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9326\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9236\n",
      "Epoch 8/10, Train Loss: 0.0967, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9232\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0392, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9319\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        81\n",
      "    positive       0.97      0.94      0.95       164\n",
      "\n",
      "    accuracy                           0.94       245\n",
      "   macro avg       0.93      0.94      0.93       245\n",
      "weighted avg       0.94      0.94      0.94       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9437, F1 Micro: 0.9437, F1 Macro: 0.8917\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.90      0.93      0.91       152\n",
      "    positive       0.79      0.71      0.75        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.81      0.80      0.80       216\n",
      "weighted avg       0.86      0.87      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.94      0.98      0.96       152\n",
      "    positive       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 97.31462144851685 s\n",
      "Averaged - Iteration 274: Accuracy: 0.8872, F1 Micro: 0.8872, F1 Macro: 0.7\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 14.836122751235962 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.583, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5066, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4656, Accuracy: 0.808, F1 Micro: 0.8916, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3902, Accuracy: 0.8876, F1 Micro: 0.9333, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.328, Accuracy: 0.9368, F1 Micro: 0.961, F1 Macro: 0.9596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2574, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1993, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1484, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1245, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1112, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.558, Accuracy: 0.6789, F1 Micro: 0.6789, F1 Macro: 0.4044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3798, Accuracy: 0.8862, F1 Micro: 0.8862, F1 Macro: 0.8712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1662, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1331, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.94\n",
      "Epoch 5/10, Train Loss: 0.1191, Accuracy: 0.9187, F1 Micro: 0.9187, F1 Macro: 0.9106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9392\n",
      "Epoch 7/10, Train Loss: 0.0605, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8768\n",
      "Epoch 8/10, Train Loss: 0.0865, Accuracy: 0.9146, F1 Micro: 0.9146, F1 Macro: 0.9059\n",
      "Epoch 9/10, Train Loss: 0.06, Accuracy: 0.8902, F1 Micro: 0.8902, F1 Macro: 0.882\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9244\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.91      0.92        79\n",
      "    positive       0.96      0.96      0.96       167\n",
      "\n",
      "    accuracy                           0.95       246\n",
      "   macro avg       0.94      0.94      0.94       246\n",
      "weighted avg       0.95      0.95      0.95       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8935\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.50      0.63        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.75      0.79       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.74      0.77        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.86      0.76      0.81        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 101.71579074859619 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5616, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5073, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4696, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3917, Accuracy: 0.8884, F1 Micro: 0.9332, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3211, Accuracy: 0.9323, F1 Micro: 0.9584, F1 Macro: 0.9567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2468, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1921, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1433, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9705\n",
      "Epoch 9/10, Train Loss: 0.1179, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1073, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5896, Accuracy: 0.6892, F1 Micro: 0.6892, F1 Macro: 0.408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4689, Accuracy: 0.8805, F1 Micro: 0.8805, F1 Macro: 0.8574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2309, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.164, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9218\n",
      "Epoch 5/10, Train Loss: 0.1112, Accuracy: 0.8964, F1 Micro: 0.8964, F1 Macro: 0.8701\n",
      "Epoch 6/10, Train Loss: 0.0728, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9069\n",
      "Epoch 7/10, Train Loss: 0.0582, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9151\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.898\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8968\n",
      "Epoch 10/10, Train Loss: 0.068, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.8944\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        78\n",
      "    positive       0.96      0.94      0.95       173\n",
      "\n",
      "    accuracy                           0.93       251\n",
      "   macro avg       0.92      0.93      0.92       251\n",
      "weighted avg       0.93      0.93      0.93       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.876\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.67      0.59        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.86      0.71      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.78      0.77       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.91      0.84        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.88      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.86      0.86        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 100.32985186576843 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5611, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4988, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4554, Accuracy: 0.8021, F1 Micro: 0.8886, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3852, Accuracy: 0.8876, F1 Micro: 0.9324, F1 Macro: 0.9305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3178, Accuracy: 0.9308, F1 Micro: 0.9576, F1 Macro: 0.9561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2504, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1924, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1464, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9744\n",
      "Epoch 9/10, Train Loss: 0.1241, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9711\n",
      "Epoch 10/10, Train Loss: 0.1095, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9729\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5751, Accuracy: 0.6707, F1 Micro: 0.6707, F1 Macro: 0.4015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4408, Accuracy: 0.874, F1 Micro: 0.874, F1 Macro: 0.8618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.246, Accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.9032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1359, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9364\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.9268, F1 Micro: 0.9268, F1 Macro: 0.9143\n",
      "Epoch 6/10, Train Loss: 0.1645, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9162\n",
      "Epoch 7/10, Train Loss: 0.103, Accuracy: 0.9187, F1 Micro: 0.9187, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0888, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9458\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9264\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9312\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9458\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        81\n",
      "    positive       0.98      0.95      0.96       165\n",
      "\n",
      "    accuracy                           0.95       246\n",
      "   macro avg       0.94      0.95      0.95       246\n",
      "weighted avg       0.95      0.95      0.95       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8945\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.83      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.83      0.81        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.86      0.78      0.82        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.86      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      0.99      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 98.15548396110535 s\n",
      "Averaged - Iteration 333: Accuracy: 0.8993, F1 Micro: 0.8993, F1 Macro: 0.7376\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 13.680283784866333 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5584, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4881, Accuracy: 0.8013, F1 Micro: 0.8882, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4163, Accuracy: 0.8668, F1 Micro: 0.9215, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3339, Accuracy: 0.9323, F1 Micro: 0.9585, F1 Macro: 0.9573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2624, Accuracy: 0.9435, F1 Micro: 0.965, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2109, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9707\n",
      "Epoch 7/10, Train Loss: 0.1585, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1255, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.108, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Epoch 10/10, Train Loss: 0.089, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9709\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.96      0.93       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6444, Accuracy: 0.6774, F1 Micro: 0.6774, F1 Macro: 0.4038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.41, Accuracy: 0.879, F1 Micro: 0.879, F1 Macro: 0.8679\n",
      "Epoch 3/10, Train Loss: 0.2305, Accuracy: 0.8427, F1 Micro: 0.8427, F1 Macro: 0.8358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1905, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.911\n",
      "Epoch 5/10, Train Loss: 0.1453, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9276\n",
      "Epoch 7/10, Train Loss: 0.0478, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9241\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.919\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9142\n",
      "Epoch 10/10, Train Loss: 0.036, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9142\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.90        80\n",
      "    positive       0.97      0.93      0.95       168\n",
      "\n",
      "    accuracy                           0.94       248\n",
      "   macro avg       0.92      0.94      0.93       248\n",
      "weighted avg       0.94      0.94      0.94       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9437, F1 Micro: 0.9437, F1 Macro: 0.8794\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.75      0.56        12\n",
      "     neutral       0.92      0.92      0.92       152\n",
      "    positive       0.86      0.73      0.79        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.74      0.80      0.76       216\n",
      "weighted avg       0.88      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.83      0.79        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.79      0.85        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 103.84556341171265 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5568, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4979, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4314, Accuracy: 0.8371, F1 Micro: 0.906, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.341, Accuracy: 0.9286, F1 Micro: 0.956, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2636, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.206, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Epoch 7/10, Train Loss: 0.1524, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.969\n",
      "Epoch 8/10, Train Loss: 0.1182, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1013, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Epoch 10/10, Train Loss: 0.0862, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6201, Accuracy: 0.6708, F1 Micro: 0.6708, F1 Macro: 0.4015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4048, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.8994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2354, Accuracy: 0.9136, F1 Micro: 0.9136, F1 Macro: 0.9053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1437, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.908\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.8999\n",
      "Epoch 6/10, Train Loss: 0.0612, Accuracy: 0.9053, F1 Micro: 0.9053, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0455, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9225\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.908\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.9114\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        80\n",
      "    positive       0.97      0.93      0.95       163\n",
      "\n",
      "    accuracy                           0.93       243\n",
      "   macro avg       0.92      0.93      0.92       243\n",
      "weighted avg       0.93      0.93      0.93       243\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.8803\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.94      0.79        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.87      0.85       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.83      0.54        12\n",
      "     neutral       0.92      0.92      0.92       152\n",
      "    positive       0.92      0.69      0.79        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.75      0.82      0.75       216\n",
      "weighted avg       0.89      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.86      0.86        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 110.18354153633118 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5571, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4888, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4176, Accuracy: 0.869, F1 Micro: 0.9227, F1 Macro: 0.9211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3374, Accuracy: 0.9278, F1 Micro: 0.9558, F1 Macro: 0.9544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2651, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2104, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9704\n",
      "Epoch 7/10, Train Loss: 0.1564, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1258, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1051, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "Epoch 10/10, Train Loss: 0.0896, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9706\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.89      0.97      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5773, Accuracy: 0.6762, F1 Micro: 0.6762, F1 Macro: 0.4034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3839, Accuracy: 0.8975, F1 Micro: 0.8975, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2301, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1125, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9226\n",
      "Epoch 5/10, Train Loss: 0.1132, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1051, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9361\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.9149\n",
      "Epoch 8/10, Train Loss: 0.0715, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.9089\n",
      "Epoch 9/10, Train Loss: 0.0395, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.93\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9102\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        79\n",
      "    positive       0.98      0.93      0.96       165\n",
      "\n",
      "    accuracy                           0.94       244\n",
      "   macro avg       0.93      0.95      0.94       244\n",
      "weighted avg       0.95      0.94      0.94       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.8771\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.36      0.75      0.49        12\n",
      "     neutral       0.92      0.91      0.92       152\n",
      "    positive       0.88      0.67      0.76        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.72      0.78      0.72       216\n",
      "weighted avg       0.88      0.85      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.90      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 107.74566912651062 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.7612\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 12.773428440093994 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5661, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4172, Accuracy: 0.8839, F1 Micro: 0.9308, F1 Macro: 0.9303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3155, Accuracy: 0.933, F1 Micro: 0.9586, F1 Macro: 0.9571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2513, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1891, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1486, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9728\n",
      "Epoch 8/10, Train Loss: 0.122, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Epoch 9/10, Train Loss: 0.1029, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0817, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.91      0.92       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5709, Accuracy: 0.8647, F1 Micro: 0.8647, F1 Macro: 0.8355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3228, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.892\n",
      "Epoch 3/10, Train Loss: 0.1703, Accuracy: 0.8647, F1 Micro: 0.8647, F1 Macro: 0.854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1663, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9263\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.8992\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9089\n",
      "Epoch 8/10, Train Loss: 0.0889, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9009\n",
      "Epoch 9/10, Train Loss: 0.0463, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9156\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9222\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        81\n",
      "    positive       0.97      0.94      0.95       185\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.92      0.94      0.93       266\n",
      "weighted avg       0.94      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8994\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.94      0.91      0.92       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.83      0.81       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.89       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 111.7461986541748 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5609, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4863, Accuracy: 0.7932, F1 Micro: 0.884, F1 Macro: 0.8824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4285, Accuracy: 0.8698, F1 Micro: 0.9228, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3181, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2384, Accuracy: 0.9368, F1 Micro: 0.9606, F1 Macro: 0.9593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1849, Accuracy: 0.9464, F1 Micro: 0.9664, F1 Macro: 0.9641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.143, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9701\n",
      "Epoch 8/10, Train Loss: 0.1111, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9698\n",
      "Epoch 9/10, Train Loss: 0.0978, Accuracy: 0.9487, F1 Micro: 0.9676, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0807, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5814, Accuracy: 0.8462, F1 Micro: 0.8462, F1 Macro: 0.8095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2922, Accuracy: 0.8731, F1 Micro: 0.8731, F1 Macro: 0.864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2159, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1251, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9218\n",
      "Epoch 5/10, Train Loss: 0.0875, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9177\n",
      "Epoch 6/10, Train Loss: 0.0818, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Epoch 8/10, Train Loss: 0.0562, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9033\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9182\n",
      "Epoch 10/10, Train Loss: 0.0488, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.911\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.99      0.90        81\n",
      "    positive       0.99      0.91      0.95       179\n",
      "\n",
      "    accuracy                           0.93       260\n",
      "   macro avg       0.91      0.95      0.92       260\n",
      "weighted avg       0.94      0.93      0.93       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8871\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.94      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.75      0.64        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.82      0.79       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.85      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 113.06561589241028 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5546, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4796, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4172, Accuracy: 0.8981, F1 Micro: 0.9384, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3141, Accuracy: 0.9278, F1 Micro: 0.9558, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2441, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1897, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1487, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "Epoch 8/10, Train Loss: 0.1192, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "Epoch 9/10, Train Loss: 0.0995, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9727\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.96      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.563, Accuracy: 0.7808, F1 Micro: 0.7808, F1 Macro: 0.6998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3814, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8935\n",
      "Epoch 3/10, Train Loss: 0.1832, Accuracy: 0.8731, F1 Micro: 0.8731, F1 Macro: 0.8646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1318, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0926, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9227\n",
      "Epoch 7/10, Train Loss: 0.1031, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0822, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9313\n",
      "Epoch 10/10, Train Loss: 0.06, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9121\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9313\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.98      0.91        82\n",
      "    positive       0.99      0.92      0.95       178\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.92      0.95      0.93       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8907\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.83      0.81       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.87      0.83        23\n",
      "     neutral       0.96      0.97      0.97       152\n",
      "    positive       0.86      0.78      0.82        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 114.68425941467285 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.7799\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 11.647965908050537 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5562, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4828, Accuracy: 0.808, F1 Micro: 0.8915, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3979, Accuracy: 0.9107, F1 Micro: 0.946, F1 Macro: 0.9451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.295, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2336, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9701\n",
      "Epoch 6/10, Train Loss: 0.1736, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.144, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Epoch 8/10, Train Loss: 0.1114, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "Epoch 9/10, Train Loss: 0.0902, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9716\n",
      "Epoch 10/10, Train Loss: 0.0767, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9703\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.91      0.92       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6002, Accuracy: 0.6846, F1 Micro: 0.6846, F1 Macro: 0.4064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.338, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.898\n",
      "Epoch 3/10, Train Loss: 0.2187, Accuracy: 0.8962, F1 Micro: 0.8962, F1 Macro: 0.8794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1519, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.078, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0832, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9347\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9195\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9227\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9347\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.91        82\n",
      "    positive       0.98      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.94      0.93       260\n",
      "weighted avg       0.95      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.8834\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.75      0.53        12\n",
      "     neutral       0.94      0.87      0.90       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.72      0.82      0.75       216\n",
      "weighted avg       0.88      0.85      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 115.04834818840027 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5538, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4862, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.404, Accuracy: 0.8958, F1 Micro: 0.9371, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2964, Accuracy: 0.9397, F1 Micro: 0.9624, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2245, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9674\n",
      "Epoch 6/10, Train Loss: 0.1645, Accuracy: 0.9479, F1 Micro: 0.9673, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1326, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1062, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "Epoch 9/10, Train Loss: 0.0836, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9717\n",
      "Epoch 10/10, Train Loss: 0.073, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.97\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.92      0.92       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5329, Accuracy: 0.7821, F1 Micro: 0.7821, F1 Macro: 0.6883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3187, Accuracy: 0.8949, F1 Micro: 0.8949, F1 Macro: 0.8692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2352, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1475, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1095, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1122, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9279\n",
      "Epoch 9/10, Train Loss: 0.0645, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9246\n",
      "Epoch 10/10, Train Loss: 0.0571, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9167\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9279\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.91      0.90        80\n",
      "    positive       0.96      0.95      0.95       177\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.93      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.88\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.92      0.52        12\n",
      "     neutral       0.96      0.87      0.91       152\n",
      "    positive       0.81      0.75      0.78        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.71      0.85      0.74       216\n",
      "weighted avg       0.89      0.84      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.89      0.78      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 118.36947989463806 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.549, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4752, Accuracy: 0.8006, F1 Micro: 0.8879, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3923, Accuracy: 0.9033, F1 Micro: 0.9417, F1 Macro: 0.9405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2881, Accuracy: 0.9353, F1 Micro: 0.96, F1 Macro: 0.9581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2291, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1688, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1371, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "Epoch 8/10, Train Loss: 0.1083, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "Epoch 9/10, Train Loss: 0.0863, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9726\n",
      "Epoch 10/10, Train Loss: 0.0782, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9706\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5757, Accuracy: 0.7671, F1 Micro: 0.7671, F1 Macro: 0.6852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3247, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9289\n",
      "Epoch 3/10, Train Loss: 0.1917, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1547, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9467\n",
      "Epoch 5/10, Train Loss: 0.1011, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9421\n",
      "Epoch 6/10, Train Loss: 0.0885, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9421\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9339\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9263\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9268\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9225\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        82\n",
      "    positive       0.99      0.94      0.96       167\n",
      "\n",
      "    accuracy                           0.95       249\n",
      "   macro avg       0.94      0.96      0.95       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.8715\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.36      0.83      0.50        12\n",
      "     neutral       0.95      0.88      0.91       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.72      0.83      0.74       216\n",
      "weighted avg       0.89      0.85      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.78      0.77        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.82      0.80      0.81        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.79      0.85        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 109.08179140090942 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9162, F1 Micro: 0.9162, F1 Macro: 0.7922\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 9.949123620986938 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5487, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.467, Accuracy: 0.8199, F1 Micro: 0.897, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3726, Accuracy: 0.9152, F1 Micro: 0.9485, F1 Macro: 0.947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2669, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1928, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "Epoch 6/10, Train Loss: 0.1444, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1178, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Epoch 8/10, Train Loss: 0.0954, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6084, Accuracy: 0.8605, F1 Micro: 0.8605, F1 Macro: 0.8251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2855, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9265\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.8996\n",
      "Epoch 4/10, Train Loss: 0.1362, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9115\n",
      "Epoch 5/10, Train Loss: 0.114, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9151\n",
      "Epoch 6/10, Train Loss: 0.123, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9023\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9066\n",
      "Epoch 8/10, Train Loss: 0.0793, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0743, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9106\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.91        83\n",
      "    positive       0.98      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.93      0.95      0.94       258\n",
      "weighted avg       0.95      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8989\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.79      0.82      0.80       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 118.06670212745667 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5379, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.808, F1 Micro: 0.8915, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3792, Accuracy: 0.9159, F1 Micro: 0.9485, F1 Macro: 0.9467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2638, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1866, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1401, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1101, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9728\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0634, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5746, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2865, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2069, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9138\n",
      "Epoch 4/10, Train Loss: 0.1487, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.926\n",
      "Epoch 6/10, Train Loss: 0.0904, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9108\n",
      "Epoch 7/10, Train Loss: 0.0695, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9068\n",
      "Epoch 8/10, Train Loss: 0.0596, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9209\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9148\n",
      "Epoch 10/10, Train Loss: 0.0617, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9145\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        80\n",
      "    positive       0.98      0.93      0.95       184\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.92      0.94      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8909\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.79      0.82      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.83      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.67      0.73        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.81      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.88      0.91       216\n",
      "weighted avg       0.98      0.98      0.97       216\n",
      "\n",
      "Total train time: 125.02610969543457 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5348, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4636, Accuracy: 0.8192, F1 Micro: 0.8971, F1 Macro: 0.896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.365, Accuracy: 0.9234, F1 Micro: 0.953, F1 Macro: 0.9515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2677, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1902, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1458, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1206, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 8/10, Train Loss: 0.0948, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9748\n",
      "Epoch 9/10, Train Loss: 0.078, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9749\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6024, Accuracy: 0.689, F1 Micro: 0.689, F1 Macro: 0.4522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3269, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1731, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9339\n",
      "Epoch 4/10, Train Loss: 0.1357, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9297\n",
      "Epoch 5/10, Train Loss: 0.1245, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9251\n",
      "Epoch 6/10, Train Loss: 0.1067, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9216\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9063\n",
      "Epoch 8/10, Train Loss: 0.0558, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9172\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9255\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9222\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        83\n",
      "    positive       0.97      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.93      0.94      0.93       254\n",
      "weighted avg       0.94      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8959\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.75      0.58        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.75      0.81      0.77       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.96      0.83        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.90      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 114.93332982063293 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9199, F1 Micro: 0.9199, F1 Macro: 0.8037\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 9.937347173690796 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5464, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.8185, F1 Micro: 0.8967, F1 Macro: 0.8959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3672, Accuracy: 0.9211, F1 Micro: 0.9512, F1 Macro: 0.949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2748, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1963, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Epoch 6/10, Train Loss: 0.158, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1256, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0987, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9733\n",
      "Epoch 9/10, Train Loss: 0.0802, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.94      0.90      0.92       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5513, Accuracy: 0.7799, F1 Micro: 0.7799, F1 Macro: 0.6648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.288, Accuracy: 0.8657, F1 Micro: 0.8657, F1 Macro: 0.857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2049, Accuracy: 0.8881, F1 Micro: 0.8881, F1 Macro: 0.8794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1334, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.117, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9408\n",
      "Epoch 6/10, Train Loss: 0.0992, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9179\n",
      "Epoch 7/10, Train Loss: 0.0936, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9225\n",
      "Epoch 8/10, Train Loss: 0.0665, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9265\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9251\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9283\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        84\n",
      "    positive       0.98      0.94      0.96       184\n",
      "\n",
      "    accuracy                           0.95       268\n",
      "   macro avg       0.93      0.95      0.94       268\n",
      "weighted avg       0.95      0.95      0.95       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9082\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.89      0.92       152\n",
      "    positive       0.73      0.85      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.83      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 125.4731388092041 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5453, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4857, Accuracy: 0.8073, F1 Micro: 0.8909, F1 Macro: 0.8896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3728, Accuracy: 0.9278, F1 Micro: 0.9554, F1 Macro: 0.9537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2721, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1915, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1494, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1189, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0929, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9729\n",
      "Epoch 10/10, Train Loss: 0.0643, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5629, Accuracy: 0.7719, F1 Micro: 0.7719, F1 Macro: 0.664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3031, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 3/10, Train Loss: 0.2185, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.8968\n",
      "Epoch 4/10, Train Loss: 0.134, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0886, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0663, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.935\n",
      "Epoch 8/10, Train Loss: 0.0643, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9309\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        82\n",
      "    positive       0.98      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9083\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.83      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 126.69282817840576 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5413, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4716, Accuracy: 0.817, F1 Micro: 0.8959, F1 Macro: 0.8948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.363, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2694, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1918, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9716\n",
      "Epoch 6/10, Train Loss: 0.1539, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1227, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "Epoch 8/10, Train Loss: 0.0983, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "Epoch 9/10, Train Loss: 0.0805, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9726\n",
      "Epoch 10/10, Train Loss: 0.072, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9702\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      1.00      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5506, Accuracy: 0.8394, F1 Micro: 0.8394, F1 Macro: 0.8193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3101, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1959, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1392, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9407\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9324\n",
      "Epoch 7/10, Train Loss: 0.0706, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9163\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9289\n",
      "Epoch 9/10, Train Loss: 0.0552, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9293\n",
      "Epoch 10/10, Train Loss: 0.0694, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9336\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        81\n",
      "    positive       0.96      0.96      0.96       168\n",
      "\n",
      "    accuracy                           0.95       249\n",
      "   macro avg       0.94      0.94      0.94       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.8814\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.75      0.71        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.96      0.67      0.79        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.85      0.80      0.82       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.83      0.53        12\n",
      "     neutral       0.94      0.89      0.91       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.73      0.83      0.75       216\n",
      "weighted avg       0.89      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 119.95013856887817 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.8132\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 9.322893381118774 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5414, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4663, Accuracy: 0.84, F1 Micro: 0.9078, F1 Macro: 0.9072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3445, Accuracy: 0.9301, F1 Micro: 0.9568, F1 Macro: 0.955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2526, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1882, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1467, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1138, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0758, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5829, Accuracy: 0.8315, F1 Micro: 0.8315, F1 Macro: 0.77\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.281, Accuracy: 0.8727, F1 Micro: 0.8727, F1 Macro: 0.8641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1822, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Epoch 4/10, Train Loss: 0.1813, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9322\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9242\n",
      "Epoch 6/10, Train Loss: 0.1328, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0935, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1102, Accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9476\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9451\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9373\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.92      0.93        84\n",
      "    positive       0.96      0.97      0.97       183\n",
      "\n",
      "    accuracy                           0.96       267\n",
      "   macro avg       0.95      0.94      0.95       267\n",
      "weighted avg       0.95      0.96      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9169\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 132.01650214195251 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5432, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4731, Accuracy: 0.814, F1 Micro: 0.8941, F1 Macro: 0.8931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3519, Accuracy: 0.9234, F1 Micro: 0.9524, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2519, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.9673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1808, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1356, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1093, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Epoch 9/10, Train Loss: 0.0762, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5467, Accuracy: 0.8517, F1 Micro: 0.8517, F1 Macro: 0.8203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3094, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2252, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9395\n",
      "Epoch 4/10, Train Loss: 0.1629, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "Epoch 5/10, Train Loss: 0.1481, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 7/10, Train Loss: 0.0961, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0868, Accuracy: 0.962, F1 Micro: 0.962, F1 Macro: 0.956\n",
      "Epoch 9/10, Train Loss: 0.1015, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9395\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.962, F1 Micro: 0.962, F1 Macro: 0.956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        82\n",
      "    positive       0.98      0.97      0.97       181\n",
      "\n",
      "    accuracy                           0.96       263\n",
      "   macro avg       0.95      0.96      0.96       263\n",
      "weighted avg       0.96      0.96      0.96       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9132\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.83      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 126.21540474891663 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5357, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4597, Accuracy: 0.8207, F1 Micro: 0.8976, F1 Macro: 0.896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3419, Accuracy: 0.9278, F1 Micro: 0.9554, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2553, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1883, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1473, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1102, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0931, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5528, Accuracy: 0.8238, F1 Micro: 0.8238, F1 Macro: 0.7794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2753, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2255, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9273\n",
      "Epoch 4/10, Train Loss: 0.1654, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.8994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 6/10, Train Loss: 0.1076, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 7/10, Train Loss: 0.0911, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0789, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9355\n",
      "Epoch 9/10, Train Loss: 0.0671, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9314\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9141\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        85\n",
      "    positive       0.97      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.94      0.94       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8814\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.96      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.69      0.71        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.96      0.90      0.93       152\n",
      "    positive       0.80      0.85      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.86      0.81       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.86      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.39464735984802 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9256, F1 Micro: 0.9256, F1 Macro: 0.8215\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 8.371622085571289 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4586, Accuracy: 0.8519, F1 Micro: 0.9134, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3378, Accuracy: 0.9315, F1 Micro: 0.9573, F1 Macro: 0.9552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2355, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1753, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1407, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1069, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0729, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.96      0.92      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5781, Accuracy: 0.717, F1 Micro: 0.717, F1 Macro: 0.5191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2893, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1782, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9371\n",
      "Epoch 4/10, Train Loss: 0.1327, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9327\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9137\n",
      "Epoch 6/10, Train Loss: 0.154, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9207\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9327\n",
      "Epoch 8/10, Train Loss: 0.0658, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0867, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0643, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9364\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        85\n",
      "    positive       0.98      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.93      0.95      0.94       265\n",
      "weighted avg       0.95      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9086\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.83      0.69        12\n",
      "     neutral       0.96      0.89      0.92       152\n",
      "    positive       0.78      0.88      0.83        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.87      0.81       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.81828117370605 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5503, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4767, Accuracy: 0.8222, F1 Micro: 0.8987, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3574, Accuracy: 0.9226, F1 Micro: 0.9522, F1 Macro: 0.9508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.245, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1788, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1383, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 8/10, Train Loss: 0.0882, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9722\n",
      "Epoch 9/10, Train Loss: 0.0741, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Epoch 10/10, Train Loss: 0.0653, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5431, Accuracy: 0.8864, F1 Micro: 0.8864, F1 Macro: 0.8603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.293, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1591, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1186, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1139, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1113, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9339\n",
      "Epoch 7/10, Train Loss: 0.1093, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9241\n",
      "Epoch 8/10, Train Loss: 0.0752, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9241\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0543, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.94\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        84\n",
      "    positive       0.98      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.93      0.95      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9113\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.97      0.91      0.94       152\n",
      "    positive       0.83      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.76      0.86      0.80       216\n",
      "weighted avg       0.91      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 130.27992129325867 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5454, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4551, Accuracy: 0.8519, F1 Micro: 0.9137, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3382, Accuracy: 0.9308, F1 Micro: 0.9573, F1 Macro: 0.9556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2373, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1815, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1436, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1078, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 8/10, Train Loss: 0.0889, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0742, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9721\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5212, Accuracy: 0.8594, F1 Micro: 0.8594, F1 Macro: 0.8275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2697, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9166\n",
      "Epoch 3/10, Train Loss: 0.1692, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1445, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9307\n",
      "Epoch 5/10, Train Loss: 0.1647, Accuracy: 0.8828, F1 Micro: 0.8828, F1 Macro: 0.8755\n",
      "Epoch 6/10, Train Loss: 0.1524, Accuracy: 0.8867, F1 Micro: 0.8867, F1 Macro: 0.8794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0852, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0879, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0741, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.938\n",
      "Epoch 10/10, Train Loss: 0.05, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9188\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.938\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        83\n",
      "    positive       0.96      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.94      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.8909\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.26      0.83      0.39        12\n",
      "     neutral       0.96      0.85      0.90       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.71      0.80      0.70       216\n",
      "weighted avg       0.90      0.82      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.12990260124207 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9279, F1 Micro: 0.9279, F1 Macro: 0.8283\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 7.892491340637207 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5404, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4495, Accuracy: 0.8527, F1 Micro: 0.9144, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3207, Accuracy: 0.936, F1 Micro: 0.9602, F1 Macro: 0.9582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2265, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9721\n",
      "Epoch 5/10, Train Loss: 0.1605, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1277, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9751\n",
      "Epoch 7/10, Train Loss: 0.1061, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9743\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0683, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9809\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.95      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.526, Accuracy: 0.8969, F1 Micro: 0.8969, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2242, Accuracy: 0.9046, F1 Micro: 0.9046, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1795, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.937\n",
      "Epoch 4/10, Train Loss: 0.181, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 5/10, Train Loss: 0.1405, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1255, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9487\n",
      "Epoch 7/10, Train Loss: 0.0942, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9357\n",
      "Epoch 8/10, Train Loss: 0.0985, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9379\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.928\n",
      "Epoch 10/10, Train Loss: 0.0594, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9323\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        85\n",
      "    positive       0.98      0.95      0.97       177\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.96      0.95       262\n",
      "weighted avg       0.96      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9172\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.85      0.87      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 135.76721334457397 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5446, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4584, Accuracy: 0.8423, F1 Micro: 0.9091, F1 Macro: 0.9078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3327, Accuracy: 0.936, F1 Micro: 0.9601, F1 Macro: 0.9586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2302, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.9674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1625, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Epoch 6/10, Train Loss: 0.1297, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Epoch 7/10, Train Loss: 0.1025, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0813, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9739\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9745\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5697, Accuracy: 0.8745, F1 Micro: 0.8745, F1 Macro: 0.8455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.282, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1644, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "Epoch 4/10, Train Loss: 0.1547, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1331, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "Epoch 6/10, Train Loss: 0.0908, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1127, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9399\n",
      "Epoch 8/10, Train Loss: 0.0713, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.92\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.916\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9399\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        84\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8845\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.96      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.83      0.65        12\n",
      "     neutral       0.94      0.89      0.92       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.85      0.79       216\n",
      "weighted avg       0.88      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.73      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.92      0.80        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.85      0.85       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.56197261810303 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5373, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4422, Accuracy: 0.8594, F1 Micro: 0.9179, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3232, Accuracy: 0.9368, F1 Micro: 0.9607, F1 Macro: 0.9591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2339, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1653, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1301, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1081, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0843, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Epoch 9/10, Train Loss: 0.0666, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5408, Accuracy: 0.8726, F1 Micro: 0.8726, F1 Macro: 0.847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2635, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1859, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "Epoch 4/10, Train Loss: 0.1342, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "Epoch 5/10, Train Loss: 0.1051, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Epoch 6/10, Train Loss: 0.1318, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.952\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9274\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        83\n",
      "    positive       0.98      0.95      0.97       176\n",
      "\n",
      "    accuracy                           0.96       259\n",
      "   macro avg       0.95      0.96      0.95       259\n",
      "weighted avg       0.96      0.96      0.96       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9013\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      1.00      0.83        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.91      0.86       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.40146160125732 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.8339\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 7.297613143920898 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4427, Accuracy: 0.8705, F1 Micro: 0.924, F1 Macro: 0.9233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3122, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.9628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2182, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1662, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1189, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9737\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0641, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "Epoch 10/10, Train Loss: 0.0544, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9729\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.91      0.92       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5602, Accuracy: 0.8856, F1 Micro: 0.8856, F1 Macro: 0.861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2724, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1913, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9299\n",
      "Epoch 4/10, Train Loss: 0.1353, Accuracy: 0.9114, F1 Micro: 0.9114, F1 Macro: 0.9028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1078, Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.9407\n",
      "Epoch 6/10, Train Loss: 0.0971, Accuracy: 0.9041, F1 Micro: 0.9041, F1 Macro: 0.8951\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9327\n",
      "Epoch 8/10, Train Loss: 0.0794, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9255\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9331\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9299, F1 Micro: 0.9299, F1 Macro: 0.9216\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.9407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        85\n",
      "    positive       0.97      0.95      0.96       186\n",
      "\n",
      "    accuracy                           0.95       271\n",
      "   macro avg       0.94      0.95      0.94       271\n",
      "weighted avg       0.95      0.95      0.95       271\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8948\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.83      0.69        12\n",
      "     neutral       0.95      0.89      0.92       152\n",
      "    positive       0.74      0.81      0.77        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.76      0.84      0.79       216\n",
      "weighted avg       0.88      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.51212215423584 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5443, Accuracy: 0.7939, F1 Micro: 0.8844, F1 Macro: 0.8829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4443, Accuracy: 0.8467, F1 Micro: 0.911, F1 Macro: 0.9093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3152, Accuracy: 0.9382, F1 Micro: 0.9616, F1 Macro: 0.96\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2173, Accuracy: 0.9494, F1 Micro: 0.9682, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1665, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1158, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Epoch 7/10, Train Loss: 0.1005, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9722\n",
      "Epoch 8/10, Train Loss: 0.0771, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0656, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0533, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5599, Accuracy: 0.8848, F1 Micro: 0.8848, F1 Macro: 0.8636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2514, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.201, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9372\n",
      "Epoch 4/10, Train Loss: 0.1315, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9182\n",
      "Epoch 5/10, Train Loss: 0.1468, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9312\n",
      "Epoch 6/10, Train Loss: 0.1202, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9096\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9332\n",
      "Epoch 8/10, Train Loss: 0.0818, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9253\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9372\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       184\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.95      0.94       269\n",
      "weighted avg       0.95      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9034\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.77      0.79      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.85      0.81       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 132.82698488235474 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5429, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4342, Accuracy: 0.875, F1 Micro: 0.9261, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3149, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2205, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1197, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 7/10, Train Loss: 0.1037, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0781, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9744\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5426, Accuracy: 0.877, F1 Micro: 0.877, F1 Macro: 0.862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1875, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1768, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1389, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9414\n",
      "Epoch 6/10, Train Loss: 0.1027, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1176, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.951\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.926\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9421\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9127\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.97       170\n",
      "\n",
      "    accuracy                           0.96       252\n",
      "   macro avg       0.94      0.96      0.95       252\n",
      "weighted avg       0.96      0.96      0.96       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8795\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.91      0.87        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.92      0.59        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.86      0.78       216\n",
      "weighted avg       0.90      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 136.90885710716248 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9311, F1 Micro: 0.9311, F1 Macro: 0.8381\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 6.7942235469818115 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4451, Accuracy: 0.8854, F1 Micro: 0.9319, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3113, Accuracy: 0.9345, F1 Micro: 0.9593, F1 Macro: 0.9573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2189, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1139, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0745, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 9/10, Train Loss: 0.0687, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9598, F1 Micro: 0.9745, F1 Macro: 0.9722\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5149, Accuracy: 0.8674, F1 Micro: 0.8674, F1 Macro: 0.8336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2422, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9269\n",
      "Epoch 3/10, Train Loss: 0.206, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9245\n",
      "Epoch 4/10, Train Loss: 0.1725, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.163, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9326\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Epoch 7/10, Train Loss: 0.0882, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.8999\n",
      "Epoch 8/10, Train Loss: 0.1161, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0801, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "Epoch 10/10, Train Loss: 0.0924, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       179\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.93      0.95      0.94       264\n",
      "weighted avg       0.95      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8953\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      0.92      0.55        12\n",
      "     neutral       0.96      0.88      0.92       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.73      0.86      0.76       216\n",
      "weighted avg       0.90      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.2747712135315 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5427, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4495, Accuracy: 0.8787, F1 Micro: 0.9275, F1 Macro: 0.9255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3119, Accuracy: 0.9345, F1 Micro: 0.9592, F1 Macro: 0.9574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2131, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1552, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9759\n",
      "Epoch 6/10, Train Loss: 0.1137, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Epoch 7/10, Train Loss: 0.0869, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0529, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.91      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5273, Accuracy: 0.8839, F1 Micro: 0.8839, F1 Macro: 0.865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2643, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1915, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9373\n",
      "Epoch 4/10, Train Loss: 0.151, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9178\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.8952\n",
      "Epoch 6/10, Train Loss: 0.1494, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9254\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.925\n",
      "Epoch 8/10, Train Loss: 0.0989, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9293\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9269\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9136\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.97      0.92        86\n",
      "    positive       0.98      0.93      0.96       181\n",
      "\n",
      "    accuracy                           0.94       267\n",
      "   macro avg       0.93      0.95      0.94       267\n",
      "weighted avg       0.95      0.94      0.94       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9022\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.76      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.85      0.83       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       1.00      0.73      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.17658185958862 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5362, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4388, Accuracy: 0.9062, F1 Micro: 0.9436, F1 Macro: 0.9422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3092, Accuracy: 0.9435, F1 Micro: 0.9651, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2167, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1595, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1152, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9751\n",
      "Epoch 8/10, Train Loss: 0.0774, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9754\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0533, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5388, Accuracy: 0.868, F1 Micro: 0.868, F1 Macro: 0.8341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2495, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9459\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Epoch 4/10, Train Loss: 0.1384, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9169\n",
      "Epoch 5/10, Train Loss: 0.0851, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0812, Accuracy: 0.96, F1 Micro: 0.96, F1 Macro: 0.9552\n",
      "Epoch 7/10, Train Loss: 0.1018, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.938\n",
      "Epoch 8/10, Train Loss: 0.0893, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.96, F1 Micro: 0.96, F1 Macro: 0.9552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94        81\n",
      "    positive       0.99      0.95      0.97       169\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.95      0.96      0.96       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9028\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.88      0.76        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.87      0.85       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.92      0.63        12\n",
      "     neutral       0.94      0.91      0.92       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.86      0.78       216\n",
      "weighted avg       0.89      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 130.69299793243408 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9324, F1 Micro: 0.9324, F1 Macro: 0.8422\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 6.409355401992798 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5392, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4489, Accuracy: 0.872, F1 Micro: 0.9243, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3198, Accuracy: 0.942, F1 Micro: 0.964, F1 Macro: 0.9627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2115, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9755\n",
      "Epoch 6/10, Train Loss: 0.1181, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.095, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.082, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.0566, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9751\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5566, Accuracy: 0.8669, F1 Micro: 0.8669, F1 Macro: 0.8319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2586, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1799, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9204\n",
      "Epoch 6/10, Train Loss: 0.1392, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9268\n",
      "Epoch 7/10, Train Loss: 0.1177, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 8/10, Train Loss: 0.0951, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8922\n",
      "Epoch 10/10, Train Loss: 0.0744, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9223\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.8894\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.73      0.80        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.84      0.88      0.86        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.94      0.90      0.92       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.85      0.80       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 138.1023108959198 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5384, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4576, Accuracy: 0.8616, F1 Micro: 0.9186, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3252, Accuracy: 0.9427, F1 Micro: 0.9643, F1 Macro: 0.9629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.208, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1534, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Epoch 6/10, Train Loss: 0.1172, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9752\n",
      "Epoch 7/10, Train Loss: 0.0937, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0808, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0621, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0544, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.535, Accuracy: 0.8876, F1 Micro: 0.8876, F1 Macro: 0.8729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2798, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.185, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1479, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9214\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9179\n",
      "Epoch 7/10, Train Loss: 0.1147, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9218\n",
      "Epoch 9/10, Train Loss: 0.0701, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9175\n",
      "Epoch 10/10, Train Loss: 0.0555, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9113\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.87      0.89        86\n",
      "    positive       0.94      0.96      0.95       181\n",
      "\n",
      "    accuracy                           0.93       267\n",
      "   macro avg       0.93      0.92      0.92       267\n",
      "weighted avg       0.93      0.93      0.93       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8989\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.88      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.85      0.82       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.4581561088562 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5331, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4312, Accuracy: 0.8914, F1 Micro: 0.9352, F1 Macro: 0.9341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3083, Accuracy: 0.9472, F1 Micro: 0.9673, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2089, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1517, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Epoch 6/10, Train Loss: 0.1164, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.097, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0813, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 9/10, Train Loss: 0.0653, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 10/10, Train Loss: 0.0576, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5201, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9028\n",
      "Epoch 2/10, Train Loss: 0.2521, Accuracy: 0.8774, F1 Micro: 0.8774, F1 Macro: 0.8698\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.8926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1636, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1465, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1167, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9269\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9179\n",
      "Epoch 8/10, Train Loss: 0.0777, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.922\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9224\n",
      "Epoch 10/10, Train Loss: 0.0567, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9112\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9269\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.93      0.90        85\n",
      "    positive       0.96      0.94      0.95       176\n",
      "\n",
      "    accuracy                           0.93       261\n",
      "   macro avg       0.92      0.93      0.93       261\n",
      "weighted avg       0.94      0.93      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8854\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.83      0.65        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.85      0.79       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 137.4223415851593 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9335, F1 Micro: 0.9335, F1 Macro: 0.8453\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.496324062347412 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5405, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4328, Accuracy: 0.9107, F1 Micro: 0.9461, F1 Macro: 0.9453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2987, Accuracy: 0.9457, F1 Micro: 0.9664, F1 Macro: 0.965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2012, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1196, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0937, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.0749, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 9/10, Train Loss: 0.0606, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9745\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5417, Accuracy: 0.8986, F1 Micro: 0.8986, F1 Macro: 0.8852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2598, Accuracy: 0.942, F1 Micro: 0.942, F1 Macro: 0.9344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2054, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1297, Accuracy: 0.9493, F1 Micro: 0.9493, F1 Macro: 0.9423\n",
      "Epoch 5/10, Train Loss: 0.144, Accuracy: 0.942, F1 Micro: 0.942, F1 Macro: 0.9348\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9384, F1 Micro: 0.9384, F1 Macro: 0.9284\n",
      "Epoch 7/10, Train Loss: 0.1065, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9502\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9205\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.942, F1 Micro: 0.942, F1 Macro: 0.9315\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.97       190\n",
      "\n",
      "    accuracy                           0.96       276\n",
      "   macro avg       0.94      0.96      0.95       276\n",
      "weighted avg       0.96      0.96      0.96       276\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9054\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.96      0.88      0.92       152\n",
      "    positive       0.75      0.88      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.87      0.82       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.87602925300598 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5419, Accuracy: 0.7932, F1 Micro: 0.8836, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4356, Accuracy: 0.9062, F1 Micro: 0.9427, F1 Macro: 0.941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2957, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1948, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1383, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1149, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0574, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.0544, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5153, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2501, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1811, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9258\n",
      "Epoch 4/10, Train Loss: 0.1554, Accuracy: 0.9148, F1 Micro: 0.9148, F1 Macro: 0.9064\n",
      "Epoch 5/10, Train Loss: 0.125, Accuracy: 0.9148, F1 Micro: 0.9148, F1 Macro: 0.9022\n",
      "Epoch 6/10, Train Loss: 0.128, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0806, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.061, Accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.9403\n",
      "Epoch 10/10, Train Loss: 0.0579, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.929\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.9403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        86\n",
      "    positive       0.96      0.96      0.96       184\n",
      "\n",
      "    accuracy                           0.95       270\n",
      "   macro avg       0.94      0.94      0.94       270\n",
      "weighted avg       0.95      0.95      0.95       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9087\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.88      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.85      0.80       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 148.66725897789001 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.535, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4279, Accuracy: 0.8988, F1 Micro: 0.9392, F1 Macro: 0.9377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3011, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2003, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1412, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1176, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0959, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0739, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0615, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.498, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2526, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9343\n",
      "Epoch 3/10, Train Loss: 0.2247, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1657, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1144, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9443\n",
      "Epoch 6/10, Train Loss: 0.125, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9238\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Epoch 8/10, Train Loss: 0.0811, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9392\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9381\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9234\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        85\n",
      "    positive       0.99      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.96      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.895\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.92      0.56        12\n",
      "     neutral       0.94      0.91      0.92       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.85      0.77       216\n",
      "weighted avg       0.90      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.4449954032898 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.8487\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.2521631717681885 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5408, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4286, Accuracy: 0.8876, F1 Micro: 0.9335, F1 Macro: 0.933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2825, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1952, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1391, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 6/10, Train Loss: 0.1059, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5005, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.9018\n",
      "Epoch 2/10, Train Loss: 0.2501, Accuracy: 0.8937, F1 Micro: 0.8937, F1 Macro: 0.8867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2163, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1624, Accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9518\n",
      "Epoch 5/10, Train Loss: 0.1092, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9424\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9185\n",
      "Epoch 7/10, Train Loss: 0.094, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9327\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9346\n",
      "Epoch 9/10, Train Loss: 0.0601, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9222\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9466\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        84\n",
      "    positive       0.98      0.95      0.97       170\n",
      "\n",
      "    accuracy                           0.96       254\n",
      "   macro avg       0.95      0.96      0.95       254\n",
      "weighted avg       0.96      0.96      0.96       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9089\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.79      0.86      0.81       216\n",
      "weighted avg       0.90      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 146.8430037498474 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5302, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.442, Accuracy: 0.9115, F1 Micro: 0.946, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2837, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1915, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1357, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.068, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9784\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.512, Accuracy: 0.8779, F1 Micro: 0.8779, F1 Macro: 0.8696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2321, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1772, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9404\n",
      "Epoch 4/10, Train Loss: 0.1418, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9054\n",
      "Epoch 5/10, Train Loss: 0.1727, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "Epoch 6/10, Train Loss: 0.103, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9349\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 9/10, Train Loss: 0.0636, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9226\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.93      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.8998\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.96      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.81      0.68        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.82      0.84      0.82       216\n",
      "weighted avg       0.93      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.83      0.69        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.79      0.86      0.82       216\n",
      "weighted avg       0.90      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.94061279296875 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5281, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4256, Accuracy: 0.9048, F1 Micro: 0.9428, F1 Macro: 0.9414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2787, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1962, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1373, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1059, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 7/10, Train Loss: 0.0844, Accuracy: 0.9665, F1 Micro: 0.9787, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0736, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.98\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Epoch 10/10, Train Loss: 0.0521, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9793\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5144, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2692, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2131, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1455, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9298\n",
      "Epoch 5/10, Train Loss: 0.1535, Accuracy: 0.9127, F1 Micro: 0.9127, F1 Macro: 0.9067\n",
      "Epoch 6/10, Train Loss: 0.1232, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9127\n",
      "Epoch 7/10, Train Loss: 0.1006, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8973\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9256\n",
      "Epoch 9/10, Train Loss: 0.0722, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9131\n",
      "Epoch 10/10, Train Loss: 0.069, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9095\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        84\n",
      "    positive       0.97      0.93      0.95       168\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.92      0.94      0.93       252\n",
      "weighted avg       0.94      0.94      0.94       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8822\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.91      0.87        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.88      0.74        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.86      0.84       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.83      0.54        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.75      0.83      0.76       216\n",
      "weighted avg       0.91      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.449862241745 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.8514\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 4.606061220169067 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5405, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4179, Accuracy: 0.9241, F1 Micro: 0.9535, F1 Macro: 0.9523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2749, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1885, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9759\n",
      "Epoch 6/10, Train Loss: 0.1055, Accuracy: 0.9576, F1 Micro: 0.973, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0857, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0517, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5063, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9068\n",
      "Epoch 2/10, Train Loss: 0.2519, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.8938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2063, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9218\n",
      "Epoch 4/10, Train Loss: 0.1501, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1729, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Epoch 6/10, Train Loss: 0.1251, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9286\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9326\n",
      "Epoch 8/10, Train Loss: 0.0668, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9088\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9278\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9162\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       182\n",
      "\n",
      "    accuracy                           0.94       267\n",
      "   macro avg       0.93      0.95      0.94       267\n",
      "weighted avg       0.95      0.94      0.94       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8998\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.77      0.79      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.84      0.79       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.70587921142578 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5452, Accuracy: 0.7902, F1 Micro: 0.8823, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4355, Accuracy: 0.9085, F1 Micro: 0.9439, F1 Macro: 0.9419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2848, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1374, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9776\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 8/10, Train Loss: 0.0687, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.053, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9805\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.489, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9156\n",
      "Epoch 2/10, Train Loss: 0.2306, Accuracy: 0.9108, F1 Micro: 0.9108, F1 Macro: 0.9028\n",
      "Epoch 3/10, Train Loss: 0.1994, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1409, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9257\n",
      "Epoch 5/10, Train Loss: 0.1094, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9144\n",
      "Epoch 7/10, Train Loss: 0.088, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.924\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9144\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9174\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.91      0.90        87\n",
      "    positive       0.96      0.95      0.95       182\n",
      "\n",
      "    accuracy                           0.93       269\n",
      "   macro avg       0.92      0.93      0.92       269\n",
      "weighted avg       0.93      0.93      0.93       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9138\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.83      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.0987207889557 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.537, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4172, Accuracy: 0.9152, F1 Micro: 0.948, F1 Macro: 0.9459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2772, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1876, Accuracy: 0.9591, F1 Micro: 0.9746, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1419, Accuracy: 0.9643, F1 Micro: 0.9778, F1 Macro: 0.9766\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9748\n",
      "Epoch 7/10, Train Loss: 0.085, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0726, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9746\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.92      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4941, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9372\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9169\n",
      "Epoch 3/10, Train Loss: 0.1588, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Epoch 4/10, Train Loss: 0.1609, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.113, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9399\n",
      "Epoch 6/10, Train Loss: 0.107, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9284\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9209\n",
      "Epoch 8/10, Train Loss: 0.095, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9272\n",
      "Epoch 9/10, Train Loss: 0.08, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9284\n",
      "Epoch 10/10, Train Loss: 0.0598, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9365\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9399\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        85\n",
      "    positive       0.97      0.96      0.96       181\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.94      0.94      0.94       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9129\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.96      0.93      0.94       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.86      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.03084206581116 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.8544\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.48119592666626 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5447, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4101, Accuracy: 0.9144, F1 Micro: 0.9481, F1 Macro: 0.9469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2817, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.187, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9743\n",
      "Epoch 5/10, Train Loss: 0.138, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.089, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0735, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9794\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5036, Accuracy: 0.8621, F1 Micro: 0.8621, F1 Macro: 0.8546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2338, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2018, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9277\n",
      "Epoch 4/10, Train Loss: 0.1613, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1148, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Epoch 6/10, Train Loss: 0.1278, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1161, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.098, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Epoch 9/10, Train Loss: 0.0921, Accuracy: 0.9004, F1 Micro: 0.9004, F1 Macro: 0.8859\n",
      "Epoch 10/10, Train Loss: 0.0548, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9229\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        86\n",
      "    positive       0.96      0.95      0.95       175\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.93      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8907\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.75      0.55        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.75      0.82      0.77       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 151.39272546768188 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.54, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4173, Accuracy: 0.9174, F1 Micro: 0.9494, F1 Macro: 0.9476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2776, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1806, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1331, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1015, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9751\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5042, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2313, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9277\n",
      "Epoch 3/10, Train Loss: 0.1634, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1543, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.927\n",
      "Epoch 5/10, Train Loss: 0.1076, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9098\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9073\n",
      "Epoch 7/10, Train Loss: 0.0849, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9225\n",
      "Epoch 10/10, Train Loss: 0.0545, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9111\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        85\n",
      "    positive       0.98      0.92      0.95       171\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.92      0.94      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8854\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.88      0.72        16\n",
      "     neutral       0.98      0.97      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.82      0.86      0.83       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.83      0.59        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.84      0.78       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.87      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 147.22848463058472 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5351, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3967, Accuracy: 0.9219, F1 Micro: 0.9525, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.273, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1795, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Epoch 9/10, Train Loss: 0.0555, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4623, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2391, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1683, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 4/10, Train Loss: 0.1389, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Epoch 5/10, Train Loss: 0.1081, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9225\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0746, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        83\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.93      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9018\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.88      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.94      0.86        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.85      0.82       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 144.63885641098022 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9374, F1 Micro: 0.9374, F1 Macro: 0.8563\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.56078839302063 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5293, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4081, Accuracy: 0.9301, F1 Micro: 0.9571, F1 Macro: 0.9558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2625, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1797, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9787\n",
      "Epoch 6/10, Train Loss: 0.0969, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5001, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2604, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Epoch 3/10, Train Loss: 0.1718, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 4/10, Train Loss: 0.1698, Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.8989\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1368, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9392\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.935\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9264\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9181\n",
      "Epoch 10/10, Train Loss: 0.0787, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9214\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        84\n",
      "    positive       0.98      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.93      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.8979\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.96      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.83      0.59        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.91      0.75      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.84      0.78       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.98      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.97299218177795 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5279, Accuracy: 0.7924, F1 Micro: 0.8831, F1 Macro: 0.8813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.415, Accuracy: 0.9219, F1 Micro: 0.9521, F1 Macro: 0.9503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2565, Accuracy: 0.9479, F1 Micro: 0.9675, F1 Macro: 0.9663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1746, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1274, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Epoch 6/10, Train Loss: 0.0951, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0729, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.979\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4751, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2267, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1839, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "Epoch 4/10, Train Loss: 0.165, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9113\n",
      "Epoch 5/10, Train Loss: 0.1204, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9198\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9136\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9572, F1 Micro: 0.9572, F1 Macro: 0.9523\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9572, F1 Micro: 0.9572, F1 Macro: 0.9523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        85\n",
      "    positive       0.98      0.95      0.97       172\n",
      "\n",
      "    accuracy                           0.96       257\n",
      "   macro avg       0.95      0.96      0.95       257\n",
      "weighted avg       0.96      0.96      0.96       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9109\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.81      0.72        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.86      0.84       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.79      0.85      0.80       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 161.2505965232849 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5164, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3942, Accuracy: 0.9286, F1 Micro: 0.9564, F1 Macro: 0.955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2518, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1758, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 5/10, Train Loss: 0.131, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9805\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9799\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.94      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4917, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9132\n",
      "Epoch 2/10, Train Loss: 0.2398, Accuracy: 0.9057, F1 Micro: 0.9057, F1 Macro: 0.8981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1803, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.919\n",
      "Epoch 4/10, Train Loss: 0.1911, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9203\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9266\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9323\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9132\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        87\n",
      "    positive       0.97      0.94      0.95       178\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.93      0.94      0.93       265\n",
      "weighted avg       0.94      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.8967\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.88      0.78        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.84      0.84      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.92      0.69        12\n",
      "     neutral       0.97      0.91      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.77      0.88      0.81       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 157.5653908252716 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.8585\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.910968065261841 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5278, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4116, Accuracy: 0.9323, F1 Micro: 0.9584, F1 Macro: 0.9574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2708, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1255, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       1.00      0.99      1.00       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4739, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8983\n",
      "Epoch 2/10, Train Loss: 0.2424, Accuracy: 0.8955, F1 Micro: 0.8955, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1957, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9291\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9103\n",
      "Epoch 5/10, Train Loss: 0.1148, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0962, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9371\n",
      "Epoch 9/10, Train Loss: 0.0711, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9327\n",
      "Epoch 10/10, Train Loss: 0.0672, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8988\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       182\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.93      0.95      0.94       268\n",
      "weighted avg       0.95      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.8987\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       1.00      0.99      1.00       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.93      0.97      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.92      0.76        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.76      0.75      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.86      0.81       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 166.29618573188782 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5284, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4223, Accuracy: 0.9278, F1 Micro: 0.9555, F1 Macro: 0.9536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2644, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0955, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.9613, F1 Micro: 0.9754, F1 Macro: 0.9733\n",
      "Epoch 8/10, Train Loss: 0.0639, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9729\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5087, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.217, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.186, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9369\n",
      "Epoch 4/10, Train Loss: 0.1318, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.916\n",
      "Epoch 5/10, Train Loss: 0.1374, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9135\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.913\n",
      "Epoch 10/10, Train Loss: 0.0348, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9209\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        85\n",
      "    positive       0.98      0.94      0.96       184\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.95      0.94       269\n",
      "weighted avg       0.95      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9035\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.88      0.80        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.86      0.87      0.86       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.95      0.89      0.92       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.85      0.78       216\n",
      "weighted avg       0.89      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.5036609172821 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5223, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4034, Accuracy: 0.9241, F1 Micro: 0.9533, F1 Macro: 0.9517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2679, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.178, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.128, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1052, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9788\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4864, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9165\n",
      "Epoch 2/10, Train Loss: 0.2492, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.9062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1982, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1829, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9379\n",
      "Epoch 5/10, Train Loss: 0.0957, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9339\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Epoch 7/10, Train Loss: 0.1098, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.928\n",
      "Epoch 8/10, Train Loss: 0.0855, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0706, Accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.945\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.96      0.96       183\n",
      "\n",
      "    accuracy                           0.95       269\n",
      "   macro avg       0.94      0.95      0.94       269\n",
      "weighted avg       0.95      0.95      0.95       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9048\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.77      0.77      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.84      0.80       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 157.54995441436768 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.8605\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.386148691177368 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5323, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3968, Accuracy: 0.9241, F1 Micro: 0.9535, F1 Macro: 0.9519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2523, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0571, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4471, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9183\n",
      "Epoch 2/10, Train Loss: 0.2132, Accuracy: 0.8826, F1 Micro: 0.8826, F1 Macro: 0.8745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1693, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9314\n",
      "Epoch 4/10, Train Loss: 0.1733, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Epoch 5/10, Train Loss: 0.1136, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9166\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.917\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0665, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9326\n",
      "Epoch 9/10, Train Loss: 0.0656, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9201\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9282\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9326\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        85\n",
      "    positive       0.98      0.93      0.95       179\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.92      0.95      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9041\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.83      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.85      0.76        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.85      0.87      0.85       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 160.63206148147583 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5294, Accuracy: 0.7917, F1 Micro: 0.8828, F1 Macro: 0.881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3908, Accuracy: 0.9375, F1 Micro: 0.9614, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2411, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1631, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Epoch 5/10, Train Loss: 0.1217, Accuracy: 0.9583, F1 Micro: 0.9736, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0899, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9621, F1 Micro: 0.9758, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.058, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "Epoch 10/10, Train Loss: 0.0433, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9762\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4867, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.224, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9257\n",
      "Epoch 3/10, Train Loss: 0.1546, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9139\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9148\n",
      "Epoch 5/10, Train Loss: 0.1097, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9135\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9092\n",
      "Epoch 7/10, Train Loss: 0.1008, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9174\n",
      "Epoch 8/10, Train Loss: 0.0765, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0619, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "Epoch 10/10, Train Loss: 0.0515, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9068\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        87\n",
      "    positive       0.97      0.93      0.95       177\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.92      0.94      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8857\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.75      0.58        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.75      0.82      0.77       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.87      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 165.7540934085846 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5214, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3782, Accuracy: 0.9271, F1 Micro: 0.955, F1 Macro: 0.9533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2458, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1721, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9753\n",
      "Epoch 5/10, Train Loss: 0.1286, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.9665, F1 Micro: 0.9787, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0603, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0565, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4858, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2532, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9366\n",
      "Epoch 3/10, Train Loss: 0.1568, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 4/10, Train Loss: 0.136, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1235, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9355\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.102, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9441\n",
      "Epoch 8/10, Train Loss: 0.0739, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9318\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9237\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9233\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.92      0.92        86\n",
      "    positive       0.96      0.97      0.96       181\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.95      0.94      0.94       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9115\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.92      0.93       152\n",
      "    positive       0.80      0.85      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.87      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.77082347869873 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.8622\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 2.115708351135254 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5177, Accuracy: 0.7999, F1 Micro: 0.8875, F1 Macro: 0.8861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3826, Accuracy: 0.9249, F1 Micro: 0.953, F1 Macro: 0.9491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2401, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1672, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 6/10, Train Loss: 0.0974, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Epoch 9/10, Train Loss: 0.05, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4888, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9264\n",
      "Epoch 2/10, Train Loss: 0.2284, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9113\n",
      "Epoch 3/10, Train Loss: 0.1558, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1266, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9316\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9268\n",
      "Epoch 7/10, Train Loss: 0.0779, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0717, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "Epoch 9/10, Train Loss: 0.0469, Accuracy: 0.9027, F1 Micro: 0.9027, F1 Macro: 0.8954\n",
      "Epoch 10/10, Train Loss: 0.0561, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9218\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        84\n",
      "    positive       0.98      0.93      0.95       173\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.92      0.94      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.899\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.42      0.92      0.58        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.93      0.77      0.84        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.87      0.78       216\n",
      "weighted avg       0.91      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.86      0.87      0.86       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.06656002998352 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5221, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3982, Accuracy: 0.9174, F1 Micro: 0.948, F1 Macro: 0.9436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2413, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1661, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1204, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.965, F1 Micro: 0.9777, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.974, F1 Micro: 0.9836, F1 Macro: 0.9828\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.974, F1 Micro: 0.9836, F1 Macro: 0.9828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.98      0.96       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4927, Accuracy: 0.8969, F1 Micro: 0.8969, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2351, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1575, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "Epoch 4/10, Train Loss: 0.133, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9154\n",
      "Epoch 5/10, Train Loss: 0.1065, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0781, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9242\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Epoch 8/10, Train Loss: 0.0815, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9009\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.91      0.90        86\n",
      "    positive       0.95      0.95      0.95       176\n",
      "\n",
      "    accuracy                           0.94       262\n",
      "   macro avg       0.93      0.93      0.93       262\n",
      "weighted avg       0.94      0.94      0.94       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9014\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.79      0.82      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.84       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.83      0.53        12\n",
      "     neutral       0.95      0.90      0.93       152\n",
      "    positive       0.91      0.81      0.86        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.75      0.85      0.77       216\n",
      "weighted avg       0.91      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.43726301193237 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.513, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3754, Accuracy: 0.9167, F1 Micro: 0.9474, F1 Macro: 0.9411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2377, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1662, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1051, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4824, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2015, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "Epoch 3/10, Train Loss: 0.1625, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9188\n",
      "Epoch 4/10, Train Loss: 0.1266, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1058, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9139\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8833\n",
      "Epoch 8/10, Train Loss: 0.0803, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9282\n",
      "Epoch 10/10, Train Loss: 0.0688, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9187\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        84\n",
      "    positive       0.94      0.97      0.95       172\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.93      0.92      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.8953\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.82      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.83      0.61        12\n",
      "     neutral       0.95      0.92      0.93       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.77      0.85      0.80       216\n",
      "weighted avg       0.91      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.87      0.83        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.90      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 157.99468731880188 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.8638\n",
      "Total runtime: 9992.36607170105 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79ea09",
   "metadata": {
    "papermill": {
     "duration": 0.469572,
     "end_time": "2025-03-08T13:17:54.529519",
     "exception": false,
     "start_time": "2025-03-08T13:17:54.059947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677ceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 2\n",
      "Random seed: [81, 90, 11]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6696, Accuracy: 0.6935, F1 Micro: 0.8002, F1 Macro: 0.707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5881, Accuracy: 0.7775, F1 Micro: 0.8718, F1 Macro: 0.8663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5506, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8808\n",
      "Epoch 4/10, Train Loss: 0.528, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4866, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4714, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.48, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4383, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 9/10, Train Loss: 0.4116, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3836, Accuracy: 0.7946, F1 Micro: 0.8847, F1 Macro: 0.8832\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7946, F1 Micro: 0.8847, F1 Macro: 0.8832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.79      0.99      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.80      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6963, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5281, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5145, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4748, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4531, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5539, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.5383, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4717, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3387, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3776, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.4762\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.4762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.20      0.29         5\n",
      "    positive       0.56      0.83      0.67         6\n",
      "\n",
      "    accuracy                           0.55        11\n",
      "   macro avg       0.53      0.52      0.48        11\n",
      "weighted avg       0.53      0.55      0.49        11\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3111\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.06      0.11        16\n",
      "     neutral       0.79      0.99      0.88       167\n",
      "    positive       0.50      0.06      0.11        33\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.60      0.37      0.36       216\n",
      "weighted avg       0.72      0.78      0.70       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       0.50      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.40      0.34      0.29       216\n",
      "weighted avg       0.62      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       0.33      0.02      0.05        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.35      0.34      0.29       216\n",
      "weighted avg       0.57      0.71      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 65.25001049041748 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6698, Accuracy: 0.7924, F1 Micro: 0.8815, F1 Macro: 0.8788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5861, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Epoch 3/10, Train Loss: 0.5481, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.5332, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 5/10, Train Loss: 0.4877, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 6/10, Train Loss: 0.4812, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4841, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 8/10, Train Loss: 0.4614, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 9/10, Train Loss: 0.4247, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4075, Accuracy: 0.7932, F1 Micro: 0.884, F1 Macro: 0.8825\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7932, F1 Micro: 0.884, F1 Macro: 0.8825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      0.99      0.87       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.72      1.00      0.84       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.812, Accuracy: 0.375, F1 Micro: 0.375, F1 Macro: 0.3651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6454, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3987, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3302, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2917, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2519, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.216, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1941, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0953, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         2\n",
      "    positive       0.75      1.00      0.86         6\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.38      0.50      0.43         8\n",
      "weighted avg       0.56      0.75      0.64         8\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7924, F1 Micro: 0.7924, F1 Macro: 0.3069\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.78      0.99      0.87       167\n",
      "    positive       0.50      0.03      0.06        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.43      0.34      0.31       216\n",
      "weighted avg       0.68      0.77      0.68       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      1.00      0.84       152\n",
      "    positive       0.67      0.10      0.17        41\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.46      0.37      0.34       216\n",
      "weighted avg       0.64      0.72      0.62       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 55.45538306236267 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6693, Accuracy: 0.782, F1 Micro: 0.8767, F1 Macro: 0.875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5726, Accuracy: 0.7894, F1 Micro: 0.8816, F1 Macro: 0.8798\n",
      "Epoch 3/10, Train Loss: 0.5386, Accuracy: 0.7879, F1 Micro: 0.8813, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5272, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4812, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4653, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4676, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Epoch 8/10, Train Loss: 0.4355, Accuracy: 0.7902, F1 Micro: 0.8816, F1 Macro: 0.8795\n",
      "Epoch 9/10, Train Loss: 0.4096, Accuracy: 0.7879, F1 Micro: 0.8795, F1 Macro: 0.8767\n",
      "Epoch 10/10, Train Loss: 0.3739, Accuracy: 0.7879, F1 Micro: 0.8786, F1 Macro: 0.8755\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      0.99      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.88      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6057, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4393, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4544, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4236, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3367, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Epoch 6/10, Train Loss: 0.3349, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 7/10, Train Loss: 0.2332, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 8/10, Train Loss: 0.2243, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 9/10, Train Loss: 0.1859, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 10/10, Train Loss: 0.1717, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         4\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         4\n",
      "   macro avg       0.50      0.50      0.50         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7762, F1 Micro: 0.7762, F1 Macro: 0.3216\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.19      0.26        16\n",
      "     neutral       0.78      0.98      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.40      0.39      0.38       216\n",
      "weighted avg       0.63      0.77      0.69       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.03      0.08      0.05        12\n",
      "     neutral       0.74      0.89      0.81       152\n",
      "    positive       1.00      0.06      0.11        52\n",
      "\n",
      "    accuracy                           0.64       216\n",
      "   macro avg       0.59      0.34      0.32       216\n",
      "weighted avg       0.76      0.64      0.60       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.14      0.09      0.11        23\n",
      "     neutral       0.73      0.97      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.69       216\n",
      "   macro avg       0.29      0.35      0.31       216\n",
      "weighted avg       0.53      0.69      0.60       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 51.52443861961365 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7868, F1 Micro: 0.7868, F1 Macro: 0.3132\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 8.273263454437256 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6103, Accuracy: 0.7924, F1 Micro: 0.8826, F1 Macro: 0.8805\n",
      "Epoch 2/10, Train Loss: 0.5351, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.5061, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.4865, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4228, Accuracy: 0.8051, F1 Micro: 0.8898, F1 Macro: 0.8882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3957, Accuracy: 0.8423, F1 Micro: 0.9078, F1 Macro: 0.906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3723, Accuracy: 0.8757, F1 Micro: 0.9253, F1 Macro: 0.9233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2934, Accuracy: 0.8943, F1 Micro: 0.9357, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2682, Accuracy: 0.9137, F1 Micro: 0.9473, F1 Macro: 0.9453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2392, Accuracy: 0.9182, F1 Micro: 0.9497, F1 Macro: 0.9478\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9182, F1 Micro: 0.9497, F1 Macro: 0.9478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.96      0.99      0.98       187\n",
      "     machine       0.93      0.94      0.94       175\n",
      "      others       0.85      0.94      0.89       158\n",
      "        part       0.88      0.99      0.93       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.94      0.99      0.97       191\n",
      "\n",
      "   micro avg       0.92      0.98      0.95      1061\n",
      "   macro avg       0.92      0.98      0.95      1061\n",
      "weighted avg       0.92      0.98      0.95      1061\n",
      " samples avg       0.93      0.98      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5783, Accuracy: 0.7095, F1 Micro: 0.7095, F1 Macro: 0.415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5272, Accuracy: 0.7095, F1 Micro: 0.7095, F1 Macro: 0.415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4312, Accuracy: 0.819, F1 Micro: 0.819, F1 Macro: 0.7602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2808, Accuracy: 0.8524, F1 Micro: 0.8524, F1 Macro: 0.8295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.181, Accuracy: 0.8714, F1 Micro: 0.8714, F1 Macro: 0.8476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1375, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0753, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.122, Accuracy: 0.8952, F1 Micro: 0.8952, F1 Macro: 0.8753\n",
      "Epoch 9/10, Train Loss: 0.069, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8781\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.82      0.83        61\n",
      "    positive       0.93      0.93      0.93       149\n",
      "\n",
      "    accuracy                           0.90       210\n",
      "   macro avg       0.88      0.88      0.88       210\n",
      "weighted avg       0.90      0.90      0.90       210\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.9043, F1 Micro: 0.9043, F1 Macro: 0.7842\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.97      0.99      0.98       181\n",
      "    positive       0.95      0.79      0.86        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.93      0.95      0.94       167\n",
      "    positive       0.68      0.64      0.66        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.80      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.33      0.38        12\n",
      "     neutral       0.85      0.95      0.89       152\n",
      "    positive       0.78      0.56      0.65        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.69      0.61      0.64       216\n",
      "weighted avg       0.81      0.82      0.81       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.39      0.55        23\n",
      "     neutral       0.88      0.99      0.93       152\n",
      "    positive       0.80      0.68      0.74        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.86      0.69      0.74       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.54      0.70        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.79      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.50      0.61        14\n",
      "     neutral       0.94      0.99      0.97       185\n",
      "    positive       0.82      0.53      0.64        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.67      0.74       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Total train time: 79.21706557273865 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6123, Accuracy: 0.7842, F1 Micro: 0.879, F1 Macro: 0.877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5339, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5128, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.494, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4308, Accuracy: 0.8058, F1 Micro: 0.8895, F1 Macro: 0.8874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4163, Accuracy: 0.8155, F1 Micro: 0.8926, F1 Macro: 0.8891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3849, Accuracy: 0.846, F1 Micro: 0.9085, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3026, Accuracy: 0.8832, F1 Micro: 0.9291, F1 Macro: 0.9257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2723, Accuracy: 0.8951, F1 Micro: 0.9348, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.24, Accuracy: 0.9092, F1 Micro: 0.9437, F1 Macro: 0.9404\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9092, F1 Micro: 0.9437, F1 Macro: 0.9404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      0.99      0.98       187\n",
      "     machine       0.92      0.95      0.94       175\n",
      "      others       0.88      0.84      0.86       158\n",
      "        part       0.86      0.99      0.92       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.94      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.92      0.96      0.94      1061\n",
      "   macro avg       0.92      0.96      0.94      1061\n",
      "weighted avg       0.93      0.96      0.94      1061\n",
      " samples avg       0.93      0.96      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6077, Accuracy: 0.7249, F1 Micro: 0.7249, F1 Macro: 0.4203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4862, Accuracy: 0.7249, F1 Micro: 0.7249, F1 Macro: 0.4203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3571, Accuracy: 0.8646, F1 Micro: 0.8646, F1 Macro: 0.8277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2323, Accuracy: 0.8908, F1 Micro: 0.8908, F1 Macro: 0.8638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1443, Accuracy: 0.9039, F1 Micro: 0.9039, F1 Macro: 0.885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9083, F1 Micro: 0.9083, F1 Macro: 0.8808\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9039, F1 Micro: 0.9039, F1 Macro: 0.8784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9083, F1 Micro: 0.9083, F1 Macro: 0.8845\n",
      "Epoch 9/10, Train Loss: 0.0261, Accuracy: 0.9039, F1 Micro: 0.9039, F1 Macro: 0.884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0827, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.8944\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.8944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.83      0.85        63\n",
      "    positive       0.93      0.95      0.94       166\n",
      "\n",
      "    accuracy                           0.92       229\n",
      "   macro avg       0.90      0.89      0.89       229\n",
      "weighted avg       0.92      0.92      0.92       229\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8974, F1 Micro: 0.8974, F1 Macro: 0.7853\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.91      0.96      0.94       167\n",
      "    positive       0.73      0.58      0.64        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.78      0.81       216\n",
      "weighted avg       0.88      0.89      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.42      0.48        12\n",
      "     neutral       0.87      0.84      0.86       152\n",
      "    positive       0.62      0.71      0.66        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.68      0.66      0.66       216\n",
      "weighted avg       0.79      0.79      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.43      0.61        23\n",
      "     neutral       0.86      0.99      0.92       152\n",
      "    positive       0.83      0.61      0.70        41\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.90      0.68      0.74       216\n",
      "weighted avg       0.87      0.86      0.85       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.62      0.73        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.70      0.82      0.76        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.85      0.81      0.82       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.43      0.57        14\n",
      "     neutral       0.93      1.00      0.97       185\n",
      "    positive       0.82      0.53      0.64        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.65      0.73       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Total train time: 85.54468631744385 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.632, Accuracy: 0.7746, F1 Micro: 0.8716, F1 Macro: 0.867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5308, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5115, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Epoch 4/10, Train Loss: 0.4978, Accuracy: 0.7917, F1 Micro: 0.8815, F1 Macro: 0.8787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4303, Accuracy: 0.7984, F1 Micro: 0.8852, F1 Macro: 0.8829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.425, Accuracy: 0.8043, F1 Micro: 0.887, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3886, Accuracy: 0.8251, F1 Micro: 0.8967, F1 Macro: 0.8931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3222, Accuracy: 0.8393, F1 Micro: 0.9048, F1 Macro: 0.9025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2855, Accuracy: 0.8624, F1 Micro: 0.9156, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2606, Accuracy: 0.8765, F1 Micro: 0.9245, F1 Macro: 0.9216\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.8765, F1 Micro: 0.9245, F1 Macro: 0.9216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.94      1.00      0.97       187\n",
      "     machine       0.91      0.90      0.90       175\n",
      "      others       0.87      0.87      0.87       158\n",
      "        part       0.83      0.97      0.89       158\n",
      "       price       0.89      0.99      0.94       192\n",
      "     service       0.91      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.89      0.96      0.92      1061\n",
      "   macro avg       0.89      0.96      0.92      1061\n",
      "weighted avg       0.89      0.96      0.92      1061\n",
      " samples avg       0.90      0.96      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5646, Accuracy: 0.7716, F1 Micro: 0.7716, F1 Macro: 0.4355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4925, Accuracy: 0.797, F1 Micro: 0.797, F1 Macro: 0.6172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3932, Accuracy: 0.8122, F1 Micro: 0.8122, F1 Macro: 0.7434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2567, Accuracy: 0.8325, F1 Micro: 0.8325, F1 Macro: 0.7883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1116, Accuracy: 0.868, F1 Micro: 0.868, F1 Macro: 0.8184\n",
      "Epoch 6/10, Train Loss: 0.1442, Accuracy: 0.8579, F1 Micro: 0.8579, F1 Macro: 0.8214\n",
      "Epoch 7/10, Train Loss: 0.0568, Accuracy: 0.8528, F1 Micro: 0.8528, F1 Macro: 0.7928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1056, Accuracy: 0.8832, F1 Micro: 0.8832, F1 Macro: 0.8469\n",
      "Epoch 9/10, Train Loss: 0.0953, Accuracy: 0.8376, F1 Micro: 0.8376, F1 Macro: 0.8083\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.8477, F1 Micro: 0.8477, F1 Macro: 0.8167\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.8832, F1 Micro: 0.8832, F1 Macro: 0.8469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.87      0.77        45\n",
      "    positive       0.96      0.89      0.92       152\n",
      "\n",
      "    accuracy                           0.88       197\n",
      "   macro avg       0.83      0.88      0.85       197\n",
      "weighted avg       0.90      0.88      0.89       197\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.865, F1 Micro: 0.865, F1 Macro: 0.6782\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.64      0.74        11\n",
      "     neutral       0.95      1.00      0.97       181\n",
      "    positive       1.00      0.71      0.83        24\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.78      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.81      0.63        16\n",
      "     neutral       0.90      0.90      0.90       167\n",
      "    positive       0.68      0.52      0.59        33\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.70      0.74      0.71       216\n",
      "weighted avg       0.84      0.83      0.83       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.36      0.42      0.38        12\n",
      "     neutral       0.86      0.87      0.87       152\n",
      "    positive       0.67      0.63      0.65        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.63      0.64      0.63       216\n",
      "weighted avg       0.79      0.79      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.52      0.62        23\n",
      "     neutral       0.83      0.97      0.89       152\n",
      "    positive       0.86      0.44      0.58        41\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.81      0.64      0.70       216\n",
      "weighted avg       0.82      0.82      0.81       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.15      0.25        13\n",
      "     neutral       0.89      0.99      0.94       186\n",
      "    positive       0.67      0.24      0.35        17\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.74      0.46      0.51       216\n",
      "weighted avg       0.86      0.88      0.85       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.29      0.42        14\n",
      "     neutral       0.91      1.00      0.95       185\n",
      "    positive       1.00      0.47      0.64        17\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.59      0.67       216\n",
      "weighted avg       0.91      0.91      0.89       216\n",
      "\n",
      "Total train time: 76.81799936294556 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8378, F1 Micro: 0.8378, F1 Macro: 0.5312\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 14.585591793060303 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6065, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5258, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4947, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4543, Accuracy: 0.8155, F1 Micro: 0.8953, F1 Macro: 0.8942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4127, Accuracy: 0.8616, F1 Micro: 0.9184, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3369, Accuracy: 0.907, F1 Micro: 0.9438, F1 Macro: 0.9424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2978, Accuracy: 0.9286, F1 Micro: 0.9561, F1 Macro: 0.9545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2564, Accuracy: 0.9353, F1 Micro: 0.9598, F1 Macro: 0.9583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2128, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1778, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9674\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9674\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.96       175\n",
      "      others       0.90      0.94      0.92       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.98      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6033, Accuracy: 0.6793, F1 Micro: 0.6793, F1 Macro: 0.4045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4369, Accuracy: 0.7173, F1 Micro: 0.7173, F1 Macro: 0.5198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3126, Accuracy: 0.903, F1 Micro: 0.903, F1 Macro: 0.8918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1357, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0685, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.922\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.9241, F1 Micro: 0.9241, F1 Macro: 0.9146\n",
      "Epoch 8/10, Train Loss: 0.0768, Accuracy: 0.9156, F1 Micro: 0.9156, F1 Macro: 0.9073\n",
      "Epoch 9/10, Train Loss: 0.0653, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9214\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.87      0.89        76\n",
      "    positive       0.94      0.96      0.95       161\n",
      "\n",
      "    accuracy                           0.93       237\n",
      "   macro avg       0.93      0.92      0.92       237\n",
      "weighted avg       0.93      0.93      0.93       237\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9406, F1 Micro: 0.9406, F1 Macro: 0.8791\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.90      0.95      0.92       152\n",
      "    positive       0.82      0.71      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.61      0.70        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.86      0.78      0.82        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.79      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.86      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.83      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 89.93258547782898 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6033, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5222, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4953, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4551, Accuracy: 0.8043, F1 Micro: 0.8897, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.421, Accuracy: 0.8564, F1 Micro: 0.9163, F1 Macro: 0.9151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.354, Accuracy: 0.901, F1 Micro: 0.9407, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3038, Accuracy: 0.9241, F1 Micro: 0.953, F1 Macro: 0.9507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2582, Accuracy: 0.9315, F1 Micro: 0.9576, F1 Macro: 0.9557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.212, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9617\n",
      "Epoch 10/10, Train Loss: 0.1809, Accuracy: 0.9405, F1 Micro: 0.9628, F1 Macro: 0.9608\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.92      0.97      0.95       175\n",
      "      others       0.91      0.92      0.91       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.563, Accuracy: 0.6864, F1 Micro: 0.6864, F1 Macro: 0.407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3978, Accuracy: 0.7839, F1 Micro: 0.7839, F1 Macro: 0.6833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2988, Accuracy: 0.8602, F1 Micro: 0.8602, F1 Macro: 0.8288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1879, Accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1399, Accuracy: 0.9068, F1 Micro: 0.9068, F1 Macro: 0.8953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9054\n",
      "Epoch 7/10, Train Loss: 0.067, Accuracy: 0.8983, F1 Micro: 0.8983, F1 Macro: 0.8827\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9153, F1 Micro: 0.9153, F1 Macro: 0.9049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.072, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9149\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.911, F1 Micro: 0.911, F1 Macro: 0.8978\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.96      0.89        74\n",
      "    positive       0.98      0.91      0.94       162\n",
      "\n",
      "    accuracy                           0.92       236\n",
      "   macro avg       0.90      0.93      0.91       236\n",
      "weighted avg       0.93      0.92      0.93       236\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.8357\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.81      0.67        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.87      0.61      0.71        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.80      0.80      0.78       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.91      0.89      0.90       152\n",
      "    positive       0.77      0.71      0.74        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.74      0.81      0.77       216\n",
      "weighted avg       0.85      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.65      0.71        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.89      0.76      0.82        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.80      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.85      0.84      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.78      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Total train time: 89.13900923728943 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6018, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Epoch 2/10, Train Loss: 0.5339, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.501, Accuracy: 0.7932, F1 Micro: 0.8841, F1 Macro: 0.8825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4667, Accuracy: 0.8095, F1 Micro: 0.8909, F1 Macro: 0.8886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4342, Accuracy: 0.8333, F1 Micro: 0.9039, F1 Macro: 0.9025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3749, Accuracy: 0.8653, F1 Micro: 0.9192, F1 Macro: 0.9171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3253, Accuracy: 0.8973, F1 Micro: 0.9372, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2747, Accuracy: 0.9167, F1 Micro: 0.9489, F1 Macro: 0.947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2254, Accuracy: 0.9249, F1 Micro: 0.9531, F1 Macro: 0.9501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1886, Accuracy: 0.9412, F1 Micro: 0.9636, F1 Macro: 0.962\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9412, F1 Micro: 0.9636, F1 Macro: 0.962\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.98       187\n",
      "     machine       0.93      0.98      0.95       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.91      0.98      0.95       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5872, Accuracy: 0.677, F1 Micro: 0.677, F1 Macro: 0.4037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4279, Accuracy: 0.6903, F1 Micro: 0.6903, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3657, Accuracy: 0.8717, F1 Micro: 0.8717, F1 Macro: 0.8481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1752, Accuracy: 0.8982, F1 Micro: 0.8982, F1 Macro: 0.8878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.9002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0458, Accuracy: 0.9292, F1 Micro: 0.9292, F1 Macro: 0.9226\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9159, F1 Micro: 0.9159, F1 Macro: 0.9049\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9071, F1 Micro: 0.9071, F1 Macro: 0.8969\n",
      "Epoch 9/10, Train Loss: 0.1184, Accuracy: 0.9204, F1 Micro: 0.9204, F1 Macro: 0.9134\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9159, F1 Micro: 0.9159, F1 Macro: 0.9035\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9292, F1 Micro: 0.9292, F1 Macro: 0.9226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.99      0.90        73\n",
      "    positive       0.99      0.90      0.95       153\n",
      "\n",
      "    accuracy                           0.93       226\n",
      "   macro avg       0.91      0.94      0.92       226\n",
      "weighted avg       0.94      0.93      0.93       226\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.929, F1 Micro: 0.929, F1 Macro: 0.8485\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.97      1.00      0.99       181\n",
      "    positive       0.95      0.83      0.89        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.98      0.88      0.93       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.88      0.64      0.74        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.79      0.82       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.82      0.69      0.75        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.80      0.80       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.61      0.72        23\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.83      0.73      0.78        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.78      0.82       216\n",
      "weighted avg       0.89      0.90      0.89       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.76        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.79      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.83      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 87.73762249946594 s\n",
      "Averaged - Iteration 208: Accuracy: 0.8691, F1 Micro: 0.8691, F1 Macro: 0.639\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 13.79374885559082 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5934, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5164, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4675, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3959, Accuracy: 0.8661, F1 Micro: 0.9208, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3357, Accuracy: 0.9174, F1 Micro: 0.9498, F1 Macro: 0.9488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2822, Accuracy: 0.936, F1 Micro: 0.9605, F1 Macro: 0.9592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2362, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1908, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9684\n",
      "Epoch 9/10, Train Loss: 0.159, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1238, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9708\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.88      0.97      0.92       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5329, Accuracy: 0.6777, F1 Micro: 0.6777, F1 Macro: 0.4039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4667, Accuracy: 0.876, F1 Micro: 0.876, F1 Macro: 0.8551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2205, Accuracy: 0.905, F1 Micro: 0.905, F1 Macro: 0.8909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1218, Accuracy: 0.9215, F1 Micro: 0.9215, F1 Macro: 0.9131\n",
      "Epoch 5/10, Train Loss: 0.1076, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9072\n",
      "Epoch 6/10, Train Loss: 0.0767, Accuracy: 0.905, F1 Micro: 0.905, F1 Macro: 0.8901\n",
      "Epoch 7/10, Train Loss: 0.0979, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9008\n",
      "Epoch 8/10, Train Loss: 0.0986, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8887\n",
      "Epoch 9/10, Train Loss: 0.0744, Accuracy: 0.8967, F1 Micro: 0.8967, F1 Macro: 0.8837\n",
      "Epoch 10/10, Train Loss: 0.0558, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9088\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9215, F1 Micro: 0.9215, F1 Macro: 0.9131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.95      0.89        78\n",
      "    positive       0.97      0.91      0.94       164\n",
      "\n",
      "    accuracy                           0.92       242\n",
      "   macro avg       0.90      0.93      0.91       242\n",
      "weighted avg       0.93      0.92      0.92       242\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.8813\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.67      0.64        12\n",
      "     neutral       0.88      0.98      0.93       152\n",
      "    positive       0.91      0.60      0.72        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.80      0.75      0.76       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.77      0.74        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 94.13263368606567 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5852, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.516, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4752, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4237, Accuracy: 0.8497, F1 Micro: 0.9127, F1 Macro: 0.9115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3583, Accuracy: 0.9107, F1 Micro: 0.9462, F1 Macro: 0.9447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2967, Accuracy: 0.9368, F1 Micro: 0.9608, F1 Macro: 0.9594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2367, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1898, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9677\n",
      "Epoch 9/10, Train Loss: 0.154, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1238, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9691\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.88      0.97      0.92       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5223, Accuracy: 0.6609, F1 Micro: 0.6609, F1 Macro: 0.3979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4109, Accuracy: 0.8627, F1 Micro: 0.8627, F1 Macro: 0.8416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.248, Accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9211\n",
      "Epoch 4/10, Train Loss: 0.1418, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9063\n",
      "Epoch 5/10, Train Loss: 0.1024, Accuracy: 0.8841, F1 Micro: 0.8841, F1 Macro: 0.8785\n",
      "Epoch 6/10, Train Loss: 0.0853, Accuracy: 0.897, F1 Micro: 0.897, F1 Macro: 0.8785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0881, Accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9207\n",
      "Epoch 8/10, Train Loss: 0.0749, Accuracy: 0.8927, F1 Micro: 0.8927, F1 Macro: 0.8784\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9207\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9207\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        79\n",
      "    positive       0.97      0.92      0.94       154\n",
      "\n",
      "    accuracy                           0.93       233\n",
      "   macro avg       0.91      0.93      0.92       233\n",
      "weighted avg       0.93      0.93      0.93       233\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.8841\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.94      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.88      0.98      0.93       152\n",
      "    positive       0.91      0.62      0.74        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.78      0.80       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 94.42159485816956 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.582, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5256, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4847, Accuracy: 0.7939, F1 Micro: 0.8844, F1 Macro: 0.8829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4336, Accuracy: 0.8266, F1 Micro: 0.9006, F1 Macro: 0.8993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3837, Accuracy: 0.8832, F1 Micro: 0.9306, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3163, Accuracy: 0.9144, F1 Micro: 0.9474, F1 Macro: 0.9458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2557, Accuracy: 0.9412, F1 Micro: 0.9635, F1 Macro: 0.9623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2007, Accuracy: 0.9457, F1 Micro: 0.9664, F1 Macro: 0.9649\n",
      "Epoch 9/10, Train Loss: 0.1603, Accuracy: 0.9457, F1 Micro: 0.966, F1 Macro: 0.9639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1269, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9713\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9713\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.88      0.98      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5263, Accuracy: 0.6552, F1 Micro: 0.6552, F1 Macro: 0.3958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.406, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.228, Accuracy: 0.9353, F1 Micro: 0.9353, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1395, Accuracy: 0.9397, F1 Micro: 0.9397, F1 Macro: 0.9343\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.9041\n",
      "Epoch 6/10, Train Loss: 0.0745, Accuracy: 0.9181, F1 Micro: 0.9181, F1 Macro: 0.9067\n",
      "Epoch 7/10, Train Loss: 0.1009, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.9045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1092, Accuracy: 0.9397, F1 Micro: 0.9397, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0636, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9389\n",
      "Epoch 10/10, Train Loss: 0.0404, Accuracy: 0.9397, F1 Micro: 0.9397, F1 Macro: 0.934\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        80\n",
      "    positive       0.97      0.94      0.96       152\n",
      "\n",
      "    accuracy                           0.94       232\n",
      "   macro avg       0.93      0.95      0.94       232\n",
      "weighted avg       0.95      0.94      0.94       232\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8944\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.82      0.84       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.88      0.98      0.93       152\n",
      "    positive       0.91      0.62      0.74        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.87      0.78      0.81       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.92      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 96.94033193588257 s\n",
      "Averaged - Iteration 274: Accuracy: 0.888, F1 Micro: 0.888, F1 Macro: 0.7009\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 12.802496433258057 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5754, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5022, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4414, Accuracy: 0.8311, F1 Micro: 0.9033, F1 Macro: 0.902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.362, Accuracy: 0.904, F1 Micro: 0.9414, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2981, Accuracy: 0.933, F1 Micro: 0.9588, F1 Macro: 0.9575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2241, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1815, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.969\n",
      "Epoch 8/10, Train Loss: 0.1401, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.9674\n",
      "Epoch 9/10, Train Loss: 0.1217, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9679\n",
      "Epoch 10/10, Train Loss: 0.1015, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9677\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.88      0.98      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.576, Accuracy: 0.6681, F1 Micro: 0.6681, F1 Macro: 0.4005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3928, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2507, Accuracy: 0.9009, F1 Micro: 0.9009, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.2026, Accuracy: 0.8621, F1 Micro: 0.8621, F1 Macro: 0.8559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.178, Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.916\n",
      "Epoch 6/10, Train Loss: 0.0855, Accuracy: 0.9181, F1 Micro: 0.9181, F1 Macro: 0.9111\n",
      "Epoch 7/10, Train Loss: 0.0945, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9141\n",
      "Epoch 8/10, Train Loss: 0.0855, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.9018\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9131\n",
      "Epoch 10/10, Train Loss: 0.0559, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9151\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.916\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        77\n",
      "    positive       0.93      0.96      0.95       155\n",
      "\n",
      "    accuracy                           0.93       232\n",
      "   macro avg       0.92      0.91      0.92       232\n",
      "weighted avg       0.93      0.93      0.93       232\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9329, F1 Micro: 0.9329, F1 Macro: 0.8553\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.88      0.72        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.83      0.61      0.70        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.80      0.82      0.79       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.29      0.83      0.43        12\n",
      "     neutral       0.93      0.90      0.92       152\n",
      "    positive       0.94      0.62      0.74        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.72      0.78      0.70       216\n",
      "weighted avg       0.90      0.83      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.87      0.83        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.79      0.85        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 98.35309910774231 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5678, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5037, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4516, Accuracy: 0.811, F1 Micro: 0.8931, F1 Macro: 0.8917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3797, Accuracy: 0.9107, F1 Micro: 0.946, F1 Macro: 0.945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3006, Accuracy: 0.9368, F1 Micro: 0.9609, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2245, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9669\n",
      "Epoch 7/10, Train Loss: 0.179, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.14, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9715\n",
      "Epoch 9/10, Train Loss: 0.1184, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.969\n",
      "Epoch 10/10, Train Loss: 0.1059, Accuracy: 0.9494, F1 Micro: 0.9681, F1 Macro: 0.9658\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5376, Accuracy: 0.6787, F1 Micro: 0.6787, F1 Macro: 0.4043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3707, Accuracy: 0.8956, F1 Micro: 0.8956, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2059, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9159\n",
      "Epoch 4/10, Train Loss: 0.1547, Accuracy: 0.9197, F1 Micro: 0.9197, F1 Macro: 0.9107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9277\n",
      "Epoch 6/10, Train Loss: 0.0884, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9163\n",
      "Epoch 7/10, Train Loss: 0.0649, Accuracy: 0.9116, F1 Micro: 0.9116, F1 Macro: 0.8966\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9247\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.9197, F1 Micro: 0.9197, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0631, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9336\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.99      0.91        80\n",
      "    positive       0.99      0.92      0.95       169\n",
      "\n",
      "    accuracy                           0.94       249\n",
      "   macro avg       0.92      0.95      0.93       249\n",
      "weighted avg       0.95      0.94      0.94       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.8828\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.94      0.86        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.35      0.92      0.51        12\n",
      "     neutral       0.96      0.87      0.91       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.71      0.85      0.74       216\n",
      "weighted avg       0.89      0.84      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.87      0.83        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.97      0.68      0.80        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 104.1474072933197 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5762, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5117, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4731, Accuracy: 0.7954, F1 Micro: 0.8852, F1 Macro: 0.8836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4171, Accuracy: 0.8631, F1 Micro: 0.9197, F1 Macro: 0.919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3326, Accuracy: 0.9211, F1 Micro: 0.9514, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2499, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.9628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1981, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1528, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9699\n",
      "Epoch 9/10, Train Loss: 0.1298, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.11, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9706\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5407, Accuracy: 0.6917, F1 Micro: 0.6917, F1 Macro: 0.4433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3537, Accuracy: 0.8696, F1 Micro: 0.8696, F1 Macro: 0.8543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1791, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1579, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9119\n",
      "Epoch 5/10, Train Loss: 0.1191, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8966\n",
      "Epoch 6/10, Train Loss: 0.0957, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9078\n",
      "Epoch 7/10, Train Loss: 0.1024, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8984\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9031\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8966\n",
      "Epoch 10/10, Train Loss: 0.039, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9008\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.94      0.88        81\n",
      "    positive       0.97      0.91      0.94       172\n",
      "\n",
      "    accuracy                           0.92       253\n",
      "   macro avg       0.90      0.93      0.91       253\n",
      "weighted avg       0.93      0.92      0.92       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.8804\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.79      0.71      0.75        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.78      0.80      0.79       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.86      0.78      0.82        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 104.01007533073425 s\n",
      "Averaged - Iteration 333: Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.7353\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 11.722859621047974 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5667, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4958, Accuracy: 0.8021, F1 Micro: 0.8886, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.418, Accuracy: 0.8862, F1 Micro: 0.932, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3198, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2483, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1896, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1559, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.113, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.104, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9714\n",
      "Epoch 10/10, Train Loss: 0.0828, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9693\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5635, Accuracy: 0.684, F1 Micro: 0.684, F1 Macro: 0.4062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.348, Accuracy: 0.912, F1 Micro: 0.912, F1 Macro: 0.903\n",
      "Epoch 3/10, Train Loss: 0.2011, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.8978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1061, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9304\n",
      "Epoch 5/10, Train Loss: 0.1484, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9193\n",
      "Epoch 6/10, Train Loss: 0.0825, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9211\n",
      "Epoch 7/10, Train Loss: 0.0844, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9155\n",
      "Epoch 8/10, Train Loss: 0.1035, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.8901\n",
      "Epoch 9/10, Train Loss: 0.095, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9077\n",
      "Epoch 10/10, Train Loss: 0.0758, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9146\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.90      0.90        79\n",
      "    positive       0.95      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.93      0.93      0.93       250\n",
      "weighted avg       0.94      0.94      0.94       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.8741\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.75      0.73        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.85      0.67      0.75        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.83      0.80      0.81       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.67      0.62        12\n",
      "     neutral       0.90      0.93      0.92       152\n",
      "    positive       0.84      0.73      0.78        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.77      0.78      0.77       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 111.0037043094635 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5642, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5042, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4441, Accuracy: 0.8571, F1 Micro: 0.916, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3468, Accuracy: 0.9256, F1 Micro: 0.9543, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.263, Accuracy: 0.9449, F1 Micro: 0.9661, F1 Macro: 0.965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1957, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9691\n",
      "Epoch 7/10, Train Loss: 0.1607, Accuracy: 0.9509, F1 Micro: 0.9693, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1194, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9726\n",
      "Epoch 9/10, Train Loss: 0.1052, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "Epoch 10/10, Train Loss: 0.0858, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9715\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5857, Accuracy: 0.7148, F1 Micro: 0.7148, F1 Macro: 0.4835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3461, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.8903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1945, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9259\n",
      "Epoch 5/10, Train Loss: 0.168, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1307, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "Epoch 7/10, Train Loss: 0.1183, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.882\n",
      "Epoch 8/10, Train Loss: 0.1013, Accuracy: 0.8935, F1 Micro: 0.8935, F1 Macro: 0.883\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9079\n",
      "Epoch 10/10, Train Loss: 0.0891, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.9015\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.90        81\n",
      "    positive       0.98      0.93      0.95       182\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.92      0.94      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.8799\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.92      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.82      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.96      0.83        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.89      0.76      0.82        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.89      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 114.7111349105835 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5647, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5174, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4611, Accuracy: 0.8028, F1 Micro: 0.8889, F1 Macro: 0.8874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3756, Accuracy: 0.9137, F1 Micro: 0.9473, F1 Macro: 0.946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2804, Accuracy: 0.9427, F1 Micro: 0.9645, F1 Macro: 0.9632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2085, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1675, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9711\n",
      "Epoch 8/10, Train Loss: 0.1193, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1073, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Epoch 10/10, Train Loss: 0.0862, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9733\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5808, Accuracy: 0.6902, F1 Micro: 0.6902, F1 Macro: 0.4424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3317, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9118\n",
      "Epoch 3/10, Train Loss: 0.1943, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1293, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1344, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0907, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9435\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.8984\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "Epoch 10/10, Train Loss: 0.0688, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9146\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.99      0.93        82\n",
      "    positive       0.99      0.93      0.96       173\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.96      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8883\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.96      0.73      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.88       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.75      0.55        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.74      0.81      0.76       216\n",
      "weighted avg       0.89      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.89      0.88       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 118.25050520896912 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9056, F1 Micro: 0.9056, F1 Macro: 0.7595\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 11.6691312789917 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5671, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4952, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4192, Accuracy: 0.8973, F1 Micro: 0.9382, F1 Macro: 0.9375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3063, Accuracy: 0.936, F1 Micro: 0.9607, F1 Macro: 0.9594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2298, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.968\n",
      "Epoch 6/10, Train Loss: 0.1774, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.14, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9714\n",
      "Epoch 8/10, Train Loss: 0.1095, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0991, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9716\n",
      "Epoch 10/10, Train Loss: 0.0773, Accuracy: 0.9524, F1 Micro: 0.97, F1 Macro: 0.968\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.90      0.95      0.92       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5258, Accuracy: 0.6887, F1 Micro: 0.6887, F1 Macro: 0.4078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2951, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9136\n",
      "Epoch 3/10, Train Loss: 0.194, Accuracy: 0.8949, F1 Micro: 0.8949, F1 Macro: 0.886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1328, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9231\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1074, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9259\n",
      "Epoch 7/10, Train Loss: 0.1193, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9214\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0693, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9293\n",
      "Epoch 10/10, Train Loss: 0.0595, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9014\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9293\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.90        80\n",
      "    positive       0.98      0.93      0.95       177\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.92      0.94      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.8612\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.83      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.32      0.67      0.43        12\n",
      "     neutral       0.92      0.89      0.91       152\n",
      "    positive       0.81      0.67      0.74        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.68      0.74      0.69       216\n",
      "weighted avg       0.86      0.83      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 118.57992386817932 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5616, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5061, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4356, Accuracy: 0.8735, F1 Micro: 0.9256, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.327, Accuracy: 0.9375, F1 Micro: 0.9615, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2381, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9676\n",
      "Epoch 6/10, Train Loss: 0.1786, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.139, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1145, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.101, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0771, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9707\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5296, Accuracy: 0.7308, F1 Micro: 0.7308, F1 Macro: 0.5667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2971, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9115\n",
      "Epoch 3/10, Train Loss: 0.186, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1349, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 5/10, Train Loss: 0.1121, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.8993\n",
      "Epoch 6/10, Train Loss: 0.1224, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9145\n",
      "Epoch 7/10, Train Loss: 0.1223, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.9035\n",
      "Epoch 8/10, Train Loss: 0.0776, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9182\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.8972\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9235\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8866\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.83      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.88      0.78        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.87      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.83      0.61        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.83      0.77       216\n",
      "weighted avg       0.88      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 115.244225025177 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5617, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5187, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.467, Accuracy: 0.8348, F1 Micro: 0.9041, F1 Macro: 0.9032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3588, Accuracy: 0.9219, F1 Micro: 0.952, F1 Macro: 0.9504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2617, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1898, Accuracy: 0.9539, F1 Micro: 0.9715, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1483, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Epoch 8/10, Train Loss: 0.1161, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0983, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Epoch 10/10, Train Loss: 0.0793, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9733\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5351, Accuracy: 0.7043, F1 Micro: 0.7043, F1 Macro: 0.4969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2924, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9038\n",
      "Epoch 3/10, Train Loss: 0.1845, Accuracy: 0.9027, F1 Micro: 0.9027, F1 Macro: 0.8959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1467, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9339\n",
      "Epoch 6/10, Train Loss: 0.1317, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9073\n",
      "Epoch 7/10, Train Loss: 0.1286, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9194\n",
      "Epoch 8/10, Train Loss: 0.0893, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0905, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.935\n",
      "Epoch 10/10, Train Loss: 0.0659, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.91        84\n",
      "    positive       0.98      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.94      0.94       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8873\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.96      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.75      0.60        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.75      0.81      0.77       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.90      0.88       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 119.67420601844788 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.7765\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 10.144341945648193 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5619, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4848, Accuracy: 0.8013, F1 Micro: 0.8882, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3934, Accuracy: 0.91, F1 Micro: 0.9449, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2971, Accuracy: 0.939, F1 Micro: 0.9624, F1 Macro: 0.961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2166, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1663, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1367, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1096, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0899, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.0752, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9713\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.89      0.97      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5561, Accuracy: 0.6855, F1 Micro: 0.6855, F1 Macro: 0.4302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3016, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1531, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1501, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.928\n",
      "Epoch 5/10, Train Loss: 0.1536, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1216, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9247\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0945, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.928\n",
      "Epoch 9/10, Train Loss: 0.1049, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.9051\n",
      "Epoch 10/10, Train Loss: 0.0752, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.9029\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.90        80\n",
      "    positive       0.97      0.93      0.95       168\n",
      "\n",
      "    accuracy                           0.94       248\n",
      "   macro avg       0.92      0.94      0.93       248\n",
      "weighted avg       0.94      0.94      0.94       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9406, F1 Micro: 0.9406, F1 Macro: 0.8615\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.84      0.86       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.67      0.53        12\n",
      "     neutral       0.90      0.92      0.91       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.74      0.77      0.75       216\n",
      "weighted avg       0.87      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.74      0.79        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.83      0.83      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.68      0.88      0.77        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.85      0.80      0.81       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 125.4457094669342 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5533, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4864, Accuracy: 0.7976, F1 Micro: 0.8864, F1 Macro: 0.8849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4048, Accuracy: 0.9055, F1 Micro: 0.9429, F1 Macro: 0.9421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2961, Accuracy: 0.9397, F1 Micro: 0.9628, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2169, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1643, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9719\n",
      "Epoch 7/10, Train Loss: 0.133, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1076, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0926, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9742\n",
      "Epoch 10/10, Train Loss: 0.0801, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9731\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5387, Accuracy: 0.8353, F1 Micro: 0.8353, F1 Macro: 0.7932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2874, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1752, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1003, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9414\n",
      "Epoch 5/10, Train Loss: 0.1091, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9324\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 7/10, Train Loss: 0.0936, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9328\n",
      "Epoch 8/10, Train Loss: 0.1117, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9332\n",
      "Epoch 9/10, Train Loss: 0.0765, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9178\n",
      "Epoch 10/10, Train Loss: 0.0563, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9248\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        81\n",
      "    positive       0.97      0.96      0.96       174\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.94      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.887\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.84      0.73      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.83      0.81       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.88      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 122.1126217842102 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5601, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5013, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4369, Accuracy: 0.869, F1 Micro: 0.9229, F1 Macro: 0.9222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3252, Accuracy: 0.9427, F1 Micro: 0.9647, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2331, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.171, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1374, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1121, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Epoch 9/10, Train Loss: 0.096, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Epoch 10/10, Train Loss: 0.079, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9705\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.88      0.97      0.92       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5569, Accuracy: 0.6763, F1 Micro: 0.6763, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3648, Accuracy: 0.9461, F1 Micro: 0.9461, F1 Macro: 0.9397\n",
      "Epoch 3/10, Train Loss: 0.1962, Accuracy: 0.9046, F1 Micro: 0.9046, F1 Macro: 0.8973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1505, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0799, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9585, F1 Micro: 0.9585, F1 Macro: 0.9532\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9269\n",
      "Epoch 8/10, Train Loss: 0.0684, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9353\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9295, F1 Micro: 0.9295, F1 Macro: 0.9221\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9269\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9585, F1 Micro: 0.9585, F1 Macro: 0.9532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        79\n",
      "    positive       0.97      0.96      0.97       162\n",
      "\n",
      "    accuracy                           0.96       241\n",
      "   macro avg       0.95      0.96      0.95       241\n",
      "weighted avg       0.96      0.96      0.96       241\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.8699\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.81      0.72        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.84      0.84      0.83       216\n",
      "weighted avg       0.93      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      0.75      0.44        12\n",
      "     neutral       0.92      0.90      0.91       152\n",
      "    positive       0.89      0.65      0.76        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.71      0.77      0.70       216\n",
      "weighted avg       0.88      0.83      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.85      0.76        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.88      0.87       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 122.34656763076782 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9148, F1 Micro: 0.9148, F1 Macro: 0.7885\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 9.035679578781128 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5519, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4591, Accuracy: 0.8415, F1 Micro: 0.9088, F1 Macro: 0.9081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3403, Accuracy: 0.9219, F1 Micro: 0.9519, F1 Macro: 0.9506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.261, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1855, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1412, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1122, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Epoch 8/10, Train Loss: 0.0951, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9734\n",
      "Epoch 9/10, Train Loss: 0.0722, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9706\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.88      0.96      0.92       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5554, Accuracy: 0.836, F1 Micro: 0.836, F1 Macro: 0.7813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2621, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.163, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1276, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9361\n",
      "Epoch 5/10, Train Loss: 0.0975, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0654, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9348\n",
      "Epoch 7/10, Train Loss: 0.0555, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.924\n",
      "Epoch 8/10, Train Loss: 0.0571, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9265\n",
      "Epoch 9/10, Train Loss: 0.0696, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9183\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9072\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.90      0.91        79\n",
      "    positive       0.95      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.94      0.93      0.93       250\n",
      "weighted avg       0.94      0.94      0.94       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.8741\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.83      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.42      0.67      0.52        12\n",
      "     neutral       0.90      0.93      0.91       152\n",
      "    positive       0.88      0.67      0.76        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.73      0.76      0.73       216\n",
      "weighted avg       0.87      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.99      0.98      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.89       216\n",
      "weighted avg       0.95      0.94      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 126.70925045013428 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5499, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.48, Accuracy: 0.808, F1 Micro: 0.8916, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3629, Accuracy: 0.9286, F1 Micro: 0.956, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2751, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1906, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1444, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Epoch 7/10, Train Loss: 0.1145, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Epoch 8/10, Train Loss: 0.0949, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.972\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9726\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5851, Accuracy: 0.7649, F1 Micro: 0.7649, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3231, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2122, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9402\n",
      "Epoch 4/10, Train Loss: 0.1496, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.931\n",
      "Epoch 5/10, Train Loss: 0.1342, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.937\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9335\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9125\n",
      "Epoch 8/10, Train Loss: 0.1097, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9323\n",
      "Epoch 9/10, Train Loss: 0.0717, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.897\n",
      "Epoch 10/10, Train Loss: 0.085, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9319\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.91      0.92        80\n",
      "    positive       0.96      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.94      0.94       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8844\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.81      0.82      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.69      0.75        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.85      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 123.55968809127808 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5534, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.4953, Accuracy: 0.7887, F1 Micro: 0.8818, F1 Macro: 0.8802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4009, Accuracy: 0.9033, F1 Micro: 0.9413, F1 Macro: 0.9396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3024, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1983, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1501, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1176, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Epoch 8/10, Train Loss: 0.0985, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9734\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.89      0.96      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5346, Accuracy: 0.7912, F1 Micro: 0.7912, F1 Macro: 0.7005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2767, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9498\n",
      "Epoch 3/10, Train Loss: 0.1843, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9421\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9458\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9418\n",
      "Epoch 6/10, Train Loss: 0.1085, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0996, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0471, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0438, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9504\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9378\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.93        81\n",
      "    positive       0.98      0.95      0.97       168\n",
      "\n",
      "    accuracy                           0.96       249\n",
      "   macro avg       0.94      0.96      0.95       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8929\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.84      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.90      0.96      0.93       152\n",
      "    positive       0.86      0.71      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.86      0.81      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.91      0.88       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 128.46417546272278 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.7991\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 8.329106569290161 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5407, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4651, Accuracy: 0.8408, F1 Micro: 0.908, F1 Macro: 0.9078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3469, Accuracy: 0.9293, F1 Micro: 0.9566, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.257, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1854, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1395, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9708\n",
      "Epoch 7/10, Train Loss: 0.1224, Accuracy: 0.9531, F1 Micro: 0.9703, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9703\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.89      0.96      0.92       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5101, Accuracy: 0.856, F1 Micro: 0.856, F1 Macro: 0.8129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2148, Accuracy: 0.904, F1 Micro: 0.904, F1 Macro: 0.8958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2172, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9244\n",
      "Epoch 5/10, Train Loss: 0.1329, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0969, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9401\n",
      "Epoch 7/10, Train Loss: 0.0805, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9265\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9287\n",
      "Epoch 9/10, Train Loss: 0.0697, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9167\n",
      "Epoch 10/10, Train Loss: 0.0844, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9123\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.91      0.92        80\n",
      "    positive       0.96      0.96      0.96       170\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.94      0.94      0.94       250\n",
      "weighted avg       0.95      0.95      0.95       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.8739\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.33      0.75      0.46        12\n",
      "     neutral       0.92      0.89      0.91       152\n",
      "    positive       0.88      0.69      0.77        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.71      0.78      0.71       216\n",
      "weighted avg       0.88      0.84      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.86      0.86        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 134.58171701431274 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5373, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4811, Accuracy: 0.8333, F1 Micro: 0.9044, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3638, Accuracy: 0.9286, F1 Micro: 0.9564, F1 Macro: 0.9552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2672, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9731\n",
      "Epoch 5/10, Train Loss: 0.1871, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9725\n",
      "Epoch 6/10, Train Loss: 0.1412, Accuracy: 0.9531, F1 Micro: 0.9705, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.122, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Epoch 8/10, Train Loss: 0.1007, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9736\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5297, Accuracy: 0.8566, F1 Micro: 0.8566, F1 Macro: 0.8232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2744, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.194, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1384, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9409\n",
      "Epoch 5/10, Train Loss: 0.1398, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9254\n",
      "Epoch 6/10, Train Loss: 0.1398, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9323\n",
      "Epoch 7/10, Train Loss: 0.1165, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9207\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0864, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9413\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9319\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        81\n",
      "    positive       0.97      0.95      0.96       170\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.95      0.94       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8847\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.75      0.50        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.73      0.80      0.75       216\n",
      "weighted avg       0.90      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.36065101623535 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4978, Accuracy: 0.8162, F1 Micro: 0.8955, F1 Macro: 0.8941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4045, Accuracy: 0.8988, F1 Micro: 0.9393, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2974, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2029, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9708\n",
      "Epoch 6/10, Train Loss: 0.1511, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.129, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1021, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0848, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Epoch 10/10, Train Loss: 0.0704, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9747\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.89      0.98      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4787, Accuracy: 0.9024, F1 Micro: 0.9024, F1 Macro: 0.8896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2471, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9455\n",
      "Epoch 3/10, Train Loss: 0.1755, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9411\n",
      "Epoch 4/10, Train Loss: 0.1367, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.153, Accuracy: 0.9634, F1 Micro: 0.9634, F1 Macro: 0.9587\n",
      "Epoch 6/10, Train Loss: 0.0774, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9498\n",
      "Epoch 7/10, Train Loss: 0.0657, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9455\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9451\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9404\n",
      "Epoch 10/10, Train Loss: 0.0646, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9486\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9634, F1 Micro: 0.9634, F1 Macro: 0.9587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.96      0.94        80\n",
      "    positive       0.98      0.96      0.97       166\n",
      "\n",
      "    accuracy                           0.96       246\n",
      "   macro avg       0.95      0.96      0.96       246\n",
      "weighted avg       0.96      0.96      0.96       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8886\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.35      0.75      0.47        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.93      0.73      0.82        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.74      0.80      0.74       216\n",
      "weighted avg       0.91      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.69      0.82        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.2889919281006 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.8074\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 7.850833177566528 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5433, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.451, Accuracy: 0.8452, F1 Micro: 0.9107, F1 Macro: 0.9099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3238, Accuracy: 0.9308, F1 Micro: 0.9572, F1 Macro: 0.9556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2383, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1772, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Epoch 6/10, Train Loss: 0.1371, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.106, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Epoch 8/10, Train Loss: 0.0954, Accuracy: 0.9561, F1 Micro: 0.9722, F1 Macro: 0.97\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9704\n",
      "Epoch 10/10, Train Loss: 0.0629, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.97\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5161, Accuracy: 0.8462, F1 Micro: 0.8462, F1 Macro: 0.7954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2523, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9278\n",
      "Epoch 3/10, Train Loss: 0.1837, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.9076\n",
      "Epoch 4/10, Train Loss: 0.1344, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9231\n",
      "Epoch 5/10, Train Loss: 0.1052, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9231\n",
      "Epoch 6/10, Train Loss: 0.0902, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1473, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9317\n",
      "Epoch 8/10, Train Loss: 0.0879, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.9034\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9401\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        80\n",
      "    positive       0.96      0.96      0.96       167\n",
      "\n",
      "    accuracy                           0.95       247\n",
      "   macro avg       0.94      0.94      0.94       247\n",
      "weighted avg       0.95      0.95      0.95       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8853\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.75      0.73        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.83      0.54        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.74      0.82      0.76       216\n",
      "weighted avg       0.89      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 131.21322560310364 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5456, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4733, Accuracy: 0.808, F1 Micro: 0.8916, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3478, Accuracy: 0.9301, F1 Micro: 0.9568, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2477, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1826, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.973\n",
      "Epoch 6/10, Train Loss: 0.1408, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Epoch 7/10, Train Loss: 0.1036, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0633, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5369, Accuracy: 0.864, F1 Micro: 0.864, F1 Macro: 0.8381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.24, Accuracy: 0.912, F1 Micro: 0.912, F1 Macro: 0.9035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2045, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1553, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9505\n",
      "Epoch 5/10, Train Loss: 0.146, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1244, Accuracy: 0.96, F1 Micro: 0.96, F1 Macro: 0.9549\n",
      "Epoch 7/10, Train Loss: 0.1134, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9462\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9304\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9462\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9288\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.96, F1 Micro: 0.96, F1 Macro: 0.9549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.97      0.94        80\n",
      "    positive       0.99      0.95      0.97       170\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.95      0.96      0.95       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.8889\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.79      0.79      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.67      0.73        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.80      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.91      0.93        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.62      0.70        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.85      0.81      0.83       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 135.23824262619019 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.546, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4854, Accuracy: 0.8043, F1 Micro: 0.8894, F1 Macro: 0.8879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3833, Accuracy: 0.9249, F1 Micro: 0.954, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2663, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1919, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1455, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1067, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9748\n",
      "Epoch 8/10, Train Loss: 0.0964, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 10/10, Train Loss: 0.066, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.99      0.97      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.504, Accuracy: 0.8919, F1 Micro: 0.8919, F1 Macro: 0.8743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2255, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9468\n",
      "Epoch 3/10, Train Loss: 0.1727, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9217\n",
      "Epoch 4/10, Train Loss: 0.1632, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.93\n",
      "Epoch 5/10, Train Loss: 0.1386, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "Epoch 6/10, Train Loss: 0.0815, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9185\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "Epoch 9/10, Train Loss: 0.0822, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9387\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        81\n",
      "    positive       0.98      0.96      0.97       178\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.95      0.95       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9015\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.75      0.62        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.77      0.82      0.79       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.99      0.97      0.98       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.92      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.13762998580933 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.8151\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 7.2213404178619385 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5372, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4456, Accuracy: 0.8884, F1 Micro: 0.9329, F1 Macro: 0.9315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3104, Accuracy: 0.9412, F1 Micro: 0.9636, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2327, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1733, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1355, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.972\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5106, Accuracy: 0.8919, F1 Micro: 0.8919, F1 Macro: 0.8808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2435, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1954, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 4/10, Train Loss: 0.1222, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9295\n",
      "Epoch 5/10, Train Loss: 0.093, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9099\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9185\n",
      "Epoch 8/10, Train Loss: 0.0588, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9189\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.93\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8929\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        82\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.93      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.8669\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.82      0.82        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.88      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.69      0.73        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.76      0.79      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.83      0.65        12\n",
      "     neutral       0.92      0.91      0.91       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.76      0.83      0.78       216\n",
      "weighted avg       0.88      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.91      0.84        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.90      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 140.45770573616028 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5416, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4712, Accuracy: 0.8557, F1 Micro: 0.9159, F1 Macro: 0.9147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3316, Accuracy: 0.936, F1 Micro: 0.9602, F1 Macro: 0.9589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2382, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1722, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9716\n",
      "Epoch 6/10, Train Loss: 0.1401, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1079, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9747\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.9561, F1 Micro: 0.9722, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "Epoch 10/10, Train Loss: 0.0628, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9764\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.99      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4894, Accuracy: 0.8911, F1 Micro: 0.8911, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2471, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1845, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "Epoch 4/10, Train Loss: 0.128, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9024\n",
      "Epoch 5/10, Train Loss: 0.1034, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9385\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9293\n",
      "Epoch 8/10, Train Loss: 0.0739, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9194\n",
      "Epoch 9/10, Train Loss: 0.0416, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9388\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        82\n",
      "    positive       0.98      0.94      0.96       175\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.93      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.905\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.78      0.84      0.79       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.99      0.97      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 141.96058773994446 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5431, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4826, Accuracy: 0.8229, F1 Micro: 0.8977, F1 Macro: 0.8959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3589, Accuracy: 0.9286, F1 Micro: 0.9558, F1 Macro: 0.9545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2572, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1849, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9708\n",
      "Epoch 6/10, Train Loss: 0.1412, Accuracy: 0.9524, F1 Micro: 0.97, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0929, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 10/10, Train Loss: 0.0617, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.483, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2497, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9386\n",
      "Epoch 3/10, Train Loss: 0.1226, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9141\n",
      "Epoch 4/10, Train Loss: 0.1528, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1077, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "Epoch 6/10, Train Loss: 0.0937, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9319\n",
      "Epoch 7/10, Train Loss: 0.1192, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9132\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9113\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.907\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        82\n",
      "    positive       0.97      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.94      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8796\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.91      0.87        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.62      0.71        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.79      0.79      0.79        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.80      0.82       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.81      0.85      0.82       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.84      0.88      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.69      0.82        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 139.58898830413818 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.8208\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 6.3798298835754395 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.54, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4325, Accuracy: 0.8988, F1 Micro: 0.9387, F1 Macro: 0.9375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3125, Accuracy: 0.9405, F1 Micro: 0.9632, F1 Macro: 0.9618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2217, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9726\n",
      "Epoch 5/10, Train Loss: 0.1617, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1224, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Epoch 7/10, Train Loss: 0.0988, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0821, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 10/10, Train Loss: 0.061, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.479, Accuracy: 0.8923, F1 Micro: 0.8923, F1 Macro: 0.8745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2567, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1684, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "Epoch 4/10, Train Loss: 0.1371, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.944\n",
      "Epoch 5/10, Train Loss: 0.1325, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9381\n",
      "Epoch 7/10, Train Loss: 0.0999, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0774, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "Epoch 9/10, Train Loss: 0.0673, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "Epoch 10/10, Train Loss: 0.0575, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.95      0.95       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9027\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.83      0.73      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.83      0.79       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.7939167022705 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5355, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4478, Accuracy: 0.8728, F1 Micro: 0.925, F1 Macro: 0.9238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3318, Accuracy: 0.9405, F1 Micro: 0.9634, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.228, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9715\n",
      "Epoch 5/10, Train Loss: 0.1634, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1272, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9734\n",
      "Epoch 8/10, Train Loss: 0.0818, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9754\n",
      "Epoch 9/10, Train Loss: 0.0728, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5535, Accuracy: 0.8699, F1 Micro: 0.8699, F1 Macro: 0.8374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2536, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9404\n",
      "Epoch 3/10, Train Loss: 0.1787, Accuracy: 0.9309, F1 Micro: 0.9309, F1 Macro: 0.9242\n",
      "Epoch 4/10, Train Loss: 0.1347, Accuracy: 0.9146, F1 Micro: 0.9146, F1 Macro: 0.9078\n",
      "Epoch 5/10, Train Loss: 0.1561, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9289\n",
      "Epoch 6/10, Train Loss: 0.0851, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9367\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9328\n",
      "Epoch 8/10, Train Loss: 0.1077, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0658, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9498\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9451\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        81\n",
      "    positive       0.98      0.96      0.97       165\n",
      "\n",
      "    accuracy                           0.96       246\n",
      "   macro avg       0.95      0.95      0.95       246\n",
      "weighted avg       0.96      0.96      0.96       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9036\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.81      0.80       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 137.26781249046326 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5401, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4711, Accuracy: 0.8207, F1 Micro: 0.8976, F1 Macro: 0.8964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3708, Accuracy: 0.9189, F1 Micro: 0.9498, F1 Macro: 0.9472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2504, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1734, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1045, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0848, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9752\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0638, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.94      0.92       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4832, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.8854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2492, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 3/10, Train Loss: 0.1722, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "Epoch 4/10, Train Loss: 0.1797, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1262, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9525\n",
      "Epoch 6/10, Train Loss: 0.1274, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8959\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9244\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9005\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        84\n",
      "    positive       0.99      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.96       259\n",
      "   macro avg       0.94      0.96      0.95       259\n",
      "weighted avg       0.96      0.96      0.96       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8785\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.79      0.79      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.80      0.77      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.76        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.75      0.88      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.83      0.85       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 146.13686180114746 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.8265\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 5.9200098514556885 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5432, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4284, Accuracy: 0.9077, F1 Micro: 0.9437, F1 Macro: 0.9425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2962, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2024, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1548, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0926, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0772, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 10/10, Train Loss: 0.0543, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5484, Accuracy: 0.8863, F1 Micro: 0.8863, F1 Macro: 0.8701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2719, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Epoch 3/10, Train Loss: 0.1687, Accuracy: 0.9059, F1 Micro: 0.9059, F1 Macro: 0.8975\n",
      "Epoch 4/10, Train Loss: 0.1628, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9206\n",
      "Epoch 5/10, Train Loss: 0.1367, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9201\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9302\n",
      "Epoch 7/10, Train Loss: 0.1243, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9215\n",
      "Epoch 8/10, Train Loss: 0.0947, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9302\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9228\n",
      "Epoch 10/10, Train Loss: 0.0509, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9118\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        82\n",
      "    positive       0.99      0.93      0.96       173\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8892\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.82      0.78        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.90      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.83      0.65        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.85      0.80       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 144.5988585948944 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5397, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4449, Accuracy: 0.8936, F1 Micro: 0.9366, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.313, Accuracy: 0.9435, F1 Micro: 0.965, F1 Macro: 0.9633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.205, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1212, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0944, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "Epoch 8/10, Train Loss: 0.0781, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9792\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9788\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.94      0.94       158\n",
      "        part       0.99      0.99      0.99       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5469, Accuracy: 0.8815, F1 Micro: 0.8815, F1 Macro: 0.8643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2624, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1885, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "Epoch 4/10, Train Loss: 0.1493, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9056\n",
      "Epoch 5/10, Train Loss: 0.1422, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9273\n",
      "Epoch 6/10, Train Loss: 0.1107, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9062\n",
      "Epoch 7/10, Train Loss: 0.0938, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0692, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9366\n",
      "Epoch 9/10, Train Loss: 0.0751, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9326\n",
      "Epoch 10/10, Train Loss: 0.0581, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.929\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        86\n",
      "    positive       0.97      0.95      0.96       184\n",
      "\n",
      "    accuracy                           0.94       270\n",
      "   macro avg       0.93      0.94      0.94       270\n",
      "weighted avg       0.95      0.94      0.94       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9173\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.89      0.86       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.99      0.98      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.80      0.94      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.16524839401245 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5458, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4789, Accuracy: 0.8251, F1 Micro: 0.9, F1 Macro: 0.8989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3512, Accuracy: 0.9286, F1 Micro: 0.9556, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2265, Accuracy: 0.9405, F1 Micro: 0.9626, F1 Macro: 0.9591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1664, Accuracy: 0.9635, F1 Micro: 0.9773, F1 Macro: 0.9763\n",
      "Epoch 6/10, Train Loss: 0.1222, Accuracy: 0.9568, F1 Micro: 0.9726, F1 Macro: 0.9698\n",
      "Epoch 7/10, Train Loss: 0.0954, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0803, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 9/10, Train Loss: 0.0655, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4903, Accuracy: 0.891, F1 Micro: 0.891, F1 Macro: 0.8726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2188, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9325\n",
      "Epoch 3/10, Train Loss: 0.1728, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9037\n",
      "Epoch 4/10, Train Loss: 0.1398, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1663, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9304\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9236\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0564, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9361\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        86\n",
      "    positive       0.97      0.95      0.96       180\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.93      0.94      0.94       266\n",
      "weighted avg       0.94      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9183\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 149.1954164505005 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.8324\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.708690881729126 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5347, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4194, Accuracy: 0.9003, F1 Micro: 0.9385, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2776, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1926, Accuracy: 0.9606, F1 Micro: 0.9755, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.143, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.0554, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5242, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2172, Accuracy: 0.9015, F1 Micro: 0.9015, F1 Macro: 0.8941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1835, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9333\n",
      "Epoch 4/10, Train Loss: 0.1505, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9363\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "Epoch 7/10, Train Loss: 0.1018, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.08, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9373\n",
      "Epoch 9/10, Train Loss: 0.0586, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9407\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.93      0.95      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9122\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.86      0.84       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.35627555847168 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5386, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4393, Accuracy: 0.9077, F1 Micro: 0.9442, F1 Macro: 0.9426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.295, Accuracy: 0.9501, F1 Micro: 0.9692, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1981, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1434, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1135, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9764\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "Epoch 9/10, Train Loss: 0.062, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0553, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.92      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5303, Accuracy: 0.8926, F1 Micro: 0.8926, F1 Macro: 0.8795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2579, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1812, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1317, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1165, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0708, Accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.945\n",
      "Epoch 9/10, Train Loss: 0.066, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.9454\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.9454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.96      0.96       183\n",
      "\n",
      "    accuracy                           0.95       270\n",
      "   macro avg       0.94      0.95      0.95       270\n",
      "weighted avg       0.95      0.95      0.95       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9273\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.92      0.94       152\n",
      "    positive       0.77      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.87      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.54370713233948 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.545, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4716, Accuracy: 0.8512, F1 Micro: 0.9134, F1 Macro: 0.9127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3294, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2146, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1173, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 9/10, Train Loss: 0.0651, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9759\n",
      "Epoch 10/10, Train Loss: 0.0582, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4973, Accuracy: 0.8992, F1 Micro: 0.8992, F1 Macro: 0.8838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2666, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1804, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9403\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1041, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0594, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        85\n",
      "    positive       0.98      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.96      0.95       258\n",
      "weighted avg       0.96      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9144\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.92      0.69        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.87      0.81       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 154.8119957447052 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9314, F1 Micro: 0.9314, F1 Macro: 0.8381\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.08794093132019 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5351, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.408, Accuracy: 0.9025, F1 Micro: 0.9399, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2889, Accuracy: 0.9449, F1 Micro: 0.9659, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2017, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9755\n",
      "Epoch 5/10, Train Loss: 0.1482, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9719\n",
      "Epoch 6/10, Train Loss: 0.1141, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Epoch 8/10, Train Loss: 0.0787, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.974\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.9583, F1 Micro: 0.9735, F1 Macro: 0.9706\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9728\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.502, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2613, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1741, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1404, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9382\n",
      "Epoch 5/10, Train Loss: 0.1538, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9136\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9256\n",
      "Epoch 10/10, Train Loss: 0.0405, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.934\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        84\n",
      "    positive       0.97      0.95      0.96       168\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.94      0.95      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8875\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.91      0.87        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.81      0.72        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.85      0.70      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.82      0.83      0.82       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.83      0.57        12\n",
      "     neutral       0.94      0.91      0.92       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.84      0.77       216\n",
      "weighted avg       0.89      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 153.93996596336365 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5381, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4278, Accuracy: 0.9107, F1 Micro: 0.9458, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3002, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2068, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9753\n",
      "Epoch 5/10, Train Loss: 0.1526, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9713\n",
      "Epoch 6/10, Train Loss: 0.118, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0959, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 9/10, Train Loss: 0.0648, Accuracy: 0.9635, F1 Micro: 0.9768, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5175, Accuracy: 0.8872, F1 Micro: 0.8872, F1 Macro: 0.8772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2539, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.182, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1397, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9204\n",
      "Epoch 5/10, Train Loss: 0.1466, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.8996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1172, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1196, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9248\n",
      "Epoch 8/10, Train Loss: 0.0779, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9165\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9095\n",
      "Epoch 10/10, Train Loss: 0.0709, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9173\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        86\n",
      "    positive       0.98      0.92      0.95       180\n",
      "\n",
      "    accuracy                           0.93       266\n",
      "   macro avg       0.92      0.94      0.92       266\n",
      "weighted avg       0.94      0.93      0.93       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9069\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.81      0.82      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.72989964485168 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5386, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4603, Accuracy: 0.8467, F1 Micro: 0.9108, F1 Macro: 0.9101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3341, Accuracy: 0.9412, F1 Micro: 0.9635, F1 Macro: 0.9615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2269, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1597, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "Epoch 6/10, Train Loss: 0.1237, Accuracy: 0.9568, F1 Micro: 0.9726, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0984, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0835, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0627, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.97      0.91      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5085, Accuracy: 0.9036, F1 Micro: 0.9036, F1 Macro: 0.8922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.9143, F1 Micro: 0.9143, F1 Macro: 0.9058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1909, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9352\n",
      "Epoch 4/10, Train Loss: 0.1277, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.917\n",
      "Epoch 5/10, Train Loss: 0.1339, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.931\n",
      "Epoch 6/10, Train Loss: 0.1393, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9294\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9233\n",
      "Epoch 8/10, Train Loss: 0.0631, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9199\n",
      "Epoch 9/10, Train Loss: 0.0485, Accuracy: 0.9214, F1 Micro: 0.9214, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9333\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.90      0.91        88\n",
      "    positive       0.95      0.96      0.96       192\n",
      "\n",
      "    accuracy                           0.94       280\n",
      "   macro avg       0.94      0.93      0.93       280\n",
      "weighted avg       0.94      0.94      0.94       280\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9206\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.62      0.74        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.75      0.82      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.81      0.83       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.97      0.91      0.94       152\n",
      "    positive       0.77      0.90      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 156.59307622909546 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9327, F1 Micro: 0.9327, F1 Macro: 0.8423\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.606164455413818 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5322, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4002, Accuracy: 0.907, F1 Micro: 0.942, F1 Macro: 0.9399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2841, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1947, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1491, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1131, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5172, Accuracy: 0.8851, F1 Micro: 0.8851, F1 Macro: 0.8753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2255, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1455, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1265, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1087, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9224\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9086\n",
      "Epoch 7/10, Train Loss: 0.0845, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0893, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Epoch 10/10, Train Loss: 0.0957, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9273\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        85\n",
      "    positive       0.97      0.94      0.95       176\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.94      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9046\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.84      0.79       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 168.9535632133484 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4222, Accuracy: 0.9137, F1 Micro: 0.9466, F1 Macro: 0.9447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2912, Accuracy: 0.9464, F1 Micro: 0.9669, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.197, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1473, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 6/10, Train Loss: 0.1068, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0872, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0606, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.0525, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5431, Accuracy: 0.8664, F1 Micro: 0.8664, F1 Macro: 0.8359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2841, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "Epoch 3/10, Train Loss: 0.1741, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.164, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9308\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9154\n",
      "Epoch 7/10, Train Loss: 0.0913, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9194\n",
      "Epoch 8/10, Train Loss: 0.0946, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9262\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.077, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        85\n",
      "    positive       0.97      0.94      0.95       177\n",
      "\n",
      "    accuracy                           0.94       262\n",
      "   macro avg       0.93      0.94      0.93       262\n",
      "weighted avg       0.94      0.94      0.94       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9005\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.75      0.64        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.78      0.83      0.80       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 162.5243079662323 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5426, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4523, Accuracy: 0.8698, F1 Micro: 0.9229, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3294, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2142, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1536, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 7/10, Train Loss: 0.0956, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.0525, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4579, Accuracy: 0.896, F1 Micro: 0.896, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.243, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1485, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Epoch 4/10, Train Loss: 0.1427, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9326\n",
      "Epoch 5/10, Train Loss: 0.0971, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1114, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9462\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.938\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.938\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9265\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.97       168\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.94      0.96      0.95       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8866\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.91      0.87        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.78      0.85      0.80       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.91      0.81        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.90      0.88       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 157.30946230888367 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.8455\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.143787384033203 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.526, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3975, Accuracy: 0.9196, F1 Micro: 0.95, F1 Macro: 0.9472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.264, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1892, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1358, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.98\n",
      "Epoch 8/10, Train Loss: 0.0678, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9764\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9765\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.97      1.00      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.505, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2356, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1467, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "Epoch 4/10, Train Loss: 0.1502, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "Epoch 6/10, Train Loss: 0.1191, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9398\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 10/10, Train Loss: 0.0591, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        85\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.93      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9072\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.92      0.71        12\n",
      "     neutral       0.94      0.90      0.92       152\n",
      "    positive       0.83      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.88      0.82       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 163.23276925086975 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5254, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4079, Accuracy: 0.9204, F1 Micro: 0.951, F1 Macro: 0.9488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2705, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1891, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1327, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1113, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9796\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0502, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.94      0.97      0.96       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4746, Accuracy: 0.9019, F1 Micro: 0.9019, F1 Macro: 0.8948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.235, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2191, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.938\n",
      "Epoch 4/10, Train Loss: 0.1542, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1089, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9374\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9294\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9291\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9219\n",
      "Epoch 9/10, Train Loss: 0.0694, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9215\n",
      "Epoch 10/10, Train Loss: 0.0548, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9294\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        88\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.93      0.95      0.94       265\n",
      "weighted avg       0.95      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9237\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.95      0.97      0.96       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.90      0.89       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 167.36299180984497 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5267, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4417, Accuracy: 0.8876, F1 Micro: 0.9326, F1 Macro: 0.9306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2981, Accuracy: 0.9494, F1 Micro: 0.9687, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2049, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1362, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1184, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0809, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "Epoch 8/10, Train Loss: 0.067, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 9/10, Train Loss: 0.0593, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0484, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.979\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4736, Accuracy: 0.8538, F1 Micro: 0.8538, F1 Macro: 0.8478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2448, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1686, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "Epoch 4/10, Train Loss: 0.1395, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9243\n",
      "Epoch 5/10, Train Loss: 0.1427, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9199\n",
      "Epoch 6/10, Train Loss: 0.1079, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9243\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9358\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9065\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.86      0.84       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.98      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.4959237575531 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.8492\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.51901912689209 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5329, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4044, Accuracy: 0.9211, F1 Micro: 0.9511, F1 Macro: 0.9488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2759, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1867, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1359, Accuracy: 0.9702, F1 Micro: 0.9814, F1 Macro: 0.9804\n",
      "Epoch 6/10, Train Loss: 0.1121, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0862, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.9643, F1 Micro: 0.9773, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.049, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5028, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2427, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "Epoch 3/10, Train Loss: 0.1591, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9356\n",
      "Epoch 4/10, Train Loss: 0.1529, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9119\n",
      "Epoch 5/10, Train Loss: 0.139, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1098, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.086, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.94\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9114\n",
      "Epoch 10/10, Train Loss: 0.068, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9236\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       170\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9139\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.90       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.88      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 163.83828449249268 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5335, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4145, Accuracy: 0.9144, F1 Micro: 0.9478, F1 Macro: 0.9463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.279, Accuracy: 0.9561, F1 Micro: 0.9728, F1 Macro: 0.9717\n",
      "Epoch 4/10, Train Loss: 0.1847, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.14, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9777\n",
      "Epoch 6/10, Train Loss: 0.1146, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9761\n",
      "Epoch 7/10, Train Loss: 0.0834, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.98\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4837, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.9035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2239, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Epoch 3/10, Train Loss: 0.1655, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1469, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.138, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "Epoch 6/10, Train Loss: 0.1382, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9391\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9067\n",
      "Epoch 8/10, Train Loss: 0.0695, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.938\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Epoch 10/10, Train Loss: 0.0563, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9124\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.8982\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.80      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 163.88379430770874 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5391, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4514, Accuracy: 0.8743, F1 Micro: 0.9247, F1 Macro: 0.9236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3165, Accuracy: 0.9494, F1 Micro: 0.9687, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1978, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "Epoch 10/10, Train Loss: 0.0496, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5261, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.8953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2556, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1836, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1298, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.138, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.944\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Epoch 7/10, Train Loss: 0.0669, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9172\n",
      "Epoch 8/10, Train Loss: 0.0799, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9251\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 10/10, Train Loss: 0.0587, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        86\n",
      "    positive       0.97      0.96      0.96       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9024\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.92      0.76        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.88      0.84       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.85      0.87      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 167.3252730369568 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.8522\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.371279716491699 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5319, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.394, Accuracy: 0.9211, F1 Micro: 0.9518, F1 Macro: 0.9502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2689, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1841, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1354, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9759\n",
      "Epoch 7/10, Train Loss: 0.0791, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4889, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2597, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.168, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9368\n",
      "Epoch 4/10, Train Loss: 0.1219, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9174\n",
      "Epoch 5/10, Train Loss: 0.1229, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.908\n",
      "Epoch 6/10, Train Loss: 0.1134, Accuracy: 0.9081, F1 Micro: 0.9081, F1 Macro: 0.8996\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.9338, F1 Micro: 0.9338, F1 Macro: 0.9257\n",
      "Epoch 8/10, Train Loss: 0.0837, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.9059\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "Epoch 10/10, Train Loss: 0.0816, Accuracy: 0.8897, F1 Micro: 0.8897, F1 Macro: 0.8798\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9368\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        87\n",
      "    positive       0.96      0.96      0.96       185\n",
      "\n",
      "    accuracy                           0.94       272\n",
      "   macro avg       0.94      0.94      0.94       272\n",
      "weighted avg       0.95      0.94      0.94       272\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8807\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.78      0.73      0.75        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.74      0.83      0.77       216\n",
      "weighted avg       0.88      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 170.14213180541992 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5326, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4075, Accuracy: 0.9278, F1 Micro: 0.9559, F1 Macro: 0.9545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2701, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1363, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1011, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9814\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5207, Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.8974\n",
      "Epoch 2/10, Train Loss: 0.237, Accuracy: 0.8988, F1 Micro: 0.8988, F1 Macro: 0.8924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2057, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1143, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0954, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9385\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Epoch 7/10, Train Loss: 0.1095, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9194\n",
      "Epoch 9/10, Train Loss: 0.0452, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Epoch 10/10, Train Loss: 0.0737, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9113\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.91      0.92        86\n",
      "    positive       0.95      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.94      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9038\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.87      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 171.14755630493164 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.7999, F1 Micro: 0.8872, F1 Macro: 0.8857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4405, Accuracy: 0.881, F1 Micro: 0.929, F1 Macro: 0.9273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2976, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1968, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1424, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.971, F1 Micro: 0.9816, F1 Macro: 0.9803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "Epoch 9/10, Train Loss: 0.054, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4753, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2423, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1603, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.166, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1145, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9196\n",
      "Epoch 8/10, Train Loss: 0.0794, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.94\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9321\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9281\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9114\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.80      0.84      0.81       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 174.02253484725952 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9372, F1 Micro: 0.9372, F1 Macro: 0.8545\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.814171075820923 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5211, Accuracy: 0.8058, F1 Micro: 0.8905, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3844, Accuracy: 0.9323, F1 Micro: 0.9582, F1 Macro: 0.9563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2542, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1758, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.128, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 6/10, Train Loss: 0.096, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0428, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4867, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2448, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9322\n",
      "Epoch 3/10, Train Loss: 0.1691, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9127\n",
      "Epoch 4/10, Train Loss: 0.1216, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9217\n",
      "Epoch 5/10, Train Loss: 0.1174, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9213\n",
      "Epoch 6/10, Train Loss: 0.1064, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9265\n",
      "Epoch 7/10, Train Loss: 0.1124, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Epoch 8/10, Train Loss: 0.0886, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9057\n",
      "Epoch 10/10, Train Loss: 0.0631, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9178\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9322\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        87\n",
      "    positive       0.97      0.94      0.95       177\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.93      0.94      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9093\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.88      0.88      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 171.05641388893127 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5185, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3943, Accuracy: 0.9278, F1 Micro: 0.9554, F1 Macro: 0.9531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2519, Accuracy: 0.9591, F1 Micro: 0.9746, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1742, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "Epoch 5/10, Train Loss: 0.1295, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9754, F1 Micro: 0.9845, F1 Macro: 0.9836\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9754, F1 Micro: 0.9845, F1 Macro: 0.9836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.98      0.98      0.98       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4871, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.213, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9339\n",
      "Epoch 3/10, Train Loss: 0.1647, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Epoch 4/10, Train Loss: 0.1445, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9015\n",
      "Epoch 5/10, Train Loss: 0.1245, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9078\n",
      "Epoch 6/10, Train Loss: 0.1311, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0951, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0845, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9369\n",
      "Epoch 9/10, Train Loss: 0.0922, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Epoch 10/10, Train Loss: 0.0572, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.92        87\n",
      "    positive       0.97      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.93      0.94      0.94       266\n",
      "weighted avg       0.95      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9271\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 175.25503301620483 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5259, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4353, Accuracy: 0.8914, F1 Micro: 0.9346, F1 Macro: 0.9333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2891, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.194, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1357, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9792\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Epoch 7/10, Train Loss: 0.0789, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4777, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2428, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1816, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9442\n",
      "Epoch 4/10, Train Loss: 0.1374, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "Epoch 5/10, Train Loss: 0.091, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Epoch 9/10, Train Loss: 0.0809, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9349\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9442\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9171\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 170.13678407669067 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.8575\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples:25 \n",
      "Sampling duration: 2.32047438621521 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5287, Accuracy: 0.8013, F1 Micro: 0.8882, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3834, Accuracy: 0.9092, F1 Micro: 0.9425, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2516, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1691, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 5/10, Train Loss: 0.1255, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.977\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.974, F1 Micro: 0.9836, F1 Macro: 0.9828\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 10/10, Train Loss: 0.0444, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.974, F1 Micro: 0.9836, F1 Macro: 0.9828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.98      0.95       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.504, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2168, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1771, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 4/10, Train Loss: 0.1549, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Epoch 5/10, Train Loss: 0.1133, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Epoch 6/10, Train Loss: 0.0821, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9115\n",
      "Epoch 7/10, Train Loss: 0.1051, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9068\n",
      "Epoch 8/10, Train Loss: 0.0706, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9121\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9119\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        86\n",
      "    positive       0.97      0.94      0.95       174\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.94      0.93       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.904\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.83      0.82       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.88      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 171.20805287361145 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5267, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4023, Accuracy: 0.9219, F1 Micro: 0.9511, F1 Macro: 0.9486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2571, Accuracy: 0.9561, F1 Micro: 0.9728, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.17, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.099, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9769\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4994, Accuracy: 0.8973, F1 Micro: 0.8973, F1 Macro: 0.89\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2447, Accuracy: 0.8973, F1 Micro: 0.8973, F1 Macro: 0.89\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2135, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1381, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0897, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0749, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Epoch 10/10, Train Loss: 0.0599, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9097\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.96      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.78      0.76      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.81      0.84       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 183.4822907447815 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5335, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4354, Accuracy: 0.901, F1 Micro: 0.9392, F1 Macro: 0.9374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2846, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1841, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1339, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9787\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.98      0.98      0.98       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4631, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2052, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1453, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1176, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1124, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9494\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.941\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9491\n",
      "Epoch 10/10, Train Loss: 0.0548, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.902\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9491\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.97       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.94      0.96      0.95       264\n",
      "weighted avg       0.96      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.8894\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.81      0.88      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.83      0.82       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.62      0.70        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.75      0.88      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.85      0.83      0.83       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 181.96477007865906 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.8595\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 1.8480525016784668 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5223, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3669, Accuracy: 0.9308, F1 Micro: 0.9573, F1 Macro: 0.9552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2383, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.158, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1152, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.077, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "Epoch 9/10, Train Loss: 0.0499, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0442, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.976\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4356, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2355, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1664, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "Epoch 4/10, Train Loss: 0.1443, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9322\n",
      "Epoch 5/10, Train Loss: 0.1038, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.923\n",
      "Epoch 6/10, Train Loss: 0.0787, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9073\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9234\n",
      "Epoch 8/10, Train Loss: 0.1039, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9185\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9274\n",
      "Epoch 10/10, Train Loss: 0.06, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9234\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        85\n",
      "    positive       0.96      0.95      0.96       174\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9018\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.42      0.83      0.56        12\n",
      "     neutral       0.94      0.89      0.92       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.85      0.77       216\n",
      "weighted avg       0.90      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 184.21752190589905 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5248, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.378, Accuracy: 0.9301, F1 Micro: 0.9567, F1 Macro: 0.9547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.24, Accuracy: 0.9561, F1 Micro: 0.9728, F1 Macro: 0.9719\n",
      "Epoch 4/10, Train Loss: 0.1568, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0974, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9665, F1 Micro: 0.9787, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.975\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.487, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2518, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1819, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9248\n",
      "Epoch 4/10, Train Loss: 0.1327, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0989, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9406\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9322\n",
      "Epoch 8/10, Train Loss: 0.0822, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9174\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9265\n",
      "Epoch 10/10, Train Loss: 0.0619, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9086\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       175\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.93      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.891\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.92      0.54        12\n",
      "     neutral       0.96      0.89      0.92       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.74      0.86      0.76       216\n",
      "weighted avg       0.90      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.92      0.77        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.87      0.86       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 179.23309016227722 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5303, Accuracy: 0.7946, F1 Micro: 0.8848, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4266, Accuracy: 0.9129, F1 Micro: 0.9473, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2753, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1024, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 8/10, Train Loss: 0.0632, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4586, Accuracy: 0.9101, F1 Micro: 0.9101, F1 Macro: 0.8937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2215, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9451\n",
      "Epoch 3/10, Train Loss: 0.1929, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.933\n",
      "Epoch 4/10, Train Loss: 0.1436, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9407\n",
      "Epoch 5/10, Train Loss: 0.1415, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9293\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9258\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9333\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9333\n",
      "Epoch 9/10, Train Loss: 0.0509, Accuracy: 0.9101, F1 Micro: 0.9101, F1 Macro: 0.9024\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9326\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.96      0.96       180\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.94      0.95      0.95       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9213\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 178.46629977226257 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.8614\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 1.195662260055542 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5203, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3683, Accuracy: 0.9315, F1 Micro: 0.9578, F1 Macro: 0.9555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2296, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1604, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1185, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 7/10, Train Loss: 0.0724, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 8/10, Train Loss: 0.0626, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Epoch 9/10, Train Loss: 0.0482, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4731, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1403, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1139, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9487\n",
      "Epoch 5/10, Train Loss: 0.1166, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9292\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9198\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.8957\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.96      0.95       260\n",
      "weighted avg       0.96      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8856\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.84      0.84       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.81      0.83      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.87      0.78        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.88      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.88      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 176.8312804698944 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5209, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3772, Accuracy: 0.9353, F1 Micro: 0.9598, F1 Macro: 0.9577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2318, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1648, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1192, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0883, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "Epoch 7/10, Train Loss: 0.0724, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9802\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4426, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Epoch 2/10, Train Loss: 0.2063, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1665, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9329\n",
      "Epoch 4/10, Train Loss: 0.1289, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.107, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0614, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9407\n",
      "Epoch 8/10, Train Loss: 0.074, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9269\n",
      "Epoch 9/10, Train Loss: 0.051, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9438\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.91      0.92        87\n",
      "    positive       0.96      0.97      0.96       177\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.95      0.94      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9128\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.86      0.87      0.86       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.92      0.79        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.84      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.89      0.85       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 188.09968209266663 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5273, Accuracy: 0.7984, F1 Micro: 0.8866, F1 Macro: 0.8851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4232, Accuracy: 0.9241, F1 Micro: 0.9534, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2555, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1725, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 7/10, Train Loss: 0.074, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "Epoch 9/10, Train Loss: 0.0483, Accuracy: 0.9673, F1 Micro: 0.9792, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9717, F1 Micro: 0.9821, F1 Macro: 0.981\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9821, F1 Macro: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4687, Accuracy: 0.893, F1 Micro: 0.893, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2188, Accuracy: 0.9188, F1 Micro: 0.9188, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.147, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9345\n",
      "Epoch 4/10, Train Loss: 0.1238, Accuracy: 0.9188, F1 Micro: 0.9188, F1 Macro: 0.9117\n",
      "Epoch 5/10, Train Loss: 0.1246, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1068, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0884, Accuracy: 0.9446, F1 Micro: 0.9446, F1 Macro: 0.9385\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9345\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9143\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9345\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9446, F1 Micro: 0.9446, F1 Macro: 0.9385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        88\n",
      "    positive       0.98      0.93      0.96       183\n",
      "\n",
      "    accuracy                           0.94       271\n",
      "   macro avg       0.93      0.95      0.94       271\n",
      "weighted avg       0.95      0.94      0.95       271\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9288\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 183.94961977005005 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.8633\n",
      "Total runtime: 10699.908027648926 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e866f",
   "metadata": {
    "papermill": {
     "duration": 0.46027,
     "end_time": "2025-03-08T13:17:56.488489",
     "exception": false,
     "start_time": "2025-03-08T13:17:56.028219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d1f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 3\n",
      "Random seed: [14, 61, 33]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c63bb79e0a43e2adffc6e1b2989e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6796, Accuracy: 0.776, F1 Micro: 0.8715, F1 Macro: 0.8667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5844, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5397, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5372, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4843, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4798, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4852, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4458, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4178, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Epoch 10/10, Train Loss: 0.4018, Accuracy: 0.7932, F1 Micro: 0.8837, F1 Macro: 0.8819\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6006, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.431, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3108, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2303, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1708, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1375, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1066, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0732, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.38      0.50      0.43         4\n",
      "weighted avg       0.56      0.75      0.64         4\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7886, F1 Micro: 0.7886, F1 Macro: 0.3053\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      0.97      0.82       152\n",
      "    positive       1.00      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.69       216\n",
      "   macro avg       0.57      0.33      0.29       216\n",
      "weighted avg       0.74      0.69      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.33      0.04      0.08        23\n",
      "     neutral       0.72      1.00      0.84       152\n",
      "    positive       0.67      0.05      0.09        41\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.57      0.36      0.34       216\n",
      "weighted avg       0.67      0.72      0.62       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 68.56853604316711 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7078, Accuracy: 0.7024, F1 Micro: 0.8155, F1 Macro: 0.7878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6021, Accuracy: 0.7872, F1 Micro: 0.8807, F1 Macro: 0.8791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5455, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5268, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4897, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Epoch 6/10, Train Loss: 0.4716, Accuracy: 0.7879, F1 Micro: 0.8805, F1 Macro: 0.8784\n",
      "Epoch 7/10, Train Loss: 0.4659, Accuracy: 0.7827, F1 Micro: 0.8767, F1 Macro: 0.8734\n",
      "Epoch 8/10, Train Loss: 0.4462, Accuracy: 0.7827, F1 Micro: 0.8759, F1 Macro: 0.872\n",
      "Epoch 9/10, Train Loss: 0.4178, Accuracy: 0.7812, F1 Micro: 0.8752, F1 Macro: 0.8715\n",
      "Epoch 10/10, Train Loss: 0.3894, Accuracy: 0.7812, F1 Micro: 0.8751, F1 Macro: 0.8716\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.72      0.99      0.84       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.814, Accuracy: 0.0, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6518, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5041, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3836, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2981, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2348, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1847, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1552, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1217, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1058, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         6\n",
      "   macro avg       0.50      0.50      0.50         6\n",
      "weighted avg       1.00      1.00      1.00         6\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7739, F1 Micro: 0.7739, F1 Macro: 0.3064\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.71      0.84      0.77       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      0.88      0.78       152\n",
      "    positive       0.83      0.10      0.17        52\n",
      "\n",
      "    accuracy                           0.64       216\n",
      "   macro avg       0.51      0.32      0.32       216\n",
      "weighted avg       0.70      0.64      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.12      0.09      0.10        23\n",
      "     neutral       0.72      0.95      0.82       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.68       216\n",
      "   macro avg       0.28      0.35      0.31       216\n",
      "weighted avg       0.52      0.68      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      0.99      0.92       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.79       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 60.16339063644409 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6904, Accuracy: 0.7835, F1 Micro: 0.8752, F1 Macro: 0.8707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5708, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5489, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5207, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4748, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4754, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.48, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4383, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4139, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3882, Accuracy: 0.7939, F1 Micro: 0.8841, F1 Macro: 0.8826\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7939, F1 Micro: 0.8841, F1 Macro: 0.8826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.80      0.98      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.72      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4833, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3188, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1606, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1135, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2591, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2411, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1873, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0508, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         5\n",
      "    positive       0.62      1.00      0.76         8\n",
      "\n",
      "    accuracy                           0.62        13\n",
      "   macro avg       0.31      0.50      0.38        13\n",
      "weighted avg       0.38      0.62      0.47        13\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7909, F1 Micro: 0.7909, F1 Macro: 0.3068\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.79      0.99      0.88       167\n",
      "    positive       0.29      0.06      0.10        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.36      0.35      0.33       216\n",
      "weighted avg       0.65      0.77      0.69       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.74      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.40      0.05      0.09        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.37      0.35      0.31       216\n",
      "weighted avg       0.58      0.71      0.60       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 73.59020900726318 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7845, F1 Micro: 0.7845, F1 Macro: 0.3062\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 8.192321538925171 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6203, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5141, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4769, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4589, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4203, Accuracy: 0.8006, F1 Micro: 0.8876, F1 Macro: 0.886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3998, Accuracy: 0.811, F1 Micro: 0.8917, F1 Macro: 0.8896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3633, Accuracy: 0.8251, F1 Micro: 0.8991, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3015, Accuracy: 0.8571, F1 Micro: 0.9157, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2709, Accuracy: 0.875, F1 Micro: 0.9249, F1 Macro: 0.9228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2336, Accuracy: 0.8854, F1 Micro: 0.9309, F1 Macro: 0.9291\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.8854, F1 Micro: 0.9309, F1 Macro: 0.9291\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.89      1.00      0.94       187\n",
      "     machine       0.88      0.99      0.93       175\n",
      "      others       0.85      0.90      0.87       158\n",
      "        part       0.88      0.97      0.92       158\n",
      "       price       0.96      0.98      0.97       192\n",
      "     service       0.87      1.00      0.93       191\n",
      "\n",
      "   micro avg       0.89      0.98      0.93      1061\n",
      "   macro avg       0.89      0.98      0.93      1061\n",
      "weighted avg       0.89      0.98      0.93      1061\n",
      " samples avg       0.89      0.98      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6521, Accuracy: 0.7396, F1 Micro: 0.7396, F1 Macro: 0.4252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5256, Accuracy: 0.7633, F1 Micro: 0.7633, F1 Macro: 0.5144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3934, Accuracy: 0.858, F1 Micro: 0.858, F1 Macro: 0.8068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2412, Accuracy: 0.8994, F1 Micro: 0.8994, F1 Macro: 0.8739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1571, Accuracy: 0.9172, F1 Micro: 0.9172, F1 Macro: 0.8968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0608, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9048\n",
      "Epoch 8/10, Train Loss: 0.0307, Accuracy: 0.9172, F1 Micro: 0.9172, F1 Macro: 0.8955\n",
      "Epoch 9/10, Train Loss: 0.0273, Accuracy: 0.9172, F1 Micro: 0.9172, F1 Macro: 0.8955\n",
      "Epoch 10/10, Train Loss: 0.0382, Accuracy: 0.8994, F1 Micro: 0.8994, F1 Macro: 0.88\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.93      0.86        44\n",
      "    positive       0.97      0.92      0.95       125\n",
      "\n",
      "    accuracy                           0.92       169\n",
      "   macro avg       0.89      0.93      0.90       169\n",
      "weighted avg       0.93      0.92      0.92       169\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8765, F1 Micro: 0.8765, F1 Macro: 0.6786\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.27      0.43        11\n",
      "     neutral       0.90      1.00      0.95       181\n",
      "    positive       1.00      0.46      0.63        24\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.97      0.58      0.67       216\n",
      "weighted avg       0.91      0.90      0.88       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.56      0.69        16\n",
      "     neutral       0.87      0.99      0.93       167\n",
      "    positive       0.94      0.45      0.61        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.90      0.67      0.74       216\n",
      "weighted avg       0.89      0.88      0.86       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.58      0.54        12\n",
      "     neutral       0.85      0.91      0.88       152\n",
      "    positive       0.72      0.56      0.63        52\n",
      "\n",
      "    accuracy                           0.81       216\n",
      "   macro avg       0.69      0.68      0.68       216\n",
      "weighted avg       0.80      0.81      0.80       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.65      0.70        23\n",
      "     neutral       0.87      0.97      0.92       152\n",
      "    positive       0.88      0.56      0.69        41\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.84      0.73      0.77       216\n",
      "weighted avg       0.86      0.86      0.85       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.62      0.70        13\n",
      "     neutral       0.96      0.98      0.97       186\n",
      "    positive       0.75      0.71      0.73        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.84      0.77      0.80       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.87      1.00      0.93       185\n",
      "    positive       1.00      0.18      0.30        17\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.62      0.39      0.41       216\n",
      "weighted avg       0.82      0.87      0.82       216\n",
      "\n",
      "Total train time: 82.6062216758728 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6412, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5252, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4826, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Epoch 4/10, Train Loss: 0.4693, Accuracy: 0.7879, F1 Micro: 0.8804, F1 Macro: 0.8782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4383, Accuracy: 0.7946, F1 Micro: 0.8834, F1 Macro: 0.881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4246, Accuracy: 0.7984, F1 Micro: 0.8852, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3841, Accuracy: 0.8162, F1 Micro: 0.8942, F1 Macro: 0.8919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3253, Accuracy: 0.8438, F1 Micro: 0.9083, F1 Macro: 0.9066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2888, Accuracy: 0.8624, F1 Micro: 0.9184, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2566, Accuracy: 0.8854, F1 Micro: 0.9311, F1 Macro: 0.9294\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8854, F1 Micro: 0.9311, F1 Macro: 0.9294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.94      1.00      0.97       187\n",
      "     machine       0.85      1.00      0.92       175\n",
      "      others       0.86      0.91      0.88       158\n",
      "        part       0.84      0.97      0.90       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.89      0.98      0.93      1061\n",
      "   macro avg       0.89      0.98      0.93      1061\n",
      "weighted avg       0.89      0.98      0.93      1061\n",
      " samples avg       0.89      0.98      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.637, Accuracy: 0.7239, F1 Micro: 0.7239, F1 Macro: 0.4199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6035, Accuracy: 0.7239, F1 Micro: 0.7239, F1 Macro: 0.4199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5003, Accuracy: 0.7301, F1 Micro: 0.7301, F1 Macro: 0.4432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4725, Accuracy: 0.816, F1 Micro: 0.816, F1 Macro: 0.7271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3171, Accuracy: 0.8834, F1 Micro: 0.8834, F1 Macro: 0.8571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1999, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.894\n",
      "Epoch 7/10, Train Loss: 0.1605, Accuracy: 0.8896, F1 Micro: 0.8896, F1 Macro: 0.8509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.8995\n",
      "Epoch 9/10, Train Loss: 0.102, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.8954\n",
      "Epoch 10/10, Train Loss: 0.0887, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.8857\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.8995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.84      0.85        45\n",
      "    positive       0.94      0.95      0.95       118\n",
      "\n",
      "    accuracy                           0.92       163\n",
      "   macro avg       0.90      0.90      0.90       163\n",
      "weighted avg       0.92      0.92      0.92       163\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8758, F1 Micro: 0.8758, F1 Macro: 0.6723\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.55      0.71        11\n",
      "     neutral       0.95      1.00      0.97       181\n",
      "    positive       1.00      0.79      0.88        24\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.98      0.78      0.85       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.38      0.55        16\n",
      "     neutral       0.85      1.00      0.92       167\n",
      "    positive       0.77      0.30      0.43        33\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.87      0.56      0.63       216\n",
      "weighted avg       0.85      0.85      0.82       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.58      0.61        12\n",
      "     neutral       0.86      0.91      0.89       152\n",
      "    positive       0.67      0.56      0.61        52\n",
      "\n",
      "    accuracy                           0.81       216\n",
      "   macro avg       0.72      0.69      0.70       216\n",
      "weighted avg       0.80      0.81      0.80       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.48      0.59        23\n",
      "     neutral       0.84      0.97      0.90       152\n",
      "    positive       0.88      0.56      0.69        41\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.84      0.67      0.73       216\n",
      "weighted avg       0.84      0.84      0.83       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.62      0.73        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.75      0.71      0.73        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.77      0.81       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 76.66058397293091 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5973, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5097, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4776, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4596, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4173, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.394, Accuracy: 0.8147, F1 Micro: 0.8948, F1 Macro: 0.8934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3559, Accuracy: 0.8326, F1 Micro: 0.9041, F1 Macro: 0.903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2991, Accuracy: 0.875, F1 Micro: 0.9258, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2681, Accuracy: 0.8943, F1 Micro: 0.9358, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2282, Accuracy: 0.907, F1 Micro: 0.9433, F1 Macro: 0.9413\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.907, F1 Micro: 0.9433, F1 Macro: 0.9413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.93      1.00      0.96       187\n",
      "     machine       0.89      1.00      0.94       175\n",
      "      others       0.88      0.91      0.89       158\n",
      "        part       0.87      0.98      0.92       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.91      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.91      0.98      0.94      1061\n",
      "   macro avg       0.91      0.98      0.94      1061\n",
      "weighted avg       0.91      0.98      0.94      1061\n",
      " samples avg       0.91      0.98      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5954, Accuracy: 0.6856, F1 Micro: 0.6856, F1 Macro: 0.4067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4759, Accuracy: 0.7474, F1 Micro: 0.7474, F1 Macro: 0.637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3612, Accuracy: 0.8608, F1 Micro: 0.8608, F1 Macro: 0.8364\n",
      "Epoch 4/10, Train Loss: 0.2627, Accuracy: 0.8351, F1 Micro: 0.8351, F1 Macro: 0.8243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.184, Accuracy: 0.9021, F1 Micro: 0.9021, F1 Macro: 0.8897\n",
      "Epoch 6/10, Train Loss: 0.0747, Accuracy: 0.8763, F1 Micro: 0.8763, F1 Macro: 0.8659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0722, Accuracy: 0.9021, F1 Micro: 0.9021, F1 Macro: 0.8905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0492, Accuracy: 0.9072, F1 Micro: 0.9072, F1 Macro: 0.8959\n",
      "Epoch 9/10, Train Loss: 0.0657, Accuracy: 0.8814, F1 Micro: 0.8814, F1 Macro: 0.8711\n",
      "Epoch 10/10, Train Loss: 0.0256, Accuracy: 0.866, F1 Micro: 0.866, F1 Macro: 0.8556\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9072, F1 Micro: 0.9072, F1 Macro: 0.8959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.92      0.86        61\n",
      "    positive       0.96      0.90      0.93       133\n",
      "\n",
      "    accuracy                           0.91       194\n",
      "   macro avg       0.89      0.91      0.90       194\n",
      "weighted avg       0.91      0.91      0.91       194\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8927, F1 Micro: 0.8927, F1 Macro: 0.7537\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.55      0.71        11\n",
      "     neutral       0.93      1.00      0.96       181\n",
      "    positive       1.00      0.62      0.77        24\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.98      0.72      0.81       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.88      1.00      0.94       167\n",
      "    positive       1.00      0.39      0.57        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.91      0.71      0.77       216\n",
      "weighted avg       0.90      0.89      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.83      0.57        12\n",
      "     neutral       0.88      0.91      0.90       152\n",
      "    positive       0.78      0.56      0.65        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.70      0.77      0.71       216\n",
      "weighted avg       0.84      0.82      0.82       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.65      0.70        23\n",
      "     neutral       0.87      0.98      0.92       152\n",
      "    positive       0.92      0.56      0.70        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.85      0.73      0.77       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.69      0.65      0.67        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.81      0.80      0.80       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.36      0.53        14\n",
      "     neutral       0.91      1.00      0.95       185\n",
      "    positive       0.86      0.35      0.50        17\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.57      0.66       216\n",
      "weighted avg       0.91      0.91      0.89       216\n",
      "\n",
      "Total train time: 83.1953980922699 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8331, F1 Micro: 0.8331, F1 Macro: 0.5039\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 12.186979532241821 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5917, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.4738, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.4774, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4294, Accuracy: 0.8036, F1 Micro: 0.8891, F1 Macro: 0.8876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3944, Accuracy: 0.8199, F1 Micro: 0.897, F1 Macro: 0.8954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3506, Accuracy: 0.8631, F1 Micro: 0.9186, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2757, Accuracy: 0.8996, F1 Micro: 0.9391, F1 Macro: 0.9373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2335, Accuracy: 0.9182, F1 Micro: 0.95, F1 Macro: 0.9483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2012, Accuracy: 0.933, F1 Micro: 0.9586, F1 Macro: 0.9568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1648, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9606\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.91      0.93      0.92       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.94      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6005, Accuracy: 0.687, F1 Micro: 0.687, F1 Macro: 0.4072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4767, Accuracy: 0.8652, F1 Micro: 0.8652, F1 Macro: 0.8346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3291, Accuracy: 0.887, F1 Micro: 0.887, F1 Macro: 0.8768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1923, Accuracy: 0.9217, F1 Micro: 0.9217, F1 Macro: 0.9103\n",
      "Epoch 5/10, Train Loss: 0.1383, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.8997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1495, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9162\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.9217, F1 Micro: 0.9217, F1 Macro: 0.9103\n",
      "Epoch 8/10, Train Loss: 0.1067, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0824, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0985, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9214\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.94      0.89        72\n",
      "    positive       0.97      0.92      0.95       158\n",
      "\n",
      "    accuracy                           0.93       230\n",
      "   macro avg       0.91      0.93      0.92       230\n",
      "weighted avg       0.93      0.93      0.93       230\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.8493\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.64      0.74        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.79      0.83       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.91      0.93      0.92       152\n",
      "    positive       0.82      0.71      0.76        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.78      0.80      0.78       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.73      0.81        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.79      0.65      0.71        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.89      0.80      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        14\n",
      "     neutral       0.94      1.00      0.97       185\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.95      0.72      0.80       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Total train time: 89.15809082984924 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6063, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4856, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4882, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.449, Accuracy: 0.7976, F1 Micro: 0.8853, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4168, Accuracy: 0.8043, F1 Micro: 0.8884, F1 Macro: 0.8862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3706, Accuracy: 0.8415, F1 Micro: 0.9076, F1 Macro: 0.906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3046, Accuracy: 0.8891, F1 Micro: 0.9338, F1 Macro: 0.9324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2573, Accuracy: 0.9189, F1 Micro: 0.95, F1 Macro: 0.9479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2188, Accuracy: 0.936, F1 Micro: 0.9604, F1 Macro: 0.9585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1787, Accuracy: 0.939, F1 Micro: 0.9623, F1 Macro: 0.9608\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.939, F1 Micro: 0.9623, F1 Macro: 0.9608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.92      0.98      0.95       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.90      0.99      0.94       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.94      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.94      0.99      0.96      1061\n",
      "   macro avg       0.94      0.99      0.96      1061\n",
      "weighted avg       0.94      0.99      0.96      1061\n",
      " samples avg       0.94      0.99      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6551, Accuracy: 0.6818, F1 Micro: 0.6818, F1 Macro: 0.4054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5204, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3836, Accuracy: 0.8773, F1 Micro: 0.8773, F1 Macro: 0.8533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2918, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1291, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9106\n",
      "Epoch 6/10, Train Loss: 0.1011, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0962, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9084\n",
      "Epoch 8/10, Train Loss: 0.0919, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9077\n",
      "Epoch 9/10, Train Loss: 0.1132, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9001\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9136, F1 Micro: 0.9136, F1 Macro: 0.9042\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.83      0.87        70\n",
      "    positive       0.92      0.97      0.94       150\n",
      "\n",
      "    accuracy                           0.92       220\n",
      "   macro avg       0.92      0.90      0.91       220\n",
      "weighted avg       0.92      0.92      0.92       220\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.84\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.92      0.98      0.95       167\n",
      "    positive       0.90      0.58      0.70        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.79      0.81       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.83      0.67      0.74        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.79      0.78       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.70      0.76        23\n",
      "     neutral       0.89      0.99      0.94       152\n",
      "    positive       0.93      0.66      0.77        41\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.89      0.78      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.57      0.73        14\n",
      "     neutral       0.94      1.00      0.97       185\n",
      "    positive       0.82      0.53      0.64        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.70      0.78       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Total train time: 90.26979446411133 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5846, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.473, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4767, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4254, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3876, Accuracy: 0.8177, F1 Micro: 0.896, F1 Macro: 0.8943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3423, Accuracy: 0.8884, F1 Micro: 0.9332, F1 Macro: 0.932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2751, Accuracy: 0.9107, F1 Micro: 0.9459, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2281, Accuracy: 0.9241, F1 Micro: 0.9532, F1 Macro: 0.9511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1973, Accuracy: 0.9308, F1 Micro: 0.9571, F1 Macro: 0.9548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1626, Accuracy: 0.9412, F1 Micro: 0.9635, F1 Macro: 0.9618\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9412, F1 Micro: 0.9635, F1 Macro: 0.9618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.92      0.93      0.92       158\n",
      "        part       0.91      0.98      0.94       158\n",
      "       price       0.96      1.00      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5833, Accuracy: 0.6812, F1 Micro: 0.6812, F1 Macro: 0.4052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4505, Accuracy: 0.8777, F1 Micro: 0.8777, F1 Macro: 0.8639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3044, Accuracy: 0.8821, F1 Micro: 0.8821, F1 Macro: 0.8721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2731, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9142\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9079\n",
      "Epoch 6/10, Train Loss: 0.1489, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9079\n",
      "Epoch 7/10, Train Loss: 0.0834, Accuracy: 0.9039, F1 Micro: 0.9039, F1 Macro: 0.8955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0918, Accuracy: 0.9345, F1 Micro: 0.9345, F1 Macro: 0.9249\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.9207\n",
      "Epoch 10/10, Train Loss: 0.0206, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.9222\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9345, F1 Micro: 0.9345, F1 Macro: 0.9249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.90      0.90        73\n",
      "    positive       0.95      0.95      0.95       156\n",
      "\n",
      "    accuracy                           0.93       229\n",
      "   macro avg       0.92      0.93      0.92       229\n",
      "weighted avg       0.93      0.93      0.93       229\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9329, F1 Micro: 0.9329, F1 Macro: 0.8658\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.82      0.87       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.78      0.72        23\n",
      "     neutral       0.91      0.98      0.94       152\n",
      "    positive       0.92      0.56      0.70        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.77      0.79       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.69      0.82        13\n",
      "     neutral       0.96      1.00      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.80      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.81      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 87.60133051872253 s\n",
      "Averaged - Iteration 208: Accuracy: 0.8653, F1 Micro: 0.8653, F1 Macro: 0.6198\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 13.073842763900757 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5737, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.497, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4489, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.405, Accuracy: 0.8333, F1 Micro: 0.9027, F1 Macro: 0.9006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3462, Accuracy: 0.8683, F1 Micro: 0.9213, F1 Macro: 0.919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2919, Accuracy: 0.9152, F1 Micro: 0.9476, F1 Macro: 0.9454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2284, Accuracy: 0.9315, F1 Micro: 0.9576, F1 Macro: 0.9557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.197, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9632\n",
      "Epoch 9/10, Train Loss: 0.1526, Accuracy: 0.939, F1 Micro: 0.9619, F1 Macro: 0.9593\n",
      "Epoch 10/10, Train Loss: 0.1284, Accuracy: 0.942, F1 Micro: 0.9638, F1 Macro: 0.9615\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.97      0.95       175\n",
      "      others       0.90      0.94      0.92       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5475, Accuracy: 0.6904, F1 Micro: 0.6904, F1 Macro: 0.4084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.432, Accuracy: 0.8619, F1 Micro: 0.8619, F1 Macro: 0.8213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3274, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9055\n",
      "Epoch 4/10, Train Loss: 0.1973, Accuracy: 0.9079, F1 Micro: 0.9079, F1 Macro: 0.8953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9021\n",
      "Epoch 6/10, Train Loss: 0.1314, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.901\n",
      "Epoch 7/10, Train Loss: 0.0874, Accuracy: 0.9079, F1 Micro: 0.9079, F1 Macro: 0.8946\n",
      "Epoch 8/10, Train Loss: 0.1282, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0961, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9074\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8916\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.88      0.87        74\n",
      "    positive       0.95      0.94      0.94       165\n",
      "\n",
      "    accuracy                           0.92       239\n",
      "   macro avg       0.91      0.91      0.91       239\n",
      "weighted avg       0.92      0.92      0.92       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9244, F1 Micro: 0.9244, F1 Macro: 0.8381\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.69      0.71        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.77      0.70      0.73        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.82      0.78      0.80       216\n",
      "weighted avg       0.90      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.83      0.51        12\n",
      "     neutral       0.92      0.89      0.90       152\n",
      "    positive       0.86      0.69      0.77        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.72      0.80      0.73       216\n",
      "weighted avg       0.87      0.84      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.78      0.77        23\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.73      0.81        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.81      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 92.88681602478027 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5897, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5058, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.464, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4247, Accuracy: 0.8199, F1 Micro: 0.8968, F1 Macro: 0.8953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3651, Accuracy: 0.8728, F1 Micro: 0.9244, F1 Macro: 0.9225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3028, Accuracy: 0.9234, F1 Micro: 0.9525, F1 Macro: 0.9502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2397, Accuracy: 0.9368, F1 Micro: 0.9609, F1 Macro: 0.9592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2027, Accuracy: 0.9405, F1 Micro: 0.9629, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1586, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9618\n",
      "Epoch 10/10, Train Loss: 0.1315, Accuracy: 0.9427, F1 Micro: 0.964, F1 Macro: 0.9615\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.92      0.98      0.95       175\n",
      "      others       0.93      0.87      0.90       158\n",
      "        part       0.94      0.97      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.97      0.96      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.96      0.97      0.96      1061\n",
      " samples avg       0.96      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6005, Accuracy: 0.6914, F1 Micro: 0.6914, F1 Macro: 0.4088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5106, Accuracy: 0.7461, F1 Micro: 0.7461, F1 Macro: 0.58\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3415, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9166\n",
      "Epoch 4/10, Train Loss: 0.1726, Accuracy: 0.9062, F1 Micro: 0.9062, F1 Macro: 0.8937\n",
      "Epoch 5/10, Train Loss: 0.1649, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8807\n",
      "Epoch 6/10, Train Loss: 0.1338, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.8951\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0867, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0713, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0561, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9225\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.90      0.89        79\n",
      "    positive       0.95      0.95      0.95       177\n",
      "\n",
      "    accuracy                           0.93       256\n",
      "   macro avg       0.92      0.92      0.92       256\n",
      "weighted avg       0.93      0.93      0.93       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.8617\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.81      0.68        16\n",
      "     neutral       0.95      0.96      0.96       167\n",
      "    positive       0.83      0.61      0.70        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.79      0.79      0.78       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.92      0.76        12\n",
      "     neutral       0.94      0.86      0.90       152\n",
      "    positive       0.70      0.81      0.75        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.76      0.86      0.80       216\n",
      "weighted avg       0.87      0.85      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.86      0.78      0.82        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.83      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 100.07779216766357 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5687, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4981, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4494, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4009, Accuracy: 0.8333, F1 Micro: 0.9029, F1 Macro: 0.9007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3409, Accuracy: 0.8914, F1 Micro: 0.9346, F1 Macro: 0.9329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2833, Accuracy: 0.9256, F1 Micro: 0.9539, F1 Macro: 0.9514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2231, Accuracy: 0.936, F1 Micro: 0.96, F1 Macro: 0.9578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1854, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9662\n",
      "Epoch 9/10, Train Loss: 0.1469, Accuracy: 0.9427, F1 Micro: 0.9641, F1 Macro: 0.962\n",
      "Epoch 10/10, Train Loss: 0.1249, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9616\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9662\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.92      0.97      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.97      1061\n",
      "weighted avg       0.95      0.98      0.97      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5516, Accuracy: 0.6831, F1 Micro: 0.6831, F1 Macro: 0.4059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4074, Accuracy: 0.8807, F1 Micro: 0.8807, F1 Macro: 0.8677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2449, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.91\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9274\n",
      "Epoch 5/10, Train Loss: 0.1035, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1114, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9434\n",
      "Epoch 7/10, Train Loss: 0.0897, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9316\n",
      "Epoch 8/10, Train Loss: 0.0368, Accuracy: 0.9465, F1 Micro: 0.9465, F1 Macro: 0.9384\n",
      "Epoch 9/10, Train Loss: 0.0903, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0715, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9426\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.91      0.92        77\n",
      "    positive       0.96      0.97      0.96       166\n",
      "\n",
      "    accuracy                           0.95       243\n",
      "   macro avg       0.95      0.94      0.94       243\n",
      "weighted avg       0.95      0.95      0.95       243\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9329, F1 Micro: 0.9329, F1 Macro: 0.8633\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.85      0.70      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.81      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.92      0.56        12\n",
      "     neutral       0.96      0.86      0.90       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.72      0.86      0.76       216\n",
      "weighted avg       0.89      0.85      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.74      0.81        23\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.86      0.76      0.81        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.79      0.85        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 94.81058073043823 s\n",
      "Averaged - Iteration 274: Accuracy: 0.8814, F1 Micro: 0.8814, F1 Macro: 0.6784\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 12.779695272445679 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5572, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.482, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4267, Accuracy: 0.8214, F1 Micro: 0.8978, F1 Macro: 0.8964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3758, Accuracy: 0.8765, F1 Micro: 0.9256, F1 Macro: 0.9236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2963, Accuracy: 0.9219, F1 Micro: 0.952, F1 Macro: 0.9506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2339, Accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1702, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1333, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1087, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9691\n",
      "Epoch 10/10, Train Loss: 0.096, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9688\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.92      0.92       158\n",
      "        part       0.94      0.99      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5665, Accuracy: 0.8765, F1 Micro: 0.8765, F1 Macro: 0.852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.331, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.207, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1492, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9303\n",
      "Epoch 5/10, Train Loss: 0.1326, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.9015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1242, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9437\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.9424, F1 Micro: 0.9424, F1 Macro: 0.9339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0842, Accuracy: 0.9547, F1 Micro: 0.9547, F1 Macro: 0.9492\n",
      "Epoch 9/10, Train Loss: 0.0819, Accuracy: 0.9465, F1 Micro: 0.9465, F1 Macro: 0.9392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9547, F1 Micro: 0.9547, F1 Macro: 0.9486\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9547, F1 Micro: 0.9547, F1 Macro: 0.9486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        78\n",
      "    positive       0.98      0.96      0.97       165\n",
      "\n",
      "    accuracy                           0.95       243\n",
      "   macro avg       0.94      0.95      0.95       243\n",
      "weighted avg       0.96      0.95      0.95       243\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8918\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.92      0.93      0.92       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.84      0.82       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.91      0.93        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.80      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 106.90693593025208 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5697, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4841, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4496, Accuracy: 0.8013, F1 Micro: 0.8881, F1 Macro: 0.8866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3975, Accuracy: 0.8638, F1 Micro: 0.9198, F1 Macro: 0.919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3086, Accuracy: 0.9249, F1 Micro: 0.9535, F1 Macro: 0.9513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2401, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1719, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1364, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1131, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0967, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.94      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6552, Accuracy: 0.6721, F1 Micro: 0.6721, F1 Macro: 0.4019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5012, Accuracy: 0.8016, F1 Micro: 0.8016, F1 Macro: 0.7189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2969, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9369\n",
      "Epoch 4/10, Train Loss: 0.1986, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.902\n",
      "Epoch 5/10, Train Loss: 0.1643, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9236\n",
      "Epoch 6/10, Train Loss: 0.0999, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9389\n",
      "Epoch 8/10, Train Loss: 0.1074, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9206\n",
      "Epoch 9/10, Train Loss: 0.0619, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0819, Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9459\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        81\n",
      "    positive       0.98      0.95      0.96       166\n",
      "\n",
      "    accuracy                           0.95       247\n",
      "   macro avg       0.94      0.95      0.95       247\n",
      "weighted avg       0.95      0.95      0.95       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9016\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.89      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 107.40908908843994 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5522, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4779, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4266, Accuracy: 0.8289, F1 Micro: 0.9018, F1 Macro: 0.9009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3654, Accuracy: 0.9025, F1 Micro: 0.9407, F1 Macro: 0.9387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2831, Accuracy: 0.9308, F1 Micro: 0.957, F1 Macro: 0.9547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2236, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1611, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Epoch 8/10, Train Loss: 0.1334, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9663\n",
      "Epoch 9/10, Train Loss: 0.1071, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0912, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9687\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.94      0.97      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5942, Accuracy: 0.6803, F1 Micro: 0.6803, F1 Macro: 0.4049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4629, Accuracy: 0.8402, F1 Micro: 0.8402, F1 Macro: 0.8309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2558, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.14, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9442\n",
      "Epoch 5/10, Train Loss: 0.109, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1076, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9435\n",
      "Epoch 7/10, Train Loss: 0.0954, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.8814\n",
      "Epoch 8/10, Train Loss: 0.0654, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9331\n",
      "Epoch 9/10, Train Loss: 0.0578, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9405\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9405\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        78\n",
      "    positive       0.96      0.96      0.96       166\n",
      "\n",
      "    accuracy                           0.95       244\n",
      "   macro avg       0.94      0.94      0.94       244\n",
      "weighted avg       0.95      0.95      0.95       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.8892\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.88      0.73      0.80        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.89      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.85      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 103.22156739234924 s\n",
      "Averaged - Iteration 333: Accuracy: 0.8945, F1 Micro: 0.8945, F1 Macro: 0.7216\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 11.692078113555908 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5472, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4683, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.412, Accuracy: 0.8475, F1 Micro: 0.91, F1 Macro: 0.908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3279, Accuracy: 0.9129, F1 Micro: 0.9466, F1 Macro: 0.9448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2396, Accuracy: 0.942, F1 Micro: 0.9641, F1 Macro: 0.9626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1828, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9683\n",
      "Epoch 7/10, Train Loss: 0.1423, Accuracy: 0.9472, F1 Micro: 0.9667, F1 Macro: 0.9639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.114, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0894, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9714\n",
      "Epoch 10/10, Train Loss: 0.0783, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.9682\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.96       175\n",
      "      others       0.91      0.94      0.92       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5708, Accuracy: 0.7953, F1 Micro: 0.7953, F1 Macro: 0.7022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3422, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9374\n",
      "Epoch 3/10, Train Loss: 0.181, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9121\n",
      "Epoch 4/10, Train Loss: 0.1462, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9126\n",
      "Epoch 5/10, Train Loss: 0.096, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9209\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.082, Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9547\n",
      "Epoch 8/10, Train Loss: 0.0965, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9172\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9506\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9463\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9547\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        80\n",
      "    positive       0.98      0.97      0.97       174\n",
      "\n",
      "    accuracy                           0.96       254\n",
      "   macro avg       0.95      0.96      0.95       254\n",
      "weighted avg       0.96      0.96      0.96       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.8917\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.92      0.92      0.92       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.81      0.79       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.96      0.83        23\n",
      "     neutral       0.96      0.96      0.96       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.90      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 107.37010216712952 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5646, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4798, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4318, Accuracy: 0.8296, F1 Micro: 0.9017, F1 Macro: 0.9004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3503, Accuracy: 0.9122, F1 Micro: 0.9463, F1 Macro: 0.9441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2526, Accuracy: 0.9278, F1 Micro: 0.9549, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1935, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1503, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1212, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.094, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9726\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.96       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6131, Accuracy: 0.6802, F1 Micro: 0.6802, F1 Macro: 0.4387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3802, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.924\n",
      "Epoch 4/10, Train Loss: 0.1472, Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0946, Accuracy: 0.9595, F1 Micro: 0.9595, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1015, Accuracy: 0.9595, F1 Micro: 0.9595, F1 Macro: 0.9535\n",
      "Epoch 7/10, Train Loss: 0.1042, Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9449\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9422\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9349\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9369\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9595, F1 Micro: 0.9595, F1 Macro: 0.9535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      0.90      0.94        82\n",
      "    positive       0.95      0.99      0.97       165\n",
      "\n",
      "    accuracy                           0.96       247\n",
      "   macro avg       0.96      0.95      0.95       247\n",
      "weighted avg       0.96      0.96      0.96       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8837\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.79      0.79      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.83      0.59        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.84      0.79       216\n",
      "weighted avg       0.91      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      1.00      0.82        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.90      0.87       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 116.56259632110596 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5426, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4681, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4045, Accuracy: 0.8609, F1 Micro: 0.918, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3257, Accuracy: 0.9286, F1 Micro: 0.9561, F1 Macro: 0.9542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2314, Accuracy: 0.9397, F1 Micro: 0.9626, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1787, Accuracy: 0.9539, F1 Micro: 0.9714, F1 Macro: 0.9699\n",
      "Epoch 7/10, Train Loss: 0.1445, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1161, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "Epoch 9/10, Train Loss: 0.0888, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9706\n",
      "Epoch 10/10, Train Loss: 0.08, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9701\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5676, Accuracy: 0.764, F1 Micro: 0.764, F1 Macro: 0.6393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.306, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1363, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1385, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9452\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9128\n",
      "Epoch 6/10, Train Loss: 0.1004, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9249\n",
      "Epoch 7/10, Train Loss: 0.097, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9181\n",
      "Epoch 8/10, Train Loss: 0.1076, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1024, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9499\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9412\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9499\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        80\n",
      "    positive       0.98      0.96      0.97       170\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.95      0.95      0.95       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8907\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.92      0.73        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.87      0.82       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.96      0.97      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.89       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 109.6713707447052 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9031, F1 Micro: 0.9031, F1 Macro: 0.7494\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 11.126128673553467 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5408, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4611, Accuracy: 0.8006, F1 Micro: 0.8879, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4089, Accuracy: 0.8452, F1 Micro: 0.9078, F1 Macro: 0.905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3211, Accuracy: 0.9263, F1 Micro: 0.9544, F1 Macro: 0.9521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2515, Accuracy: 0.936, F1 Micro: 0.9601, F1 Macro: 0.958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1844, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9667\n",
      "Epoch 7/10, Train Loss: 0.1444, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9663\n",
      "Epoch 8/10, Train Loss: 0.1164, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0882, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0798, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5924, Accuracy: 0.7976, F1 Micro: 0.7976, F1 Macro: 0.7091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3213, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9458\n",
      "Epoch 3/10, Train Loss: 0.1513, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9164\n",
      "Epoch 4/10, Train Loss: 0.1478, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1227, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1117, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0628, Accuracy: 0.9643, F1 Micro: 0.9643, F1 Macro: 0.9592\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9403\n",
      "Epoch 9/10, Train Loss: 0.0807, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9464\n",
      "Epoch 10/10, Train Loss: 0.0606, Accuracy: 0.9603, F1 Micro: 0.9603, F1 Macro: 0.9551\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9643, F1 Micro: 0.9643, F1 Macro: 0.9592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.96      0.94        80\n",
      "    positive       0.98      0.97      0.97       172\n",
      "\n",
      "    accuracy                           0.96       252\n",
      "   macro avg       0.95      0.96      0.96       252\n",
      "weighted avg       0.96      0.96      0.96       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9099\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.89      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.94      0.99      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 117.99242496490479 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.554, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4672, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4131, Accuracy: 0.8356, F1 Micro: 0.9036, F1 Macro: 0.9017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.327, Accuracy: 0.9308, F1 Micro: 0.9575, F1 Macro: 0.9557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2507, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1883, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1471, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.97\n",
      "Epoch 8/10, Train Loss: 0.1176, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9667\n",
      "Epoch 9/10, Train Loss: 0.0919, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9692\n",
      "Epoch 10/10, Train Loss: 0.0808, Accuracy: 0.9524, F1 Micro: 0.97, F1 Macro: 0.9675\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6162, Accuracy: 0.7592, F1 Micro: 0.7592, F1 Macro: 0.6424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4106, Accuracy: 0.8898, F1 Micro: 0.8898, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2231, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9219\n",
      "Epoch 4/10, Train Loss: 0.2089, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.089, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.9443\n",
      "Epoch 7/10, Train Loss: 0.0735, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9406\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.941\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.9453\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.9453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        81\n",
      "    positive       0.97      0.95      0.96       164\n",
      "\n",
      "    accuracy                           0.95       245\n",
      "   macro avg       0.94      0.95      0.95       245\n",
      "weighted avg       0.95      0.95      0.95       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.8804\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.92      0.59        12\n",
      "     neutral       0.94      0.88      0.91       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.73      0.86      0.77       216\n",
      "weighted avg       0.89      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.88      0.89       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 116.28257083892822 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5367, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4571, Accuracy: 0.7999, F1 Micro: 0.8875, F1 Macro: 0.886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3965, Accuracy: 0.8668, F1 Micro: 0.9196, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.309, Accuracy: 0.9382, F1 Micro: 0.9619, F1 Macro: 0.9601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2365, Accuracy: 0.939, F1 Micro: 0.9619, F1 Macro: 0.9596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1835, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1416, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.115, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0897, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0782, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9743\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5868, Accuracy: 0.8441, F1 Micro: 0.8441, F1 Macro: 0.7916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3298, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1611, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1369, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9437\n",
      "Epoch 5/10, Train Loss: 0.1608, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Epoch 7/10, Train Loss: 0.0639, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.072, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9472\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0406, Accuracy: 0.9582, F1 Micro: 0.9582, F1 Macro: 0.9526\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9582, F1 Micro: 0.9582, F1 Macro: 0.9526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        83\n",
      "    positive       0.99      0.95      0.97       180\n",
      "\n",
      "    accuracy                           0.96       263\n",
      "   macro avg       0.94      0.96      0.95       263\n",
      "weighted avg       0.96      0.96      0.96       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9192\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.75      0.83      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.88      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.91      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 123.66561889648438 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9097, F1 Micro: 0.9097, F1 Macro: 0.7714\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 9.736251592636108 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.542, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.8065, F1 Micro: 0.8903, F1 Macro: 0.8886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3948, Accuracy: 0.8713, F1 Micro: 0.922, F1 Macro: 0.919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.291, Accuracy: 0.9301, F1 Micro: 0.9566, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.213, Accuracy: 0.9442, F1 Micro: 0.9651, F1 Macro: 0.9626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1763, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1238, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1063, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0897, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9722\n",
      "Epoch 10/10, Train Loss: 0.0719, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.971\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.90      0.91       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5341, Accuracy: 0.8839, F1 Micro: 0.8839, F1 Macro: 0.8569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2214, Accuracy: 0.9139, F1 Micro: 0.9139, F1 Macro: 0.9049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1667, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1034, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9441\n",
      "Epoch 5/10, Train Loss: 0.1067, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Epoch 6/10, Train Loss: 0.1172, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9444\n",
      "Epoch 8/10, Train Loss: 0.0903, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9407\n",
      "Epoch 9/10, Train Loss: 0.0363, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9384\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9326\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        84\n",
      "    positive       0.98      0.95      0.96       183\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.94      0.95      0.94       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9059\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.90      0.92       152\n",
      "    positive       0.75      0.81      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.85      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 127.6949737071991 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5548, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4836, Accuracy: 0.7969, F1 Micro: 0.8859, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4097, Accuracy: 0.8802, F1 Micro: 0.9285, F1 Macro: 0.9273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3032, Accuracy: 0.9249, F1 Micro: 0.9536, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.216, Accuracy: 0.9442, F1 Micro: 0.965, F1 Macro: 0.962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1787, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1282, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9724\n",
      "Epoch 8/10, Train Loss: 0.1051, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0879, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "Epoch 10/10, Train Loss: 0.0746, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.968\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.91      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6231, Accuracy: 0.7115, F1 Micro: 0.7115, F1 Macro: 0.5005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3868, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1755, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1355, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9427\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Epoch 6/10, Train Loss: 0.0976, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1182, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9415\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 9/10, Train Loss: 0.0567, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0615, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9476\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        83\n",
      "    positive       0.98      0.95      0.97       177\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.95      0.95       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8976\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.92      0.71        12\n",
      "     neutral       0.96      0.89      0.92       152\n",
      "    positive       0.77      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.88      0.81       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 126.58491969108582 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5365, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4688, Accuracy: 0.8095, F1 Micro: 0.8921, F1 Macro: 0.8906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.379, Accuracy: 0.8936, F1 Micro: 0.9346, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2733, Accuracy: 0.9301, F1 Micro: 0.9563, F1 Macro: 0.9536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2013, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9691\n",
      "Epoch 6/10, Train Loss: 0.1678, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.121, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.971\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.9524, F1 Micro: 0.97, F1 Macro: 0.9674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.086, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.075, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9705\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5234, Accuracy: 0.8837, F1 Micro: 0.8837, F1 Macro: 0.8579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3062, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9428\n",
      "Epoch 3/10, Train Loss: 0.1761, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1165, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1393, Accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9556\n",
      "Epoch 6/10, Train Loss: 0.0896, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9428\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9187\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.947\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9467\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9428\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        82\n",
      "    positive       0.98      0.97      0.97       176\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.95      0.96      0.96       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8959\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      1.00      0.65        12\n",
      "     neutral       0.96      0.88      0.92       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.90      0.79       216\n",
      "weighted avg       0.90      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.84      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 121.56026434898376 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.7875\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 8.941875696182251 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5405, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4664, Accuracy: 0.8095, F1 Micro: 0.8923, F1 Macro: 0.8909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3661, Accuracy: 0.8973, F1 Micro: 0.9369, F1 Macro: 0.9339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2738, Accuracy: 0.9442, F1 Micro: 0.9651, F1 Macro: 0.9627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.195, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1436, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1182, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0977, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.90      0.99      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4732, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2227, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1397, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1163, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9504\n",
      "Epoch 5/10, Train Loss: 0.0966, Accuracy: 0.9268, F1 Micro: 0.9268, F1 Macro: 0.9204\n",
      "Epoch 6/10, Train Loss: 0.0826, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9461\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9331\n",
      "Epoch 8/10, Train Loss: 0.0541, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9331\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.9593, F1 Micro: 0.9593, F1 Macro: 0.9543\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9593, F1 Micro: 0.9593, F1 Macro: 0.9543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        81\n",
      "    positive       0.98      0.96      0.97       165\n",
      "\n",
      "    accuracy                           0.96       246\n",
      "   macro avg       0.95      0.96      0.95       246\n",
      "weighted avg       0.96      0.96      0.96       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.8985\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.97      0.96      0.96       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.75      0.58        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.95      0.73      0.83        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.78      0.81      0.78       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.8188157081604 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5481, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4722, Accuracy: 0.8028, F1 Micro: 0.8875, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3754, Accuracy: 0.9129, F1 Micro: 0.9466, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2783, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1975, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1492, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1227, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9727\n",
      "Epoch 8/10, Train Loss: 0.0971, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0748, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6061, Accuracy: 0.6929, F1 Micro: 0.6929, F1 Macro: 0.4541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3444, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.9008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1708, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.9013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1442, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.145, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1376, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0959, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9427\n",
      "Epoch 8/10, Train Loss: 0.0799, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9427\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92        82\n",
      "    positive       0.98      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       254\n",
      "   macro avg       0.93      0.95      0.94       254\n",
      "weighted avg       0.95      0.95      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8998\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.84      0.84       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.83      0.61        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.75      0.84      0.78       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 134.71572828292847 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5381, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.458, Accuracy: 0.8132, F1 Micro: 0.8939, F1 Macro: 0.8925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3529, Accuracy: 0.9159, F1 Micro: 0.9477, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2629, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1907, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1454, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1178, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0934, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9744\n",
      "Epoch 10/10, Train Loss: 0.0637, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.95      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.539, Accuracy: 0.8127, F1 Micro: 0.8127, F1 Macro: 0.7469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2783, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9314\n",
      "Epoch 3/10, Train Loss: 0.163, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9258\n",
      "Epoch 4/10, Train Loss: 0.1184, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9179\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9463\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9323\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9417\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9384\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        82\n",
      "    positive       0.98      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.95      0.95       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9036\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.88      0.78        16\n",
      "     neutral       0.96      0.96      0.96       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.88      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.92      0.73        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.80      0.88      0.83       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.95      0.98      0.96       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 126.50942468643188 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.8\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 8.087431907653809 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5381, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4679, Accuracy: 0.8065, F1 Micro: 0.8908, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3758, Accuracy: 0.9003, F1 Micro: 0.9389, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2649, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1902, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1517, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1138, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0724, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Epoch 10/10, Train Loss: 0.0647, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9704\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5157, Accuracy: 0.8933, F1 Micro: 0.8933, F1 Macro: 0.8762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2815, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1872, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1426, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1132, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9508\n",
      "Epoch 6/10, Train Loss: 0.094, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1017, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9508\n",
      "Epoch 8/10, Train Loss: 0.0669, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9261\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9368\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9334\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        82\n",
      "    positive       0.98      0.96      0.97       171\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.95      0.96      0.95       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9452, F1 Micro: 0.9452, F1 Macro: 0.8869\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      0.92      0.55        12\n",
      "     neutral       0.94      0.88      0.91       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.74      0.86      0.76       216\n",
      "weighted avg       0.90      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 134.82351803779602 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5455, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4761, Accuracy: 0.8043, F1 Micro: 0.8893, F1 Macro: 0.8877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.395, Accuracy: 0.9025, F1 Micro: 0.9409, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2809, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.201, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1566, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1187, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0992, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9729\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9698\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5699, Accuracy: 0.746, F1 Micro: 0.746, F1 Macro: 0.5947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3133, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.921\n",
      "Epoch 3/10, Train Loss: 0.2182, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8993\n",
      "Epoch 4/10, Train Loss: 0.1711, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9454\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "Epoch 8/10, Train Loss: 0.057, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9504\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        81\n",
      "    positive       0.98      0.96      0.97       171\n",
      "\n",
      "    accuracy                           0.96       252\n",
      "   macro avg       0.95      0.95      0.95       252\n",
      "weighted avg       0.96      0.96      0.96       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8945\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      1.00      0.55        12\n",
      "     neutral       0.96      0.86      0.91       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.74      0.89      0.77       216\n",
      "weighted avg       0.91      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.77      0.74        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.4742922782898 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5337, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.456, Accuracy: 0.814, F1 Micro: 0.8945, F1 Macro: 0.8933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3613, Accuracy: 0.8996, F1 Micro: 0.9383, F1 Macro: 0.9341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2542, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1867, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Epoch 6/10, Train Loss: 0.15, Accuracy: 0.9501, F1 Micro: 0.9685, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0929, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9774\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9719\n",
      "Epoch 10/10, Train Loss: 0.0632, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9726\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5272, Accuracy: 0.8924, F1 Micro: 0.8924, F1 Macro: 0.8757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2771, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9327\n",
      "Epoch 3/10, Train Loss: 0.1517, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1805, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9424\n",
      "Epoch 5/10, Train Loss: 0.0856, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9323\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0849, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.077, Accuracy: 0.9562, F1 Micro: 0.9562, F1 Macro: 0.9512\n",
      "Epoch 9/10, Train Loss: 0.0462, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.9562, F1 Micro: 0.9562, F1 Macro: 0.9515\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9562, F1 Micro: 0.9562, F1 Macro: 0.9515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        83\n",
      "    positive       0.99      0.95      0.97       168\n",
      "\n",
      "    accuracy                           0.96       251\n",
      "   macro avg       0.94      0.96      0.95       251\n",
      "weighted avg       0.96      0.96      0.96       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9012\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.92      0.59        12\n",
      "     neutral       0.95      0.89      0.92       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.87      0.79       216\n",
      "weighted avg       0.90      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 131.8521854877472 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9218, F1 Micro: 0.9218, F1 Macro: 0.8094\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 7.729357719421387 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5305, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4411, Accuracy: 0.8274, F1 Micro: 0.9005, F1 Macro: 0.8992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3489, Accuracy: 0.9122, F1 Micro: 0.9465, F1 Macro: 0.9446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.247, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1765, Accuracy: 0.9509, F1 Micro: 0.9691, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1415, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1087, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0876, Accuracy: 0.9546, F1 Micro: 0.9712, F1 Macro: 0.9682\n",
      "Epoch 9/10, Train Loss: 0.0746, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9741\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5289, Accuracy: 0.8842, F1 Micro: 0.8842, F1 Macro: 0.8644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2506, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9217\n",
      "Epoch 3/10, Train Loss: 0.1594, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9124\n",
      "Epoch 4/10, Train Loss: 0.1366, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9135\n",
      "Epoch 5/10, Train Loss: 0.1331, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.888\n",
      "Epoch 6/10, Train Loss: 0.0777, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9333\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9174\n",
      "Epoch 9/10, Train Loss: 0.0809, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9262\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9202\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.90      0.91        83\n",
      "    positive       0.95      0.96      0.96       176\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.93      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8917\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.85      0.81       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 133.00494837760925 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4506, Accuracy: 0.8155, F1 Micro: 0.8952, F1 Macro: 0.8938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3606, Accuracy: 0.9211, F1 Micro: 0.9515, F1 Macro: 0.9494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2529, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9698\n",
      "Epoch 5/10, Train Loss: 0.1807, Accuracy: 0.9479, F1 Micro: 0.9671, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1381, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1085, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.073, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9728\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5678, Accuracy: 0.8086, F1 Micro: 0.8086, F1 Macro: 0.7318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2903, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9338\n",
      "Epoch 3/10, Train Loss: 0.1902, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1541, Accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.9514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1271, Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9562\n",
      "Epoch 6/10, Train Loss: 0.1053, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9394\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9352\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9107\n",
      "Epoch 10/10, Train Loss: 0.0781, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94        83\n",
      "    positive       0.99      0.95      0.97       173\n",
      "\n",
      "    accuracy                           0.96       256\n",
      "   macro avg       0.95      0.96      0.96       256\n",
      "weighted avg       0.96      0.96      0.96       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9028\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.46      0.92      0.61        12\n",
      "     neutral       0.95      0.89      0.92       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.87      0.79       216\n",
      "weighted avg       0.90      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 134.3022119998932 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5278, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4378, Accuracy: 0.8371, F1 Micro: 0.9056, F1 Macro: 0.9039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3433, Accuracy: 0.9256, F1 Micro: 0.9542, F1 Macro: 0.9518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2419, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1359, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1113, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 8/10, Train Loss: 0.0887, Accuracy: 0.9554, F1 Micro: 0.9716, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4797, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.248, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1464, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1137, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1406, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9345\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9258\n",
      "Epoch 7/10, Train Loss: 0.0971, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9176\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9161\n",
      "Epoch 9/10, Train Loss: 0.057, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0394, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9368\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9368\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.89      0.91        83\n",
      "    positive       0.95      0.97      0.96       173\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.93      0.94       256\n",
      "weighted avg       0.95      0.95      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.8938\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.92      0.71        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.86      0.85      0.85        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.89      0.83       216\n",
      "weighted avg       0.91      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.69      0.75        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 139.27138233184814 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9244, F1 Micro: 0.9244, F1 Macro: 0.8173\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 6.873712062835693 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.539, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4468, Accuracy: 0.8356, F1 Micro: 0.9036, F1 Macro: 0.9018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.345, Accuracy: 0.9338, F1 Micro: 0.9589, F1 Macro: 0.9566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2353, Accuracy: 0.9568, F1 Micro: 0.9733, F1 Macro: 0.9722\n",
      "Epoch 5/10, Train Loss: 0.1787, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0671, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.98      0.97      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5105, Accuracy: 0.8902, F1 Micro: 0.8902, F1 Macro: 0.8667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2269, Accuracy: 0.9059, F1 Micro: 0.9059, F1 Macro: 0.8993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1934, Accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9611\n",
      "Epoch 4/10, Train Loss: 0.1351, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9516\n",
      "Epoch 5/10, Train Loss: 0.1269, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9519\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9371\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9286\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9527\n",
      "Epoch 9/10, Train Loss: 0.0678, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9609\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.97      0.95        86\n",
      "    positive       0.98      0.96      0.97       169\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.96      0.96      0.96       255\n",
      "weighted avg       0.97      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9358\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.98      0.97      0.97       167\n",
      "    positive       0.85      0.88      0.87        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.85      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.62947869300842 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5435, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4548, Accuracy: 0.8304, F1 Micro: 0.9021, F1 Macro: 0.9009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3485, Accuracy: 0.9375, F1 Micro: 0.9614, F1 Macro: 0.9599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2414, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1858, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1315, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9583, F1 Micro: 0.9736, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0704, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5738, Accuracy: 0.878, F1 Micro: 0.878, F1 Macro: 0.8528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3457, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2026, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1833, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0724, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9218\n",
      "Epoch 7/10, Train Loss: 0.1098, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 8/10, Train Loss: 0.1101, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9099\n",
      "Epoch 10/10, Train Loss: 0.0725, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.96      0.90        82\n",
      "    positive       0.98      0.91      0.95       172\n",
      "\n",
      "    accuracy                           0.93       254\n",
      "   macro avg       0.91      0.94      0.92       254\n",
      "weighted avg       0.94      0.93      0.93       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.899\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.93      0.77      0.84        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.79      0.85      0.80       216\n",
      "weighted avg       0.91      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 141.17177176475525 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5314, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4418, Accuracy: 0.8445, F1 Micro: 0.909, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3282, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2272, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.176, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1054, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "Epoch 9/10, Train Loss: 0.0675, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.505, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.8703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2535, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.204, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9472\n",
      "Epoch 4/10, Train Loss: 0.1166, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9429\n",
      "Epoch 5/10, Train Loss: 0.1305, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9383\n",
      "Epoch 6/10, Train Loss: 0.0896, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8914\n",
      "Epoch 7/10, Train Loss: 0.1191, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9435\n",
      "Epoch 8/10, Train Loss: 0.078, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.948\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9287\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.99      0.93        84\n",
      "    positive       0.99      0.93      0.96       169\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.96      0.95       253\n",
      "weighted avg       0.96      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.8984\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.92      0.69        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.79      0.87      0.82       216\n",
      "weighted avg       0.91      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.11647701263428 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.8251\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 6.354593515396118 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5243, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4412, Accuracy: 0.8549, F1 Micro: 0.9147, F1 Macro: 0.9131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3212, Accuracy: 0.9308, F1 Micro: 0.9574, F1 Macro: 0.9551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2157, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9713\n",
      "Epoch 5/10, Train Loss: 0.1575, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1259, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5052, Accuracy: 0.8631, F1 Micro: 0.8631, F1 Macro: 0.8264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.314, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 3/10, Train Loss: 0.1969, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9208\n",
      "Epoch 4/10, Train Loss: 0.194, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1315, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1338, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9395\n",
      "Epoch 7/10, Train Loss: 0.1065, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9361\n",
      "Epoch 8/10, Train Loss: 0.0786, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9305\n",
      "Epoch 9/10, Train Loss: 0.0616, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9244\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        85\n",
      "    positive       0.97      0.96      0.96       178\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.94      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9042\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.76        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.83      0.86       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 147.2245352268219 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5302, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4554, Accuracy: 0.8199, F1 Micro: 0.8968, F1 Macro: 0.8954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3436, Accuracy: 0.9196, F1 Micro: 0.9504, F1 Macro: 0.9474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2358, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1644, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1306, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9713\n",
      "Epoch 8/10, Train Loss: 0.0832, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 9/10, Train Loss: 0.0705, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.055, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5474, Accuracy: 0.8627, F1 Micro: 0.8627, F1 Macro: 0.8269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2717, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1769, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1336, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1385, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9186\n",
      "Epoch 7/10, Train Loss: 0.1055, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0706, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9056\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.86      0.83       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 148.11159253120422 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5226, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4311, Accuracy: 0.8772, F1 Micro: 0.9268, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3079, Accuracy: 0.933, F1 Micro: 0.9586, F1 Macro: 0.9563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2092, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9714\n",
      "Epoch 5/10, Train Loss: 0.1495, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1258, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0996, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 8/10, Train Loss: 0.0786, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Epoch 9/10, Train Loss: 0.0655, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5022, Accuracy: 0.8783, F1 Micro: 0.8783, F1 Macro: 0.847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.322, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9361\n",
      "Epoch 3/10, Train Loss: 0.1832, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.196, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9485\n",
      "Epoch 6/10, Train Loss: 0.1495, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "Epoch 7/10, Train Loss: 0.1252, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "Epoch 8/10, Train Loss: 0.0851, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9291\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        84\n",
      "    positive       0.98      0.95      0.97       179\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.96      0.95       263\n",
      "weighted avg       0.96      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9158\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.83      0.84        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.86      0.88      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 147.26182317733765 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9293, F1 Micro: 0.9293, F1 Macro: 0.8315\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 5.833921670913696 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5307, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4291, Accuracy: 0.8609, F1 Micro: 0.9182, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3066, Accuracy: 0.9435, F1 Micro: 0.965, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.215, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1537, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1217, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 7/10, Train Loss: 0.0947, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0751, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Epoch 10/10, Train Loss: 0.0504, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4639, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9062\n",
      "Epoch 2/10, Train Loss: 0.2424, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.8981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1734, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "Epoch 4/10, Train Loss: 0.1641, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "Epoch 5/10, Train Loss: 0.1327, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9164\n",
      "Epoch 7/10, Train Loss: 0.1099, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.924\n",
      "Epoch 8/10, Train Loss: 0.1164, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "Epoch 9/10, Train Loss: 0.09, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "Epoch 10/10, Train Loss: 0.092, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.931\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.905\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.79      0.85      0.81       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 148.60493111610413 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5382, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.443, Accuracy: 0.843, F1 Micro: 0.9089, F1 Macro: 0.9074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3183, Accuracy: 0.9375, F1 Micro: 0.9609, F1 Macro: 0.959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2218, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1629, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.127, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0763, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 10/10, Train Loss: 0.053, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5591, Accuracy: 0.8721, F1 Micro: 0.8721, F1 Macro: 0.8443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.28, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1843, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1359, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1312, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9386\n",
      "Epoch 7/10, Train Loss: 0.0934, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0538, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9428\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9382\n",
      "Epoch 10/10, Train Loss: 0.0572, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9382\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        84\n",
      "    positive       0.97      0.96      0.96       174\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.94      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.905\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.81      0.86      0.83       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 153.72335529327393 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5264, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4155, Accuracy: 0.8847, F1 Micro: 0.9308, F1 Macro: 0.9286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2924, Accuracy: 0.9345, F1 Micro: 0.9591, F1 Macro: 0.9567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2059, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1514, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1171, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 7/10, Train Loss: 0.0955, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 8/10, Train Loss: 0.0727, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5279, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2528, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9436\n",
      "Epoch 3/10, Train Loss: 0.2034, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 4/10, Train Loss: 0.1348, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 5/10, Train Loss: 0.1458, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Epoch 6/10, Train Loss: 0.1203, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "Epoch 7/10, Train Loss: 0.0816, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9157\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9345\n",
      "Epoch 10/10, Train Loss: 0.068, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9286\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        84\n",
      "    positive       0.98      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9105\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.78      0.85      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 145.6234152317047 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.8369\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.406259059906006 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5278, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4296, Accuracy: 0.8586, F1 Micro: 0.9142, F1 Macro: 0.9103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3172, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1993, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1577, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 7/10, Train Loss: 0.0907, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.075, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9769\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4793, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2197, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1479, Accuracy: 0.9531, F1 Micro: 0.9531, F1 Macro: 0.9475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1475, Accuracy: 0.9648, F1 Micro: 0.9648, F1 Macro: 0.9609\n",
      "Epoch 5/10, Train Loss: 0.1049, Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9562\n",
      "Epoch 6/10, Train Loss: 0.082, Accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.9514\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9429\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9372\n",
      "Epoch 9/10, Train Loss: 0.0627, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.932\n",
      "Epoch 10/10, Train Loss: 0.0577, Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9557\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9648, F1 Micro: 0.9648, F1 Macro: 0.9609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95        85\n",
      "    positive       0.99      0.96      0.97       171\n",
      "\n",
      "    accuracy                           0.96       256\n",
      "   macro avg       0.96      0.97      0.96       256\n",
      "weighted avg       0.97      0.96      0.97       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9115\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.73      0.80        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.92      0.63        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.86      0.79       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      1.00      0.98        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 155.56110858917236 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4336, Accuracy: 0.869, F1 Micro: 0.9214, F1 Macro: 0.9184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3176, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1983, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1582, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1186, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 7/10, Train Loss: 0.0919, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5439, Accuracy: 0.8701, F1 Micro: 0.8701, F1 Macro: 0.8597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2645, Accuracy: 0.9055, F1 Micro: 0.9055, F1 Macro: 0.8973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1766, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1301, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9388\n",
      "Epoch 5/10, Train Loss: 0.147, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1278, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9385\n",
      "Epoch 7/10, Train Loss: 0.0653, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9226\n",
      "Epoch 8/10, Train Loss: 0.092, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0897, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9381\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9346\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        83\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.93      0.94      0.94       254\n",
      "weighted avg       0.95      0.94      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9028\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.92      0.59        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.86      0.78       216\n",
      "weighted avg       0.90      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 161.09647727012634 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5221, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.415, Accuracy: 0.8921, F1 Micro: 0.934, F1 Macro: 0.9307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2991, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9687\n",
      "Epoch 4/10, Train Loss: 0.1883, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1106, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "Epoch 8/10, Train Loss: 0.0723, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Epoch 9/10, Train Loss: 0.0593, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5165, Accuracy: 0.8947, F1 Micro: 0.8947, F1 Macro: 0.8854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2378, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9321\n",
      "Epoch 3/10, Train Loss: 0.2062, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.8954\n",
      "Epoch 4/10, Train Loss: 0.1828, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9276\n",
      "Epoch 5/10, Train Loss: 0.1274, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9256\n",
      "Epoch 6/10, Train Loss: 0.1408, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0868, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9415\n",
      "Epoch 8/10, Train Loss: 0.1084, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9325\n",
      "Epoch 9/10, Train Loss: 0.0718, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9496\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9496\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        87\n",
      "    positive       0.98      0.95      0.97       179\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.94      0.96      0.95       266\n",
      "weighted avg       0.96      0.95      0.96       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9301\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 149.99874830245972 s\n",
      "Averaged - Iteration 673: Accuracy: 0.933, F1 Micro: 0.933, F1 Macro: 0.8421\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.8849382400512695 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5378, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4229, Accuracy: 0.8914, F1 Micro: 0.9338, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3035, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1997, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1531, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1142, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0926, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 8/10, Train Loss: 0.0763, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.98      0.98      0.98       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4796, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "Epoch 2/10, Train Loss: 0.2486, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1667, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "Epoch 4/10, Train Loss: 0.1687, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "Epoch 5/10, Train Loss: 0.1343, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 6/10, Train Loss: 0.0992, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9223\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9212\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.92\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9204\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9028\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.90      0.85      0.88        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.83      0.69        12\n",
      "     neutral       0.93      0.92      0.92       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.84      0.80       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 155.00751543045044 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5421, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4359, Accuracy: 0.8832, F1 Micro: 0.9293, F1 Macro: 0.927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3143, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.9627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2076, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1591, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1173, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 8/10, Train Loss: 0.0748, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0663, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5413, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.8856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2788, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.933\n",
      "Epoch 3/10, Train Loss: 0.1826, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9258\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1487, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.931\n",
      "Epoch 6/10, Train Loss: 0.1023, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9178\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 8/10, Train Loss: 0.0837, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9188\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0555, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9366\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.92        86\n",
      "    positive       0.97      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.94       267\n",
      "   macro avg       0.93      0.94      0.94       267\n",
      "weighted avg       0.95      0.94      0.94       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9162\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 161.46414184570312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5332, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4175, Accuracy: 0.904, F1 Micro: 0.941, F1 Macro: 0.9384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2946, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1931, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1488, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1097, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.98\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4893, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.8923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2522, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1579, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "Epoch 4/10, Train Loss: 0.1454, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9354\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9089\n",
      "Epoch 8/10, Train Loss: 0.0599, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.91      0.91        87\n",
      "    positive       0.95      0.96      0.96       176\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.94      0.93      0.94       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9139\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.95427441596985 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9345, F1 Micro: 0.9345, F1 Macro: 0.8464\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.561395168304443 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5331, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4404, Accuracy: 0.8943, F1 Micro: 0.9355, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2934, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2012, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1474, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 7/10, Train Loss: 0.0888, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0577, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9788\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4693, Accuracy: 0.8922, F1 Micro: 0.8922, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1931, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9361\n",
      "Epoch 3/10, Train Loss: 0.1464, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "Epoch 4/10, Train Loss: 0.1194, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9372\n",
      "Epoch 6/10, Train Loss: 0.122, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9288\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0926, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9419\n",
      "Epoch 10/10, Train Loss: 0.0585, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        87\n",
      "    positive       0.98      0.94      0.96       182\n",
      "\n",
      "    accuracy                           0.95       269\n",
      "   macro avg       0.93      0.95      0.94       269\n",
      "weighted avg       0.95      0.95      0.95       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9099\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 163.04367017745972 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5378, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4501, Accuracy: 0.8921, F1 Micro: 0.9346, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2981, Accuracy: 0.9494, F1 Micro: 0.9687, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2025, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1502, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1143, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.978\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.8911, F1 Micro: 0.8911, F1 Macro: 0.8715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2817, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1831, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1906, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1157, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0767, Accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.9463\n",
      "Epoch 8/10, Train Loss: 0.0906, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.941\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9292\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.942\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.9463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.96       166\n",
      "\n",
      "    accuracy                           0.95       248\n",
      "   macro avg       0.94      0.95      0.95       248\n",
      "weighted avg       0.95      0.95      0.95       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8947\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.88      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.42      0.83      0.56        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.83      0.76       216\n",
      "weighted avg       0.90      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 161.23952102661133 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5282, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4327, Accuracy: 0.9048, F1 Micro: 0.9417, F1 Macro: 0.9392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2855, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1976, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1437, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0873, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.92      0.93      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4842, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2222, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9374\n",
      "Epoch 3/10, Train Loss: 0.1945, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9199\n",
      "Epoch 4/10, Train Loss: 0.1148, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.141, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9385\n",
      "Epoch 6/10, Train Loss: 0.1142, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0564, Accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.9627, F1 Micro: 0.9627, F1 Macro: 0.9577\n",
      "Epoch 9/10, Train Loss: 0.0443, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9239\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9208\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9627, F1 Micro: 0.9627, F1 Macro: 0.9577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.97      0.94        86\n",
      "    positive       0.98      0.96      0.97       182\n",
      "\n",
      "    accuracy                           0.96       268\n",
      "   macro avg       0.95      0.96      0.96       268\n",
      "weighted avg       0.96      0.96      0.96       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9108\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.90      0.85      0.88        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.77      0.77      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.82      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 167.12520051002502 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.8499\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.8932039737701416 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5292, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4197, Accuracy: 0.9077, F1 Micro: 0.9436, F1 Macro: 0.9416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2789, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1928, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Epoch 5/10, Train Loss: 0.1368, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9734\n",
      "Epoch 6/10, Train Loss: 0.1066, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0842, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 8/10, Train Loss: 0.0688, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9734\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9759\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.479, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2105, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.181, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.929\n",
      "Epoch 4/10, Train Loss: 0.1204, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1247, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1205, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9429\n",
      "Epoch 7/10, Train Loss: 0.0932, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.939\n",
      "Epoch 8/10, Train Loss: 0.0756, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9353\n",
      "Epoch 10/10, Train Loss: 0.0653, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.92      0.92        84\n",
      "    positive       0.96      0.97      0.96       178\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.94      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.896\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.89      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.83      0.59        12\n",
      "     neutral       0.94      0.90      0.92       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.74      0.83      0.77       216\n",
      "weighted avg       0.89      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 159.79324102401733 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5342, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4242, Accuracy: 0.9018, F1 Micro: 0.9408, F1 Macro: 0.939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2888, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1944, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1377, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9741\n",
      "Epoch 6/10, Train Loss: 0.1049, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0864, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0586, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9781\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.8841, F1 Micro: 0.8841, F1 Macro: 0.8745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3018, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1876, Accuracy: 0.9348, F1 Micro: 0.9348, F1 Macro: 0.9274\n",
      "Epoch 4/10, Train Loss: 0.1539, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9084\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9185\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9185\n",
      "Epoch 7/10, Train Loss: 0.0948, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9219\n",
      "Epoch 8/10, Train Loss: 0.0915, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9459\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9189\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.96      0.97       189\n",
      "\n",
      "    accuracy                           0.95       276\n",
      "   macro avg       0.94      0.95      0.95       276\n",
      "weighted avg       0.95      0.95      0.95       276\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9138\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.75      0.81      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.85      0.84       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.91      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 167.00434112548828 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5234, Accuracy: 0.7984, F1 Micro: 0.8865, F1 Macro: 0.8849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4123, Accuracy: 0.9182, F1 Micro: 0.9494, F1 Macro: 0.9467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2712, Accuracy: 0.9442, F1 Micro: 0.9651, F1 Macro: 0.963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1867, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0853, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0688, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.981\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.464, Accuracy: 0.8914, F1 Micro: 0.8914, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2832, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2057, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9403\n",
      "Epoch 4/10, Train Loss: 0.1618, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9183\n",
      "Epoch 5/10, Train Loss: 0.139, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9166\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9132\n",
      "Epoch 7/10, Train Loss: 0.119, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9171\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9171\n",
      "Epoch 9/10, Train Loss: 0.0704, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9363\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9166\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        87\n",
      "    positive       0.96      0.96      0.96       180\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.94      0.94      0.94       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9173\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 165.69792222976685 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9367, F1 Micro: 0.9367, F1 Macro: 0.8532\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.5111234188079834 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5218, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4224, Accuracy: 0.8943, F1 Micro: 0.9349, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2929, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1933, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1429, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 6/10, Train Loss: 0.1079, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4628, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.237, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1813, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9485\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9441\n",
      "Epoch 5/10, Train Loss: 0.1173, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "Epoch 7/10, Train Loss: 0.0879, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9441\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9431\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9358\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9332\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.95      0.95       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9024\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.78      0.85      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.84      0.80       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 166.34446144104004 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5299, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4282, Accuracy: 0.907, F1 Micro: 0.9433, F1 Macro: 0.9412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2863, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1887, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 6/10, Train Loss: 0.1066, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 7/10, Train Loss: 0.0851, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9739\n",
      "Epoch 9/10, Train Loss: 0.0567, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5393, Accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2572, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1793, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.109, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9386\n",
      "Epoch 5/10, Train Loss: 0.0955, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.934\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9101\n",
      "Epoch 7/10, Train Loss: 0.0958, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9141\n",
      "Epoch 8/10, Train Loss: 0.086, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9386\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9256\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        84\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.94      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.901\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.78      0.85      0.81       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 157.259907245636 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5153, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4034, Accuracy: 0.9137, F1 Micro: 0.9461, F1 Macro: 0.9427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2702, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1867, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1389, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0846, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 8/10, Train Loss: 0.0697, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Epoch 9/10, Train Loss: 0.0563, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4739, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2369, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 3/10, Train Loss: 0.1513, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.134, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9447\n",
      "Epoch 5/10, Train Loss: 0.0906, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9352\n",
      "Epoch 8/10, Train Loss: 0.0552, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 9/10, Train Loss: 0.0455, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        86\n",
      "    positive       0.97      0.96      0.96       175\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9035\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.83      0.69        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.85      0.81       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 165.65347814559937 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9376, F1 Micro: 0.9376, F1 Macro: 0.8557\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.3334105014801025 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5236, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4077, Accuracy: 0.9107, F1 Micro: 0.9452, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2571, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1723, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1339, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.971, F1 Micro: 0.9819, F1 Macro: 0.981\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9819, F1 Macro: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4156, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.938\n",
      "Epoch 2/10, Train Loss: 0.2158, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1743, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9374\n",
      "Epoch 4/10, Train Loss: 0.1392, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9249\n",
      "Epoch 5/10, Train Loss: 0.109, Accuracy: 0.8952, F1 Micro: 0.8952, F1 Macro: 0.8899\n",
      "Epoch 6/10, Train Loss: 0.1105, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9199\n",
      "Epoch 7/10, Train Loss: 0.0882, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9177\n",
      "Epoch 8/10, Train Loss: 0.1097, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9177\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.9136\n",
      "Epoch 10/10, Train Loss: 0.0632, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9338\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        85\n",
      "    positive       0.96      0.96      0.96       163\n",
      "\n",
      "    accuracy                           0.94       248\n",
      "   macro avg       0.94      0.94      0.94       248\n",
      "weighted avg       0.94      0.94      0.94       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.8963\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.75      0.73        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.75      0.60        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.93      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.81      0.78       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 160.49054980278015 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5295, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4218, Accuracy: 0.9129, F1 Micro: 0.9468, F1 Macro: 0.9449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2679, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 5/10, Train Loss: 0.1382, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9782\n",
      "Epoch 6/10, Train Loss: 0.0988, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5129, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2453, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9171\n",
      "Epoch 3/10, Train Loss: 0.148, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1415, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0912, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9401\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 8/10, Train Loss: 0.0907, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "Epoch 10/10, Train Loss: 0.0721, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9176\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        85\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.93      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.8944\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.82      0.82        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.88      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.83      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 171.451815366745 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5224, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4027, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2544, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1728, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1376, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1024, Accuracy: 0.9673, F1 Micro: 0.9796, F1 Macro: 0.9787\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.475, Accuracy: 0.8897, F1 Micro: 0.8897, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2715, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Epoch 3/10, Train Loss: 0.1689, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9204\n",
      "Epoch 4/10, Train Loss: 0.167, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Epoch 5/10, Train Loss: 0.1446, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "Epoch 6/10, Train Loss: 0.0975, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.928\n",
      "Epoch 7/10, Train Loss: 0.0781, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "Epoch 8/10, Train Loss: 0.0543, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9089\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9309\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9089\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9042\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.80      0.77      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.85      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      1.00      0.99       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 163.19682216644287 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9384, F1 Micro: 0.9384, F1 Macro: 0.8579\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.740241289138794 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5207, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4006, Accuracy: 0.9159, F1 Micro: 0.948, F1 Macro: 0.9452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2542, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.174, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9747\n",
      "Epoch 5/10, Train Loss: 0.1284, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9742\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.076, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0511, Accuracy: 0.9732, F1 Micro: 0.9831, F1 Macro: 0.982\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9732, F1 Micro: 0.9831, F1 Macro: 0.982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4683, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9246\n",
      "Epoch 2/10, Train Loss: 0.2151, Accuracy: 0.9064, F1 Micro: 0.9064, F1 Macro: 0.8986\n",
      "Epoch 3/10, Train Loss: 0.1538, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 4/10, Train Loss: 0.1231, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0825, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9258\n",
      "Epoch 6/10, Train Loss: 0.091, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.925\n",
      "Epoch 8/10, Train Loss: 0.0774, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9101\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9202\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9101, F1 Micro: 0.9101, F1 Macro: 0.9015\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        87\n",
      "    positive       0.97      0.93      0.95       180\n",
      "\n",
      "    accuracy                           0.93       267\n",
      "   macro avg       0.92      0.94      0.92       267\n",
      "weighted avg       0.94      0.93      0.93       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9032\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 171.38713765144348 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5286, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4103, Accuracy: 0.9196, F1 Micro: 0.9506, F1 Macro: 0.9485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.263, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1779, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1324, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0963, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0736, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5041, Accuracy: 0.8927, F1 Micro: 0.8927, F1 Macro: 0.8813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2543, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.9042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1809, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9241\n",
      "Epoch 4/10, Train Loss: 0.1348, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1015, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9277\n",
      "Epoch 9/10, Train Loss: 0.0417, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9241\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9224\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        84\n",
      "    positive       0.99      0.93      0.96       177\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.93      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9116\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 176.01346826553345 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5157, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3906, Accuracy: 0.9263, F1 Micro: 0.9539, F1 Macro: 0.9504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2472, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1707, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1282, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9751\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9778\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0529, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.048, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4591, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2188, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.158, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1338, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9223\n",
      "Epoch 5/10, Train Loss: 0.0927, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Epoch 8/10, Train Loss: 0.1056, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Epoch 10/10, Train Loss: 0.0557, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        85\n",
      "    positive       0.98      0.93      0.95       178\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.92      0.95      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9212\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.83      0.87        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.46321439743042 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.8605\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.294233798980713 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5382, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4111, Accuracy: 0.9085, F1 Micro: 0.9441, F1 Macro: 0.9421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.269, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1695, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1288, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1025, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9789\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0526, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.981\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9799\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.95      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4516, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1918, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9245\n",
      "Epoch 3/10, Train Loss: 0.1466, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1145, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.94\n",
      "Epoch 5/10, Train Loss: 0.1111, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.921\n",
      "Epoch 6/10, Train Loss: 0.0721, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.94\n",
      "Epoch 8/10, Train Loss: 0.0812, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.063, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9403\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9355\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.94      0.95      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9193\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.85      0.85      0.85        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.87      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 177.1435899734497 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4164, Accuracy: 0.91, F1 Micro: 0.9446, F1 Macro: 0.9421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2735, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1741, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1286, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9788\n",
      "Epoch 9/10, Train Loss: 0.0537, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4902, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2375, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Epoch 3/10, Train Loss: 0.1471, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.9057\n",
      "Epoch 4/10, Train Loss: 0.1299, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.913\n",
      "Epoch 5/10, Train Loss: 0.1041, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9101\n",
      "Epoch 6/10, Train Loss: 0.0825, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9221\n",
      "Epoch 7/10, Train Loss: 0.0673, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9195\n",
      "Epoch 8/10, Train Loss: 0.1024, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "Epoch 9/10, Train Loss: 0.0588, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0426, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9284\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.92      0.90        87\n",
      "    positive       0.96      0.95      0.95       182\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.92      0.93      0.93       269\n",
      "weighted avg       0.94      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9206\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 181.24768352508545 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5268, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3923, Accuracy: 0.9211, F1 Micro: 0.9514, F1 Macro: 0.9488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2553, Accuracy: 0.9568, F1 Micro: 0.9733, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1626, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1235, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4971, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2122, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1745, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9393\n",
      "Epoch 4/10, Train Loss: 0.1366, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9336\n",
      "Epoch 5/10, Train Loss: 0.1099, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9273\n",
      "Epoch 6/10, Train Loss: 0.1038, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9317\n",
      "Epoch 7/10, Train Loss: 0.0617, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9435\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "Epoch 10/10, Train Loss: 0.0574, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9233\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        85\n",
      "    positive       0.97      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.8953\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.75      0.60        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.78      0.83      0.80       216\n",
      "weighted avg       0.90      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 172.79448628425598 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.8628\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 1.780271053314209 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5255, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3852, Accuracy: 0.9345, F1 Micro: 0.9596, F1 Macro: 0.9584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2405, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1645, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1182, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.9744\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.95      0.94      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4392, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.217, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1708, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1196, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.946\n",
      "Epoch 5/10, Train Loss: 0.1095, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.93\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0723, Accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9494\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9333\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9337\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9139, F1 Micro: 0.9139, F1 Macro: 0.9067\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9166\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        88\n",
      "    positive       0.97      0.96      0.97       179\n",
      "\n",
      "    accuracy                           0.96       267\n",
      "   macro avg       0.95      0.95      0.95       267\n",
      "weighted avg       0.96      0.96      0.96       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9149\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.89      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 185.8737828731537 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5279, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3975, Accuracy: 0.9226, F1 Micro: 0.952, F1 Macro: 0.9498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2505, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1639, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.975\n",
      "Epoch 5/10, Train Loss: 0.1227, Accuracy: 0.9598, F1 Micro: 0.9745, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0956, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9794\n",
      "Epoch 8/10, Train Loss: 0.0608, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9771\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.979\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.482, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2149, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9324\n",
      "Epoch 3/10, Train Loss: 0.1647, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9144\n",
      "Epoch 4/10, Train Loss: 0.1331, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.916\n",
      "Epoch 5/10, Train Loss: 0.097, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.924\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9361\n",
      "Epoch 9/10, Train Loss: 0.0824, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9101\n",
      "Epoch 10/10, Train Loss: 0.0617, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.913\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        86\n",
      "    positive       0.96      0.96      0.96       183\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.94      0.94       269\n",
      "weighted avg       0.94      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9063\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.88      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.92      0.80        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.89      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 181.21365761756897 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5214, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3832, Accuracy: 0.9353, F1 Micro: 0.96, F1 Macro: 0.9582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2336, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1565, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1179, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Epoch 6/10, Train Loss: 0.0927, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.974\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9736\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9781\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4561, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2265, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 3/10, Train Loss: 0.1462, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9265\n",
      "Epoch 4/10, Train Loss: 0.1193, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9233\n",
      "Epoch 5/10, Train Loss: 0.1171, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0686, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.076, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9389\n",
      "Epoch 8/10, Train Loss: 0.0774, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9359\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9125\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        84\n",
      "    positive       0.97      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.94      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8971\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.94      0.86        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.89      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.92      0.69        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.86      0.81       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.92      0.77        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.87      0.86       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 175.71962308883667 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.8647\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 1.1996126174926758 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5306, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3971, Accuracy: 0.9293, F1 Micro: 0.9567, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2493, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1677, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9789\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9748\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4273, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "Epoch 2/10, Train Loss: 0.2161, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1425, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "Epoch 4/10, Train Loss: 0.1474, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.933\n",
      "Epoch 5/10, Train Loss: 0.1143, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.084, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9484\n",
      "Epoch 8/10, Train Loss: 0.0681, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "Epoch 9/10, Train Loss: 0.0653, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0412, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9531\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.97      0.94        86\n",
      "    positive       0.98      0.95      0.97       176\n",
      "\n",
      "    accuracy                           0.96       262\n",
      "   macro avg       0.95      0.96      0.95       262\n",
      "weighted avg       0.96      0.96      0.96       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.93\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 176.5331335067749 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5341, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4087, Accuracy: 0.9226, F1 Micro: 0.9525, F1 Macro: 0.9506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2605, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1259, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0958, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 7/10, Train Loss: 0.077, Accuracy: 0.9613, F1 Micro: 0.9754, F1 Macro: 0.9733\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9735\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.9742\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.506, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9258\n",
      "Epoch 2/10, Train Loss: 0.2513, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1516, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9249\n",
      "Epoch 4/10, Train Loss: 0.1231, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1393, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "Epoch 6/10, Train Loss: 0.1129, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0816, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9345\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9189\n",
      "Epoch 10/10, Train Loss: 0.0554, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.8728\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        84\n",
      "    positive       0.97      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8587\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.55      0.67        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.79      0.92      0.85        24\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.82      0.84       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.62      0.67        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.82      0.79      0.80       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.83      0.85      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.78      0.84        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.86      0.88      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.54      0.64        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.71      0.88      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.83      0.81      0.81       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 174.81871581077576 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5233, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3837, Accuracy: 0.933, F1 Micro: 0.9584, F1 Macro: 0.9561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2351, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1628, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1174, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 6/10, Train Loss: 0.0948, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9739\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9769\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9751\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4592, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2242, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9354\n",
      "Epoch 3/10, Train Loss: 0.1191, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "Epoch 4/10, Train Loss: 0.1423, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9231\n",
      "Epoch 5/10, Train Loss: 0.1003, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Epoch 6/10, Train Loss: 0.0976, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9231\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9105\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9105\n",
      "Epoch 9/10, Train Loss: 0.063, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9131\n",
      "Epoch 10/10, Train Loss: 0.0431, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9181\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        84\n",
      "    positive       0.98      0.93      0.96       173\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.95      0.94       257\n",
      "weighted avg       0.95      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8928\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.91      0.87        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      0.92      0.55        12\n",
      "     neutral       0.94      0.89      0.92       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.74      0.85      0.76       216\n",
      "weighted avg       0.89      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.87      0.88      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 168.50120878219604 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.8661\n",
      "Total runtime: 10708.90365743637 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c1054",
   "metadata": {
    "papermill": {
     "duration": 0.461668,
     "end_time": "2025-03-08T13:17:58.426788",
     "exception": false,
     "start_time": "2025-03-08T13:17:57.965120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16a966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 4\n",
      "Random seed: [3, 44, 85]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6645, Accuracy: 0.7865, F1 Micro: 0.8804, F1 Macro: 0.8788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5755, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5509, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5294, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4827, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4817, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 7/10, Train Loss: 0.4898, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 8/10, Train Loss: 0.4555, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4326, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4, Accuracy: 0.7946, F1 Micro: 0.8843, F1 Macro: 0.8825\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7946, F1 Micro: 0.8843, F1 Macro: 0.8825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.73      0.96      0.83       158\n",
      "        part       0.72      1.00      0.84       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.88      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7166, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5281, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4526, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3997, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3523, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3078, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2888, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2379, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1964, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1981, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.95      1.00      0.97        18\n",
      "\n",
      "    accuracy                           0.95        19\n",
      "   macro avg       0.47      0.50      0.49        19\n",
      "weighted avg       0.90      0.95      0.92        19\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.794, F1 Micro: 0.794, F1 Macro: 0.3165\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.73      0.96      0.83       152\n",
      "    positive       0.60      0.17      0.27        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.44      0.38      0.37       216\n",
      "weighted avg       0.66      0.72      0.65       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      1.00      0.84       152\n",
      "    positive       0.75      0.07      0.13        41\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.49      0.36      0.32       216\n",
      "weighted avg       0.65      0.72      0.61       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 63.85168194770813 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6903, Accuracy: 0.7671, F1 Micro: 0.8669, F1 Macro: 0.8648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5862, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 3/10, Train Loss: 0.5474, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.5251, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 5/10, Train Loss: 0.4831, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 6/10, Train Loss: 0.4764, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4815, Accuracy: 0.7887, F1 Micro: 0.8818, F1 Macro: 0.8803\n",
      "Epoch 8/10, Train Loss: 0.4411, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4087, Accuracy: 0.7969, F1 Micro: 0.8856, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3783, Accuracy: 0.8043, F1 Micro: 0.889, F1 Macro: 0.8873\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8043, F1 Micro: 0.889, F1 Macro: 0.8873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.81      0.97      0.88       175\n",
      "      others       0.73      0.99      0.84       158\n",
      "        part       0.74      0.99      0.85       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.81      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.89      1061\n",
      "weighted avg       0.81      0.99      0.89      1061\n",
      " samples avg       0.81      0.99      0.89      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6552, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5933, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5783, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5062, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4427, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3945, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4686, Accuracy: 0.6857, F1 Micro: 0.6857, F1 Macro: 0.4804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4872, Accuracy: 0.7429, F1 Micro: 0.7429, F1 Macro: 0.6504\n",
      "Epoch 9/10, Train Loss: 0.3907, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.6196\n",
      "Epoch 10/10, Train Loss: 0.4138, Accuracy: 0.6286, F1 Micro: 0.6286, F1 Macro: 0.5956\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.7429, F1 Micro: 0.7429, F1 Macro: 0.6504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.33      0.47        12\n",
      "    positive       0.73      0.96      0.83        23\n",
      "\n",
      "    accuracy                           0.74        35\n",
      "   macro avg       0.77      0.64      0.65        35\n",
      "weighted avg       0.76      0.74      0.71        35\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7986, F1 Micro: 0.7986, F1 Macro: 0.3478\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.38      0.44        16\n",
      "     neutral       0.80      0.98      0.88       167\n",
      "    positive       0.50      0.03      0.06        33\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.62      0.46      0.46       216\n",
      "weighted avg       0.74      0.79      0.72       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.73      0.99      0.84       152\n",
      "    positive       0.62      0.10      0.17        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.45      0.36      0.33       216\n",
      "weighted avg       0.66      0.72      0.63       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.74      0.99      0.85       152\n",
      "    positive       0.58      0.17      0.26        41\n",
      "\n",
      "    accuracy                           0.73       216\n",
      "   macro avg       0.44      0.39      0.37       216\n",
      "weighted avg       0.63      0.73      0.65       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 52.37716364860535 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6548, Accuracy: 0.7827, F1 Micro: 0.878, F1 Macro: 0.8767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5764, Accuracy: 0.7879, F1 Micro: 0.8814, F1 Macro: 0.8799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5377, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5305, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4931, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4768, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4845, Accuracy: 0.7879, F1 Micro: 0.8814, F1 Macro: 0.8799\n",
      "Epoch 8/10, Train Loss: 0.4606, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4249, Accuracy: 0.7924, F1 Micro: 0.8834, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4039, Accuracy: 0.7969, F1 Micro: 0.8852, F1 Macro: 0.8834\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7969, F1 Micro: 0.8852, F1 Macro: 0.8834\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.80      0.99      0.88       175\n",
      "      others       0.72      0.97      0.83       158\n",
      "        part       0.72      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6588, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5725, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4665, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5629, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4718, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3767, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3798, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3906, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Epoch 9/10, Train Loss: 0.4707, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.326, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.5214\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.5214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.14      0.22         7\n",
      "    positive       0.73      0.94      0.82        17\n",
      "\n",
      "    accuracy                           0.71        24\n",
      "   macro avg       0.61      0.54      0.52        24\n",
      "weighted avg       0.66      0.71      0.65        24\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.794, F1 Micro: 0.794, F1 Macro: 0.3226\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.06      0.11        16\n",
      "     neutral       0.79      0.99      0.88       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.43      0.35      0.33       216\n",
      "weighted avg       0.65      0.77      0.69       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.72      0.98      0.83       152\n",
      "    positive       0.70      0.13      0.23        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.47      0.37      0.35       216\n",
      "weighted avg       0.68      0.72      0.64       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.50      0.10      0.16        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.41      0.36      0.33       216\n",
      "weighted avg       0.60      0.71      0.62       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 60.89115786552429 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7955, F1 Micro: 0.7955, F1 Macro: 0.3289\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 8.267948865890503 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6152, Accuracy: 0.7783, F1 Micro: 0.8742, F1 Macro: 0.8705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5095, Accuracy: 0.7879, F1 Micro: 0.88, F1 Macro: 0.8775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5069, Accuracy: 0.7917, F1 Micro: 0.8831, F1 Macro: 0.8815\n",
      "Epoch 4/10, Train Loss: 0.4873, Accuracy: 0.7932, F1 Micro: 0.8828, F1 Macro: 0.8805\n",
      "Epoch 5/10, Train Loss: 0.4578, Accuracy: 0.779, F1 Micro: 0.8717, F1 Macro: 0.8639\n",
      "Epoch 6/10, Train Loss: 0.425, Accuracy: 0.7887, F1 Micro: 0.8763, F1 Macro: 0.8686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3856, Accuracy: 0.8088, F1 Micro: 0.888, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3389, Accuracy: 0.8229, F1 Micro: 0.8951, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3119, Accuracy: 0.8527, F1 Micro: 0.9114, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2649, Accuracy: 0.8743, F1 Micro: 0.9224, F1 Macro: 0.9165\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.8743, F1 Micro: 0.9224, F1 Macro: 0.9165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.89      1.00      0.94       187\n",
      "     machine       0.88      0.98      0.93       175\n",
      "      others       0.92      0.74      0.82       158\n",
      "        part       0.84      0.92      0.88       158\n",
      "       price       0.94      1.00      0.97       192\n",
      "     service       0.93      1.00      0.96       191\n",
      "\n",
      "   micro avg       0.90      0.95      0.92      1061\n",
      "   macro avg       0.90      0.94      0.92      1061\n",
      "weighted avg       0.90      0.95      0.92      1061\n",
      " samples avg       0.91      0.95      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.621, Accuracy: 0.7512, F1 Micro: 0.7512, F1 Macro: 0.4289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5491, Accuracy: 0.7512, F1 Micro: 0.7512, F1 Macro: 0.4289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.458, Accuracy: 0.7512, F1 Micro: 0.7512, F1 Macro: 0.4289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3391, Accuracy: 0.788, F1 Micro: 0.788, F1 Macro: 0.589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.317, Accuracy: 0.8249, F1 Micro: 0.8249, F1 Macro: 0.7789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2793, Accuracy: 0.8433, F1 Micro: 0.8433, F1 Macro: 0.8042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2099, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.8077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1527, Accuracy: 0.871, F1 Micro: 0.871, F1 Macro: 0.8419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1082, Accuracy: 0.9032, F1 Micro: 0.9032, F1 Macro: 0.8714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9078, F1 Micro: 0.9078, F1 Macro: 0.8782\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9078, F1 Micro: 0.9078, F1 Macro: 0.8782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.83      0.82        54\n",
      "    positive       0.94      0.93      0.94       163\n",
      "\n",
      "    accuracy                           0.91       217\n",
      "   macro avg       0.87      0.88      0.88       217\n",
      "weighted avg       0.91      0.91      0.91       217\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8657, F1 Micro: 0.8657, F1 Macro: 0.7155\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.36      0.53        11\n",
      "     neutral       0.89      1.00      0.94       181\n",
      "    positive       1.00      0.38      0.55        24\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.96      0.58      0.67       216\n",
      "weighted avg       0.91      0.90      0.88       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        16\n",
      "     neutral       0.88      0.98      0.93       167\n",
      "    positive       0.71      0.45      0.56        33\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.86      0.65      0.72       216\n",
      "weighted avg       0.86      0.87      0.85       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.75      0.60        12\n",
      "     neutral       0.92      0.74      0.82       152\n",
      "    positive       0.54      0.79      0.64        52\n",
      "\n",
      "    accuracy                           0.75       216\n",
      "   macro avg       0.65      0.76      0.69       216\n",
      "weighted avg       0.80      0.75      0.76       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.43      0.59        23\n",
      "     neutral       0.85      0.93      0.89       152\n",
      "    positive       0.68      0.61      0.64        41\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.81      0.66      0.71       216\n",
      "weighted avg       0.82      0.82      0.81       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.38      0.53        13\n",
      "     neutral       0.93      1.00      0.97       186\n",
      "    positive       0.91      0.59      0.71        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.66      0.74       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.93      1.00      0.96       185\n",
      "    positive       1.00      0.41      0.58        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.97      0.68      0.78       216\n",
      "weighted avg       0.94      0.93      0.92       216\n",
      "\n",
      "Total train time: 82.07567858695984 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6383, Accuracy: 0.7894, F1 Micro: 0.881, F1 Macro: 0.8788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.514, Accuracy: 0.7894, F1 Micro: 0.8819, F1 Macro: 0.8802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4982, Accuracy: 0.7939, F1 Micro: 0.8841, F1 Macro: 0.8823\n",
      "Epoch 4/10, Train Loss: 0.4747, Accuracy: 0.7946, F1 Micro: 0.8833, F1 Macro: 0.881\n",
      "Epoch 5/10, Train Loss: 0.4457, Accuracy: 0.7976, F1 Micro: 0.8815, F1 Macro: 0.8754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.41, Accuracy: 0.8192, F1 Micro: 0.893, F1 Macro: 0.888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3522, Accuracy: 0.8318, F1 Micro: 0.9, F1 Macro: 0.8954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3186, Accuracy: 0.8542, F1 Micro: 0.9119, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.289, Accuracy: 0.8735, F1 Micro: 0.9224, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2446, Accuracy: 0.8914, F1 Micro: 0.9322, F1 Macro: 0.9258\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8914, F1 Micro: 0.9322, F1 Macro: 0.9258\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.87      1.00      0.93       187\n",
      "     machine       0.91      0.95      0.93       175\n",
      "      others       0.94      0.70      0.80       158\n",
      "        part       0.87      0.99      0.93       158\n",
      "       price       0.95      1.00      0.97       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.92      0.95      0.93      1061\n",
      "   macro avg       0.92      0.94      0.93      1061\n",
      "weighted avg       0.92      0.95      0.93      1061\n",
      " samples avg       0.92      0.95      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5869, Accuracy: 0.7344, F1 Micro: 0.7344, F1 Macro: 0.4234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5804, Accuracy: 0.7344, F1 Micro: 0.7344, F1 Macro: 0.4234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5324, Accuracy: 0.7344, F1 Micro: 0.7344, F1 Macro: 0.4234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3919, Accuracy: 0.7676, F1 Micro: 0.7676, F1 Macro: 0.553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3335, Accuracy: 0.8299, F1 Micro: 0.8299, F1 Macro: 0.7762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2589, Accuracy: 0.8299, F1 Micro: 0.8299, F1 Macro: 0.7891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.21, Accuracy: 0.8506, F1 Micro: 0.8506, F1 Macro: 0.8203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1694, Accuracy: 0.8631, F1 Micro: 0.8631, F1 Macro: 0.8254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1291, Accuracy: 0.8672, F1 Micro: 0.8672, F1 Macro: 0.8361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1044, Accuracy: 0.8797, F1 Micro: 0.8797, F1 Macro: 0.8534\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8797, F1 Micro: 0.8797, F1 Macro: 0.8534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.86      0.79        64\n",
      "    positive       0.95      0.89      0.92       177\n",
      "\n",
      "    accuracy                           0.88       241\n",
      "   macro avg       0.84      0.87      0.85       241\n",
      "weighted avg       0.89      0.88      0.88       241\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8812, F1 Micro: 0.8812, F1 Macro: 0.7429\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.09      0.17        11\n",
      "     neutral       0.88      1.00      0.94       181\n",
      "    positive       0.89      0.33      0.48        24\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.92      0.47      0.53       216\n",
      "weighted avg       0.89      0.88      0.85       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        16\n",
      "     neutral       0.91      0.96      0.93       167\n",
      "    positive       0.79      0.58      0.67        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.76      0.78       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.75      0.53        12\n",
      "     neutral       0.94      0.70      0.80       152\n",
      "    positive       0.55      0.85      0.67        52\n",
      "\n",
      "    accuracy                           0.74       216\n",
      "   macro avg       0.63      0.77      0.67       216\n",
      "weighted avg       0.82      0.74      0.76       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.61      0.74        23\n",
      "     neutral       0.87      0.99      0.93       152\n",
      "    positive       0.83      0.59      0.69        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.88      0.73      0.78       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.54      0.64        13\n",
      "     neutral       0.94      1.00      0.97       186\n",
      "    positive       0.90      0.53      0.67        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.69      0.76       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 81.85784983634949 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6253, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.517, Accuracy: 0.7879, F1 Micro: 0.8814, F1 Macro: 0.8798\n",
      "Epoch 3/10, Train Loss: 0.5024, Accuracy: 0.7879, F1 Micro: 0.8794, F1 Macro: 0.8763\n",
      "Epoch 4/10, Train Loss: 0.4838, Accuracy: 0.7909, F1 Micro: 0.881, F1 Macro: 0.8781\n",
      "Epoch 5/10, Train Loss: 0.457, Accuracy: 0.7924, F1 Micro: 0.8811, F1 Macro: 0.8776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4301, Accuracy: 0.8013, F1 Micro: 0.8851, F1 Macro: 0.8817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3833, Accuracy: 0.8147, F1 Micro: 0.8919, F1 Macro: 0.8889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3503, Accuracy: 0.8333, F1 Micro: 0.9007, F1 Macro: 0.8972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3145, Accuracy: 0.8557, F1 Micro: 0.913, F1 Macro: 0.9098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2717, Accuracy: 0.8795, F1 Micro: 0.9259, F1 Macro: 0.9225\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.8795, F1 Micro: 0.9259, F1 Macro: 0.9225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.87      0.99      0.93       187\n",
      "     machine       0.90      0.98      0.94       175\n",
      "      others       0.92      0.80      0.85       158\n",
      "        part       0.83      0.97      0.89       158\n",
      "       price       0.90      0.97      0.94       192\n",
      "     service       0.97      0.99      0.98       191\n",
      "\n",
      "   micro avg       0.90      0.95      0.93      1061\n",
      "   macro avg       0.90      0.95      0.92      1061\n",
      "weighted avg       0.90      0.95      0.93      1061\n",
      " samples avg       0.90      0.95      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5304, Accuracy: 0.7429, F1 Micro: 0.7429, F1 Macro: 0.4262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5115, Accuracy: 0.7762, F1 Micro: 0.7762, F1 Macro: 0.5723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4303, Accuracy: 0.8048, F1 Micro: 0.8048, F1 Macro: 0.7291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.303, Accuracy: 0.8333, F1 Micro: 0.8333, F1 Macro: 0.7805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2605, Accuracy: 0.8476, F1 Micro: 0.8476, F1 Macro: 0.8094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1481, Accuracy: 0.8714, F1 Micro: 0.8714, F1 Macro: 0.8286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2258, Accuracy: 0.8952, F1 Micro: 0.8952, F1 Macro: 0.8661\n",
      "Epoch 8/10, Train Loss: 0.1035, Accuracy: 0.8905, F1 Micro: 0.8905, F1 Macro: 0.8623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0811, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.8837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0747, Accuracy: 0.9143, F1 Micro: 0.9143, F1 Macro: 0.8939\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9143, F1 Micro: 0.9143, F1 Macro: 0.8939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.93      0.85        54\n",
      "    positive       0.97      0.91      0.94       156\n",
      "\n",
      "    accuracy                           0.91       210\n",
      "   macro avg       0.88      0.92      0.89       210\n",
      "weighted avg       0.92      0.91      0.92       210\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8719, F1 Micro: 0.8719, F1 Macro: 0.7032\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.18      0.31        11\n",
      "     neutral       0.88      0.99      0.93       181\n",
      "    positive       0.89      0.33      0.48        24\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.92      0.50      0.58       216\n",
      "weighted avg       0.89      0.88      0.85       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.90      0.98      0.94       167\n",
      "    positive       0.89      0.48      0.63        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.76      0.80       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.92      0.80      0.85       152\n",
      "    positive       0.59      0.79      0.68        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.70      0.78      0.73       216\n",
      "weighted avg       0.82      0.79      0.80       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.57      0.68        23\n",
      "     neutral       0.84      0.97      0.90       152\n",
      "    positive       0.79      0.46      0.58        41\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.83      0.67      0.72       216\n",
      "weighted avg       0.83      0.83      0.82       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.25      0.08      0.12        13\n",
      "     neutral       0.90      0.97      0.94       186\n",
      "    positive       0.55      0.35      0.43        17\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.57      0.47      0.49       216\n",
      "weighted avg       0.83      0.87      0.85       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.97      0.99      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 76.53453660011292 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8342, F1 Micro: 0.8342, F1 Macro: 0.5247\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 14.00605320930481 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5843, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.5099, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4802, Accuracy: 0.7946, F1 Micro: 0.8847, F1 Macro: 0.8831\n",
      "Epoch 4/10, Train Loss: 0.4557, Accuracy: 0.7939, F1 Micro: 0.8823, F1 Macro: 0.8793\n",
      "Epoch 5/10, Train Loss: 0.4163, Accuracy: 0.7999, F1 Micro: 0.8836, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3749, Accuracy: 0.8445, F1 Micro: 0.9065, F1 Macro: 0.9032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.298, Accuracy: 0.8847, F1 Micro: 0.9293, F1 Macro: 0.9256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2591, Accuracy: 0.901, F1 Micro: 0.9389, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2058, Accuracy: 0.9092, F1 Micro: 0.943, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1783, Accuracy: 0.9211, F1 Micro: 0.9511, F1 Macro: 0.9485\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9211, F1 Micro: 0.9511, F1 Macro: 0.9485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.98       187\n",
      "     machine       0.90      0.97      0.93       175\n",
      "      others       0.88      0.88      0.88       158\n",
      "        part       0.92      0.97      0.94       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.95      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.93      0.97      0.95      1061\n",
      "   macro avg       0.93      0.97      0.95      1061\n",
      "weighted avg       0.93      0.97      0.95      1061\n",
      " samples avg       0.93      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6447, Accuracy: 0.6913, F1 Micro: 0.6913, F1 Macro: 0.4087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5287, Accuracy: 0.7348, F1 Micro: 0.7348, F1 Macro: 0.5516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3767, Accuracy: 0.8913, F1 Micro: 0.8913, F1 Macro: 0.875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2552, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.116, Accuracy: 0.9217, F1 Micro: 0.9217, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1411, Accuracy: 0.9348, F1 Micro: 0.9348, F1 Macro: 0.9245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1031, Accuracy: 0.9391, F1 Micro: 0.9391, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1265, Accuracy: 0.9391, F1 Micro: 0.9391, F1 Macro: 0.9303\n",
      "Epoch 9/10, Train Loss: 0.1272, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.8981\n",
      "Epoch 10/10, Train Loss: 0.0888, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9197\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9391, F1 Micro: 0.9391, F1 Macro: 0.9303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        71\n",
      "    positive       0.97      0.94      0.96       159\n",
      "\n",
      "    accuracy                           0.94       230\n",
      "   macro avg       0.92      0.94      0.93       230\n",
      "weighted avg       0.94      0.94      0.94       230\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9128, F1 Micro: 0.9128, F1 Macro: 0.8236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.97      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.90      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.90      0.97      0.93       167\n",
      "    positive       0.74      0.52      0.61        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.72      0.77       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.58      0.61        12\n",
      "     neutral       0.88      0.88      0.88       152\n",
      "    positive       0.69      0.71      0.70        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.73      0.72      0.73       216\n",
      "weighted avg       0.82      0.82      0.82       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.85      0.71      0.77        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       0.75      0.53      0.62        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.74      0.79       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.95      1.00      0.98       185\n",
      "    positive       1.00      0.65      0.79        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.79      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Total train time: 87.58491659164429 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5995, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5018, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.468, Accuracy: 0.7976, F1 Micro: 0.886, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4334, Accuracy: 0.8251, F1 Micro: 0.8988, F1 Macro: 0.8969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3845, Accuracy: 0.8579, F1 Micro: 0.9156, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.334, Accuracy: 0.8981, F1 Micro: 0.9379, F1 Macro: 0.9359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2586, Accuracy: 0.9234, F1 Micro: 0.9524, F1 Macro: 0.9499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2244, Accuracy: 0.9308, F1 Micro: 0.9565, F1 Macro: 0.9533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1798, Accuracy: 0.933, F1 Micro: 0.958, F1 Macro: 0.9548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1537, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9631\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9631\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.92      0.89      0.90       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6027, Accuracy: 0.6892, F1 Micro: 0.6892, F1 Macro: 0.408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4975, Accuracy: 0.7211, F1 Micro: 0.7211, F1 Macro: 0.5089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3544, Accuracy: 0.8805, F1 Micro: 0.8805, F1 Macro: 0.8642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1952, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1638, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1069, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9228\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9107\n",
      "Epoch 9/10, Train Loss: 0.0285, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.895\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9005\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9228\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        78\n",
      "    positive       0.97      0.93      0.95       173\n",
      "\n",
      "    accuracy                           0.93       251\n",
      "   macro avg       0.91      0.93      0.92       251\n",
      "weighted avg       0.94      0.93      0.93       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.8536\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.85      0.67      0.75        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.80      0.82       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.25      0.40        12\n",
      "     neutral       0.92      0.89      0.90       152\n",
      "    positive       0.64      0.81      0.71        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.85      0.65      0.67       216\n",
      "weighted avg       0.86      0.83      0.83       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.88      0.71      0.78        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 91.42718505859375 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5908, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.52, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4927, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4677, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4312, Accuracy: 0.8185, F1 Micro: 0.8961, F1 Macro: 0.8946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4036, Accuracy: 0.8497, F1 Micro: 0.911, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3169, Accuracy: 0.8966, F1 Micro: 0.9372, F1 Macro: 0.9353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2647, Accuracy: 0.9226, F1 Micro: 0.9521, F1 Macro: 0.9497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.214, Accuracy: 0.9308, F1 Micro: 0.9569, F1 Macro: 0.954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.185, Accuracy: 0.9345, F1 Micro: 0.959, F1 Macro: 0.9568\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9345, F1 Micro: 0.959, F1 Macro: 0.9568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.93      0.94       175\n",
      "      others       0.92      0.89      0.90       158\n",
      "        part       0.91      0.99      0.95       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.97      0.96      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.95      0.97      0.96      1061\n",
      " samples avg       0.95      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6461, Accuracy: 0.7045, F1 Micro: 0.7045, F1 Macro: 0.4133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5282, Accuracy: 0.8381, F1 Micro: 0.8381, F1 Macro: 0.8085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3526, Accuracy: 0.8947, F1 Micro: 0.8947, F1 Macro: 0.8726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2111, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.182, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9268\n",
      "Epoch 6/10, Train Loss: 0.172, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.924\n",
      "Epoch 7/10, Train Loss: 0.0624, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9201\n",
      "Epoch 8/10, Train Loss: 0.0675, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9319\n",
      "Epoch 10/10, Train Loss: 0.0839, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9119\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.90      0.90        73\n",
      "    positive       0.96      0.96      0.96       174\n",
      "\n",
      "    accuracy                           0.94       247\n",
      "   macro avg       0.93      0.93      0.93       247\n",
      "weighted avg       0.94      0.94      0.94       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9244, F1 Micro: 0.9244, F1 Macro: 0.849\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.91      0.87        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.83      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.94      0.93      0.94       167\n",
      "    positive       0.70      0.70      0.70        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.81      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.92      0.89      0.91       152\n",
      "    positive       0.72      0.75      0.74        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.76      0.80      0.78       216\n",
      "weighted avg       0.86      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.96      0.63      0.76        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.83      0.86       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.54      0.64        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.73      0.79       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 89.84626126289368 s\n",
      "Averaged - Iteration 208: Accuracy: 0.8638, F1 Micro: 0.8638, F1 Macro: 0.6305\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 13.677143573760986 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5807, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.5254, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4811, Accuracy: 0.7976, F1 Micro: 0.8861, F1 Macro: 0.8845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4344, Accuracy: 0.8051, F1 Micro: 0.888, F1 Macro: 0.8855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3795, Accuracy: 0.8519, F1 Micro: 0.9111, F1 Macro: 0.9082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3214, Accuracy: 0.8988, F1 Micro: 0.938, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2577, Accuracy: 0.9211, F1 Micro: 0.9509, F1 Macro: 0.9484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1989, Accuracy: 0.9308, F1 Micro: 0.9564, F1 Macro: 0.9539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1629, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9646\n",
      "Epoch 10/10, Train Loss: 0.1299, Accuracy: 0.9397, F1 Micro: 0.9621, F1 Macro: 0.9591\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.91      0.94      0.92       158\n",
      "        part       0.92      0.99      0.95       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.97      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5728, Accuracy: 0.6738, F1 Micro: 0.6738, F1 Macro: 0.4026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4773, Accuracy: 0.7382, F1 Micro: 0.7382, F1 Macro: 0.5903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3333, Accuracy: 0.8884, F1 Micro: 0.8884, F1 Macro: 0.8747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1983, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1443, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1752, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9243\n",
      "Epoch 7/10, Train Loss: 0.1612, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9121\n",
      "Epoch 8/10, Train Loss: 0.1241, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0615, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9265\n",
      "Epoch 10/10, Train Loss: 0.0529, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9148\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90        76\n",
      "    positive       0.95      0.96      0.95       157\n",
      "\n",
      "    accuracy                           0.94       233\n",
      "   macro avg       0.93      0.93      0.93       233\n",
      "weighted avg       0.94      0.94      0.94       233\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.8329\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.75      0.73        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.95      0.61      0.74        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.78      0.81       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      0.75      0.44        12\n",
      "     neutral       0.94      0.85      0.89       152\n",
      "    positive       0.80      0.77      0.78        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.68      0.79      0.71       216\n",
      "weighted avg       0.87      0.82      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.87      0.77        23\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.93      0.63      0.75        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.83      0.82       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.79      0.81        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.65      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.81      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 97.89350438117981 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.586, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5216, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4633, Accuracy: 0.8065, F1 Micro: 0.8896, F1 Macro: 0.8876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4058, Accuracy: 0.8542, F1 Micro: 0.9141, F1 Macro: 0.9124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3276, Accuracy: 0.9115, F1 Micro: 0.9458, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2788, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2308, Accuracy: 0.939, F1 Micro: 0.9618, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1769, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1498, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9672\n",
      "Epoch 10/10, Train Loss: 0.1152, Accuracy: 0.9464, F1 Micro: 0.9661, F1 Macro: 0.9635\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.91      0.99      0.95       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6008, Accuracy: 0.6652, F1 Micro: 0.6652, F1 Macro: 0.3995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4976, Accuracy: 0.8652, F1 Micro: 0.8652, F1 Macro: 0.8361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3015, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1921, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9465\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9217, F1 Micro: 0.9217, F1 Macro: 0.9152\n",
      "Epoch 6/10, Train Loss: 0.152, Accuracy: 0.9217, F1 Micro: 0.9217, F1 Macro: 0.9121\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9102\n",
      "Epoch 8/10, Train Loss: 0.0715, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9229\n",
      "Epoch 9/10, Train Loss: 0.0511, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9238\n",
      "Epoch 10/10, Train Loss: 0.0397, Accuracy: 0.9391, F1 Micro: 0.9391, F1 Macro: 0.9329\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        77\n",
      "    positive       0.97      0.96      0.96       153\n",
      "\n",
      "    accuracy                           0.95       230\n",
      "   macro avg       0.95      0.95      0.95       230\n",
      "weighted avg       0.95      0.95      0.95       230\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9282, F1 Micro: 0.9282, F1 Macro: 0.8505\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.81      0.68        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.91      0.61      0.73        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.82      0.80      0.79       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      1.00      0.47        12\n",
      "     neutral       0.97      0.85      0.91       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.72      0.87      0.73       216\n",
      "weighted avg       0.91      0.83      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.96      0.80        23\n",
      "     neutral       0.95      0.98      0.96       152\n",
      "    positive       0.96      0.63      0.76        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.86      0.84       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.72      0.76      0.74        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.85      0.84      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 93.95363354682922 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5877, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.5247, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4824, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4519, Accuracy: 0.8088, F1 Micro: 0.8913, F1 Macro: 0.8897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3895, Accuracy: 0.8646, F1 Micro: 0.9198, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3308, Accuracy: 0.9182, F1 Micro: 0.95, F1 Macro: 0.9478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2632, Accuracy: 0.9375, F1 Micro: 0.9612, F1 Macro: 0.9592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2068, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1676, Accuracy: 0.942, F1 Micro: 0.964, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1312, Accuracy: 0.9435, F1 Micro: 0.9644, F1 Macro: 0.9618\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9435, F1 Micro: 0.9644, F1 Macro: 0.9618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.95      0.86      0.90       158\n",
      "        part       0.93      0.98      0.95       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.97      0.96      1061\n",
      "   macro avg       0.96      0.97      0.96      1061\n",
      "weighted avg       0.96      0.97      0.96      1061\n",
      " samples avg       0.96      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5777, Accuracy: 0.6988, F1 Micro: 0.6988, F1 Macro: 0.4349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4855, Accuracy: 0.8726, F1 Micro: 0.8726, F1 Macro: 0.8406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2268, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1856, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1815, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9166\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.8971\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.8996\n",
      "Epoch 9/10, Train Loss: 0.092, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9005\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8949\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.94      0.89        80\n",
      "    positive       0.97      0.92      0.95       179\n",
      "\n",
      "    accuracy                           0.93       259\n",
      "   macro avg       0.91      0.93      0.92       259\n",
      "weighted avg       0.93      0.93      0.93       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.8599\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.87      0.61      0.71        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.80      0.81       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.96      0.86      0.91       152\n",
      "    positive       0.67      0.90      0.77        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.84      0.81      0.81       216\n",
      "weighted avg       0.88      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.91      0.82        23\n",
      "     neutral       0.93      0.98      0.95       152\n",
      "    positive       0.96      0.63      0.76        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.84      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.79      0.85        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.83      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 97.53656649589539 s\n",
      "Averaged - Iteration 274: Accuracy: 0.8796, F1 Micro: 0.8796, F1 Macro: 0.6848\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 12.448424339294434 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5593, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4949, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4689, Accuracy: 0.8058, F1 Micro: 0.8895, F1 Macro: 0.8877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4119, Accuracy: 0.8549, F1 Micro: 0.9128, F1 Macro: 0.9101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3126, Accuracy: 0.904, F1 Micro: 0.9406, F1 Macro: 0.9381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2516, Accuracy: 0.936, F1 Micro: 0.96, F1 Macro: 0.9581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1916, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9662\n",
      "Epoch 8/10, Train Loss: 0.1482, Accuracy: 0.9412, F1 Micro: 0.9628, F1 Macro: 0.9596\n",
      "Epoch 9/10, Train Loss: 0.1287, Accuracy: 0.9427, F1 Micro: 0.9637, F1 Macro: 0.9602\n",
      "Epoch 10/10, Train Loss: 0.0966, Accuracy: 0.9479, F1 Micro: 0.967, F1 Macro: 0.9637\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9662\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.92      0.92      0.92       158\n",
      "        part       0.93      0.98      0.96       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6069, Accuracy: 0.6892, F1 Micro: 0.6892, F1 Macro: 0.408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4418, Accuracy: 0.8566, F1 Micro: 0.8566, F1 Macro: 0.8168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2398, Accuracy: 0.8964, F1 Micro: 0.8964, F1 Macro: 0.8764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2094, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1382, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1524, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1162, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9016\n",
      "Epoch 8/10, Train Loss: 0.1256, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0649, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0666, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.904\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.90      0.87        78\n",
      "    positive       0.95      0.92      0.94       173\n",
      "\n",
      "    accuracy                           0.92       251\n",
      "   macro avg       0.90      0.91      0.90       251\n",
      "weighted avg       0.92      0.92      0.92       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.8462\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.75      0.71        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.83      0.80      0.81       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.32      0.83      0.47        12\n",
      "     neutral       0.96      0.82      0.89       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.69      0.83      0.72       216\n",
      "weighted avg       0.88      0.82      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.83      0.81        23\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.76      0.82        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.90      0.88       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.86      0.80        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.85      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 105.59668302536011 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5686, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4906, Accuracy: 0.7999, F1 Micro: 0.8868, F1 Macro: 0.8852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4528, Accuracy: 0.8341, F1 Micro: 0.9045, F1 Macro: 0.9034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3692, Accuracy: 0.8996, F1 Micro: 0.9382, F1 Macro: 0.9355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.284, Accuracy: 0.9338, F1 Micro: 0.9588, F1 Macro: 0.9572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2272, Accuracy: 0.9464, F1 Micro: 0.9664, F1 Macro: 0.9652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1773, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9694\n",
      "Epoch 8/10, Train Loss: 0.1406, Accuracy: 0.9412, F1 Micro: 0.9628, F1 Macro: 0.9595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1216, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0937, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9741\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.97      0.95       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5757, Accuracy: 0.6811, F1 Micro: 0.6811, F1 Macro: 0.4052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4426, Accuracy: 0.8583, F1 Micro: 0.8583, F1 Macro: 0.8193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2239, Accuracy: 0.9055, F1 Micro: 0.9055, F1 Macro: 0.8882\n",
      "Epoch 4/10, Train Loss: 0.2079, Accuracy: 0.8898, F1 Micro: 0.8898, F1 Macro: 0.8643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1437, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1289, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0902, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0746, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9126\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.8947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9118\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9118\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.84      0.88        81\n",
      "    positive       0.93      0.97      0.95       173\n",
      "\n",
      "    accuracy                           0.93       254\n",
      "   macro avg       0.92      0.90      0.91       254\n",
      "weighted avg       0.92      0.93      0.92       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8804\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.85      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.62      0.71        16\n",
      "     neutral       0.94      0.97      0.95       167\n",
      "    positive       0.74      0.70      0.72        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.76      0.80       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.90      0.83      0.86        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.90      0.90      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.86      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 107.38289475440979 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5758, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5001, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4822, Accuracy: 0.7999, F1 Micro: 0.8873, F1 Macro: 0.8859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4245, Accuracy: 0.8646, F1 Micro: 0.92, F1 Macro: 0.9189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3291, Accuracy: 0.9234, F1 Micro: 0.9526, F1 Macro: 0.9506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2659, Accuracy: 0.933, F1 Micro: 0.9577, F1 Macro: 0.9552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2027, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1608, Accuracy: 0.9479, F1 Micro: 0.967, F1 Macro: 0.9641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.13, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0979, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9678\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9678\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.97      0.95       175\n",
      "      others       0.94      0.91      0.93       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5943, Accuracy: 0.6836, F1 Micro: 0.6836, F1 Macro: 0.406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4429, Accuracy: 0.8594, F1 Micro: 0.8594, F1 Macro: 0.8341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2327, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9207\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9124\n",
      "Epoch 6/10, Train Loss: 0.1562, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9058\n",
      "Epoch 7/10, Train Loss: 0.1085, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8862\n",
      "Epoch 8/10, Train Loss: 0.094, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.8835\n",
      "Epoch 9/10, Train Loss: 0.0843, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8874\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.8875\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9207\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.94      0.89        81\n",
      "    positive       0.97      0.93      0.95       175\n",
      "\n",
      "    accuracy                           0.93       256\n",
      "   macro avg       0.91      0.93      0.92       256\n",
      "weighted avg       0.93      0.93      0.93       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9367, F1 Micro: 0.9367, F1 Macro: 0.8584\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        16\n",
      "     neutral       0.94      0.97      0.96       167\n",
      "    positive       0.82      0.70      0.75        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.81      0.82       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.33      0.47        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.70      0.87      0.78        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.82      0.70      0.73       216\n",
      "weighted avg       0.88      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.93      0.68      0.79        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 103.60045313835144 s\n",
      "Averaged - Iteration 333: Accuracy: 0.8912, F1 Micro: 0.8912, F1 Macro: 0.7202\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 11.655288934707642 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5632, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4802, Accuracy: 0.7961, F1 Micro: 0.8855, F1 Macro: 0.884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4407, Accuracy: 0.8185, F1 Micro: 0.8948, F1 Macro: 0.8924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.357, Accuracy: 0.8914, F1 Micro: 0.9337, F1 Macro: 0.9311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2691, Accuracy: 0.936, F1 Micro: 0.9601, F1 Macro: 0.9583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.202, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.97\n",
      "Epoch 7/10, Train Loss: 0.1514, Accuracy: 0.9479, F1 Micro: 0.9672, F1 Macro: 0.9651\n",
      "Epoch 8/10, Train Loss: 0.1226, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0968, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Epoch 10/10, Train Loss: 0.0803, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.89      0.97      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5454, Accuracy: 0.6653, F1 Micro: 0.6653, F1 Macro: 0.3995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3401, Accuracy: 0.8939, F1 Micro: 0.8939, F1 Macro: 0.8847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2046, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9323\n",
      "Epoch 4/10, Train Loss: 0.1505, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9241\n",
      "Epoch 5/10, Train Loss: 0.1439, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9241\n",
      "Epoch 6/10, Train Loss: 0.1382, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9271\n",
      "Epoch 7/10, Train Loss: 0.1387, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9287\n",
      "Epoch 8/10, Train Loss: 0.091, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9236\n",
      "Epoch 9/10, Train Loss: 0.097, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9137\n",
      "Epoch 10/10, Train Loss: 0.0666, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9287\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        82\n",
      "    positive       0.97      0.94      0.95       163\n",
      "\n",
      "    accuracy                           0.94       245\n",
      "   macro avg       0.93      0.94      0.93       245\n",
      "weighted avg       0.94      0.94      0.94       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8925\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.75      0.64        12\n",
      "     neutral       0.90      0.95      0.92       152\n",
      "    positive       0.90      0.69      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.80      0.78       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.92      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 106.1630003452301 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5682, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4715, Accuracy: 0.808, F1 Micro: 0.8914, F1 Macro: 0.89\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3968, Accuracy: 0.8817, F1 Micro: 0.9295, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3082, Accuracy: 0.9382, F1 Micro: 0.9617, F1 Macro: 0.9603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2332, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1742, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Epoch 7/10, Train Loss: 0.1349, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "Epoch 8/10, Train Loss: 0.1115, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.094, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Epoch 10/10, Train Loss: 0.0829, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.88      0.98      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5184, Accuracy: 0.6787, F1 Micro: 0.6787, F1 Macro: 0.4043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4323, Accuracy: 0.8916, F1 Micro: 0.8916, F1 Macro: 0.8697\n",
      "Epoch 3/10, Train Loss: 0.261, Accuracy: 0.8795, F1 Micro: 0.8795, F1 Macro: 0.8713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1675, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1723, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9498\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9351\n",
      "Epoch 7/10, Train Loss: 0.1305, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9367\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9371\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9209\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.932\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        80\n",
      "    positive       0.98      0.96      0.97       169\n",
      "\n",
      "    accuracy                           0.96       249\n",
      "   macro avg       0.95      0.95      0.95       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.8965\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.90      0.98      0.94       152\n",
      "    positive       0.92      0.69      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.78      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.95      0.94      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 108.64385652542114 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5688, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4874, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4535, Accuracy: 0.814, F1 Micro: 0.8938, F1 Macro: 0.8922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3813, Accuracy: 0.9055, F1 Micro: 0.9428, F1 Macro: 0.9413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2834, Accuracy: 0.939, F1 Micro: 0.9622, F1 Macro: 0.9607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2046, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.157, Accuracy: 0.9501, F1 Micro: 0.9686, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1274, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0998, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0831, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5634, Accuracy: 0.6759, F1 Micro: 0.6759, F1 Macro: 0.4033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3953, Accuracy: 0.8775, F1 Micro: 0.8775, F1 Macro: 0.8548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2952, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1508, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9245\n",
      "Epoch 5/10, Train Loss: 0.1536, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9212\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9138\n",
      "Epoch 7/10, Train Loss: 0.1297, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9198\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9177\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.924\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.90        82\n",
      "    positive       0.96      0.94      0.95       171\n",
      "\n",
      "    accuracy                           0.93       253\n",
      "   macro avg       0.92      0.93      0.92       253\n",
      "weighted avg       0.93      0.93      0.93       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8967\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.58      0.74        12\n",
      "     neutral       0.90      0.96      0.93       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.92      0.77      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 117.75918769836426 s\n",
      "Averaged - Iteration 387: Accuracy: 0.901, F1 Micro: 0.901, F1 Macro: 0.7494\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 11.139630556106567 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5516, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4938, Accuracy: 0.7939, F1 Micro: 0.8843, F1 Macro: 0.8828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4366, Accuracy: 0.817, F1 Micro: 0.8953, F1 Macro: 0.8936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3519, Accuracy: 0.9062, F1 Micro: 0.9425, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2607, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1921, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.967\n",
      "Epoch 7/10, Train Loss: 0.1441, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9668\n",
      "Epoch 8/10, Train Loss: 0.1135, Accuracy: 0.9494, F1 Micro: 0.9681, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0961, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "Epoch 10/10, Train Loss: 0.0788, Accuracy: 0.9516, F1 Micro: 0.9694, F1 Macro: 0.9671\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.92      0.92       158\n",
      "        part       0.96      0.97      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5409, Accuracy: 0.6834, F1 Micro: 0.6834, F1 Macro: 0.406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3644, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2301, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.154, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1592, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.142, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9465\n",
      "Epoch 7/10, Train Loss: 0.1037, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9253\n",
      "Epoch 8/10, Train Loss: 0.1073, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.9023\n",
      "Epoch 9/10, Train Loss: 0.0906, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9189\n",
      "Epoch 10/10, Train Loss: 0.0955, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9212\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        82\n",
      "    positive       0.97      0.97      0.97       177\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.95      0.95      0.95       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.8746\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.69      0.73        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.81      0.83       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.83      0.65        12\n",
      "     neutral       0.92      0.88      0.90       152\n",
      "    positive       0.78      0.77      0.78        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.74      0.83      0.77       216\n",
      "weighted avg       0.86      0.85      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.88      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.93      0.90        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 117.07287764549255 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5639, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4808, Accuracy: 0.7999, F1 Micro: 0.8874, F1 Macro: 0.8859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3938, Accuracy: 0.8787, F1 Micro: 0.9278, F1 Macro: 0.9266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2986, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2221, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1667, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1296, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9741\n",
      "Epoch 8/10, Train Loss: 0.1055, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9707\n",
      "Epoch 9/10, Train Loss: 0.0951, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9722\n",
      "Epoch 10/10, Train Loss: 0.0754, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9708\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.88      0.99      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5997, Accuracy: 0.6582, F1 Micro: 0.6582, F1 Macro: 0.3969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4185, Accuracy: 0.8776, F1 Micro: 0.8776, F1 Macro: 0.8559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2862, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9351\n",
      "Epoch 4/10, Train Loss: 0.1713, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9143\n",
      "Epoch 5/10, Train Loss: 0.1503, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9226\n",
      "Epoch 6/10, Train Loss: 0.139, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.103, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1056, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9351\n",
      "Epoch 9/10, Train Loss: 0.0884, Accuracy: 0.9367, F1 Micro: 0.9367, F1 Macro: 0.9314\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.921\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.92        81\n",
      "    positive       0.97      0.94      0.95       156\n",
      "\n",
      "    accuracy                           0.94       237\n",
      "   macro avg       0.93      0.94      0.94       237\n",
      "weighted avg       0.94      0.94      0.94       237\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.8683\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.75      0.71        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.80      0.81       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      0.75      0.44        12\n",
      "     neutral       0.92      0.92      0.92       152\n",
      "    positive       0.97      0.65      0.78        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.73      0.77      0.71       216\n",
      "weighted avg       0.90      0.85      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.87      0.82        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.87      0.83      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.89      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 113.16850185394287 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5526, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4974, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4386, Accuracy: 0.8274, F1 Micro: 0.9011, F1 Macro: 0.8999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.349, Accuracy: 0.9271, F1 Micro: 0.955, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2576, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1891, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9708\n",
      "Epoch 7/10, Train Loss: 0.1443, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.97\n",
      "Epoch 8/10, Train Loss: 0.1161, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9677\n",
      "Epoch 9/10, Train Loss: 0.0948, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9675\n",
      "Epoch 10/10, Train Loss: 0.0773, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9675\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.91      0.98      0.95       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.95      1.00      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5876, Accuracy: 0.6623, F1 Micro: 0.6623, F1 Macro: 0.3984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.373, Accuracy: 0.9307, F1 Micro: 0.9307, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1782, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9342\n",
      "Epoch 4/10, Train Loss: 0.1092, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9525\n",
      "Epoch 6/10, Train Loss: 0.0793, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9335\n",
      "Epoch 7/10, Train Loss: 0.0989, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9252\n",
      "Epoch 8/10, Train Loss: 0.0989, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9207\n",
      "Epoch 9/10, Train Loss: 0.1006, Accuracy: 0.9437, F1 Micro: 0.9437, F1 Macro: 0.9373\n",
      "Epoch 10/10, Train Loss: 0.0755, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9289\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.94        78\n",
      "    positive       0.99      0.95      0.97       153\n",
      "\n",
      "    accuracy                           0.96       231\n",
      "   macro avg       0.95      0.96      0.95       231\n",
      "weighted avg       0.96      0.96      0.96       231\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9329, F1 Micro: 0.9329, F1 Macro: 0.8561\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.54      0.81      0.65        16\n",
      "     neutral       0.94      0.97      0.96       167\n",
      "    positive       0.90      0.55      0.68        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.79      0.78      0.76       216\n",
      "weighted avg       0.91      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.36      0.83      0.50        12\n",
      "     neutral       0.93      0.89      0.91       152\n",
      "    positive       0.93      0.73      0.82        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.74      0.82      0.74       216\n",
      "weighted avg       0.89      0.85      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.87      0.77        23\n",
      "     neutral       0.95      0.97      0.96       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.87      0.86       216\n",
      "weighted avg       0.93      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 110.32820916175842 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9061, F1 Micro: 0.9061, F1 Macro: 0.7661\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 9.536176204681396 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5372, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4968, Accuracy: 0.8021, F1 Micro: 0.8878, F1 Macro: 0.8859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4125, Accuracy: 0.8653, F1 Micro: 0.919, F1 Macro: 0.9168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3112, Accuracy: 0.9189, F1 Micro: 0.9496, F1 Macro: 0.9465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2356, Accuracy: 0.9464, F1 Micro: 0.9662, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1701, Accuracy: 0.9472, F1 Micro: 0.9669, F1 Macro: 0.9649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1336, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1052, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5495, Accuracy: 0.6863, F1 Micro: 0.6863, F1 Macro: 0.4507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3419, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9431\n",
      "Epoch 3/10, Train Loss: 0.1979, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9186\n",
      "Epoch 4/10, Train Loss: 0.1427, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9393\n",
      "Epoch 5/10, Train Loss: 0.1071, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9268\n",
      "Epoch 6/10, Train Loss: 0.1535, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Epoch 7/10, Train Loss: 0.0784, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1209, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9431\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8982\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.84      0.73      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.81      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.93      0.90        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 122.87497091293335 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5394, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4811, Accuracy: 0.8371, F1 Micro: 0.9056, F1 Macro: 0.9052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3698, Accuracy: 0.9167, F1 Micro: 0.9494, F1 Macro: 0.9482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2675, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2053, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1548, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Epoch 7/10, Train Loss: 0.1251, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0992, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9763\n",
      "Epoch 9/10, Train Loss: 0.0816, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9759\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5764, Accuracy: 0.696, F1 Micro: 0.696, F1 Macro: 0.4759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3532, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1589, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9465\n",
      "Epoch 4/10, Train Loss: 0.1386, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.8996\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9337\n",
      "Epoch 6/10, Train Loss: 0.1364, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9291\n",
      "Epoch 7/10, Train Loss: 0.0957, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9269\n",
      "Epoch 8/10, Train Loss: 0.12, Accuracy: 0.912, F1 Micro: 0.912, F1 Macro: 0.8975\n",
      "Epoch 9/10, Train Loss: 0.1177, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9193\n",
      "Epoch 10/10, Train Loss: 0.0727, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9207\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        81\n",
      "    positive       0.99      0.94      0.96       169\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.94      0.96      0.95       250\n",
      "weighted avg       0.96      0.95      0.95       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8821\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.81      0.74        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.85      0.70      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.83      0.83      0.82       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      0.75      0.51        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.75      0.80      0.75       216\n",
      "weighted avg       0.90      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 115.79092144966125 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5488, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5027, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4255, Accuracy: 0.8549, F1 Micro: 0.9151, F1 Macro: 0.9143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3217, Accuracy: 0.9308, F1 Micro: 0.9575, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2363, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1739, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9708\n",
      "Epoch 7/10, Train Loss: 0.1369, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9694\n",
      "Epoch 8/10, Train Loss: 0.1131, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0876, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "Epoch 10/10, Train Loss: 0.075, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9722\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.97      0.95       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.574, Accuracy: 0.8745, F1 Micro: 0.8745, F1 Macro: 0.851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2776, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.173, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Epoch 4/10, Train Loss: 0.1829, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1146, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Epoch 6/10, Train Loss: 0.13, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9294\n",
      "Epoch 7/10, Train Loss: 0.127, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9369\n",
      "Epoch 8/10, Train Loss: 0.1183, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9294\n",
      "Epoch 9/10, Train Loss: 0.0803, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.924\n",
      "Epoch 10/10, Train Loss: 0.0734, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.929\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        82\n",
      "    positive       0.97      0.95      0.96       165\n",
      "\n",
      "    accuracy                           0.95       247\n",
      "   macro avg       0.94      0.95      0.94       247\n",
      "weighted avg       0.95      0.95      0.95       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8854\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.94      0.96      0.95       167\n",
      "    positive       0.85      0.70      0.77        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.75      0.62        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.79      0.82      0.80       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 116.9341812133789 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9113, F1 Micro: 0.9113, F1 Macro: 0.7814\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 8.906419277191162 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5502, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4837, Accuracy: 0.8013, F1 Micro: 0.888, F1 Macro: 0.8863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.386, Accuracy: 0.8988, F1 Micro: 0.9388, F1 Macro: 0.937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2715, Accuracy: 0.9427, F1 Micro: 0.9642, F1 Macro: 0.962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2016, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1453, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1131, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9728\n",
      "Epoch 8/10, Train Loss: 0.0896, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9719\n",
      "Epoch 9/10, Train Loss: 0.0753, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0629, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5266, Accuracy: 0.712, F1 Micro: 0.712, F1 Macro: 0.5359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3048, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9219\n",
      "Epoch 3/10, Train Loss: 0.1985, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1495, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1171, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9295\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9253\n",
      "Epoch 7/10, Train Loss: 0.1211, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9151\n",
      "Epoch 8/10, Train Loss: 0.0862, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9257\n",
      "Epoch 9/10, Train Loss: 0.0956, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9219\n",
      "Epoch 10/10, Train Loss: 0.0755, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9173\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        83\n",
      "    positive       0.97      0.93      0.95       167\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.92      0.94      0.93       250\n",
      "weighted avg       0.94      0.94      0.94       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9011\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.88      0.71      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.81      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 127.09902763366699 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5575, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4678, Accuracy: 0.843, F1 Micro: 0.9093, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3454, Accuracy: 0.9263, F1 Micro: 0.9548, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2407, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1341, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9755\n",
      "Epoch 7/10, Train Loss: 0.1047, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9729\n",
      "Epoch 8/10, Train Loss: 0.0841, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9761\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5469, Accuracy: 0.6772, F1 Micro: 0.6772, F1 Macro: 0.4038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3378, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9263\n",
      "Epoch 3/10, Train Loss: 0.2085, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1645, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9335\n",
      "Epoch 5/10, Train Loss: 0.1457, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1363, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "Epoch 7/10, Train Loss: 0.1216, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Epoch 8/10, Train Loss: 0.1067, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9088\n",
      "Epoch 10/10, Train Loss: 0.0873, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9116\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.91        82\n",
      "    positive       0.98      0.93      0.96       172\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.92      0.95      0.93       254\n",
      "weighted avg       0.94      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.8958\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.81      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 128.84039044380188 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5529, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4952, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4023, Accuracy: 0.9048, F1 Micro: 0.9426, F1 Macro: 0.9414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2777, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.9643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1968, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1473, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9717\n",
      "Epoch 7/10, Train Loss: 0.1108, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9695\n",
      "Epoch 8/10, Train Loss: 0.0857, Accuracy: 0.9516, F1 Micro: 0.9694, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9722\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.97      0.95       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5514, Accuracy: 0.6718, F1 Micro: 0.6718, F1 Macro: 0.4018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3363, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1743, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9149\n",
      "Epoch 5/10, Train Loss: 0.1429, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9068\n",
      "Epoch 6/10, Train Loss: 0.1242, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9073\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9189\n",
      "Epoch 9/10, Train Loss: 0.0577, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        85\n",
      "    positive       0.98      0.91      0.95       174\n",
      "\n",
      "    accuracy                           0.93       259\n",
      "   macro avg       0.91      0.94      0.92       259\n",
      "weighted avg       0.94      0.93      0.93       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.9014\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.97      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.97      0.71      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 131.877427816391 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9158, F1 Micro: 0.9158, F1 Macro: 0.7945\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 7.901094198226929 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5421, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4808, Accuracy: 0.8103, F1 Micro: 0.8905, F1 Macro: 0.888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3886, Accuracy: 0.9003, F1 Micro: 0.9386, F1 Macro: 0.9355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2743, Accuracy: 0.9405, F1 Micro: 0.9631, F1 Macro: 0.9619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1988, Accuracy: 0.9501, F1 Micro: 0.9687, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1484, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1194, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 8/10, Train Loss: 0.0965, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9737\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5504, Accuracy: 0.8441, F1 Micro: 0.8441, F1 Macro: 0.8014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2848, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.189, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1445, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1532, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "Epoch 6/10, Train Loss: 0.1208, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1016, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.8984\n",
      "Epoch 10/10, Train Loss: 0.0634, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.8991\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        83\n",
      "    positive       0.98      0.92      0.95       180\n",
      "\n",
      "    accuracy                           0.93       263\n",
      "   macro avg       0.91      0.94      0.92       263\n",
      "weighted avg       0.94      0.93      0.93       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8929\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.88      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.62      0.69        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.78      0.85      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.81      0.82       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.75896167755127 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5436, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4559, Accuracy: 0.8586, F1 Micro: 0.9162, F1 Macro: 0.9145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3447, Accuracy: 0.9301, F1 Micro: 0.957, F1 Macro: 0.9556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2447, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.969\n",
      "Epoch 5/10, Train Loss: 0.186, Accuracy: 0.9501, F1 Micro: 0.9687, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1378, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1136, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9771\n",
      "Epoch 8/10, Train Loss: 0.0943, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9737\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.9561, F1 Micro: 0.9722, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.98      0.97      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5759, Accuracy: 0.6908, F1 Micro: 0.6908, F1 Macro: 0.4705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3295, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1976, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1491, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9404\n",
      "Epoch 5/10, Train Loss: 0.1361, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Epoch 6/10, Train Loss: 0.1143, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9175\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9275\n",
      "Epoch 8/10, Train Loss: 0.0973, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.917\n",
      "Epoch 9/10, Train Loss: 0.0658, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9275\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9275\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        84\n",
      "    positive       0.99      0.93      0.96       178\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.93      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9106\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.98      0.97      0.97       167\n",
      "    positive       0.83      0.88      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.48387622833252 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5471, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4874, Accuracy: 0.8058, F1 Micro: 0.8902, F1 Macro: 0.8886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4007, Accuracy: 0.9025, F1 Micro: 0.9412, F1 Macro: 0.9397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2809, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.9629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2005, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1499, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1169, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9731\n",
      "Epoch 9/10, Train Loss: 0.0747, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9723\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9721\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5629, Accuracy: 0.8549, F1 Micro: 0.8549, F1 Macro: 0.817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3019, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1952, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1353, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 5/10, Train Loss: 0.1344, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9174\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9182\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9178\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9128\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.918\n",
      "Epoch 10/10, Train Loss: 0.052, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9059\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.98      0.91        82\n",
      "    positive       0.99      0.92      0.95       173\n",
      "\n",
      "    accuracy                           0.94       255\n",
      "   macro avg       0.92      0.95      0.93       255\n",
      "weighted avg       0.94      0.94      0.94       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.8715\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.88      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.75      0.63        16\n",
      "     neutral       0.98      0.95      0.96       167\n",
      "    positive       0.77      0.73      0.75        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.77      0.81      0.78       216\n",
      "weighted avg       0.91      0.90      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.67      0.64        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.80      0.79       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.90      0.89       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.3077473640442 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.8042\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 7.894679069519043 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.539, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4783, Accuracy: 0.8058, F1 Micro: 0.8899, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3766, Accuracy: 0.9196, F1 Micro: 0.9511, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2518, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1834, Accuracy: 0.9501, F1 Micro: 0.9687, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1378, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1073, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Epoch 8/10, Train Loss: 0.0873, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9724\n",
      "Epoch 9/10, Train Loss: 0.0693, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5374, Accuracy: 0.784, F1 Micro: 0.784, F1 Macro: 0.6893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2884, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1628, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Epoch 4/10, Train Loss: 0.1242, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1093, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1289, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.924\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9093\n",
      "Epoch 9/10, Train Loss: 0.0638, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9198\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9236\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        81\n",
      "    positive       0.98      0.93      0.96       169\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.93      0.95      0.94       250\n",
      "weighted avg       0.95      0.94      0.94       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.898\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.56      0.67        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.75      0.82      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.84      0.78      0.80       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.88      0.71      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.81      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 137.47995114326477 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5396, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4546, Accuracy: 0.8571, F1 Micro: 0.9168, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3331, Accuracy: 0.9412, F1 Micro: 0.9636, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2221, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1281, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9765\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.972\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5402, Accuracy: 0.8627, F1 Micro: 0.8627, F1 Macro: 0.8285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2979, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1663, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9513\n",
      "Epoch 4/10, Train Loss: 0.1462, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 5/10, Train Loss: 0.1368, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9294\n",
      "Epoch 6/10, Train Loss: 0.143, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9302\n",
      "Epoch 7/10, Train Loss: 0.0885, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9261\n",
      "Epoch 8/10, Train Loss: 0.0941, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Epoch 9/10, Train Loss: 0.0659, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9298\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.934\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.97       173\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.95      0.96      0.95       255\n",
      "weighted avg       0.96      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9041\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.80      0.82       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 132.57872462272644 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5402, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4766, Accuracy: 0.8058, F1 Micro: 0.8905, F1 Macro: 0.8891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3789, Accuracy: 0.9249, F1 Micro: 0.9543, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2492, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1849, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9727\n",
      "Epoch 6/10, Train Loss: 0.1365, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1092, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.0883, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Epoch 9/10, Train Loss: 0.0742, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9743\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5331, Accuracy: 0.8849, F1 Micro: 0.8849, F1 Macro: 0.8685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.265, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9382\n",
      "Epoch 3/10, Train Loss: 0.1828, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9071\n",
      "Epoch 4/10, Train Loss: 0.1724, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9421\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9173\n",
      "Epoch 7/10, Train Loss: 0.1128, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.895\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9248\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.929\n",
      "Epoch 10/10, Train Loss: 0.0833, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9099\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        82\n",
      "    positive       0.98      0.95      0.96       170\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.94      0.95      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8913\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.94      0.83        16\n",
      "     neutral       0.96      0.96      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.88      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.92      0.67        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.93      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.79      0.86      0.80       216\n",
      "weighted avg       0.91      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.04418754577637 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.8127\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 7.255678415298462 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5481, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.47, Accuracy: 0.8088, F1 Micro: 0.8907, F1 Macro: 0.8887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3579, Accuracy: 0.9182, F1 Micro: 0.9493, F1 Macro: 0.9469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2372, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1669, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1337, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Epoch 7/10, Train Loss: 0.0987, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9767\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.548, Accuracy: 0.7547, F1 Micro: 0.7547, F1 Macro: 0.614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2547, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1734, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1588, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1218, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9356\n",
      "Epoch 6/10, Train Loss: 0.1011, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9239\n",
      "Epoch 7/10, Train Loss: 0.0852, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9316\n",
      "Epoch 8/10, Train Loss: 0.0772, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9311\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9169\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9169\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.91        83\n",
      "    positive       0.98      0.94      0.96       182\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.93      0.95      0.94       265\n",
      "weighted avg       0.95      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.894\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.62      0.69        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.76      0.79      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.83      0.79      0.81       216\n",
      "weighted avg       0.91      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.83      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.96579599380493 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5493, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4518, Accuracy: 0.8698, F1 Micro: 0.9231, F1 Macro: 0.9224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3247, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2132, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1527, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1277, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0826, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0724, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.978\n",
      "Epoch 10/10, Train Loss: 0.0598, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5421, Accuracy: 0.8538, F1 Micro: 0.8538, F1 Macro: 0.822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2682, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Epoch 3/10, Train Loss: 0.1505, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9085\n",
      "Epoch 4/10, Train Loss: 0.1349, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1149, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0865, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0832, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "Epoch 8/10, Train Loss: 0.108, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0649, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        83\n",
      "    positive       0.98      0.92      0.95       177\n",
      "\n",
      "    accuracy                           0.93       260\n",
      "   macro avg       0.92      0.94      0.93       260\n",
      "weighted avg       0.94      0.93      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9075\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.82      0.86      0.84       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 144.38190746307373 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5447, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4797, Accuracy: 0.8095, F1 Micro: 0.8921, F1 Macro: 0.8905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3768, Accuracy: 0.9293, F1 Micro: 0.9563, F1 Macro: 0.9544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2494, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1763, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1398, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1021, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9747\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "Epoch 10/10, Train Loss: 0.0623, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5543, Accuracy: 0.8985, F1 Micro: 0.8985, F1 Macro: 0.887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2397, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "Epoch 3/10, Train Loss: 0.1789, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9095\n",
      "Epoch 4/10, Train Loss: 0.1555, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9217\n",
      "Epoch 5/10, Train Loss: 0.1107, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9217\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Epoch 7/10, Train Loss: 0.0949, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9453\n",
      "Epoch 9/10, Train Loss: 0.0572, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9332\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9213\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.94      0.95      0.95       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.9012\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.92      0.76        12\n",
      "     neutral       0.95      0.92      0.93       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.88      0.83       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.91      0.88       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 138.83706378936768 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.8201\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 6.843721151351929 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5361, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4569, Accuracy: 0.8251, F1 Micro: 0.8988, F1 Macro: 0.8968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3479, Accuracy: 0.9263, F1 Micro: 0.9551, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2359, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1641, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1243, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.975\n",
      "Epoch 7/10, Train Loss: 0.1017, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0659, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.975\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5569, Accuracy: 0.7103, F1 Micro: 0.7103, F1 Macro: 0.534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2837, Accuracy: 0.9127, F1 Micro: 0.9127, F1 Macro: 0.9054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1864, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1682, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.123, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1067, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Epoch 8/10, Train Loss: 0.1028, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0842, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9379\n",
      "Epoch 10/10, Train Loss: 0.0602, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        83\n",
      "    positive       0.97      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.94      0.94       252\n",
      "weighted avg       0.95      0.94      0.94       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.895\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.84      0.81       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 151.46750378608704 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5422, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4242, Accuracy: 0.8943, F1 Micro: 0.9364, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3021, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.21, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1604, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1177, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.098, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9735\n",
      "Epoch 9/10, Train Loss: 0.0675, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "Epoch 10/10, Train Loss: 0.055, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9756\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5413, Accuracy: 0.8254, F1 Micro: 0.8254, F1 Macro: 0.7742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2317, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.9013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1654, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9431\n",
      "Epoch 4/10, Train Loss: 0.1476, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9389\n",
      "Epoch 5/10, Train Loss: 0.1099, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9347\n",
      "Epoch 6/10, Train Loss: 0.1076, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9264\n",
      "Epoch 8/10, Train Loss: 0.1094, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9102\n",
      "Epoch 10/10, Train Loss: 0.0778, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.934\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        83\n",
      "    positive       0.99      0.93      0.96       169\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.93      0.96      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9008\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.83      0.57        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.76      0.84      0.78       216\n",
      "weighted avg       0.91      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.19145703315735 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5404, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4644, Accuracy: 0.8266, F1 Micro: 0.9007, F1 Macro: 0.8995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3554, Accuracy: 0.9353, F1 Micro: 0.9603, F1 Macro: 0.959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2337, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1682, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.972\n",
      "Epoch 10/10, Train Loss: 0.0564, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9753\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6039, Accuracy: 0.8706, F1 Micro: 0.8706, F1 Macro: 0.8368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2936, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1805, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1182, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1214, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9234\n",
      "Epoch 6/10, Train Loss: 0.1064, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9169\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9182\n",
      "Epoch 8/10, Train Loss: 0.0912, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9201\n",
      "Epoch 9/10, Train Loss: 0.0647, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9223\n",
      "Epoch 10/10, Train Loss: 0.0558, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9219\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.89      0.90        82\n",
      "    positive       0.95      0.95      0.95       173\n",
      "\n",
      "    accuracy                           0.93       255\n",
      "   macro avg       0.92      0.92      0.92       255\n",
      "weighted avg       0.93      0.93      0.93       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.887\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.92      0.59        12\n",
      "     neutral       0.97      0.91      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.75      0.87      0.78       216\n",
      "weighted avg       0.91      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.99      0.98      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.89       216\n",
      "weighted avg       0.95      0.94      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 143.5212516784668 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.8258\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 6.026079416275024 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5304, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4575, Accuracy: 0.8311, F1 Micro: 0.9028, F1 Macro: 0.9012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3181, Accuracy: 0.9375, F1 Micro: 0.9613, F1 Macro: 0.9595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2225, Accuracy: 0.9449, F1 Micro: 0.9654, F1 Macro: 0.9622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1588, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1179, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "Epoch 7/10, Train Loss: 0.0938, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Epoch 10/10, Train Loss: 0.0522, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.972\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.96      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.533, Accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2747, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1539, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9324\n",
      "Epoch 4/10, Train Loss: 0.131, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9271\n",
      "Epoch 5/10, Train Loss: 0.1567, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1066, Accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9559\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9425\n",
      "Epoch 8/10, Train Loss: 0.1034, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9519\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "Epoch 10/10, Train Loss: 0.0543, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9382\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        83\n",
      "    positive       0.98      0.96      0.97       172\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.95      0.96      0.96       255\n",
      "weighted avg       0.96      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8889\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.92      0.50        12\n",
      "     neutral       0.96      0.89      0.93       152\n",
      "    positive       0.88      0.71      0.79        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.73      0.84      0.74       216\n",
      "weighted avg       0.91      0.85      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 145.59215450286865 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5384, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.439, Accuracy: 0.8795, F1 Micro: 0.9288, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.293, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2111, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1463, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1137, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0762, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0616, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0536, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5673, Accuracy: 0.845, F1 Micro: 0.845, F1 Macro: 0.8003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2729, Accuracy: 0.8837, F1 Micro: 0.8837, F1 Macro: 0.8761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2231, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1365, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.931\n",
      "Epoch 5/10, Train Loss: 0.1244, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9155\n",
      "Epoch 6/10, Train Loss: 0.1044, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9236\n",
      "Epoch 7/10, Train Loss: 0.1038, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.922\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9111\n",
      "Epoch 9/10, Train Loss: 0.0895, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "Epoch 10/10, Train Loss: 0.0811, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9269\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        84\n",
      "    positive       0.98      0.93      0.95       174\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.92      0.94      0.93       258\n",
      "weighted avg       0.94      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9154\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.54185366630554 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5356, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.467, Accuracy: 0.8289, F1 Micro: 0.9021, F1 Macro: 0.901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3245, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2274, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1603, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 7/10, Train Loss: 0.0976, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Epoch 8/10, Train Loss: 0.0747, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5737, Accuracy: 0.8555, F1 Micro: 0.8555, F1 Macro: 0.8167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2835, Accuracy: 0.8897, F1 Micro: 0.8897, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2291, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1192, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9248\n",
      "Epoch 7/10, Train Loss: 0.1042, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9146\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0826, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0803, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        85\n",
      "    positive       0.97      0.94      0.95       178\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.94      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9136\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.62      0.71        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.77      0.73      0.75        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.85      0.78      0.81       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.81      0.85      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 161.45464038848877 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.8315\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.847787380218506 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5376, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.45, Accuracy: 0.8318, F1 Micro: 0.9028, F1 Macro: 0.9012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3202, Accuracy: 0.9397, F1 Micro: 0.9625, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2175, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1553, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1154, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.0612, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5059, Accuracy: 0.8849, F1 Micro: 0.8849, F1 Macro: 0.8677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2599, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9084\n",
      "Epoch 3/10, Train Loss: 0.1939, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1521, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1165, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "Epoch 6/10, Train Loss: 0.1206, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Epoch 8/10, Train Loss: 0.079, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9136\n",
      "Epoch 9/10, Train Loss: 0.0509, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0597, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        82\n",
      "    positive       0.99      0.93      0.96       170\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.95      0.94       252\n",
      "weighted avg       0.95      0.94      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.8927\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.75      0.62        12\n",
      "     neutral       0.92      0.93      0.92       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.82      0.79       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 150.9350016117096 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5424, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4231, Accuracy: 0.9025, F1 Micro: 0.9412, F1 Macro: 0.9405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2836, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1948, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1472, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Epoch 6/10, Train Loss: 0.1129, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9791\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9782\n",
      "Epoch 8/10, Train Loss: 0.0731, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 10/10, Train Loss: 0.0495, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.509, Accuracy: 0.8803, F1 Micro: 0.8803, F1 Macro: 0.854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2516, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1692, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1227, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1205, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "Epoch 6/10, Train Loss: 0.1165, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9436\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "Epoch 10/10, Train Loss: 0.0376, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9398\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        84\n",
      "    positive       0.98      0.94      0.96       175\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9099\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 150.15077877044678 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5412, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4566, Accuracy: 0.8452, F1 Micro: 0.9103, F1 Macro: 0.9094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3205, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2143, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1574, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9778\n",
      "Epoch 7/10, Train Loss: 0.0869, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9737\n",
      "Epoch 10/10, Train Loss: 0.0496, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.92      0.99      0.95       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5767, Accuracy: 0.8745, F1 Micro: 0.8745, F1 Macro: 0.8572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2585, Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1539, Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.9087\n",
      "Epoch 4/10, Train Loss: 0.1398, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1068, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.108, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.934\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.929\n",
      "Epoch 9/10, Train Loss: 0.0686, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0679, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.934\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.98      0.92        83\n",
      "    positive       0.99      0.92      0.95       164\n",
      "\n",
      "    accuracy                           0.94       247\n",
      "   macro avg       0.92      0.95      0.93       247\n",
      "weighted avg       0.94      0.94      0.94       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8929\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.88      0.70        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.95      0.55      0.69        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.80      0.79       216\n",
      "weighted avg       0.92      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.83      0.57        12\n",
      "     neutral       0.96      0.92      0.94       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.76      0.85      0.79       216\n",
      "weighted avg       0.91      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.333740234375 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.836\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.034341096878052 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5311, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4515, Accuracy: 0.8341, F1 Micro: 0.9042, F1 Macro: 0.9027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.32, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2107, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Epoch 6/10, Train Loss: 0.1194, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0906, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 8/10, Train Loss: 0.0765, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5099, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2336, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9393\n",
      "Epoch 3/10, Train Loss: 0.1718, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9154\n",
      "Epoch 4/10, Train Loss: 0.1712, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 5/10, Train Loss: 0.1221, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9206\n",
      "Epoch 6/10, Train Loss: 0.1128, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9215\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9186\n",
      "Epoch 8/10, Train Loss: 0.099, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9215\n",
      "Epoch 9/10, Train Loss: 0.0941, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Epoch 10/10, Train Loss: 0.0812, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9393\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        84\n",
      "    positive       0.98      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8843\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.83      0.59        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.75      0.84      0.78       216\n",
      "weighted avg       0.90      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      1.00      0.82        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 146.0558590888977 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5346, Accuracy: 0.7976, F1 Micro: 0.8864, F1 Macro: 0.885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4091, Accuracy: 0.8996, F1 Micro: 0.9396, F1 Macro: 0.9385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2785, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1914, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.14, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9783\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0723, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5219, Accuracy: 0.8931, F1 Micro: 0.8931, F1 Macro: 0.8821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2332, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9448\n",
      "Epoch 4/10, Train Loss: 0.1279, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Epoch 5/10, Train Loss: 0.1179, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9271\n",
      "Epoch 6/10, Train Loss: 0.1107, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.923\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9088\n",
      "Epoch 8/10, Train Loss: 0.0573, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9005\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "Epoch 10/10, Train Loss: 0.055, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        85\n",
      "    positive       0.99      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.96      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9111\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 157.1120617389679 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5357, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4513, Accuracy: 0.8348, F1 Micro: 0.905, F1 Macro: 0.904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.316, Accuracy: 0.9442, F1 Micro: 0.9655, F1 Macro: 0.964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2095, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9754\n",
      "Epoch 5/10, Train Loss: 0.1504, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Epoch 6/10, Train Loss: 0.1209, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5219, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2384, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2046, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9322\n",
      "Epoch 4/10, Train Loss: 0.1433, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 5/10, Train Loss: 0.1213, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9353\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9197\n",
      "Epoch 8/10, Train Loss: 0.0768, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0721, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0574, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        85\n",
      "    positive       0.97      0.96      0.97       174\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.95      0.95      0.95       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9227\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 156.34933757781982 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.8404\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.674431562423706 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5283, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4352, Accuracy: 0.8668, F1 Micro: 0.9208, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3142, Accuracy: 0.9427, F1 Micro: 0.9647, F1 Macro: 0.9634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1986, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9743\n",
      "Epoch 5/10, Train Loss: 0.1491, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.117, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 7/10, Train Loss: 0.086, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 8/10, Train Loss: 0.0677, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "Epoch 10/10, Train Loss: 0.0484, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9781\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4974, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2277, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1871, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9218\n",
      "Epoch 4/10, Train Loss: 0.1535, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9164\n",
      "Epoch 6/10, Train Loss: 0.0834, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0973, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 9/10, Train Loss: 0.0853, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.912\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        85\n",
      "    positive       0.97      0.94      0.95       178\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.94      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9074\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.46      1.00      0.63        12\n",
      "     neutral       0.96      0.91      0.93       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.89      0.79       216\n",
      "weighted avg       0.91      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 155.3452820777893 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5343, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4089, Accuracy: 0.9137, F1 Micro: 0.9475, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2828, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1834, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "Epoch 5/10, Train Loss: 0.1379, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0847, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9796\n",
      "Epoch 8/10, Train Loss: 0.072, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.982\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.98      0.96       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5474, Accuracy: 0.8867, F1 Micro: 0.8867, F1 Macro: 0.8711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2577, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1759, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1501, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1402, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9318\n",
      "Epoch 6/10, Train Loss: 0.1295, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9274\n",
      "Epoch 7/10, Train Loss: 0.0803, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.8984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9277\n",
      "Epoch 10/10, Train Loss: 0.0589, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9233\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        86\n",
      "    positive       0.98      0.93      0.95       170\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.92      0.94      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9205\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.98      0.96       152\n",
      "    positive       0.93      0.81      0.87        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 160.43385863304138 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5314, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4514, Accuracy: 0.8534, F1 Micro: 0.9144, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3159, Accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2021, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Epoch 5/10, Train Loss: 0.1524, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.9723\n",
      "Epoch 6/10, Train Loss: 0.1146, Accuracy: 0.9591, F1 Micro: 0.974, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0901, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0616, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.94      0.95      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4951, Accuracy: 0.8918, F1 Micro: 0.8918, F1 Macro: 0.8836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2301, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9181\n",
      "Epoch 3/10, Train Loss: 0.1601, Accuracy: 0.9104, F1 Micro: 0.9104, F1 Macro: 0.9031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1294, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9203\n",
      "Epoch 5/10, Train Loss: 0.122, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9243\n",
      "Epoch 7/10, Train Loss: 0.0976, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0825, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9056\n",
      "Epoch 10/10, Train Loss: 0.0838, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.892\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.93      0.91        87\n",
      "    positive       0.97      0.94      0.95       181\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.92      0.94      0.93       268\n",
      "weighted avg       0.94      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.8948\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.77        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.74      0.88      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.82      0.85       216\n",
      "weighted avg       0.94      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.86      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 165.1958713531494 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9332, F1 Micro: 0.9332, F1 Macro: 0.8443\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.346531629562378 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5303, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4323, Accuracy: 0.8668, F1 Micro: 0.9216, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2924, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1978, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 5/10, Train Loss: 0.1353, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1116, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Epoch 7/10, Train Loss: 0.0879, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4676, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2617, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1984, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9305\n",
      "Epoch 4/10, Train Loss: 0.1561, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9305\n",
      "Epoch 6/10, Train Loss: 0.0921, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0915, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.928\n",
      "Epoch 9/10, Train Loss: 0.0596, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.98      0.91        84\n",
      "    positive       0.99      0.92      0.95       179\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.92      0.95      0.93       263\n",
      "weighted avg       0.95      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9108\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.81      0.85      0.83       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.0925648212433 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5297, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3938, Accuracy: 0.9219, F1 Micro: 0.9524, F1 Macro: 0.9511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2548, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1848, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9752\n",
      "Epoch 5/10, Train Loss: 0.1325, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1075, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 7/10, Train Loss: 0.0841, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 8/10, Train Loss: 0.0689, Accuracy: 0.9628, F1 Micro: 0.9763, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "Epoch 10/10, Train Loss: 0.048, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9795\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.93      0.98      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.478, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1961, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "Epoch 3/10, Train Loss: 0.1447, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9084\n",
      "Epoch 4/10, Train Loss: 0.1604, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 5/10, Train Loss: 0.1482, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Epoch 6/10, Train Loss: 0.0964, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9301\n",
      "Epoch 7/10, Train Loss: 0.0983, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9213\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Epoch 9/10, Train Loss: 0.0711, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9227\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.902\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.75      0.60        12\n",
      "     neutral       0.95      0.96      0.95       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.79      0.83      0.80       216\n",
      "weighted avg       0.91      0.90      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.87      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 153.7764356136322 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5367, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4429, Accuracy: 0.8743, F1 Micro: 0.9259, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2882, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1951, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9713\n",
      "Epoch 5/10, Train Loss: 0.1367, Accuracy: 0.9539, F1 Micro: 0.9708, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1086, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9734\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5424, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9172\n",
      "Epoch 2/10, Train Loss: 0.2319, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8931\n",
      "Epoch 3/10, Train Loss: 0.1745, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1384, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1006, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0819, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0563, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "Epoch 8/10, Train Loss: 0.0621, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Epoch 9/10, Train Loss: 0.0662, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Epoch 10/10, Train Loss: 0.0795, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        85\n",
      "    positive       0.98      0.94      0.96       175\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8891\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.83      0.54        12\n",
      "     neutral       0.95      0.89      0.92       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.74      0.84      0.76       216\n",
      "weighted avg       0.90      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.92      0.80        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.87      0.86       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 160.24877500534058 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9343, F1 Micro: 0.9343, F1 Macro: 0.8474\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.8316423892974854 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5186, Accuracy: 0.7932, F1 Micro: 0.8838, F1 Macro: 0.8821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4359, Accuracy: 0.8847, F1 Micro: 0.929, F1 Macro: 0.926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.302, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1903, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9755\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1091, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.068, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4677, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9152\n",
      "Epoch 2/10, Train Loss: 0.1804, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1582, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "Epoch 4/10, Train Loss: 0.1253, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9116\n",
      "Epoch 5/10, Train Loss: 0.1237, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "Epoch 6/10, Train Loss: 0.1322, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0879, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "Epoch 8/10, Train Loss: 0.0914, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9162\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9162\n",
      "Epoch 10/10, Train Loss: 0.05, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9201\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        84\n",
      "    positive       0.98      0.93      0.95       180\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.92      0.94      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9094\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 157.85882234573364 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5198, Accuracy: 0.8006, F1 Micro: 0.8878, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3984, Accuracy: 0.9226, F1 Micro: 0.952, F1 Macro: 0.9495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2681, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1773, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "Epoch 6/10, Train Loss: 0.104, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9752\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.8846, F1 Micro: 0.8846, F1 Macro: 0.8738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2143, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.177, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1532, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9354\n",
      "Epoch 5/10, Train Loss: 0.13, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9141\n",
      "Epoch 7/10, Train Loss: 0.0881, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9199\n",
      "Epoch 8/10, Train Loss: 0.0636, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Epoch 9/10, Train Loss: 0.0703, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0459, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.8995\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.88      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.83      0.59        12\n",
      "     neutral       0.95      0.90      0.93       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.85      0.79       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.87      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 160.8152105808258 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5204, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4445, Accuracy: 0.8936, F1 Micro: 0.9354, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3076, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1955, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "Epoch 5/10, Train Loss: 0.1405, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1104, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.088, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4834, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9212\n",
      "Epoch 2/10, Train Loss: 0.2024, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1456, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9374\n",
      "Epoch 4/10, Train Loss: 0.1283, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9255\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9052\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9194\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9212\n",
      "Epoch 8/10, Train Loss: 0.0846, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9291\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9334\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9099\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.97      0.92        86\n",
      "    positive       0.98      0.93      0.96       182\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.93      0.95      0.94       268\n",
      "weighted avg       0.95      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.8993\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.92      0.71        12\n",
      "     neutral       0.96      0.89      0.93       152\n",
      "    positive       0.77      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.88      0.81       216\n",
      "weighted avg       0.90      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 157.43985676765442 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9353, F1 Micro: 0.9353, F1 Macro: 0.8503\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.6084210872650146 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.533, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4347, Accuracy: 0.8914, F1 Micro: 0.9351, F1 Macro: 0.9336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2742, Accuracy: 0.9561, F1 Micro: 0.9728, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1895, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1383, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 6/10, Train Loss: 0.1042, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.974\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4793, Accuracy: 0.9011, F1 Micro: 0.9011, F1 Macro: 0.8924\n",
      "Epoch 2/10, Train Loss: 0.2417, Accuracy: 0.8517, F1 Micro: 0.8517, F1 Macro: 0.8452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.187, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 4/10, Train Loss: 0.149, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "Epoch 5/10, Train Loss: 0.1247, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.909\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9291\n",
      "Epoch 8/10, Train Loss: 0.0615, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9172\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9011, F1 Micro: 0.9011, F1 Macro: 0.8824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9447\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9066\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.83      0.59        12\n",
      "     neutral       0.93      0.89      0.91       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.74      0.83      0.77       216\n",
      "weighted avg       0.88      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.63943552970886 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5337, Accuracy: 0.7954, F1 Micro: 0.8853, F1 Macro: 0.8838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3926, Accuracy: 0.9219, F1 Micro: 0.9522, F1 Macro: 0.951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2462, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.178, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1358, Accuracy: 0.9688, F1 Micro: 0.9805, F1 Macro: 0.9796\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9749\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 8/10, Train Loss: 0.0688, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9785\n",
      "Epoch 9/10, Train Loss: 0.0591, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9739\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9738\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9805, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5139, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2646, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1637, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.0992, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1343, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9422\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9333\n",
      "Epoch 7/10, Train Loss: 0.068, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0684, Accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9502\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9595, F1 Micro: 0.9595, F1 Macro: 0.9541\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9595, F1 Micro: 0.9595, F1 Macro: 0.9541\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.93      0.94        82\n",
      "    positive       0.96      0.98      0.97       165\n",
      "\n",
      "    accuracy                           0.96       247\n",
      "   macro avg       0.96      0.95      0.95       247\n",
      "weighted avg       0.96      0.96      0.96       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8889\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.81      0.70        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.82      0.83      0.82       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.83      0.49        12\n",
      "     neutral       0.96      0.88      0.92       152\n",
      "    positive       0.88      0.83      0.85        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.73      0.85      0.75       216\n",
      "weighted avg       0.91      0.86      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.98      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 164.0518081188202 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5372, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4427, Accuracy: 0.8854, F1 Micro: 0.932, F1 Macro: 0.9307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2796, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1913, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9739\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9767\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4453, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2308, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9287\n",
      "Epoch 3/10, Train Loss: 0.1446, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1229, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1097, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9368\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9163\n",
      "Epoch 9/10, Train Loss: 0.0848, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.902\n",
      "Epoch 10/10, Train Loss: 0.1001, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9368\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        86\n",
      "    positive       0.99      0.93      0.96       174\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9021\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.83      0.54        12\n",
      "     neutral       0.96      0.91      0.93       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.74      0.84      0.77       216\n",
      "weighted avg       0.91      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 168.2288212776184 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.8528\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.022184133529663 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5233, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4165, Accuracy: 0.8943, F1 Micro: 0.9339, F1 Macro: 0.9305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2712, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9681\n",
      "Epoch 4/10, Train Loss: 0.1844, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1346, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 7/10, Train Loss: 0.072, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0504, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4939, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2447, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1554, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1539, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1075, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9438\n",
      "Epoch 6/10, Train Loss: 0.1171, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.931\n",
      "Epoch 7/10, Train Loss: 0.0679, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9354\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9348\n",
      "Epoch 9/10, Train Loss: 0.0797, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.931\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.915\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        84\n",
      "    positive       0.99      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.96      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.8972\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.92      0.69        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.79      0.86      0.81       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 168.4484076499939 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5263, Accuracy: 0.8036, F1 Micro: 0.8894, F1 Macro: 0.888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3839, Accuracy: 0.9353, F1 Micro: 0.9598, F1 Macro: 0.9584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2482, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1727, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1308, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9814\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "Epoch 8/10, Train Loss: 0.0647, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4989, Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.8939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.231, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1269, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.935\n",
      "Epoch 4/10, Train Loss: 0.1405, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Epoch 5/10, Train Loss: 0.1233, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0841, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Epoch 8/10, Train Loss: 0.0764, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "Epoch 9/10, Train Loss: 0.0434, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.8938\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.9144, F1 Micro: 0.9144, F1 Macro: 0.9044\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.94      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.8906\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.91      0.87        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.88      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.84      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 166.2946915626526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5276, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4373, Accuracy: 0.91, F1 Micro: 0.9448, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2752, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1822, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1329, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 6/10, Train Loss: 0.0987, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.073, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5068, Accuracy: 0.9011, F1 Micro: 0.9011, F1 Macro: 0.8832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2165, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9358\n",
      "Epoch 3/10, Train Loss: 0.1488, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8944\n",
      "Epoch 4/10, Train Loss: 0.1363, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9094\n",
      "Epoch 7/10, Train Loss: 0.0978, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.92\n",
      "Epoch 8/10, Train Loss: 0.0765, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        86\n",
      "    positive       0.97      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.94      0.94       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.8959\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.96      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.88      0.78        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.86      0.84       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.83      0.61        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.84      0.78       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 166.39367365837097 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9369, F1 Micro: 0.9369, F1 Macro: 0.8548\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.536543607711792 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5297, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4199, Accuracy: 0.9003, F1 Micro: 0.9384, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2704, Accuracy: 0.9561, F1 Micro: 0.9728, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1766, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1282, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0999, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9785\n",
      "Epoch 7/10, Train Loss: 0.0746, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.95      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.465, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2059, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1525, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1343, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1286, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9415\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Epoch 7/10, Train Loss: 0.0967, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9231\n",
      "Epoch 9/10, Train Loss: 0.067, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9151\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9171\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        87\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.93      0.95      0.94       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.8956\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.96      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.62      0.69        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.77      0.73      0.75        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.78      0.80       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.85      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.90      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 174.94501447677612 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5247, Accuracy: 0.7961, F1 Micro: 0.8855, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3785, Accuracy: 0.9263, F1 Micro: 0.954, F1 Macro: 0.9518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2443, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9698\n",
      "Epoch 4/10, Train Loss: 0.165, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9799\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9785\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9809\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.97      0.96       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.52, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Epoch 3/10, Train Loss: 0.1907, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9088\n",
      "Epoch 4/10, Train Loss: 0.1353, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Epoch 5/10, Train Loss: 0.1164, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9268\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9243\n",
      "Epoch 8/10, Train Loss: 0.0732, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0609, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        86\n",
      "    positive       0.97      0.93      0.95       174\n",
      "\n",
      "    accuracy                           0.93       260\n",
      "   macro avg       0.92      0.94      0.93       260\n",
      "weighted avg       0.94      0.93      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9174\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.97      0.96       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 170.59089994430542 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5312, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4286, Accuracy: 0.9159, F1 Micro: 0.9484, F1 Macro: 0.9467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2731, Accuracy: 0.9501, F1 Micro: 0.9692, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1778, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9719\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0673, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5297, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2621, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.134, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9187\n",
      "Epoch 4/10, Train Loss: 0.1297, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0849, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9265\n",
      "Epoch 7/10, Train Loss: 0.1146, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.922\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.902\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9212\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9101\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.91        85\n",
      "    positive       0.97      0.92      0.95       168\n",
      "\n",
      "    accuracy                           0.93       253\n",
      "   macro avg       0.92      0.94      0.93       253\n",
      "weighted avg       0.94      0.93      0.93       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8873\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.83      0.51        12\n",
      "     neutral       0.95      0.92      0.93       152\n",
      "    positive       0.90      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.74      0.82      0.75       216\n",
      "weighted avg       0.90      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 166.2948546409607 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.8568\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.2060325145721436 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5268, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4117, Accuracy: 0.8862, F1 Micro: 0.9314, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2637, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1657, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.119, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 6/10, Train Loss: 0.086, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9767\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4616, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2204, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.9237\n",
      "Epoch 3/10, Train Loss: 0.1769, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9187\n",
      "Epoch 4/10, Train Loss: 0.1473, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.9115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1287, Accuracy: 0.9338, F1 Micro: 0.9338, F1 Macro: 0.9261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.9039\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.8963\n",
      "Epoch 9/10, Train Loss: 0.0642, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9157\n",
      "Epoch 10/10, Train Loss: 0.0742, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.911\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        88\n",
      "    positive       0.97      0.93      0.95       184\n",
      "\n",
      "    accuracy                           0.94       272\n",
      "   macro avg       0.92      0.94      0.93       272\n",
      "weighted avg       0.94      0.94      0.94       272\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9079\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.89      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.87      0.89      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 173.95866322517395 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5249, Accuracy: 0.8103, F1 Micro: 0.8921, F1 Macro: 0.8905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3766, Accuracy: 0.9301, F1 Micro: 0.9569, F1 Macro: 0.9553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2376, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.158, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0883, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0638, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 9/10, Train Loss: 0.0496, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.95      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4744, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2266, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.173, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9247\n",
      "Epoch 4/10, Train Loss: 0.1533, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1126, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0867, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0779, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "Epoch 8/10, Train Loss: 0.0644, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8992\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9234\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.906\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.93      0.91        87\n",
      "    positive       0.97      0.94      0.95       181\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.92      0.94      0.93       268\n",
      "weighted avg       0.94      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.903\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.96      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.86      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 180.15375542640686 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5288, Accuracy: 0.7879, F1 Micro: 0.8813, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4237, Accuracy: 0.9085, F1 Micro: 0.9446, F1 Macro: 0.9427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2611, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1642, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1198, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 6/10, Train Loss: 0.085, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Epoch 7/10, Train Loss: 0.0705, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "Epoch 9/10, Train Loss: 0.0504, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.976\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.495, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2015, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1767, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9403\n",
      "Epoch 4/10, Train Loss: 0.1235, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9329\n",
      "Epoch 5/10, Train Loss: 0.0929, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0651, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9441\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9245\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        86\n",
      "    positive       0.97      0.96      0.96       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.94      0.95      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.8976\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.92      0.59        12\n",
      "     neutral       0.96      0.89      0.93       152\n",
      "    positive       0.80      0.77      0.78        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.73      0.86      0.77       216\n",
      "weighted avg       0.90      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 178.43082809448242 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9384, F1 Micro: 0.9384, F1 Macro: 0.8588\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 1.8719842433929443 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5205, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.405, Accuracy: 0.9144, F1 Micro: 0.947, F1 Macro: 0.9449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2479, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1681, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Epoch 5/10, Train Loss: 0.1138, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 7/10, Train Loss: 0.0681, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.059, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0487, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5234, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8927\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.8931, F1 Micro: 0.8931, F1 Macro: 0.8863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1783, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1314, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.925\n",
      "Epoch 5/10, Train Loss: 0.1353, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.917\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1151, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Epoch 9/10, Train Loss: 0.0864, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9242\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9242\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        87\n",
      "    positive       0.97      0.93      0.95       175\n",
      "\n",
      "    accuracy                           0.93       262\n",
      "   macro avg       0.92      0.93      0.92       262\n",
      "weighted avg       0.93      0.93      0.93       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8976\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.84      0.84       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.84      0.78       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 175.1548171043396 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5196, Accuracy: 0.7954, F1 Micro: 0.8852, F1 Macro: 0.8837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3751, Accuracy: 0.9315, F1 Micro: 0.9571, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2309, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1622, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9769\n",
      "Epoch 5/10, Train Loss: 0.1101, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0718, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0581, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "Epoch 9/10, Train Loss: 0.05, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9789\n",
      "Epoch 10/10, Train Loss: 0.0442, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5268, Accuracy: 0.8626, F1 Micro: 0.8626, F1 Macro: 0.8305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2229, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1725, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9367\n",
      "Epoch 4/10, Train Loss: 0.1442, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9057\n",
      "Epoch 5/10, Train Loss: 0.1069, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.923\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9221\n",
      "Epoch 7/10, Train Loss: 0.0892, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9206\n",
      "Epoch 8/10, Train Loss: 0.0673, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "Epoch 10/10, Train Loss: 0.071, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9127\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        86\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.94      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.8971\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.81      0.74        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.84      0.84      0.83       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.79      0.86      0.82       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      1.00      0.82        23\n",
      "     neutral       0.99      0.97      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.92      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 176.12017035484314 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5246, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4232, Accuracy: 0.9278, F1 Micro: 0.9557, F1 Macro: 0.9541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2558, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1715, Accuracy: 0.9635, F1 Micro: 0.9773, F1 Macro: 0.9764\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9759\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9725\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9751\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.9746\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4382, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9206\n",
      "Epoch 2/10, Train Loss: 0.1969, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1751, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1606, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1312, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Epoch 6/10, Train Loss: 0.1234, Accuracy: 0.9046, F1 Micro: 0.9046, F1 Macro: 0.897\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0735, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9118\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9238\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8931\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.37      0.83      0.51        12\n",
      "     neutral       0.94      0.90      0.92       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.73      0.83      0.75       216\n",
      "weighted avg       0.90      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.45257353782654 s\n",
      "Averaged - Iteration 864: Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.8605\n",
      "Total runtime: 10518.683017492294 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1b289",
   "metadata": {
    "papermill": {
     "duration": 0.471329,
     "end_time": "2025-03-08T13:18:00.518288",
     "exception": false,
     "start_time": "2025-03-08T13:18:00.046959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eececb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 5\n",
      "Random seed: [94, 21, 5]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6559, Accuracy: 0.7463, F1 Micro: 0.8496, F1 Macro: 0.8267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5772, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5458, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5239, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.476, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4654, Accuracy: 0.7902, F1 Micro: 0.8825, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4704, Accuracy: 0.7879, F1 Micro: 0.8808, F1 Macro: 0.8789\n",
      "Epoch 8/10, Train Loss: 0.4263, Accuracy: 0.7827, F1 Micro: 0.8771, F1 Macro: 0.8742\n",
      "Epoch 9/10, Train Loss: 0.4009, Accuracy: 0.7812, F1 Micro: 0.8761, F1 Macro: 0.8731\n",
      "Epoch 10/10, Train Loss: 0.3867, Accuracy: 0.7798, F1 Micro: 0.8749, F1 Macro: 0.8721\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7902, F1 Micro: 0.8825, F1 Macro: 0.8809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      0.99      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.88      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.8458, Accuracy: 0.4, F1 Micro: 0.4, F1 Macro: 0.2857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6455, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4744, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3486, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2564, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2002, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1521, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1192, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0909, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0727, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         5\n",
      "   macro avg       0.50      0.50      0.50         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7762, F1 Micro: 0.7762, F1 Macro: 0.3084\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      0.96      0.85       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.75       216\n",
      "   macro avg       0.26      0.32      0.28       216\n",
      "weighted avg       0.59      0.75      0.66       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      0.89      0.79       152\n",
      "    positive       0.60      0.06      0.11        52\n",
      "\n",
      "    accuracy                           0.64       216\n",
      "   macro avg       0.44      0.32      0.30       216\n",
      "weighted avg       0.64      0.64      0.58       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.13      0.21        23\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.41      0.37      0.35       216\n",
      "weighted avg       0.56      0.71      0.61       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 58.94250988960266 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6744, Accuracy: 0.7552, F1 Micro: 0.8555, F1 Macro: 0.8479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5918, Accuracy: 0.785, F1 Micro: 0.8794, F1 Macro: 0.8779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5453, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5309, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4879, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4745, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4859, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4599, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4311, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Epoch 10/10, Train Loss: 0.3935, Accuracy: 0.7909, F1 Micro: 0.8827, F1 Macro: 0.8811\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6444, Accuracy: 0.8, F1 Micro: 0.8, F1 Macro: 0.4444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5577, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.531, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4857, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4167, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3914, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3635, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2985, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3418, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2611, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         5\n",
      "   macro avg       0.50      0.50      0.50         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7901, F1 Micro: 0.7901, F1 Macro: 0.3024\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.78      0.99      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      0.99      0.83       152\n",
      "    positive       1.00      0.06      0.11        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.35      0.31       216\n",
      "weighted avg       0.74      0.71      0.61       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      0.99      0.83       152\n",
      "    positive       0.50      0.02      0.05        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.40      0.34      0.29       216\n",
      "weighted avg       0.59      0.70      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 65.81516146659851 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7408, Accuracy: 0.5982, F1 Micro: 0.7065, F1 Macro: 0.6096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6394, Accuracy: 0.7723, F1 Micro: 0.8672, F1 Macro: 0.8576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.6003, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 4/10, Train Loss: 0.5587, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 5/10, Train Loss: 0.5047, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 6/10, Train Loss: 0.5075, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.5115, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 8/10, Train Loss: 0.4764, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 9/10, Train Loss: 0.4545, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4405, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.88      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7828, Accuracy: 0.0, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6312, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4728, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3577, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3441, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.307, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2576, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1603, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2043, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1563, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         1\n",
      "   macro avg       0.50      0.50      0.50         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7901, F1 Micro: 0.7901, F1 Macro: 0.2958\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.74      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 58.35481071472168 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7855, F1 Micro: 0.7855, F1 Macro: 0.3022\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 8.757826328277588 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6071, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5272, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.49, Accuracy: 0.7902, F1 Micro: 0.8824, F1 Macro: 0.8808\n",
      "Epoch 4/10, Train Loss: 0.4677, Accuracy: 0.7894, F1 Micro: 0.8805, F1 Macro: 0.8779\n",
      "Epoch 5/10, Train Loss: 0.4441, Accuracy: 0.7924, F1 Micro: 0.8815, F1 Macro: 0.8784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4249, Accuracy: 0.8028, F1 Micro: 0.8867, F1 Macro: 0.8837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4034, Accuracy: 0.8222, F1 Micro: 0.8971, F1 Macro: 0.8952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3395, Accuracy: 0.8438, F1 Micro: 0.9081, F1 Macro: 0.9064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2869, Accuracy: 0.8624, F1 Micro: 0.9168, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2624, Accuracy: 0.878, F1 Micro: 0.9257, F1 Macro: 0.9225\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.878, F1 Micro: 0.9257, F1 Macro: 0.9225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.87      1.00      0.93       187\n",
      "     machine       0.88      0.97      0.92       175\n",
      "      others       0.86      0.84      0.85       158\n",
      "        part       0.87      0.96      0.91       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.90      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.89      0.96      0.93      1061\n",
      "   macro avg       0.89      0.96      0.92      1061\n",
      "weighted avg       0.89      0.96      0.93      1061\n",
      " samples avg       0.90      0.96      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6366, Accuracy: 0.734, F1 Micro: 0.734, F1 Macro: 0.4233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5464, Accuracy: 0.734, F1 Micro: 0.734, F1 Macro: 0.4233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4264, Accuracy: 0.8191, F1 Micro: 0.8191, F1 Macro: 0.7196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3039, Accuracy: 0.8191, F1 Micro: 0.8191, F1 Macro: 0.7437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2049, Accuracy: 0.8617, F1 Micro: 0.8617, F1 Macro: 0.8409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.145, Accuracy: 0.8617, F1 Micro: 0.8617, F1 Macro: 0.8394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0629, Accuracy: 0.8723, F1 Micro: 0.8723, F1 Macro: 0.8518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.046, Accuracy: 0.8883, F1 Micro: 0.8883, F1 Macro: 0.8685\n",
      "Epoch 9/10, Train Loss: 0.0882, Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.8477\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.8617, F1 Micro: 0.8617, F1 Macro: 0.8423\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.8883, F1 Micro: 0.8883, F1 Macro: 0.8685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.94      0.82        50\n",
      "    positive       0.98      0.87      0.92       138\n",
      "\n",
      "    accuracy                           0.89       188\n",
      "   macro avg       0.85      0.90      0.87       188\n",
      "weighted avg       0.91      0.89      0.89       188\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8688, F1 Micro: 0.8688, F1 Macro: 0.6807\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.87      1.00      0.93       181\n",
      "    positive       1.00      0.38      0.55        24\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.62      0.46      0.49       216\n",
      "weighted avg       0.84      0.88      0.84       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.88      0.97      0.92       167\n",
      "    positive       0.86      0.36      0.51        33\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.82      0.72      0.73       216\n",
      "weighted avg       0.87      0.87      0.85       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.75      0.53        12\n",
      "     neutral       0.86      0.84      0.85       152\n",
      "    positive       0.61      0.54      0.57        52\n",
      "\n",
      "    accuracy                           0.76       216\n",
      "   macro avg       0.63      0.71      0.65       216\n",
      "weighted avg       0.77      0.76      0.76       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.61      0.68        23\n",
      "     neutral       0.88      0.97      0.92       152\n",
      "    positive       0.87      0.63      0.73        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.84      0.74      0.78       216\n",
      "weighted avg       0.86      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.54      0.64        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.73      0.79       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.36      0.53        14\n",
      "     neutral       0.90      1.00      0.95       185\n",
      "    positive       1.00      0.29      0.45        17\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.97      0.55      0.64       216\n",
      "weighted avg       0.91      0.90      0.88       216\n",
      "\n",
      "Total train time: 73.85298347473145 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6484, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.539, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4916, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 4/10, Train Loss: 0.478, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4482, Accuracy: 0.7984, F1 Micro: 0.885, F1 Macro: 0.8826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.424, Accuracy: 0.8021, F1 Micro: 0.8858, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3993, Accuracy: 0.8296, F1 Micro: 0.9001, F1 Macro: 0.8971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3397, Accuracy: 0.8646, F1 Micro: 0.9192, F1 Macro: 0.9175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2883, Accuracy: 0.8854, F1 Micro: 0.9301, F1 Macro: 0.9275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2624, Accuracy: 0.8936, F1 Micro: 0.9351, F1 Macro: 0.9329\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8936, F1 Micro: 0.9351, F1 Macro: 0.9329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.89      0.99      0.94       187\n",
      "     machine       0.93      0.95      0.94       175\n",
      "      others       0.86      0.87      0.87       158\n",
      "        part       0.89      0.99      0.94       158\n",
      "       price       0.94      0.99      0.96       192\n",
      "     service       0.91      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.90      0.97      0.94      1061\n",
      "   macro avg       0.90      0.97      0.93      1061\n",
      "weighted avg       0.90      0.97      0.93      1061\n",
      " samples avg       0.91      0.97      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5813, Accuracy: 0.7245, F1 Micro: 0.7245, F1 Macro: 0.4201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6057, Accuracy: 0.7245, F1 Micro: 0.7245, F1 Macro: 0.4201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.442, Accuracy: 0.8163, F1 Micro: 0.8163, F1 Macro: 0.77\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3016, Accuracy: 0.852, F1 Micro: 0.852, F1 Macro: 0.8157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.852, F1 Micro: 0.852, F1 Macro: 0.8297\n",
      "Epoch 6/10, Train Loss: 0.1286, Accuracy: 0.8469, F1 Micro: 0.8469, F1 Macro: 0.8274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.8776, F1 Micro: 0.8776, F1 Macro: 0.8559\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.852, F1 Micro: 0.852, F1 Macro: 0.8338\n",
      "Epoch 9/10, Train Loss: 0.067, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.8389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0687, Accuracy: 0.8827, F1 Micro: 0.8827, F1 Macro: 0.8585\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8827, F1 Micro: 0.8827, F1 Macro: 0.8585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.85      0.80        54\n",
      "    positive       0.94      0.89      0.92       142\n",
      "\n",
      "    accuracy                           0.88       196\n",
      "   macro avg       0.85      0.87      0.86       196\n",
      "weighted avg       0.89      0.88      0.88       196\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8827, F1 Micro: 0.8827, F1 Macro: 0.7015\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.90      0.99      0.94       181\n",
      "    positive       0.87      0.54      0.67        24\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.59      0.51      0.54       216\n",
      "weighted avg       0.85      0.89      0.86       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.92      0.96      0.94       167\n",
      "    positive       0.78      0.64      0.70        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.58      0.52        12\n",
      "     neutral       0.86      0.87      0.86       152\n",
      "    positive       0.68      0.62      0.65        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.67      0.69      0.68       216\n",
      "weighted avg       0.79      0.79      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.52      0.62        23\n",
      "     neutral       0.89      0.99      0.94       152\n",
      "    positive       0.87      0.63      0.73        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.72      0.76       216\n",
      "weighted avg       0.87      0.88      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.94      0.99      0.97       186\n",
      "    positive       0.82      0.53      0.64        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.69      0.76       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.43      0.60        14\n",
      "     neutral       0.90      1.00      0.95       185\n",
      "    positive       0.80      0.24      0.36        17\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.55      0.64       216\n",
      "weighted avg       0.90      0.90      0.88       216\n",
      "\n",
      "Total train time: 77.21419191360474 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6745, Accuracy: 0.7537, F1 Micro: 0.8532, F1 Macro: 0.8278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5552, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.508, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5003, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4702, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4546, Accuracy: 0.8051, F1 Micro: 0.8896, F1 Macro: 0.8882\n",
      "Epoch 7/10, Train Loss: 0.4485, Accuracy: 0.8065, F1 Micro: 0.8886, F1 Macro: 0.8857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3818, Accuracy: 0.8423, F1 Micro: 0.9074, F1 Macro: 0.9058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3363, Accuracy: 0.8713, F1 Micro: 0.9216, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3002, Accuracy: 0.8891, F1 Micro: 0.932, F1 Macro: 0.9295\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.8891, F1 Micro: 0.932, F1 Macro: 0.9295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.89      1.00      0.94       187\n",
      "     machine       0.92      0.93      0.92       175\n",
      "      others       0.88      0.85      0.87       158\n",
      "        part       0.89      0.97      0.93       158\n",
      "       price       0.93      0.99      0.96       192\n",
      "     service       0.91      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.90      0.96      0.93      1061\n",
      "   macro avg       0.90      0.96      0.93      1061\n",
      "weighted avg       0.90      0.96      0.93      1061\n",
      " samples avg       0.91      0.96      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5916, Accuracy: 0.7198, F1 Micro: 0.7198, F1 Macro: 0.4185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4176, Accuracy: 0.7295, F1 Micro: 0.7295, F1 Macro: 0.4953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3993, Accuracy: 0.8116, F1 Micro: 0.8116, F1 Macro: 0.7652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2715, Accuracy: 0.8551, F1 Micro: 0.8551, F1 Macro: 0.8143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1754, Accuracy: 0.8841, F1 Micro: 0.8841, F1 Macro: 0.8643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.8889, F1 Micro: 0.8889, F1 Macro: 0.8716\n",
      "Epoch 7/10, Train Loss: 0.0595, Accuracy: 0.8792, F1 Micro: 0.8792, F1 Macro: 0.8626\n",
      "Epoch 8/10, Train Loss: 0.0738, Accuracy: 0.8841, F1 Micro: 0.8841, F1 Macro: 0.8676\n",
      "Epoch 9/10, Train Loss: 0.1128, Accuracy: 0.8744, F1 Micro: 0.8744, F1 Macro: 0.8576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.8937, F1 Micro: 0.8937, F1 Macro: 0.8767\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.8937, F1 Micro: 0.8937, F1 Macro: 0.8767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.93      0.83        58\n",
      "    positive       0.97      0.88      0.92       149\n",
      "\n",
      "    accuracy                           0.89       207\n",
      "   macro avg       0.86      0.91      0.88       207\n",
      "weighted avg       0.91      0.89      0.90       207\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8819, F1 Micro: 0.8819, F1 Macro: 0.7258\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.18      0.31        11\n",
      "     neutral       0.89      1.00      0.94       181\n",
      "    positive       1.00      0.46      0.63        24\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.96      0.55      0.63       216\n",
      "weighted avg       0.91      0.90      0.88       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.91      0.93      0.92       167\n",
      "    positive       0.74      0.61      0.67        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.78      0.78       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.92      0.71        12\n",
      "     neutral       0.88      0.86      0.87       152\n",
      "    positive       0.66      0.63      0.65        52\n",
      "\n",
      "    accuracy                           0.81       216\n",
      "   macro avg       0.71      0.80      0.74       216\n",
      "weighted avg       0.81      0.81      0.81       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.65      0.71        23\n",
      "     neutral       0.89      0.97      0.93       152\n",
      "    positive       0.87      0.66      0.75        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.76      0.80       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.93      0.99      0.96       186\n",
      "    positive       0.80      0.47      0.59        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.67      0.74       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.43      0.60        14\n",
      "     neutral       0.91      1.00      0.95       185\n",
      "    positive       0.83      0.29      0.43        17\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.57      0.66       216\n",
      "weighted avg       0.91      0.91      0.89       216\n",
      "\n",
      "Total train time: 73.56284618377686 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8317, F1 Micro: 0.8317, F1 Macro: 0.5024\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 15.124352216720581 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5851, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5174, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.505, Accuracy: 0.7969, F1 Micro: 0.8856, F1 Macro: 0.884\n",
      "Epoch 4/10, Train Loss: 0.4492, Accuracy: 0.7924, F1 Micro: 0.8834, F1 Macro: 0.8817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4321, Accuracy: 0.8155, F1 Micro: 0.894, F1 Macro: 0.8921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3652, Accuracy: 0.8438, F1 Micro: 0.909, F1 Macro: 0.9078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3197, Accuracy: 0.8817, F1 Micro: 0.929, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2617, Accuracy: 0.9234, F1 Micro: 0.9528, F1 Macro: 0.9505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2041, Accuracy: 0.9293, F1 Micro: 0.9561, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1857, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9535\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.99       187\n",
      "     machine       0.91      0.97      0.94       175\n",
      "      others       0.91      0.87      0.89       158\n",
      "        part       0.89      0.98      0.93       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.94      0.97      0.96      1061\n",
      "   macro avg       0.94      0.97      0.95      1061\n",
      "weighted avg       0.94      0.97      0.96      1061\n",
      " samples avg       0.94      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.622, Accuracy: 0.6987, F1 Micro: 0.6987, F1 Macro: 0.4113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4475, Accuracy: 0.795, F1 Micro: 0.795, F1 Macro: 0.705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2931, Accuracy: 0.8828, F1 Micro: 0.8828, F1 Macro: 0.8574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.189, Accuracy: 0.8954, F1 Micro: 0.8954, F1 Macro: 0.8829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.113, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9004\n",
      "Epoch 6/10, Train Loss: 0.1291, Accuracy: 0.8912, F1 Micro: 0.8912, F1 Macro: 0.8793\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.8828, F1 Micro: 0.8828, F1 Macro: 0.8715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9052\n",
      "Epoch 9/10, Train Loss: 0.0865, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.8873\n",
      "Epoch 10/10, Train Loss: 0.0613, Accuracy: 0.8912, F1 Micro: 0.8912, F1 Macro: 0.8793\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.86      0.87        72\n",
      "    positive       0.94      0.95      0.94       167\n",
      "\n",
      "    accuracy                           0.92       239\n",
      "   macro avg       0.91      0.90      0.91       239\n",
      "weighted avg       0.92      0.92      0.92       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.8406\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.88      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        16\n",
      "     neutral       0.91      0.97      0.94       167\n",
      "    positive       0.86      0.58      0.69        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.77      0.79       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.83      0.59        12\n",
      "     neutral       0.91      0.88      0.89       152\n",
      "    positive       0.71      0.65      0.68        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.69      0.79      0.72       216\n",
      "weighted avg       0.84      0.82      0.82       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.70      0.73        23\n",
      "     neutral       0.89      0.99      0.93       152\n",
      "    positive       0.96      0.61      0.75        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.87      0.76      0.80       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 81.03108334541321 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.604, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5138, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5052, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4541, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4295, Accuracy: 0.808, F1 Micro: 0.8895, F1 Macro: 0.8866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3623, Accuracy: 0.8661, F1 Micro: 0.9212, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3186, Accuracy: 0.8906, F1 Micro: 0.9339, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.265, Accuracy: 0.9144, F1 Micro: 0.9478, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2037, Accuracy: 0.9249, F1 Micro: 0.9536, F1 Macro: 0.9516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1913, Accuracy: 0.9293, F1 Micro: 0.9565, F1 Macro: 0.9549\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9293, F1 Micro: 0.9565, F1 Macro: 0.9549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      0.99      0.98       187\n",
      "     machine       0.92      0.96      0.94       175\n",
      "      others       0.88      0.96      0.92       158\n",
      "        part       0.90      0.99      0.94       158\n",
      "       price       0.96      1.00      0.98       192\n",
      "     service       0.95      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.93      0.98      0.96      1061\n",
      "   macro avg       0.93      0.98      0.95      1061\n",
      "weighted avg       0.93      0.98      0.96      1061\n",
      " samples avg       0.93      0.98      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6139, Accuracy: 0.6901, F1 Micro: 0.6901, F1 Macro: 0.4083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4866, Accuracy: 0.7559, F1 Micro: 0.7559, F1 Macro: 0.6212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3265, Accuracy: 0.8967, F1 Micro: 0.8967, F1 Macro: 0.8782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1679, Accuracy: 0.9108, F1 Micro: 0.9108, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0993, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1224, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9038\n",
      "Epoch 7/10, Train Loss: 0.1141, Accuracy: 0.8873, F1 Micro: 0.8873, F1 Macro: 0.878\n",
      "Epoch 8/10, Train Loss: 0.0696, Accuracy: 0.9155, F1 Micro: 0.9155, F1 Macro: 0.9035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0448, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9063\n",
      "Epoch 10/10, Train Loss: 0.0385, Accuracy: 0.9108, F1 Micro: 0.9108, F1 Macro: 0.8986\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.86      0.87        66\n",
      "    positive       0.94      0.95      0.94       147\n",
      "\n",
      "    accuracy                           0.92       213\n",
      "   macro avg       0.91      0.90      0.91       213\n",
      "weighted avg       0.92      0.92      0.92       213\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.8229\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.97      0.99      0.98       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.85      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.91      0.96      0.94       167\n",
      "    positive       0.84      0.64      0.72        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.78      0.81       216\n",
      "weighted avg       0.89      0.90      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.75      0.64        12\n",
      "     neutral       0.88      0.96      0.92       152\n",
      "    positive       0.85      0.56      0.67        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.76      0.76      0.75       216\n",
      "weighted avg       0.86      0.85      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.65      0.75        23\n",
      "     neutral       0.90      0.99      0.94       152\n",
      "    positive       0.90      0.68      0.78        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.78      0.82       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.95      1.00      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.75      0.81       216\n",
      "weighted avg       0.95      0.95      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.95      1.00      0.98       185\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.96      0.79      0.86       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Total train time: 84.99764370918274 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6466, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5279, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5166, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4746, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4489, Accuracy: 0.8073, F1 Micro: 0.891, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3929, Accuracy: 0.843, F1 Micro: 0.9084, F1 Macro: 0.9068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3399, Accuracy: 0.8832, F1 Micro: 0.9296, F1 Macro: 0.9272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2821, Accuracy: 0.9196, F1 Micro: 0.9505, F1 Macro: 0.9484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2221, Accuracy: 0.9315, F1 Micro: 0.9572, F1 Macro: 0.9547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2055, Accuracy: 0.9353, F1 Micro: 0.9596, F1 Macro: 0.9575\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9353, F1 Micro: 0.9596, F1 Macro: 0.9575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.98       187\n",
      "     machine       0.92      0.96      0.94       175\n",
      "      others       0.91      0.92      0.91       158\n",
      "        part       0.90      0.97      0.94       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.97      0.96      1061\n",
      "   macro avg       0.94      0.97      0.96      1061\n",
      "weighted avg       0.95      0.97      0.96      1061\n",
      " samples avg       0.95      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5695, Accuracy: 0.6929, F1 Micro: 0.6929, F1 Macro: 0.4093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4186, Accuracy: 0.8423, F1 Micro: 0.8423, F1 Macro: 0.8036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3502, Accuracy: 0.8921, F1 Micro: 0.8921, F1 Macro: 0.8691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1551, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0794, Accuracy: 0.9378, F1 Micro: 0.9378, F1 Macro: 0.9277\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9212, F1 Micro: 0.9212, F1 Macro: 0.9114\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8983\n",
      "Epoch 8/10, Train Loss: 0.0529, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.907\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9237\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.922\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9378, F1 Micro: 0.9378, F1 Macro: 0.9277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.92      0.90        74\n",
      "    positive       0.96      0.95      0.95       167\n",
      "\n",
      "    accuracy                           0.94       241\n",
      "   macro avg       0.92      0.93      0.93       241\n",
      "weighted avg       0.94      0.94      0.94       241\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.8546\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.91      0.96      0.94       167\n",
      "    positive       0.81      0.64      0.71        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.78      0.82       216\n",
      "weighted avg       0.89      0.90      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.91      0.91      0.91       152\n",
      "    positive       0.77      0.77      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.84      0.84       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.65      0.75        23\n",
      "     neutral       0.90      0.98      0.94       152\n",
      "    positive       0.82      0.68      0.75        41\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.77      0.81       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.82      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 83.36204886436462 s\n",
      "Averaged - Iteration 208: Accuracy: 0.8616, F1 Micro: 0.8616, F1 Macro: 0.6147\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 14.692261934280396 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5892, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5212, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4668, Accuracy: 0.8013, F1 Micro: 0.8881, F1 Macro: 0.8866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4232, Accuracy: 0.8222, F1 Micro: 0.8977, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3752, Accuracy: 0.8638, F1 Micro: 0.9196, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2988, Accuracy: 0.9182, F1 Micro: 0.95, F1 Macro: 0.9487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2355, Accuracy: 0.933, F1 Micro: 0.9583, F1 Macro: 0.9556\n",
      "Epoch 8/10, Train Loss: 0.1874, Accuracy: 0.9293, F1 Micro: 0.9557, F1 Macro: 0.9525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1489, Accuracy: 0.9368, F1 Micro: 0.9607, F1 Macro: 0.9581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1297, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9629\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.92      0.97      0.95       175\n",
      "      others       0.89      0.94      0.91       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6186, Accuracy: 0.6709, F1 Micro: 0.6709, F1 Macro: 0.4015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4061, Accuracy: 0.8608, F1 Micro: 0.8608, F1 Macro: 0.8188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2296, Accuracy: 0.9241, F1 Micro: 0.9241, F1 Macro: 0.9146\n",
      "Epoch 4/10, Train Loss: 0.1336, Accuracy: 0.9156, F1 Micro: 0.9156, F1 Macro: 0.9087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1179, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9377\n",
      "Epoch 6/10, Train Loss: 0.1173, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.92\n",
      "Epoch 7/10, Train Loss: 0.1196, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9241\n",
      "Epoch 8/10, Train Loss: 0.0644, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9112\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9156, F1 Micro: 0.9156, F1 Macro: 0.9051\n",
      "Epoch 10/10, Train Loss: 0.0347, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9117\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.91      0.92        78\n",
      "    positive       0.96      0.96      0.96       159\n",
      "\n",
      "    accuracy                           0.95       237\n",
      "   macro avg       0.94      0.94      0.94       237\n",
      "weighted avg       0.95      0.95      0.95       237\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9329, F1 Micro: 0.9329, F1 Macro: 0.8681\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.92      0.98      0.95       167\n",
      "    positive       0.87      0.61      0.71        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.80      0.82       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.89      0.93      0.91       152\n",
      "    positive       0.80      0.63      0.71        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.76      0.77      0.76       216\n",
      "weighted avg       0.85      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.93      0.98      0.96       152\n",
      "    positive       0.86      0.73      0.79        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 83.50103235244751 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5983, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.524, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4703, Accuracy: 0.7932, F1 Micro: 0.8841, F1 Macro: 0.8825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4166, Accuracy: 0.8356, F1 Micro: 0.9049, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3644, Accuracy: 0.8772, F1 Micro: 0.926, F1 Macro: 0.9232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2895, Accuracy: 0.9115, F1 Micro: 0.9458, F1 Macro: 0.9442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.233, Accuracy: 0.9271, F1 Micro: 0.955, F1 Macro: 0.9534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1853, Accuracy: 0.933, F1 Micro: 0.9584, F1 Macro: 0.9563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1584, Accuracy: 0.9382, F1 Micro: 0.9616, F1 Macro: 0.9595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1311, Accuracy: 0.9397, F1 Micro: 0.9625, F1 Macro: 0.9606\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9397, F1 Micro: 0.9625, F1 Macro: 0.9606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.92      0.98      0.95       175\n",
      "      others       0.90      0.93      0.91       158\n",
      "        part       0.92      0.97      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6032, Accuracy: 0.6864, F1 Micro: 0.6864, F1 Macro: 0.407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4339, Accuracy: 0.7797, F1 Micro: 0.7797, F1 Macro: 0.6793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2428, Accuracy: 0.9025, F1 Micro: 0.9025, F1 Macro: 0.8888\n",
      "Epoch 4/10, Train Loss: 0.1424, Accuracy: 0.8814, F1 Micro: 0.8814, F1 Macro: 0.8726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1341, Accuracy: 0.9068, F1 Micro: 0.9068, F1 Macro: 0.8874\n",
      "Epoch 6/10, Train Loss: 0.1551, Accuracy: 0.8983, F1 Micro: 0.8983, F1 Macro: 0.8885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.126, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9183\n",
      "Epoch 8/10, Train Loss: 0.1142, Accuracy: 0.911, F1 Micro: 0.911, F1 Macro: 0.901\n",
      "Epoch 9/10, Train Loss: 0.045, Accuracy: 0.9153, F1 Micro: 0.9153, F1 Macro: 0.9054\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9144\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.93      0.89        74\n",
      "    positive       0.97      0.93      0.95       162\n",
      "\n",
      "    accuracy                           0.93       236\n",
      "   macro avg       0.91      0.93      0.92       236\n",
      "weighted avg       0.93      0.93      0.93       236\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.8648\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.92      0.98      0.95       167\n",
      "    positive       0.91      0.64      0.75        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.79      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.75      0.64        12\n",
      "     neutral       0.90      0.93      0.91       152\n",
      "    positive       0.84      0.69      0.76        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.77      0.79      0.77       216\n",
      "weighted avg       0.86      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.78      0.80        23\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 89.75663185119629 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6425, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.532, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4854, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4454, Accuracy: 0.814, F1 Micro: 0.8941, F1 Macro: 0.8925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4003, Accuracy: 0.8631, F1 Micro: 0.9187, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3229, Accuracy: 0.9137, F1 Micro: 0.9467, F1 Macro: 0.9441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2598, Accuracy: 0.9353, F1 Micro: 0.9599, F1 Macro: 0.9583\n",
      "Epoch 8/10, Train Loss: 0.2015, Accuracy: 0.9338, F1 Micro: 0.9586, F1 Macro: 0.9566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1727, Accuracy: 0.9382, F1 Micro: 0.9615, F1 Macro: 0.9589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1433, Accuracy: 0.9457, F1 Micro: 0.9659, F1 Macro: 0.9639\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9457, F1 Micro: 0.9659, F1 Macro: 0.9639\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.92      0.91      0.91       158\n",
      "        part       0.93      0.98      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5916, Accuracy: 0.6865, F1 Micro: 0.6865, F1 Macro: 0.4071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4415, Accuracy: 0.869, F1 Micro: 0.869, F1 Macro: 0.844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2008, Accuracy: 0.8929, F1 Micro: 0.8929, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1486, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9096\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.8968, F1 Micro: 0.8968, F1 Macro: 0.8859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1082, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9084\n",
      "Epoch 7/10, Train Loss: 0.0594, Accuracy: 0.9127, F1 Micro: 0.9127, F1 Macro: 0.9012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9127\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8947\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9127\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.89      0.88        79\n",
      "    positive       0.95      0.94      0.94       173\n",
      "\n",
      "    accuracy                           0.92       252\n",
      "   macro avg       0.91      0.91      0.91       252\n",
      "weighted avg       0.92      0.92      0.92       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.8695\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.92      0.90      0.91       152\n",
      "    positive       0.79      0.71      0.75        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.74      0.82      0.76       216\n",
      "weighted avg       0.86      0.85      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 89.93186163902283 s\n",
      "Averaged - Iteration 274: Accuracy: 0.8795, F1 Micro: 0.8795, F1 Macro: 0.6779\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 14.260364294052124 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5659, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4971, Accuracy: 0.7984, F1 Micro: 0.8866, F1 Macro: 0.8851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4537, Accuracy: 0.8199, F1 Micro: 0.8973, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3879, Accuracy: 0.8646, F1 Micro: 0.9198, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3153, Accuracy: 0.9219, F1 Micro: 0.9523, F1 Macro: 0.9505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2349, Accuracy: 0.9427, F1 Micro: 0.9643, F1 Macro: 0.9625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1805, Accuracy: 0.9449, F1 Micro: 0.966, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1401, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9669\n",
      "Epoch 9/10, Train Loss: 0.115, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0931, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9704\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5687, Accuracy: 0.6792, F1 Micro: 0.6792, F1 Macro: 0.4166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4242, Accuracy: 0.9083, F1 Micro: 0.9083, F1 Macro: 0.8993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1868, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.9156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1338, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1358, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.9161\n",
      "Epoch 6/10, Train Loss: 0.0905, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.075, Accuracy: 0.9292, F1 Micro: 0.9292, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0511, Accuracy: 0.9292, F1 Micro: 0.9292, F1 Macro: 0.9206\n",
      "Epoch 9/10, Train Loss: 0.0477, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0695, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9255\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.90        78\n",
      "    positive       0.97      0.93      0.95       162\n",
      "\n",
      "    accuracy                           0.93       240\n",
      "   macro avg       0.92      0.93      0.93       240\n",
      "weighted avg       0.94      0.93      0.93       240\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9452, F1 Micro: 0.9452, F1 Macro: 0.889\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.78      0.82       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.86      0.71      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.80      0.80       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 100.47985291481018 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5763, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4959, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4529, Accuracy: 0.8073, F1 Micro: 0.891, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3901, Accuracy: 0.8757, F1 Micro: 0.9258, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3207, Accuracy: 0.9129, F1 Micro: 0.9467, F1 Macro: 0.9449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.24, Accuracy: 0.9353, F1 Micro: 0.96, F1 Macro: 0.9587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1896, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9646\n",
      "Epoch 8/10, Train Loss: 0.145, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1193, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1001, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9691\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5792, Accuracy: 0.6709, F1 Micro: 0.6709, F1 Macro: 0.4015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4341, Accuracy: 0.8692, F1 Micro: 0.8692, F1 Macro: 0.8615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2088, Accuracy: 0.9114, F1 Micro: 0.9114, F1 Macro: 0.9029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1485, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.101, Accuracy: 0.9241, F1 Micro: 0.9241, F1 Macro: 0.9166\n",
      "Epoch 6/10, Train Loss: 0.0791, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9106\n",
      "Epoch 7/10, Train Loss: 0.0713, Accuracy: 0.9114, F1 Micro: 0.9114, F1 Macro: 0.8987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.087, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.921\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.921\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.921\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        78\n",
      "    positive       0.97      0.92      0.94       159\n",
      "\n",
      "    accuracy                           0.93       237\n",
      "   macro avg       0.91      0.93      0.92       237\n",
      "weighted avg       0.93      0.93      0.93       237\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.8831\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.81      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.90      0.95      0.93       152\n",
      "    positive       0.88      0.67      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.79      0.78       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.94      0.71      0.81        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.84      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 99.93128418922424 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6111, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5085, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4713, Accuracy: 0.8021, F1 Micro: 0.8886, F1 Macro: 0.8872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4117, Accuracy: 0.8571, F1 Micro: 0.9164, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.343, Accuracy: 0.9226, F1 Micro: 0.9525, F1 Macro: 0.951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2619, Accuracy: 0.9397, F1 Micro: 0.9624, F1 Macro: 0.9603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2019, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1536, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1293, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9711\n",
      "Epoch 10/10, Train Loss: 0.1043, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9711\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.92      0.93      0.92       158\n",
      "        part       0.95      1.00      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.577, Accuracy: 0.6694, F1 Micro: 0.6694, F1 Macro: 0.401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4027, Accuracy: 0.905, F1 Micro: 0.905, F1 Macro: 0.8943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.203, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9078\n",
      "Epoch 4/10, Train Loss: 0.1275, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9018\n",
      "Epoch 5/10, Train Loss: 0.0585, Accuracy: 0.9132, F1 Micro: 0.9132, F1 Macro: 0.901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0903, Accuracy: 0.9215, F1 Micro: 0.9215, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9256, F1 Micro: 0.9256, F1 Macro: 0.9175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0697, Accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.9218\n",
      "Epoch 9/10, Train Loss: 0.054, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9093\n",
      "Epoch 10/10, Train Loss: 0.0802, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8973\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.9218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.93      0.90        80\n",
      "    positive       0.96      0.93      0.95       162\n",
      "\n",
      "    accuracy                           0.93       242\n",
      "   macro avg       0.92      0.93      0.92       242\n",
      "weighted avg       0.93      0.93      0.93       242\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.8678\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.69      0.73        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.84      0.64      0.72        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.77      0.81       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.92      0.93      0.92       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.84      0.82       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.74      0.71        23\n",
      "     neutral       0.95      0.97      0.96       152\n",
      "    positive       0.86      0.76      0.81        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.82      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.86      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 92.46103405952454 s\n",
      "Averaged - Iteration 333: Accuracy: 0.892, F1 Micro: 0.892, F1 Macro: 0.7183\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 12.838666200637817 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5649, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4773, Accuracy: 0.8036, F1 Micro: 0.8888, F1 Macro: 0.8872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4331, Accuracy: 0.8363, F1 Micro: 0.9053, F1 Macro: 0.9045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3483, Accuracy: 0.8973, F1 Micro: 0.9383, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2671, Accuracy: 0.9435, F1 Micro: 0.9649, F1 Macro: 0.9632\n",
      "Epoch 6/10, Train Loss: 0.1974, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9611\n",
      "Epoch 7/10, Train Loss: 0.1453, Accuracy: 0.936, F1 Micro: 0.9598, F1 Macro: 0.9566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1168, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0979, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Epoch 10/10, Train Loss: 0.0844, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.972\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.98      0.94       158\n",
      "        part       0.96      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5751, Accuracy: 0.7008, F1 Micro: 0.7008, F1 Macro: 0.4984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3728, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.162, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1167, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0841, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9408\n",
      "Epoch 6/10, Train Loss: 0.0803, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 7/10, Train Loss: 0.0818, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9365\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9357\n",
      "Epoch 9/10, Train Loss: 0.0578, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9365\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9365\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        81\n",
      "    positive       0.97      0.94      0.96       163\n",
      "\n",
      "    accuracy                           0.95       244\n",
      "   macro avg       0.94      0.95      0.94       244\n",
      "weighted avg       0.95      0.95      0.95       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8969\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.75      0.62        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.92      0.69      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.80      0.78       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.96      0.97      0.97       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 96.13788104057312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5699, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4866, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4326, Accuracy: 0.8341, F1 Micro: 0.9045, F1 Macro: 0.9034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3534, Accuracy: 0.901, F1 Micro: 0.9397, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2723, Accuracy: 0.9382, F1 Micro: 0.9617, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2119, Accuracy: 0.9472, F1 Micro: 0.967, F1 Macro: 0.9652\n",
      "Epoch 7/10, Train Loss: 0.1572, Accuracy: 0.9457, F1 Micro: 0.966, F1 Macro: 0.9635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1261, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1052, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0903, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6114, Accuracy: 0.6612, F1 Micro: 0.6612, F1 Macro: 0.398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3841, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1788, Accuracy: 0.9132, F1 Micro: 0.9132, F1 Macro: 0.9055\n",
      "Epoch 4/10, Train Loss: 0.1363, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1054, Accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.9227\n",
      "Epoch 6/10, Train Loss: 0.11, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1131, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 8/10, Train Loss: 0.0951, Accuracy: 0.9256, F1 Micro: 0.9256, F1 Macro: 0.9188\n",
      "Epoch 9/10, Train Loss: 0.0447, Accuracy: 0.9256, F1 Micro: 0.9256, F1 Macro: 0.9184\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9256, F1 Micro: 0.9256, F1 Macro: 0.9184\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        82\n",
      "    positive       0.97      0.93      0.95       160\n",
      "\n",
      "    accuracy                           0.93       242\n",
      "   macro avg       0.92      0.94      0.93       242\n",
      "weighted avg       0.94      0.93      0.93       242\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.8938\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.95      0.67      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.80      0.80       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 103.21051216125488 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6012, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4937, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4653, Accuracy: 0.8147, F1 Micro: 0.8947, F1 Macro: 0.8933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3851, Accuracy: 0.8929, F1 Micro: 0.9355, F1 Macro: 0.9344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.298, Accuracy: 0.9375, F1 Micro: 0.9608, F1 Macro: 0.9591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2226, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9673\n",
      "Epoch 7/10, Train Loss: 0.1659, Accuracy: 0.9464, F1 Micro: 0.9664, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1324, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.11, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9778\n",
      "Epoch 10/10, Train Loss: 0.0949, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5695, Accuracy: 0.6776, F1 Micro: 0.6776, F1 Macro: 0.4577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3716, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2301, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9099\n",
      "Epoch 4/10, Train Loss: 0.1233, Accuracy: 0.9061, F1 Micro: 0.9061, F1 Macro: 0.9001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0985, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9319\n",
      "Epoch 6/10, Train Loss: 0.0772, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9275\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9189\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.916\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9121\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9194\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        83\n",
      "    positive       0.96      0.95      0.95       162\n",
      "\n",
      "    accuracy                           0.94       245\n",
      "   macro avg       0.93      0.93      0.93       245\n",
      "weighted avg       0.94      0.94      0.94       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8804\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.96      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.75      0.73        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.83      0.82      0.82       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.83      0.62        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.95      0.73      0.83        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.79      0.84      0.80       216\n",
      "weighted avg       0.91      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.77      0.74        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 101.68248105049133 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9015, F1 Micro: 0.9015, F1 Macro: 0.747\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 12.183520078659058 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.558, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4758, Accuracy: 0.8043, F1 Micro: 0.8897, F1 Macro: 0.8884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4272, Accuracy: 0.8571, F1 Micro: 0.9159, F1 Macro: 0.9154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3221, Accuracy: 0.9405, F1 Micro: 0.9634, F1 Macro: 0.962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2398, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1765, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.9643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1329, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1135, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0861, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.97\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.91      0.91       158\n",
      "        part       0.97      0.96      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5851, Accuracy: 0.8308, F1 Micro: 0.8308, F1 Macro: 0.7744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2873, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1663, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.0934, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9209\n",
      "Epoch 5/10, Train Loss: 0.0987, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.099, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0573, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9309\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9259\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9167\n",
      "Epoch 10/10, Train Loss: 0.057, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9213\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        82\n",
      "    positive       0.98      0.93      0.95       178\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.92      0.95      0.93       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8983\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.91      0.91      0.91       152\n",
      "    positive       0.78      0.75      0.76        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.80      0.83      0.82       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.91      0.84        23\n",
      "     neutral       0.97      0.96      0.97       152\n",
      "    positive       0.87      0.83      0.85        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.90      0.89       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 109.533194065094 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5662, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4802, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4329, Accuracy: 0.8527, F1 Micro: 0.9137, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3389, Accuracy: 0.9115, F1 Micro: 0.9456, F1 Macro: 0.9434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2585, Accuracy: 0.9397, F1 Micro: 0.9628, F1 Macro: 0.9614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1896, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1454, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9706\n",
      "Epoch 8/10, Train Loss: 0.122, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9682\n",
      "Epoch 9/10, Train Loss: 0.0947, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9681\n",
      "Epoch 10/10, Train Loss: 0.0776, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9694\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.7792, F1 Micro: 0.7792, F1 Macro: 0.697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3206, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9094\n",
      "Epoch 3/10, Train Loss: 0.1621, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.9036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1363, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9299\n",
      "Epoch 5/10, Train Loss: 0.092, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9307\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9303\n",
      "Epoch 10/10, Train Loss: 0.0254, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9255\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        79\n",
      "    positive       0.97      0.94      0.95       161\n",
      "\n",
      "    accuracy                           0.94       240\n",
      "   macro avg       0.92      0.94      0.93       240\n",
      "weighted avg       0.94      0.94      0.94       240\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.8659\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.81      0.74        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.75      0.56        12\n",
      "     neutral       0.91      0.92      0.92       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.80      0.76       216\n",
      "weighted avg       0.88      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.91      0.82        23\n",
      "     neutral       0.97      0.96      0.97       152\n",
      "    positive       0.87      0.80      0.84        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.89      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 104.55543923377991 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5945, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4885, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4564, Accuracy: 0.8341, F1 Micro: 0.9041, F1 Macro: 0.9031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3593, Accuracy: 0.9174, F1 Micro: 0.949, F1 Macro: 0.9469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2757, Accuracy: 0.942, F1 Micro: 0.9638, F1 Macro: 0.9623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2034, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.157, Accuracy: 0.9643, F1 Micro: 0.9778, F1 Macro: 0.9769\n",
      "Epoch 8/10, Train Loss: 0.1298, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9705\n",
      "Epoch 9/10, Train Loss: 0.1002, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9714\n",
      "Epoch 10/10, Train Loss: 0.0827, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9732\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9778, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.90      0.98      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5895, Accuracy: 0.7155, F1 Micro: 0.7155, F1 Macro: 0.5878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3578, Accuracy: 0.9372, F1 Micro: 0.9372, F1 Macro: 0.9317\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9148\n",
      "Epoch 4/10, Train Loss: 0.162, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1031, Accuracy: 0.9456, F1 Micro: 0.9456, F1 Macro: 0.9383\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.9372, F1 Micro: 0.9372, F1 Macro: 0.9317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0973, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0715, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9491\n",
      "Epoch 9/10, Train Loss: 0.057, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9446\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9452\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9491\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        81\n",
      "    positive       0.97      0.96      0.96       158\n",
      "\n",
      "    accuracy                           0.95       239\n",
      "   macro avg       0.95      0.95      0.95       239\n",
      "weighted avg       0.95      0.95      0.95       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8929\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.33      0.83      0.48        12\n",
      "     neutral       0.93      0.89      0.91       152\n",
      "    positive       0.93      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.73      0.81      0.73       216\n",
      "weighted avg       0.90      0.85      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 99.21091961860657 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9076, F1 Micro: 0.9076, F1 Macro: 0.7668\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 11.0722177028656 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4821, Accuracy: 0.8065, F1 Micro: 0.8908, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4238, Accuracy: 0.8661, F1 Micro: 0.9211, F1 Macro: 0.9209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3158, Accuracy: 0.942, F1 Micro: 0.9642, F1 Macro: 0.9629\n",
      "Epoch 5/10, Train Loss: 0.2226, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1677, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1347, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1063, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0934, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9735\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.92      0.92       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.599, Accuracy: 0.8538, F1 Micro: 0.8538, F1 Macro: 0.809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3061, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1365, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1037, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9193\n",
      "Epoch 5/10, Train Loss: 0.0887, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0985, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.924\n",
      "Epoch 10/10, Train Loss: 0.0715, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9313\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9077\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.85      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 114.07209205627441 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5456, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4828, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4118, Accuracy: 0.8802, F1 Micro: 0.9284, F1 Macro: 0.9273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3132, Accuracy: 0.9315, F1 Micro: 0.9576, F1 Macro: 0.9561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2238, Accuracy: 0.942, F1 Micro: 0.9637, F1 Macro: 0.9619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1724, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1382, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1105, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9728\n",
      "Epoch 9/10, Train Loss: 0.0915, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5908, Accuracy: 0.6932, F1 Micro: 0.6932, F1 Macro: 0.4547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3054, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9292\n",
      "Epoch 3/10, Train Loss: 0.1936, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1458, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1126, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9466\n",
      "Epoch 7/10, Train Loss: 0.0626, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.942\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9331\n",
      "Epoch 9/10, Train Loss: 0.081, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.942\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9171\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9466\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        81\n",
      "    positive       0.99      0.94      0.96       170\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.96      0.95       251\n",
      "weighted avg       0.96      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8948\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.82      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.91      0.94      0.93       152\n",
      "    positive       0.82      0.71      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.80      0.79       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.71      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.90      0.88       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 107.58228325843811 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5682, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4942, Accuracy: 0.7961, F1 Micro: 0.8856, F1 Macro: 0.8842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4372, Accuracy: 0.8616, F1 Micro: 0.9186, F1 Macro: 0.9182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3301, Accuracy: 0.9278, F1 Micro: 0.955, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.239, Accuracy: 0.9464, F1 Micro: 0.9664, F1 Macro: 0.9644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1814, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1462, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Epoch 8/10, Train Loss: 0.1136, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.097, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Epoch 10/10, Train Loss: 0.0763, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5632, Accuracy: 0.8907, F1 Micro: 0.8907, F1 Macro: 0.8748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3293, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9357\n",
      "Epoch 3/10, Train Loss: 0.1447, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1268, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9401\n",
      "Epoch 5/10, Train Loss: 0.0858, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9376\n",
      "Epoch 6/10, Train Loss: 0.0823, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9365\n",
      "Epoch 7/10, Train Loss: 0.0946, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0487, Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0587, Accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9508\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        82\n",
      "    positive       0.99      0.95      0.97       165\n",
      "\n",
      "    accuracy                           0.96       247\n",
      "   macro avg       0.94      0.96      0.95       247\n",
      "weighted avg       0.96      0.96      0.96       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9023\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.96      0.70      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.87       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.83      0.57        12\n",
      "     neutral       0.94      0.90      0.92       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.74      0.83      0.77       216\n",
      "weighted avg       0.89      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 107.44021987915039 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.7837\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 10.34786868095398 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.8162, F1 Micro: 0.8957, F1 Macro: 0.8946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3816, Accuracy: 0.8981, F1 Micro: 0.9385, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2753, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.9635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1864, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1488, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1171, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0908, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5615, Accuracy: 0.8833, F1 Micro: 0.8833, F1 Macro: 0.8718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2644, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9145\n",
      "Epoch 3/10, Train Loss: 0.1965, Accuracy: 0.8988, F1 Micro: 0.8988, F1 Macro: 0.891\n",
      "Epoch 4/10, Train Loss: 0.154, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1056, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9301\n",
      "Epoch 6/10, Train Loss: 0.0715, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9141\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9181\n",
      "Epoch 8/10, Train Loss: 0.0949, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0548, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.98      0.91        83\n",
      "    positive       0.99      0.92      0.95       174\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.92      0.95      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9078\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.84      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 112.85867500305176 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5433, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4799, Accuracy: 0.8103, F1 Micro: 0.8925, F1 Macro: 0.8912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3678, Accuracy: 0.9062, F1 Micro: 0.9427, F1 Macro: 0.9411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2833, Accuracy: 0.936, F1 Micro: 0.9598, F1 Macro: 0.9573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1941, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1531, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1231, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0979, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9728\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6189, Accuracy: 0.6759, F1 Micro: 0.6759, F1 Macro: 0.4033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3132, Accuracy: 0.8854, F1 Micro: 0.8854, F1 Macro: 0.877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2145, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9295\n",
      "Epoch 4/10, Train Loss: 0.1716, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1202, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "Epoch 7/10, Train Loss: 0.1147, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9415\n",
      "Epoch 8/10, Train Loss: 0.0787, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9415\n",
      "Epoch 9/10, Train Loss: 0.0894, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.933\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9334\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.96      0.95       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8917\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.75      0.73        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.91      0.92      0.92       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 114.00179672241211 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5698, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4951, Accuracy: 0.7976, F1 Micro: 0.8864, F1 Macro: 0.8849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3996, Accuracy: 0.9003, F1 Micro: 0.9396, F1 Macro: 0.9384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3007, Accuracy: 0.933, F1 Micro: 0.9581, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2102, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1655, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.1344, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1059, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 9/10, Train Loss: 0.0857, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.97      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5552, Accuracy: 0.811, F1 Micro: 0.811, F1 Macro: 0.7519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2912, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Epoch 3/10, Train Loss: 0.1878, Accuracy: 0.9055, F1 Micro: 0.9055, F1 Macro: 0.8982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1595, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1448, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9385\n",
      "Epoch 6/10, Train Loss: 0.114, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1128, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0836, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9473\n",
      "Epoch 9/10, Train Loss: 0.043, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9381\n",
      "Epoch 10/10, Train Loss: 0.0665, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        83\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       254\n",
      "   macro avg       0.94      0.96      0.95       254\n",
      "weighted avg       0.95      0.95      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8923\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.92      0.67        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.90      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.86      0.80       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.89      0.78      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.90      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.77      0.71        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.86      0.82      0.83       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 112.31079721450806 s\n",
      "Averaged - Iteration 517: Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.7963\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 9.182439804077148 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5471, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4894, Accuracy: 0.817, F1 Micro: 0.8959, F1 Macro: 0.8947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3988, Accuracy: 0.8914, F1 Micro: 0.9348, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2816, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1995, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9697\n",
      "Epoch 6/10, Train Loss: 0.1511, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1151, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5257, Accuracy: 0.8919, F1 Micro: 0.8919, F1 Macro: 0.8808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2779, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.9028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1775, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1677, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Epoch 5/10, Train Loss: 0.0983, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9258\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0842, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9341\n",
      "Epoch 8/10, Train Loss: 0.1107, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9171\n",
      "Epoch 10/10, Train Loss: 0.0389, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        83\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9107\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.85      0.85      0.85        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.91      0.76      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 118.7124891281128 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5574, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4907, Accuracy: 0.8006, F1 Micro: 0.8879, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3888, Accuracy: 0.9107, F1 Micro: 0.9457, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2789, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2004, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9687\n",
      "Epoch 6/10, Train Loss: 0.154, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1203, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0989, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5725, Accuracy: 0.7381, F1 Micro: 0.7381, F1 Macro: 0.5883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3188, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Epoch 3/10, Train Loss: 0.1425, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9264\n",
      "Epoch 4/10, Train Loss: 0.1323, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9264\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9099\n",
      "Epoch 6/10, Train Loss: 0.1216, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0747, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "Epoch 9/10, Train Loss: 0.1008, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9264\n",
      "Epoch 10/10, Train Loss: 0.0414, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9243\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       169\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.94      0.95      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.905\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.81      0.85       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.96      0.93       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 110.5010974407196 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5782, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5048, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4205, Accuracy: 0.9048, F1 Micro: 0.9424, F1 Macro: 0.941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3096, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2234, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1683, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "Epoch 7/10, Train Loss: 0.1267, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1083, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0904, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0711, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5731, Accuracy: 0.8651, F1 Micro: 0.8651, F1 Macro: 0.8386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2871, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9516\n",
      "Epoch 3/10, Train Loss: 0.167, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9473\n",
      "Epoch 4/10, Train Loss: 0.1365, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9467\n",
      "Epoch 5/10, Train Loss: 0.1407, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "Epoch 6/10, Train Loss: 0.0879, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9434\n",
      "Epoch 7/10, Train Loss: 0.1141, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9379\n",
      "Epoch 8/10, Train Loss: 0.0876, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9516\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9431\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        84\n",
      "    positive       0.98      0.95      0.97       168\n",
      "\n",
      "    accuracy                           0.96       252\n",
      "   macro avg       0.95      0.96      0.95       252\n",
      "weighted avg       0.96      0.96      0.96       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9107\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.83      0.69        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.80      0.84      0.81       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 109.28780436515808 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.8075\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 8.117871522903442 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5375, Accuracy: 0.7976, F1 Micro: 0.8858, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4785, Accuracy: 0.8237, F1 Micro: 0.8993, F1 Macro: 0.8982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3825, Accuracy: 0.9271, F1 Micro: 0.9553, F1 Macro: 0.954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2587, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1856, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1358, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 10/10, Train Loss: 0.0606, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5426, Accuracy: 0.7778, F1 Micro: 0.7778, F1 Macro: 0.6745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2866, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1805, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9483\n",
      "Epoch 4/10, Train Loss: 0.1702, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9344\n",
      "Epoch 5/10, Train Loss: 0.0948, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9435\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9441\n",
      "Epoch 7/10, Train Loss: 0.0758, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9215\n",
      "Epoch 8/10, Train Loss: 0.0908, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9348\n",
      "Epoch 9/10, Train Loss: 0.091, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Epoch 10/10, Train Loss: 0.0828, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        84\n",
      "    positive       0.98      0.95      0.97       177\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.96      0.95       261\n",
      "weighted avg       0.96      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9042\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.87      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 120.06656098365784 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5433, Accuracy: 0.7909, F1 Micro: 0.8826, F1 Macro: 0.8808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4829, Accuracy: 0.8118, F1 Micro: 0.8934, F1 Macro: 0.892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3715, Accuracy: 0.9271, F1 Micro: 0.9551, F1 Macro: 0.9533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.259, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1914, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1407, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9749\n",
      "Epoch 7/10, Train Loss: 0.1146, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9691\n",
      "Epoch 8/10, Train Loss: 0.0895, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5455, Accuracy: 0.8679, F1 Micro: 0.8679, F1 Macro: 0.8366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2733, Accuracy: 0.8679, F1 Micro: 0.8679, F1 Macro: 0.8597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2124, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1329, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9397\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9352\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0918, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9445\n",
      "Epoch 8/10, Train Loss: 0.0965, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9405\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9205\n",
      "Epoch 10/10, Train Loss: 0.0559, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9149\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        84\n",
      "    positive       0.98      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.94      0.95      0.94       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9028\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.90      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 117.78727459907532 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5623, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4879, Accuracy: 0.8147, F1 Micro: 0.8948, F1 Macro: 0.8936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3988, Accuracy: 0.907, F1 Micro: 0.9436, F1 Macro: 0.9424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.277, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.9644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2056, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1486, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Epoch 7/10, Train Loss: 0.1211, Accuracy: 0.9531, F1 Micro: 0.9704, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9765\n",
      "Epoch 9/10, Train Loss: 0.0839, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5588, Accuracy: 0.8706, F1 Micro: 0.8706, F1 Macro: 0.8423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.28, Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1919, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9313\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1158, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9336\n",
      "Epoch 6/10, Train Loss: 0.1135, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0898, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9477\n",
      "Epoch 8/10, Train Loss: 0.0755, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9519\n",
      "Epoch 10/10, Train Loss: 0.0865, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9252\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        84\n",
      "    positive       0.98      0.95      0.97       171\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.95      0.96      0.95       255\n",
      "weighted avg       0.96      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9051\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 120.3177580833435 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9238, F1 Micro: 0.9238, F1 Macro: 0.8163\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 7.982858657836914 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5421, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4829, Accuracy: 0.8311, F1 Micro: 0.9033, F1 Macro: 0.9024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3668, Accuracy: 0.9293, F1 Micro: 0.9569, F1 Macro: 0.956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2455, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1688, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1342, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Epoch 8/10, Train Loss: 0.0807, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9752\n",
      "Epoch 9/10, Train Loss: 0.0694, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5057, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9157\n",
      "Epoch 2/10, Train Loss: 0.2253, Accuracy: 0.8911, F1 Micro: 0.8911, F1 Macro: 0.8836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1738, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1294, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Epoch 5/10, Train Loss: 0.0888, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9246\n",
      "Epoch 6/10, Train Loss: 0.0963, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.944\n",
      "Epoch 8/10, Train Loss: 0.0362, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9398\n",
      "Epoch 9/10, Train Loss: 0.0529, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9398\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9234\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        84\n",
      "    positive       0.99      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.93      0.96      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9084\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.85      0.76        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.87      0.86       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 118.97601103782654 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5536, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4847, Accuracy: 0.8155, F1 Micro: 0.8951, F1 Macro: 0.8937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3588, Accuracy: 0.9204, F1 Micro: 0.9511, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2465, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1783, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1391, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Epoch 7/10, Train Loss: 0.1083, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0826, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0716, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9747\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5252, Accuracy: 0.8588, F1 Micro: 0.8588, F1 Macro: 0.8251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2498, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Epoch 3/10, Train Loss: 0.188, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9124\n",
      "Epoch 4/10, Train Loss: 0.1407, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9131\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9238\n",
      "Epoch 6/10, Train Loss: 0.1023, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9092\n",
      "Epoch 7/10, Train Loss: 0.0891, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9018\n",
      "Epoch 8/10, Train Loss: 0.1284, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0996, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.93      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9114\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.86      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 117.23712539672852 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5685, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4969, Accuracy: 0.8103, F1 Micro: 0.8925, F1 Macro: 0.891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3754, Accuracy: 0.9263, F1 Micro: 0.9545, F1 Macro: 0.9533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2611, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1858, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9755\n",
      "Epoch 6/10, Train Loss: 0.1485, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1142, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.9744\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0675, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5225, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8857\n",
      "Epoch 2/10, Train Loss: 0.221, Accuracy: 0.8808, F1 Micro: 0.8808, F1 Macro: 0.8738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1898, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1432, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1104, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9358\n",
      "Epoch 6/10, Train Loss: 0.1184, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9449\n",
      "Epoch 8/10, Train Loss: 0.0459, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9327\n",
      "Epoch 9/10, Train Loss: 0.049, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9338\n",
      "Epoch 10/10, Train Loss: 0.0544, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        86\n",
      "    positive       0.99      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.96      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9139\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 120.63271903991699 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.8242\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 7.233957767486572 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5333, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.458, Accuracy: 0.84, F1 Micro: 0.9074, F1 Macro: 0.9067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3481, Accuracy: 0.9315, F1 Micro: 0.9574, F1 Macro: 0.9551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2315, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1575, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9733\n",
      "Epoch 6/10, Train Loss: 0.1207, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1047, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.0806, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "Epoch 10/10, Train Loss: 0.0552, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.97      0.92      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5121, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.8911\n",
      "Epoch 2/10, Train Loss: 0.2251, Accuracy: 0.9041, F1 Micro: 0.9041, F1 Macro: 0.8951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1761, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9251\n",
      "Epoch 4/10, Train Loss: 0.1057, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1279, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9291\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9267\n",
      "Epoch 7/10, Train Loss: 0.0965, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0492, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9448\n",
      "Epoch 9/10, Train Loss: 0.076, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9338\n",
      "Epoch 10/10, Train Loss: 0.0814, Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.9414\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.92      0.92        87\n",
      "    positive       0.96      0.97      0.96       184\n",
      "\n",
      "    accuracy                           0.95       271\n",
      "   macro avg       0.95      0.94      0.94       271\n",
      "weighted avg       0.95      0.95      0.95       271\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9181\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.97      0.92      0.94       152\n",
      "    positive       0.79      0.88      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.91      0.89       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.92      0.80        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.89      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 128.2062954902649 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5392, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4551, Accuracy: 0.8557, F1 Micro: 0.9148, F1 Macro: 0.9135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3359, Accuracy: 0.9323, F1 Micro: 0.958, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2268, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1627, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1036, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0796, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5463, Accuracy: 0.7093, F1 Micro: 0.7093, F1 Macro: 0.5313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3161, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.924\n",
      "Epoch 3/10, Train Loss: 0.1861, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.13, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9348\n",
      "Epoch 5/10, Train Loss: 0.1351, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0937, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9393\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "Epoch 9/10, Train Loss: 0.0708, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.908\n",
      "Epoch 10/10, Train Loss: 0.0777, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9169\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9052\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 125.84536457061768 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5568, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4735, Accuracy: 0.8244, F1 Micro: 0.8987, F1 Macro: 0.8974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3713, Accuracy: 0.9308, F1 Micro: 0.9569, F1 Macro: 0.9552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2471, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1755, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9743\n",
      "Epoch 7/10, Train Loss: 0.1126, Accuracy: 0.9576, F1 Micro: 0.9731, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "Epoch 9/10, Train Loss: 0.0726, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5539, Accuracy: 0.8482, F1 Micro: 0.8482, F1 Macro: 0.8171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2757, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.919\n",
      "Epoch 3/10, Train Loss: 0.1564, Accuracy: 0.8872, F1 Micro: 0.8872, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1305, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1258, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9354\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9074\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.936\n",
      "Epoch 9/10, Train Loss: 0.1005, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9242\n",
      "Epoch 10/10, Train Loss: 0.0953, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9194\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.97      0.92        86\n",
      "    positive       0.98      0.93      0.95       171\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.95      0.94       257\n",
      "weighted avg       0.95      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9066\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 125.27921676635742 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.8308\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 6.548803806304932 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5457, Accuracy: 0.8036, F1 Micro: 0.8892, F1 Macro: 0.8878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4632, Accuracy: 0.8371, F1 Micro: 0.9064, F1 Macro: 0.9055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3392, Accuracy: 0.9457, F1 Micro: 0.9664, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2357, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1593, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9725\n",
      "Epoch 6/10, Train Loss: 0.1228, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0951, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Epoch 8/10, Train Loss: 0.076, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0542, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5271, Accuracy: 0.8775, F1 Micro: 0.8775, F1 Macro: 0.8548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2608, Accuracy: 0.8933, F1 Micro: 0.8933, F1 Macro: 0.8878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.176, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1561, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9435\n",
      "Epoch 5/10, Train Loss: 0.1291, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9341\n",
      "Epoch 6/10, Train Loss: 0.1104, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.939\n",
      "Epoch 7/10, Train Loss: 0.0965, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9194\n",
      "Epoch 8/10, Train Loss: 0.104, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.903\n",
      "Epoch 9/10, Train Loss: 0.0837, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9253\n",
      "Epoch 10/10, Train Loss: 0.08, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9383\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       167\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.95      0.94       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9158\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 123.5713050365448 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5488, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4599, Accuracy: 0.8497, F1 Micro: 0.9129, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.34, Accuracy: 0.9345, F1 Micro: 0.9593, F1 Macro: 0.9571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.237, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1648, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0976, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 8/10, Train Loss: 0.0773, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.0645, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0541, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5015, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2441, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1542, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1434, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1233, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9109\n",
      "Epoch 7/10, Train Loss: 0.0966, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9336\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9435\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9234\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9109\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        84\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.95      0.95       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9037\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 128.1388943195343 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5669, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4682, Accuracy: 0.8326, F1 Micro: 0.9039, F1 Macro: 0.903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3485, Accuracy: 0.9353, F1 Micro: 0.9599, F1 Macro: 0.958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2475, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1717, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1333, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9791\n",
      "Epoch 7/10, Train Loss: 0.1028, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.9591, F1 Micro: 0.974, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.528, Accuracy: 0.8794, F1 Micro: 0.8794, F1 Macro: 0.8568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2583, Accuracy: 0.8794, F1 Micro: 0.8794, F1 Macro: 0.8729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1938, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1568, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1395, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1141, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9364\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9251\n",
      "Epoch 9/10, Train Loss: 0.0838, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9357\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9171\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 128.08347749710083 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.8366\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 6.3207926750183105 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5323, Accuracy: 0.7984, F1 Micro: 0.8861, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4596, Accuracy: 0.8534, F1 Micro: 0.9148, F1 Macro: 0.9142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3156, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2053, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1169, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5246, Accuracy: 0.8846, F1 Micro: 0.8846, F1 Macro: 0.8738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2705, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Epoch 3/10, Train Loss: 0.1688, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1507, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1428, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9264\n",
      "Epoch 7/10, Train Loss: 0.1112, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Epoch 8/10, Train Loss: 0.0745, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9227\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9101\n",
      "Epoch 10/10, Train Loss: 0.082, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9052\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.92      0.65        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.87      0.79       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 134.44447422027588 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5379, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4464, Accuracy: 0.8884, F1 Micro: 0.9331, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3027, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2052, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1547, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1169, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0755, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9787\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5079, Accuracy: 0.9004, F1 Micro: 0.9004, F1 Macro: 0.8879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2672, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1717, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1294, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9435\n",
      "Epoch 7/10, Train Loss: 0.0888, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9233\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9215\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        85\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9053\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.83      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.88      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 133.24774384498596 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5575, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.468, Accuracy: 0.8586, F1 Micro: 0.9175, F1 Macro: 0.9163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.324, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.9587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2176, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1675, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 7/10, Train Loss: 0.0996, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0803, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0659, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "Epoch 10/10, Train Loss: 0.0584, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.876, F1 Micro: 0.876, F1 Macro: 0.8655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.289, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2004, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 4/10, Train Loss: 0.1641, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9324\n",
      "Epoch 5/10, Train Loss: 0.1346, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9163\n",
      "Epoch 6/10, Train Loss: 0.0663, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9324\n",
      "Epoch 7/10, Train Loss: 0.1214, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9324\n",
      "Epoch 8/10, Train Loss: 0.0778, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9243\n",
      "Epoch 9/10, Train Loss: 0.0835, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.931\n",
      "Epoch 10/10, Train Loss: 0.0762, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        86\n",
      "    positive       0.96      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.93      0.94      0.94       258\n",
      "weighted avg       0.94      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9029\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.85      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 127.79306077957153 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9324, F1 Micro: 0.9324, F1 Macro: 0.8412\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.7048180103302 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.8013, F1 Micro: 0.8879, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4518, Accuracy: 0.8683, F1 Micro: 0.9227, F1 Macro: 0.9224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.31, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2017, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1115, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0825, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0495, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5168, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2539, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Epoch 3/10, Train Loss: 0.175, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1467, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9571\n",
      "Epoch 6/10, Train Loss: 0.0904, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9338\n",
      "Epoch 7/10, Train Loss: 0.0974, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "Epoch 10/10, Train Loss: 0.0755, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9443\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94        85\n",
      "    positive       0.99      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.96       260\n",
      "   macro avg       0.95      0.97      0.96       260\n",
      "weighted avg       0.96      0.96      0.96       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.925\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 132.99845337867737 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5407, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4528, Accuracy: 0.869, F1 Micro: 0.923, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3188, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2065, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9759\n",
      "Epoch 5/10, Train Loss: 0.1507, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Epoch 6/10, Train Loss: 0.1161, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0892, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.074, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0603, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9816\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.468, Accuracy: 0.8992, F1 Micro: 0.8992, F1 Macro: 0.8896\n",
      "Epoch 2/10, Train Loss: 0.2615, Accuracy: 0.8837, F1 Micro: 0.8837, F1 Macro: 0.8766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2041, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9444\n",
      "Epoch 4/10, Train Loss: 0.138, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Epoch 5/10, Train Loss: 0.1445, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9561\n",
      "Epoch 7/10, Train Loss: 0.1208, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9444\n",
      "Epoch 8/10, Train Loss: 0.1132, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9519\n",
      "Epoch 9/10, Train Loss: 0.1009, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.928\n",
      "Epoch 10/10, Train Loss: 0.0679, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.928\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.94      0.94        85\n",
      "    positive       0.97      0.97      0.97       173\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.96      0.96      0.96       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9182\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.93      0.82      0.87        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 130.1892008781433 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5622, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.47, Accuracy: 0.8482, F1 Micro: 0.9121, F1 Macro: 0.9108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3329, Accuracy: 0.939, F1 Micro: 0.9618, F1 Macro: 0.9601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2223, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9753\n",
      "Epoch 5/10, Train Loss: 0.1568, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1227, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9702, F1 Micro: 0.9814, F1 Macro: 0.9804\n",
      "Epoch 10/10, Train Loss: 0.0559, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9814, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.89      0.99      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.486, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9004\n",
      "Epoch 2/10, Train Loss: 0.2291, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1783, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9397\n",
      "Epoch 4/10, Train Loss: 0.1595, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9319\n",
      "Epoch 5/10, Train Loss: 0.1233, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1151, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9469\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9187\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9187\n",
      "Epoch 9/10, Train Loss: 0.0628, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9187\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9105\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        86\n",
      "    positive       0.96      0.96      0.96       165\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.95      0.95      0.95       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9452, F1 Micro: 0.9452, F1 Macro: 0.8757\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.29      0.75      0.42        12\n",
      "     neutral       0.94      0.91      0.92       152\n",
      "    positive       0.92      0.67      0.78        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.72      0.78      0.71       216\n",
      "weighted avg       0.90      0.84      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.88      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 125.07770466804504 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.8452\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.963650941848755 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5414, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4387, Accuracy: 0.8869, F1 Micro: 0.9323, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3027, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1948, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1348, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0639, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4971, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.942\n",
      "Epoch 2/10, Train Loss: 0.2579, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9388\n",
      "Epoch 3/10, Train Loss: 0.1607, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9349\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9391\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9342\n",
      "Epoch 6/10, Train Loss: 0.1031, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9388\n",
      "Epoch 7/10, Train Loss: 0.0942, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9427\n",
      "Epoch 9/10, Train Loss: 0.0619, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9284\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9288\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        84\n",
      "    positive       0.98      0.95      0.96       167\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.95      0.94       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8936\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.82      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.75      0.55        12\n",
      "     neutral       0.93      0.92      0.92       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.81      0.76       216\n",
      "weighted avg       0.89      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 125.51525330543518 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5492, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4405, Accuracy: 0.8921, F1 Micro: 0.9346, F1 Macro: 0.9334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3133, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1992, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1397, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.9673, F1 Micro: 0.9796, F1 Macro: 0.9787\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9773\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9767\n",
      "Epoch 9/10, Train Loss: 0.0601, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4902, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2437, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1923, Accuracy: 0.965, F1 Micro: 0.965, F1 Macro: 0.9608\n",
      "Epoch 4/10, Train Loss: 0.1376, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Epoch 5/10, Train Loss: 0.1435, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9611, F1 Micro: 0.9611, F1 Macro: 0.9563\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Epoch 8/10, Train Loss: 0.0482, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9222\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Epoch 10/10, Train Loss: 0.0411, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9147\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.965, F1 Micro: 0.965, F1 Macro: 0.9608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95        84\n",
      "    positive       0.99      0.96      0.97       173\n",
      "\n",
      "    accuracy                           0.96       257\n",
      "   macro avg       0.95      0.97      0.96       257\n",
      "weighted avg       0.97      0.96      0.97       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.8987\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.62      0.74        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.80      0.73      0.76        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.78      0.82       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.67      0.80        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.93      0.81      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 131.0605673789978 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.563, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4546, Accuracy: 0.8743, F1 Micro: 0.9256, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3264, Accuracy: 0.9412, F1 Micro: 0.9636, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2135, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9748\n",
      "Epoch 5/10, Train Loss: 0.1507, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1244, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 8/10, Train Loss: 0.0725, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 10/10, Train Loss: 0.0519, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.558, Accuracy: 0.8885, F1 Micro: 0.8885, F1 Macro: 0.8657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2486, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1987, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9487\n",
      "Epoch 4/10, Train Loss: 0.1608, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9443\n",
      "Epoch 5/10, Train Loss: 0.1388, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Epoch 6/10, Train Loss: 0.1313, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1035, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "Epoch 8/10, Train Loss: 0.0869, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Epoch 9/10, Train Loss: 0.0656, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "Epoch 10/10, Train Loss: 0.0745, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9243\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.95      0.95       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8955\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.92      0.67        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.81      0.73      0.77        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.85      0.78       216\n",
      "weighted avg       0.88      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.85      0.79        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 130.12130117416382 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.8482\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.545867919921875 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5249, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.438, Accuracy: 0.8988, F1 Micro: 0.9387, F1 Macro: 0.9373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2848, Accuracy: 0.9516, F1 Micro: 0.9701, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1822, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1271, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0951, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5267, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2613, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1766, Accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.142, Accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1038, Accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9609\n",
      "Epoch 6/10, Train Loss: 0.1083, Accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9568\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.932\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9238\n",
      "Epoch 9/10, Train Loss: 0.07, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9238\n",
      "Epoch 10/10, Train Loss: 0.0712, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.934\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.97      0.95        86\n",
      "    positive       0.98      0.96      0.97       169\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.96      0.96      0.96       255\n",
      "weighted avg       0.97      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9194\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.94      0.84      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.23066520690918 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5308, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4351, Accuracy: 0.8958, F1 Micro: 0.9366, F1 Macro: 0.9347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2933, Accuracy: 0.9479, F1 Micro: 0.9678, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1903, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9736\n",
      "Epoch 5/10, Train Loss: 0.1317, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.965, F1 Micro: 0.9782, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4508, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2289, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1722, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9485\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1419, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "Epoch 6/10, Train Loss: 0.1117, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1081, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0669, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9527\n",
      "Epoch 9/10, Train Loss: 0.0736, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.934\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.94        87\n",
      "    positive       0.98      0.96      0.97       171\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.95      0.96      0.95       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9177\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.56      0.69        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.76      0.79      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.78      0.81       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.12375950813293 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.552, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4628, Accuracy: 0.8705, F1 Micro: 0.9228, F1 Macro: 0.9214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3207, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.209, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1442, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1069, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 7/10, Train Loss: 0.0902, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0763, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9797\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 10/10, Train Loss: 0.0533, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5619, Accuracy: 0.8951, F1 Micro: 0.8951, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2844, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1976, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9222\n",
      "Epoch 4/10, Train Loss: 0.1553, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9142\n",
      "Epoch 5/10, Train Loss: 0.1235, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1028, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Epoch 9/10, Train Loss: 0.0657, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9457\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        87\n",
      "    positive       0.98      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.94      0.95      0.95       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9221\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.78      0.85      0.80       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.42147994041443 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.8522\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.9717113971710205 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4229, Accuracy: 0.8802, F1 Micro: 0.9287, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2819, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1728, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 6/10, Train Loss: 0.0985, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4871, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9241\n",
      "Epoch 2/10, Train Loss: 0.2262, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1744, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9242\n",
      "Epoch 4/10, Train Loss: 0.1387, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9693, F1 Micro: 0.9693, F1 Macro: 0.9655\n",
      "Epoch 7/10, Train Loss: 0.0854, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9325\n",
      "Epoch 8/10, Train Loss: 0.0996, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9086\n",
      "Epoch 9/10, Train Loss: 0.092, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9397\n",
      "Epoch 10/10, Train Loss: 0.0666, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9352\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9693, F1 Micro: 0.9693, F1 Macro: 0.9655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.98      0.95        85\n",
      "    positive       0.99      0.97      0.98       176\n",
      "\n",
      "    accuracy                           0.97       261\n",
      "   macro avg       0.96      0.97      0.97       261\n",
      "weighted avg       0.97      0.97      0.97       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9098\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.88      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 134.63256192207336 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5364, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4163, Accuracy: 0.8876, F1 Micro: 0.9321, F1 Macro: 0.9303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2888, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1839, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1362, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 6/10, Train Loss: 0.1035, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9732\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9739\n",
      "Epoch 8/10, Train Loss: 0.0662, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9804\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4997, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2095, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1359, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9436\n",
      "Epoch 5/10, Train Loss: 0.1153, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1121, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9531\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "Epoch 10/10, Train Loss: 0.0374, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9154\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.97      0.94        86\n",
      "    positive       0.98      0.95      0.97       176\n",
      "\n",
      "    accuracy                           0.96       262\n",
      "   macro avg       0.95      0.96      0.95       262\n",
      "weighted avg       0.96      0.96      0.96       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9009\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.87      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.84      0.80       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 134.15993642807007 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5563, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4572, Accuracy: 0.8564, F1 Micro: 0.9162, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3249, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.9635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2048, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1493, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9774\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0896, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.0768, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0522, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4939, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.8996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2181, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1566, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1473, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1337, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9488\n",
      "Epoch 7/10, Train Loss: 0.0783, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9406\n",
      "Epoch 10/10, Train Loss: 0.0637, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9403\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        86\n",
      "    positive       0.97      0.96      0.96       172\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.95      0.95      0.95       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9232\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.73724341392517 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9374, F1 Micro: 0.9374, F1 Macro: 0.8553\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.798031806945801 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5267, Accuracy: 0.7976, F1 Micro: 0.8864, F1 Macro: 0.885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.414, Accuracy: 0.9055, F1 Micro: 0.9429, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2698, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1674, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1219, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4777, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2436, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1798, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9303\n",
      "Epoch 4/10, Train Loss: 0.1557, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 5/10, Train Loss: 0.1189, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9088\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        86\n",
      "    positive       0.97      0.94      0.95       176\n",
      "\n",
      "    accuracy                           0.94       262\n",
      "   macro avg       0.93      0.94      0.93       262\n",
      "weighted avg       0.94      0.94      0.94       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.8939\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.85      0.85       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.86      0.84       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.85      0.76        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.85      0.85       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.17771458625793 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.537, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4169, Accuracy: 0.9062, F1 Micro: 0.9433, F1 Macro: 0.942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2806, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1779, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9796\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9724\n",
      "Epoch 8/10, Train Loss: 0.0641, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9794\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4986, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.241, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1726, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9357\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.937\n",
      "Epoch 7/10, Train Loss: 0.0856, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9411\n",
      "Epoch 9/10, Train Loss: 0.0779, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9075\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.92      0.65        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.81      0.75      0.78        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.86      0.78       216\n",
      "weighted avg       0.89      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.5407989025116 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.553, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4399, Accuracy: 0.8996, F1 Micro: 0.9392, F1 Macro: 0.9378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.298, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1921, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1356, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Epoch 8/10, Train Loss: 0.067, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5385, Accuracy: 0.8977, F1 Micro: 0.8977, F1 Macro: 0.8877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2708, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1987, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9359\n",
      "Epoch 4/10, Train Loss: 0.1078, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1375, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9491\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9333\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.917\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9224\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Epoch 10/10, Train Loss: 0.0414, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9491\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.97       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.94      0.96      0.95       264\n",
      "weighted avg       0.96      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9172\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.1960527896881 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.8578\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.250582695007324 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5179, Accuracy: 0.8021, F1 Micro: 0.8886, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4157, Accuracy: 0.9033, F1 Micro: 0.941, F1 Macro: 0.9389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2584, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1648, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1261, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 6/10, Train Loss: 0.0914, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9753\n",
      "Epoch 7/10, Train Loss: 0.0704, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0439, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.97      0.96       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4856, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2034, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.933\n",
      "Epoch 3/10, Train Loss: 0.1527, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1724, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9357\n",
      "Epoch 5/10, Train Loss: 0.1065, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.917\n",
      "Epoch 6/10, Train Loss: 0.0864, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9206\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0473, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.936\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9492\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9492\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        87\n",
      "    positive       0.98      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.96      0.95       262\n",
      "weighted avg       0.96      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9322\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.97      0.96       152\n",
      "    positive       0.90      0.83      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 146.64328956604004 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5246, Accuracy: 0.7969, F1 Micro: 0.8859, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4086, Accuracy: 0.907, F1 Micro: 0.9425, F1 Macro: 0.9405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2664, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1721, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1284, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 6/10, Train Loss: 0.0952, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9804\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4927, Accuracy: 0.9015, F1 Micro: 0.9015, F1 Macro: 0.8921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2046, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1711, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9491\n",
      "Epoch 4/10, Train Loss: 0.1819, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9451\n",
      "Epoch 5/10, Train Loss: 0.1247, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.941\n",
      "Epoch 6/10, Train Loss: 0.1229, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9174\n",
      "Epoch 7/10, Train Loss: 0.0971, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9479\n",
      "Epoch 9/10, Train Loss: 0.0871, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9096\n",
      "Epoch 10/10, Train Loss: 0.0374, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9206\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.92      0.93        86\n",
      "    positive       0.96      0.97      0.97       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.95      0.95      0.95       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9125\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.15439772605896 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5441, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4303, Accuracy: 0.9033, F1 Micro: 0.9404, F1 Macro: 0.9377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2763, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1832, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1385, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 10/10, Train Loss: 0.0509, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9782\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5369, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2234, Accuracy: 0.9132, F1 Micro: 0.9132, F1 Macro: 0.9054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1736, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9452\n",
      "Epoch 4/10, Train Loss: 0.1648, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9252\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9247\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9294\n",
      "Epoch 7/10, Train Loss: 0.0963, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9334\n",
      "Epoch 8/10, Train Loss: 0.0697, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9176\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9319\n",
      "Epoch 10/10, Train Loss: 0.0481, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9275\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.94      0.95      0.95       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9017\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.87      0.89      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.85363364219666 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.8606\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.2020041942596436 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5321, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4198, Accuracy: 0.91, F1 Micro: 0.9456, F1 Macro: 0.945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2605, Accuracy: 0.9524, F1 Micro: 0.9706, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1671, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1234, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.99      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4552, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2299, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 3/10, Train Loss: 0.1959, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1429, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9344\n",
      "Epoch 6/10, Train Loss: 0.1154, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9224\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9196\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9277\n",
      "Epoch 9/10, Train Loss: 0.0774, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9169\n",
      "Epoch 10/10, Train Loss: 0.0612, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.928\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        85\n",
      "    positive       0.96      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.93      0.94      0.93       258\n",
      "weighted avg       0.94      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9004\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.88      0.82        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.87      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.39      1.00      0.56        12\n",
      "     neutral       0.96      0.90      0.93       152\n",
      "    positive       0.95      0.77      0.85        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.89      0.78       216\n",
      "weighted avg       0.92      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 148.28631234169006 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5353, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4051, Accuracy: 0.9085, F1 Micro: 0.9442, F1 Macro: 0.9435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2613, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.9714\n",
      "Epoch 4/10, Train Loss: 0.1713, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1292, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0935, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.9732, F1 Micro: 0.9832, F1 Macro: 0.9824\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9769\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9732, F1 Micro: 0.9832, F1 Macro: 0.9824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.547, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.242, Accuracy: 0.8972, F1 Micro: 0.8972, F1 Macro: 0.8913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1745, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "Epoch 4/10, Train Loss: 0.1454, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9314\n",
      "Epoch 5/10, Train Loss: 0.1408, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9314\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9352\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.939\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9103\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.96       168\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.95      0.95       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8861\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.80      0.85      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.84      0.86       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.32      0.92      0.48        12\n",
      "     neutral       0.95      0.89      0.92       152\n",
      "    positive       0.97      0.73      0.84        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.75      0.85      0.75       216\n",
      "weighted avg       0.92      0.86      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 141.05564332008362 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5538, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4322, Accuracy: 0.9107, F1 Micro: 0.9453, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2817, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1811, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1356, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "Epoch 7/10, Train Loss: 0.082, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9744\n",
      "Epoch 8/10, Train Loss: 0.0671, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0632, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "Epoch 10/10, Train Loss: 0.0504, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4751, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.9\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2208, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1705, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1572, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.94\n",
      "Epoch 5/10, Train Loss: 0.1053, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.928\n",
      "Epoch 7/10, Train Loss: 0.1032, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9314\n",
      "Epoch 8/10, Train Loss: 0.0559, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9277\n",
      "Epoch 9/10, Train Loss: 0.0447, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9169\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9119\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        85\n",
      "    positive       0.98      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.93      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.8948\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.83      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.83      0.53        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.93      0.73      0.82        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.75      0.83      0.76       216\n",
      "weighted avg       0.90      0.87      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.98      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 147.9043869972229 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9397, F1 Micro: 0.9397, F1 Macro: 0.8621\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.0429389476776123 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.515, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.402, Accuracy: 0.9226, F1 Micro: 0.9528, F1 Macro: 0.951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.251, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.116, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 7/10, Train Loss: 0.0706, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4445, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.246, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9287\n",
      "Epoch 3/10, Train Loss: 0.1695, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1235, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0914, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9408\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0749, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9368\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9205\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      1.00      0.80        12\n",
      "     neutral       0.95      0.94      0.95       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.92      0.86       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      1.00      0.84        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.96      0.94      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.49901247024536 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5173, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3964, Accuracy: 0.9122, F1 Micro: 0.9461, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2636, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1677, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1264, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9781\n",
      "Epoch 10/10, Train Loss: 0.0439, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4436, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9328\n",
      "Epoch 2/10, Train Loss: 0.2454, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9072\n",
      "Epoch 3/10, Train Loss: 0.1701, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.9034\n",
      "Epoch 4/10, Train Loss: 0.1604, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.911\n",
      "Epoch 5/10, Train Loss: 0.1182, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.92\n",
      "Epoch 6/10, Train Loss: 0.0829, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9144\n",
      "Epoch 7/10, Train Loss: 0.0684, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9324\n",
      "Epoch 9/10, Train Loss: 0.0657, Accuracy: 0.9338, F1 Micro: 0.9338, F1 Macro: 0.9257\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9338, F1 Micro: 0.9338, F1 Macro: 0.9257\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        86\n",
      "    positive       0.96      0.95      0.96       186\n",
      "\n",
      "    accuracy                           0.94       272\n",
      "   macro avg       0.93      0.94      0.93       272\n",
      "weighted avg       0.94      0.94      0.94       272\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9142\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.94      0.90      0.92       152\n",
      "    positive       0.75      0.81      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.85      0.81       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.9848358631134 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.535, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4216, Accuracy: 0.9115, F1 Micro: 0.9453, F1 Macro: 0.9431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2727, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1756, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.13, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.979\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 7/10, Train Loss: 0.0803, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 10/10, Train Loss: 0.0484, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5123, Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.8974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2056, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9357\n",
      "Epoch 3/10, Train Loss: 0.1627, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9251\n",
      "Epoch 4/10, Train Loss: 0.1471, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9121\n",
      "Epoch 5/10, Train Loss: 0.1065, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9186\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9446\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9024\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.99      0.93        85\n",
      "    positive       0.99      0.93      0.96       172\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.93      0.96      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.8921\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.88      0.78        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.85      0.85      0.84       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.83      0.69        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.80      0.84      0.81       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.48074793815613 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9404, F1 Micro: 0.9404, F1 Macro: 0.8641\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 2.505479335784912 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5195, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4032, Accuracy: 0.9196, F1 Micro: 0.9508, F1 Macro: 0.9493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2445, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1611, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Epoch 5/10, Train Loss: 0.1204, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0499, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.979\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4525, Accuracy: 0.9031, F1 Micro: 0.9031, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2208, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1379, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1374, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "Epoch 5/10, Train Loss: 0.139, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Epoch 6/10, Train Loss: 0.119, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9133\n",
      "Epoch 7/10, Train Loss: 0.0873, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9236\n",
      "Epoch 8/10, Train Loss: 0.0855, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9111\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9257\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.9035\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        85\n",
      "    positive       0.95      0.95      0.95       173\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.93      0.93      0.93       258\n",
      "weighted avg       0.94      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.89\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.83      0.61        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.76      0.84      0.79       216\n",
      "weighted avg       0.90      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      1.00      0.82        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.91      0.88       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 145.632807970047 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5298, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4095, Accuracy: 0.9025, F1 Micro: 0.9402, F1 Macro: 0.938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2556, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1718, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0791, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 8/10, Train Loss: 0.0621, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.91      0.98      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4711, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.933\n",
      "Epoch 2/10, Train Loss: 0.2649, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1516, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.152, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9517\n",
      "Epoch 5/10, Train Loss: 0.1108, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.938\n",
      "Epoch 6/10, Train Loss: 0.1219, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9393\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9432\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9419\n",
      "Epoch 9/10, Train Loss: 0.0903, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9307\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9435\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        84\n",
      "    positive       0.98      0.95      0.97       169\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.95      0.96      0.95       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9068\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.46      0.92      0.61        12\n",
      "     neutral       0.94      0.92      0.93       152\n",
      "    positive       0.93      0.77      0.84        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.87      0.79       216\n",
      "weighted avg       0.91      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.93      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.66190552711487 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5459, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4147, Accuracy: 0.9129, F1 Micro: 0.9461, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2586, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9751\n",
      "Epoch 4/10, Train Loss: 0.1759, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1019, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9805\n",
      "Epoch 7/10, Train Loss: 0.0842, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.97      0.96       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4243, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2104, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9212\n",
      "Epoch 3/10, Train Loss: 0.1506, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.114, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1229, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9325\n",
      "Epoch 7/10, Train Loss: 0.0821, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9248\n",
      "Epoch 8/10, Train Loss: 0.0725, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9156\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9201\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9121\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        87\n",
      "    positive       0.98      0.93      0.95       174\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.92      0.94      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.8952\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.91      0.87        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.82      0.86      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.92      0.80        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.89      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.8551368713379 s\n",
      "Averaged - Iteration 864: Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.8658\n",
      "Total runtime: 9287.776391267776 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6811367,
     "sourceId": 10950280,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10046.857916,
   "end_time": "2025-03-08T13:18:04.968965",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-08T10:30:38.111049",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "028bc68712754e2b9af5fcc5b2da9289": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_afd3cc430bb74c2d804d3a63925c0f98",
       "placeholder": "",
       "style": "IPY_MODEL_a4050f638c9548d3a7641d768f84fb05",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json:100%"
      }
     },
     "0472064ec3f04410bbf26a40362012b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "12d199880e154804ba14e0131215e766": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "138c1c44dd994f9688c6049c9e88279b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1861cafc869e4baa943001fdbde845cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a5645e69a94a441bbb6c4a1b79e93eeb",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_624937f67b154d62a0282c49ece0c870",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "1d55b5e1284b48e6861e64f877b4ed4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_acf62f5c61d14a0f90382472d5d9bcf5",
       "placeholder": "",
       "style": "IPY_MODEL_f89ca8baf9a0478394d0db644db45787",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json:100%"
      }
     },
     "2e63685bf492404cba43e56966552ef0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a05e2ccfed6c44068a3c52998b61dc1f",
        "IPY_MODEL_37ff8383cdd5465a9b99f203a8b8d526",
        "IPY_MODEL_ca619171a95446eb84368c56bfca17ca"
       ],
       "layout": "IPY_MODEL_67ba0911a9c54412bc47ab62fd3040c0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "35f20aba46474383a9722f13d784cd2e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "36374a4b140e44f8a854f9e117afb229": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_af1cab8dbcf547dd86d69345f481ec46",
        "IPY_MODEL_924176070c224fd3a0af0998067ec2df",
        "IPY_MODEL_fe7ecb451c9e4222b9c94bf206704c47"
       ],
       "layout": "IPY_MODEL_35f20aba46474383a9722f13d784cd2e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "37ff8383cdd5465a9b99f203a8b8d526": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cbc2ba69d3ac4e71a25814aad825f2fa",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fde73021717c4dd5b1f101ae552ecd8b",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "3cfc1d8f16a24dcdbcf10d39ff2d4ed7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3eb63b7e4b3441528ee29e3ef7928f23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "49059e31b7f14b33a62d2ef78a9c8523": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b494e8f0145476b87235f4dd5fe04f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4e795e3dae1040b2a6305e6401095bb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_12d199880e154804ba14e0131215e766",
       "placeholder": "",
       "style": "IPY_MODEL_612b163abb8242eab289237881b9a30f",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:100%"
      }
     },
     "53b85f921fde461d8a9c30e08090a25c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3eb63b7e4b3441528ee29e3ef7928f23",
       "placeholder": "",
       "style": "IPY_MODEL_6bfbf96b141249b68c4e32ea89cefe36",
       "tabbable": null,
       "tooltip": null,
       "value": "2.00/2.00[00:00&lt;00:00,137B/s]"
      }
     },
     "5d5343ceee394caab343162574efe754": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "612b163abb8242eab289237881b9a30f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "624937f67b154d62a0282c49ece0c870": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6382b087ca23407fa6213df7289a6100": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "651cb98179424b1585d3a65a47513166": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_028bc68712754e2b9af5fcc5b2da9289",
        "IPY_MODEL_1861cafc869e4baa943001fdbde845cf",
        "IPY_MODEL_53b85f921fde461d8a9c30e08090a25c"
       ],
       "layout": "IPY_MODEL_ee6084f3742c485190d54dcb419d7830",
       "tabbable": null,
       "tooltip": null
      }
     },
     "654942489a674dc1ab1752352d627a3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d078a92ba6a2455ba27af35e9132bc20",
       "placeholder": "",
       "style": "IPY_MODEL_72bd8401cd5941c4b133faba7cc5ea69",
       "tabbable": null,
       "tooltip": null,
       "value": "112/112[00:00&lt;00:00,10.0kB/s]"
      }
     },
     "67ba0911a9c54412bc47ab62fd3040c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6bfbf96b141249b68c4e32ea89cefe36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "72bd8401cd5941c4b133faba7cc5ea69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "74bcb43823124becb2941446a839e8cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8701b10e29364e9788bf036164b2ab30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e7dcbeadffad45e3aeb06c2bfceaddd9",
       "placeholder": "",
       "style": "IPY_MODEL_fc129072c9c6480e96f1aceca6e04af8",
       "tabbable": null,
       "tooltip": null,
       "value": "1.53k/1.53k[00:00&lt;00:00,141kB/s]"
      }
     },
     "924176070c224fd3a0af0998067ec2df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c91ace2563d549dc97865c4c87a83fed",
       "max": 497787752,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6382b087ca23407fa6213df7289a6100",
       "tabbable": null,
       "tooltip": null,
       "value": 497787752
      }
     },
     "9c2fb51b01e34f229fefbca13959f687": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e9a6d1a4590345868ad0da469213a7ad",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_138c1c44dd994f9688c6049c9e88279b",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     },
     "a05e2ccfed6c44068a3c52998b61dc1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_74bcb43823124becb2941446a839e8cc",
       "placeholder": "",
       "style": "IPY_MODEL_3cfc1d8f16a24dcdbcf10d39ff2d4ed7",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt:100%"
      }
     },
     "a2108cfc979947e6b4b784e86587e63f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a4050f638c9548d3a7641d768f84fb05": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a5645e69a94a441bbb6c4a1b79e93eeb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "acf62f5c61d14a0f90382472d5d9bcf5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "af1cab8dbcf547dd86d69345f481ec46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bfb2ccfa489f464f9da0e9524541f548",
       "placeholder": "",
       "style": "IPY_MODEL_0472064ec3f04410bbf26a40362012b6",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:100%"
      }
     },
     "afd3cc430bb74c2d804d3a63925c0f98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8eba7d123dd41ec93acf0283bc4bd2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e1b333a04db04caf94c7354f1722761f",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4b494e8f0145476b87235f4dd5fe04f0",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "ba209923c2a64a34841b8f695b4fe5b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bfb2ccfa489f464f9da0e9524541f548": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c91ace2563d549dc97865c4c87a83fed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca619171a95446eb84368c56bfca17ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_49059e31b7f14b33a62d2ef78a9c8523",
       "placeholder": "",
       "style": "IPY_MODEL_ba209923c2a64a34841b8f695b4fe5b4",
       "tabbable": null,
       "tooltip": null,
       "value": "229k/229k[00:00&lt;00:00,3.95MB/s]"
      }
     },
     "cbc2ba69d3ac4e71a25814aad825f2fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d078a92ba6a2455ba27af35e9132bc20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0f34109dcb54d258cee854f312324bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4e795e3dae1040b2a6305e6401095bb7",
        "IPY_MODEL_9c2fb51b01e34f229fefbca13959f687",
        "IPY_MODEL_8701b10e29364e9788bf036164b2ab30"
       ],
       "layout": "IPY_MODEL_a2108cfc979947e6b4b784e86587e63f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d1198eb89a0040eaa9ff9858375065ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dd55f574c62d46d4ba71c5fa7a3bed62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1b333a04db04caf94c7354f1722761f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7dcbeadffad45e3aeb06c2bfceaddd9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9a6d1a4590345868ad0da469213a7ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee6084f3742c485190d54dcb419d7830": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f89ca8baf9a0478394d0db644db45787": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fc129072c9c6480e96f1aceca6e04af8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fdbde207bbd34e299dfc81ea77d699f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1d55b5e1284b48e6861e64f877b4ed4e",
        "IPY_MODEL_b8eba7d123dd41ec93acf0283bc4bd2b",
        "IPY_MODEL_654942489a674dc1ab1752352d627a3c"
       ],
       "layout": "IPY_MODEL_dd55f574c62d46d4ba71c5fa7a3bed62",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fde73021717c4dd5b1f101ae552ecd8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fe7ecb451c9e4222b9c94bf206704c47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5d5343ceee394caab343162574efe754",
       "placeholder": "",
       "style": "IPY_MODEL_d1198eb89a0040eaa9ff9858375065ca",
       "tabbable": null,
       "tooltip": null,
       "value": "498M/498M[00:02&lt;00:00,202MB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
