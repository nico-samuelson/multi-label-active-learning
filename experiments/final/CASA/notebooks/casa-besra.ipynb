{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abc294d",
   "metadata": {
    "papermill": {
     "duration": 0.016417,
     "end_time": "2025-04-05T06:34:51.708642",
     "exception": false,
     "start_time": "2025-04-05T06:34:51.692225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9921e538",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:34:51.738300Z",
     "iopub.status.busy": "2025-04-05T06:34:51.738024Z",
     "iopub.status.idle": "2025-04-05T06:35:18.918165Z",
     "shell.execute_reply": "2025-04-05T06:35:18.917248Z"
    },
    "papermill": {
     "duration": 27.197204,
     "end_time": "2025-04-05T06:35:18.920282",
     "exception": false,
     "start_time": "2025-04-05T06:34:51.723078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, BertPreTrainedModel, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa8d0c7",
   "metadata": {
    "papermill": {
     "duration": 0.01573,
     "end_time": "2025-04-05T06:35:18.951238",
     "exception": false,
     "start_time": "2025-04-05T06:35:18.935508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be9460c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:18.983514Z",
     "iopub.status.busy": "2025-04-05T06:35:18.982934Z",
     "iopub.status.idle": "2025-04-05T06:35:18.986943Z",
     "shell.execute_reply": "2025-04-05T06:35:18.986137Z"
    },
    "papermill": {
     "duration": 0.021437,
     "end_time": "2025-04-05T06:35:18.988336",
     "exception": false,
     "start_time": "2025-04-05T06:35:18.966899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10413f58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:19.022358Z",
     "iopub.status.busy": "2025-04-05T06:35:19.022059Z",
     "iopub.status.idle": "2025-04-05T06:35:19.026776Z",
     "shell.execute_reply": "2025-04-05T06:35:19.025903Z"
    },
    "papermill": {
     "duration": 0.022242,
     "end_time": "2025-04-05T06:35:19.028143",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.005901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/working/results') == False:\n",
    "    os.mkdir('/kaggle/working/results')\n",
    "\n",
    "if os.path.exists('/kaggle/working/acquired_data') == False:\n",
    "    os.mkdir('/kaggle/working/acquired_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a68f10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:19.058300Z",
     "iopub.status.busy": "2025-04-05T06:35:19.057987Z",
     "iopub.status.idle": "2025-04-05T06:35:19.067836Z",
     "shell.execute_reply": "2025-04-05T06:35:19.067139Z"
    },
    "papermill": {
     "duration": 0.026133,
     "end_time": "2025-04-05T06:35:19.069183",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.043050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ce4b4",
   "metadata": {
    "papermill": {
     "duration": 0.014678,
     "end_time": "2025-04-05T06:35:19.098729",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.084051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f05a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:19.130436Z",
     "iopub.status.busy": "2025-04-05T06:35:19.130091Z",
     "iopub.status.idle": "2025-04-05T06:35:19.195172Z",
     "shell.execute_reply": "2025-04-05T06:35:19.193444Z"
    },
    "papermill": {
     "duration": 0.083133,
     "end_time": "2025-04-05T06:35:19.197314",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.114181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "\n",
    "# Shared resources\n",
    "aspect_accuracies = manager.list()\n",
    "aspect_f1_micros = manager.list()\n",
    "aspect_f1_macros = manager.list()\n",
    "sentiment_accuracies = manager.list()\n",
    "sentiment_f1_micros = manager.list()\n",
    "sentiment_f1_macros = manager.list()\n",
    "accuracies = manager.list()\n",
    "f1_micros = manager.list()\n",
    "f1_macros = manager.list()\n",
    "\n",
    "# Non shared resources\n",
    "filename = 'casa-besra'\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "sequence_length = 48\n",
    "\n",
    "aspect_list = ['fuel', 'machine', 'others', 'part', 'price', 'service']\n",
    "aspect_mapping = {'fuel': 0, 'machine': 1, 'others': 2, 'part': 3, 'price': 4, 'service': 5 }\n",
    "label_mapping = {\"negative\": 0, \"neutral\": 1, 'positive': 2}\n",
    "ignored_keys = ['labels', 'ori_text', 'ori_label', 'ori_indices', 'aspect']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0061307c",
   "metadata": {
    "papermill": {
     "duration": 0.014626,
     "end_time": "2025-04-05T06:35:19.227182",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.212556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LOAD AND PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602323f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:19.258948Z",
     "iopub.status.busy": "2025-04-05T06:35:19.258533Z",
     "iopub.status.idle": "2025-04-05T06:35:19.342095Z",
     "shell.execute_reply": "2025-04-05T06:35:19.341157Z"
    },
    "papermill": {
     "duration": 0.101536,
     "end_time": "2025-04-05T06:35:19.343689",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.242153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>fuel</th>\n",
       "      <th>machine</th>\n",
       "      <th>others</th>\n",
       "      <th>part</th>\n",
       "      <th>price</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saya memakai Honda Jazz GK5 tahun 2014 ( perta...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avanza kenapa jadi boros bensin begini dah ah....</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saran ku dan pengalaman ku , mending beli mobi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dari segi harga juga pajero lebih mahal 30 jut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo menurut gw enak pajero si</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      fuel   machine  \\\n",
       "0  Saya memakai Honda Jazz GK5 tahun 2014 ( perta...   neutral   neutral   \n",
       "1  Avanza kenapa jadi boros bensin begini dah ah....  negative   neutral   \n",
       "2  saran ku dan pengalaman ku , mending beli mobi...  positive  positive   \n",
       "3  Dari segi harga juga pajero lebih mahal 30 jut...   neutral   neutral   \n",
       "4                     Kalo menurut gw enak pajero si   neutral   neutral   \n",
       "\n",
       "     others     part     price  service  \n",
       "0  positive  neutral   neutral  neutral  \n",
       "1   neutral  neutral   neutral  neutral  \n",
       "2   neutral  neutral   neutral  neutral  \n",
       "3   neutral  neutral  positive  neutral  \n",
       "4  positive  neutral   neutral  neutral  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/casa-dataset/train_preprocess.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('/kaggle/input/casa-dataset/valid_preprocess.csv', encoding='latin-1')\n",
    "test_data = pd.read_csv('/kaggle/input/casa-dataset/test_preprocess.csv', encoding='latin-1')\n",
    "\n",
    "data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec7c514b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:19.375455Z",
     "iopub.status.busy": "2025-04-05T06:35:19.375131Z",
     "iopub.status.idle": "2025-04-05T06:35:19.386794Z",
     "shell.execute_reply": "2025-04-05T06:35:19.385829Z"
    },
    "papermill": {
     "duration": 0.029028,
     "end_time": "2025-04-05T06:35:19.388250",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.359222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>fuel</th>\n",
       "      <th>machine</th>\n",
       "      <th>others</th>\n",
       "      <th>part</th>\n",
       "      <th>price</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saya memakai Honda Jazz GK5 tahun 2014 ( perta...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avanza kenapa jadi boros bensin begini dah ah....</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saran ku dan pengalaman ku , mending beli mobi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dari segi harga juga pajero lebih mahal 30 jut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo menurut gw enak pajero si</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      fuel   machine  \\\n",
       "0  Saya memakai Honda Jazz GK5 tahun 2014 ( perta...   neutral   neutral   \n",
       "1  Avanza kenapa jadi boros bensin begini dah ah....  negative   neutral   \n",
       "2  saran ku dan pengalaman ku , mending beli mobi...  positive  positive   \n",
       "3  Dari segi harga juga pajero lebih mahal 30 jut...   neutral   neutral   \n",
       "4                     Kalo menurut gw enak pajero si   neutral   neutral   \n",
       "\n",
       "     others     part     price  service  \n",
       "0  positive  neutral   neutral  neutral  \n",
       "1   neutral  neutral   neutral  neutral  \n",
       "2   neutral  neutral   neutral  neutral  \n",
       "3   neutral  neutral  positive  neutral  \n",
       "4  positive  neutral   neutral  neutral  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb5e357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:19.418910Z",
     "iopub.status.busy": "2025-04-05T06:35:19.418574Z",
     "iopub.status.idle": "2025-04-05T06:35:19.427951Z",
     "shell.execute_reply": "2025-04-05T06:35:19.426950Z"
    },
    "papermill": {
     "duration": 0.025993,
     "end_time": "2025-04-05T06:35:19.429336",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.403343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e43dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:19.460612Z",
     "iopub.status.busy": "2025-04-05T06:35:19.460300Z",
     "iopub.status.idle": "2025-04-05T06:35:19.473087Z",
     "shell.execute_reply": "2025-04-05T06:35:19.472200Z"
    },
    "papermill": {
     "duration": 0.030132,
     "end_time": "2025-04-05T06:35:19.474504",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.444372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864,) (864, 6)\n",
      "(216,) (216, 6)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_data.columns[1:]\n",
    "val_labels = val_data.columns[1:]\n",
    "\n",
    "# Extract features and labels for training and validation\n",
    "X_train = train_data['sentence'].values\n",
    "y_train = train_data[train_labels].values\n",
    "X_val = val_data['sentence'].values\n",
    "y_val = val_data[val_labels].values\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4182c",
   "metadata": {
    "papermill": {
     "duration": 0.015359,
     "end_time": "2025-04-05T06:35:19.505652",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.490293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BUILD DATASET & DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b760dacc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:19.539118Z",
     "iopub.status.busy": "2025-04-05T06:35:19.538814Z",
     "iopub.status.idle": "2025-04-05T06:35:19.549138Z",
     "shell.execute_reply": "2025-04-05T06:35:19.548305Z"
    },
    "papermill": {
     "duration": 0.029333,
     "end_time": "2025-04-05T06:35:19.550425",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.521092",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AspectDetectionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, label_mapping, tokenizer, max_length=sequence_length, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        \n",
    "        original_labels = [self.label_mapping[label] for label in self.labels[idx]]\n",
    "        encoded_labels = [1 if label == 1 else 0 for label in original_labels]\n",
    "        \n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['ori_indices'] = idx\n",
    "        item['ori_text'] = self.texts[idx]\n",
    "        item['ori_label'] = torch.tensor(original_labels, dtype=torch.float)\n",
    "        item['labels'] = torch.tensor(encoded_labels, dtype=torch.float if self.use_float else torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9603df97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:19.581961Z",
     "iopub.status.busy": "2025-04-05T06:35:19.581624Z",
     "iopub.status.idle": "2025-04-05T06:35:19.592506Z",
     "shell.execute_reply": "2025-04-05T06:35:19.591749Z"
    },
    "papermill": {
     "duration": 0.028113,
     "end_time": "2025-04-05T06:35:19.593907",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.565794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentAnalysisDataset(Dataset):\n",
    "    def __init__(self, texts, labels, aspects, indices, label_mapping, tokenizer, max_length=96, use_float=True):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.aspects = aspects\n",
    "        self.indices = indices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_float = use_float\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = aspect_token + ' ' + self.aspects[idx] + ' ' + review_token + ' ' + self.texts[idx] \n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        if isinstance(self.labels[idx], str):\n",
    "            self.labels[idx] = self.label_mapping[self.labels[idx]]\n",
    "        elif torch.is_tensor(self.labels[idx]):\n",
    "            self.labels[idx] = int(self.labels[idx].item())\n",
    "\n",
    "        encoded_label = 1 if self.labels[idx] == 2 else self.labels[idx]\n",
    "        one_hot_label = F.one_hot(torch.tensor(encoded_label, dtype=torch.long), num_classes=2).float()\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['aspect'] = self.aspects[idx]\n",
    "        item['labels'] = one_hot_label\n",
    "        item['ori_indices'] = self.indices[idx]\n",
    "        item['ori_text'] = self.texts[idx]\n",
    "        item['ori_label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def get_per_class_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the spread of labels (0 and 1) for each class in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are class indices and values are [count_0, count_1].\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize a dictionary to store counts for each class\n",
    "        label_counts = defaultdict(lambda: [0, 0])  # [count_0, count_1] for each class\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update counts for each class\n",
    "            for class_idx, label in enumerate(labels):\n",
    "                label_counts[class_idx][int(label)] += 1\n",
    "\n",
    "        for key in label_counts.keys():\n",
    "            total = sum(label_counts[key])\n",
    "            label_counts[key] = [x / total for x in label_counts[key]]\n",
    "\n",
    "        return label_counts\n",
    "\n",
    "    def get_global_probs(self):\n",
    "        \"\"\"\n",
    "        Calculate the global count of 0s and 1s across all classes in the dataset.\n",
    "        Returns:\n",
    "            dict: A dictionary with keys '0' and '1' representing their global counts.\n",
    "        \"\"\"\n",
    "        global_counts = {'0': 0, '1': 0}\n",
    "\n",
    "        for i in range(len(self)):\n",
    "            # Get the labels for the i-th sample\n",
    "            labels = self[i]['labels']\n",
    "\n",
    "            # Update global counts\n",
    "            for label in labels:\n",
    "                global_counts[str(int(label))] += 1\n",
    "\n",
    "        total = global_counts['0'] + global_counts['1']\n",
    "        for key in global_counts.keys():\n",
    "            global_counts[key] /= total\n",
    "\n",
    "        return global_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc1bc3eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:19.627308Z",
     "iopub.status.busy": "2025-04-05T06:35:19.627009Z",
     "iopub.status.idle": "2025-04-05T06:35:20.636115Z",
     "shell.execute_reply": "2025-04-05T06:35:20.635349Z"
    },
    "papermill": {
     "duration": 1.028177,
     "end_time": "2025-04-05T06:35:20.637651",
     "exception": false,
     "start_time": "2025-04-05T06:35:19.609474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460baffa9940478ab98712e4b982646b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7476e149a0b4f2092d95c45e74df7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccf46d309d94c90b8c1c886f0f9062a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1021bc823af246998d54c088ac4f2226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "review_token = '[REVIEW]'\n",
    "aspect_token = '[ASPECT]'\n",
    "special_tokens_dict = {'additional_special_tokens': [review_token, aspect_token]}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43ab99e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:20.670557Z",
     "iopub.status.busy": "2025-04-05T06:35:20.670248Z",
     "iopub.status.idle": "2025-04-05T06:35:20.674861Z",
     "shell.execute_reply": "2025-04-05T06:35:20.674105Z"
    },
    "papermill": {
     "duration": 0.021944,
     "end_time": "2025-04-05T06:35:20.676154",
     "exception": false,
     "start_time": "2025-04-05T06:35:20.654210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_aspect_dataset(X_train, y_train, X_val, y_val, sequence_length, num_workers=4):\n",
    "    train_dataset = AspectDetectionDataset(X_train, y_train, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    val_dataset = AspectDetectionDataset(X_val, y_val, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=num_workers, \n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfaf6dcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:20.707175Z",
     "iopub.status.busy": "2025-04-05T06:35:20.706767Z",
     "iopub.status.idle": "2025-04-05T06:35:20.720522Z",
     "shell.execute_reply": "2025-04-05T06:35:20.719755Z"
    },
    "papermill": {
     "duration": 0.030935,
     "end_time": "2025-04-05T06:35:20.721891",
     "exception": false,
     "start_time": "2025-04-05T06:35:20.690956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_sentiment_dataset(device, train_dataset, val_dataset, aspect_detection_model, tokenizer, max_length=sequence_length):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "\n",
    "    aspect_detection_model.to(device)\n",
    "    aspect_detection_model.eval()\n",
    "\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    train_aspects = []\n",
    "    train_indices = []\n",
    "\n",
    "    val_data = []\n",
    "    val_labels = []\n",
    "    val_aspects = []\n",
    "    val_indices = []\n",
    "\n",
    "    # Transform train set\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = aspect_detection_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                for j in range(len(preds[i])):\n",
    "                    if int(preds[i][j]) != 1:\n",
    "                        train_aspects.append(aspect_list[j])\n",
    "                        train_data.append(batch['ori_text'][i])\n",
    "                        train_labels.append(batch['ori_label'][i][j])\n",
    "                        train_indices.append(batch['ori_indices'][i])\n",
    "            \n",
    "        # Transform validation set\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = aspect_detection_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                for j in range(len(preds[i])):\n",
    "                    if int(preds[i][j]) != 1:\n",
    "                        val_aspects.append(aspect_list[j])\n",
    "                        val_data.append(batch['ori_text'][i])\n",
    "                        val_labels.append(batch['ori_label'][i][j])\n",
    "                        val_indices.append(batch['ori_indices'][i])\n",
    "\n",
    "    # if len(train_data) > 0:\n",
    "    train_dataset = SentimentAnalysisDataset(train_data, train_labels, train_aspects, train_indices, label_mapping, tokenizer, max_length=max_length)\n",
    "    val_dataset = SentimentAnalysisDataset(val_data, val_labels, val_aspects, val_indices, label_mapping, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4, \n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset\n",
    "\n",
    "    # return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768739e",
   "metadata": {
    "papermill": {
     "duration": 0.015303,
     "end_time": "2025-04-05T06:35:20.752476",
     "exception": false,
     "start_time": "2025-04-05T06:35:20.737173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd142432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:20.784082Z",
     "iopub.status.busy": "2025-04-05T06:35:20.783794Z",
     "iopub.status.idle": "2025-04-05T06:35:20.787841Z",
     "shell.execute_reply": "2025-04-05T06:35:20.787085Z"
    },
    "papermill": {
     "duration": 0.021517,
     "end_time": "2025-04-05T06:35:20.789285",
     "exception": false,
     "start_time": "2025-04-05T06:35:20.767768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_data = len(X_train) + len(X_val)\n",
    "initial_train_size = int(0.05 * total_data)\n",
    "checkpoints = [\n",
    "    int(0.5 * total_data), \n",
    "    int(0.6 * total_data), \n",
    "    int(0.7 * total_data),\n",
    "    len(X_train)\n",
    "]\n",
    "min_increment = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d403c555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:20.820477Z",
     "iopub.status.busy": "2025-04-05T06:35:20.820227Z",
     "iopub.status.idle": "2025-04-05T06:35:20.825402Z",
     "shell.execute_reply": "2025-04-05T06:35:20.824656Z"
    },
    "papermill": {
     "duration": 0.022776,
     "end_time": "2025-04-05T06:35:20.826995",
     "exception": false,
     "start_time": "2025-04-05T06:35:20.804219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p, label, classes):\n",
    "    preds = torch.tensor(p.predictions) # Sigmoid and threshold for multi-label\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    hamming_accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Standard multi-label precision, recall, and F1 metrics\n",
    "    precision, recall, f1_micro, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    _, _, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    report = classification_report(\n",
    "        labels, \n",
    "        preds, \n",
    "        labels=label,\n",
    "        target_names=classes,\n",
    "        zero_division=0\n",
    "    ) \n",
    "\n",
    "    return {\n",
    "        'accuracy': hamming_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'report': report\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a15f49e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:20.859523Z",
     "iopub.status.busy": "2025-04-05T06:35:20.859221Z",
     "iopub.status.idle": "2025-04-05T06:35:20.866214Z",
     "shell.execute_reply": "2025-04-05T06:35:20.865408Z"
    },
    "papermill": {
     "duration": 0.025043,
     "end_time": "2025-04-05T06:35:20.867789",
     "exception": false,
     "start_time": "2025-04-05T06:35:20.842746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics_overall(p, classes):\n",
    "    preds = torch.tensor(p.predictions)\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "\n",
    "    # Ensure it's in the correct shape\n",
    "    if preds.shape != labels.shape:\n",
    "        raise ValueError(\"Shape mismatch: predictions and labels must have the same shape.\")\n",
    "\n",
    "    # Hamming accuracy: proportion of correctly predicted labels over total labels\n",
    "    hamming_accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    # Compute per-label (column-wise) precision, recall, F1\n",
    "    precision_list, recall_list, f1_micro_list, f1_macro_list = [], [], [], []\n",
    "    \n",
    "    for i in range(labels.shape[1]):  # Loop through each column (multi-output)\n",
    "        prec, rec, f1_micro, _ = precision_recall_fscore_support(\n",
    "            labels[:, i], preds[:, i], average='micro', zero_division=0\n",
    "        )\n",
    "        _, _, f1_macro, _ = precision_recall_fscore_support(\n",
    "            labels[:, i], preds[:, i], average='macro', zero_division=0\n",
    "        )\n",
    "\n",
    "        precision_list.append(prec)\n",
    "        recall_list.append(rec)\n",
    "        f1_micro_list.append(f1_micro)\n",
    "        f1_macro_list.append(f1_macro)\n",
    "\n",
    "    # Compute average metrics across all outputs\n",
    "    precision = sum(precision_list) / len(precision_list)\n",
    "    recall = sum(recall_list) / len(recall_list)\n",
    "    f1_micro = sum(f1_micro_list) / len(f1_micro_list)\n",
    "    f1_macro = sum(f1_macro_list) / len(f1_macro_list)\n",
    "\n",
    "    # Generate classification report per output\n",
    "    reports = [classification_report(labels[:, i], preds[:, i], target_names=classes, zero_division=0) for i in range(labels.shape[1])]\n",
    "\n",
    "    return {\n",
    "        'accuracy': hamming_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'reports': reports  # Returns list of reports, one for each output label\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c735cf7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:20.898988Z",
     "iopub.status.busy": "2025-04-05T06:35:20.898683Z",
     "iopub.status.idle": "2025-04-05T06:35:20.927964Z",
     "shell.execute_reply": "2025-04-05T06:35:20.927224Z"
    },
    "papermill": {
     "duration": 0.046395,
     "end_time": "2025-04-05T06:35:20.929244",
     "exception": false,
     "start_time": "2025-04-05T06:35:20.882849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(current_train_size, train_indices, aspect_metrics, sentiment_metrics, metrics, trials, model_num):\n",
    "    accelerator = Accelerator(mixed_precision='fp16')  # Initialize the accelerator\n",
    "    device = accelerator.device\n",
    "\n",
    "    # Setup Aspect Model\n",
    "    aspect_model = BertForSequenceClassification.from_pretrained(\n",
    "        'indobenchmark/indobert-base-p1',\n",
    "        num_labels=len(train_labels),\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    ) \n",
    "    aspect_optimizer = torch.optim.AdamW(aspect_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    for name, param in aspect_model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Setup Sentiment Model\n",
    "    sentiment_model = BertForSequenceClassification.from_pretrained(\n",
    "        'indobenchmark/indobert-base-p1',\n",
    "        num_labels=2,\n",
    "    )\n",
    "    sentiment_optimizer = torch.optim.AdamW(sentiment_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    for name, param in sentiment_model.named_parameters():\n",
    "        if \"encoder.layer\" in name:\n",
    "            layer_num = name.split(\".\")[3]\n",
    "            try:\n",
    "                if int(layer_num) < 6:\n",
    "                    param.requires_grad = False\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Prepare models\n",
    "    aspect_model, aspect_optimizer = accelerator.prepare(aspect_model, aspect_optimizer)\n",
    "    sentiment_model, sentiment_optimizer = accelerator.prepare(sentiment_model, sentiment_optimizer)\n",
    "\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define DataLoaders\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    aspect_train_loader, aspect_val_loader, aspect_train_dataset, aspect_val_dataset = build_aspect_dataset(current_X_train, current_y_train, X_val, y_val, sequence_length)\n",
    "\n",
    "    # Prepare train loaders\n",
    "    aspect_train_loader, aspect_val_loader = accelerator.prepare(\n",
    "        aspect_train_loader, aspect_val_loader\n",
    "    )\n",
    "\n",
    "    aspect_result = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # ASPECT DETECTION\n",
    "    accelerator.print(\"ASPECT DETECTION\")\n",
    "    for epoch in range(epochs):\n",
    "        aspect_model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in aspect_train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels']\n",
    "        \n",
    "            aspect_optimizer.zero_grad()\n",
    "            outputs = aspect_model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            aspect_optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        aspect_model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in aspect_val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "                labels = batch['labels']\n",
    "                \n",
    "                outputs = aspect_model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                # Gather predictions and labels from all devices\n",
    "                all_preds.append(accelerator.gather(preds))\n",
    "                all_labels.append(accelerator.gather(labels))\n",
    "\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "        result = compute_metrics(\n",
    "            type('EvalOutput', (object,), {'predictions': all_preds, 'label_ids': all_labels}),\n",
    "            None,\n",
    "            aspect_list,\n",
    "        )\n",
    "\n",
    "        if aspect_result is None or result['f1_micro'] >= aspect_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(aspect_model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                f'{filename}-aspect-{trials + 1}-model-{model_num+1}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "            aspect_result = result\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(aspect_train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "\n",
    "    accelerator.print(f\"\\nModel {model_num+1} of aspect detection, Accuracy: {round(aspect_result['accuracy'], 4)}, F1 Micro: {round(aspect_result['f1_micro'], 4)}, F1 Macro: {round(aspect_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(aspect_result['report'])\n",
    "\n",
    "    best_aspect_model = BertForSequenceClassification.from_pretrained(f'{filename}-aspect-{trials + 1}-model-{model_num+1}')\n",
    "    best_aspect_model = accelerator.prepare(best_aspect_model)\n",
    "\n",
    "    # SENTIMENT ANALYSIS ON NON NEUTRAL ASPECTS\n",
    "    accelerator.print(\"--------------------------------------------------\")\n",
    "    accelerator.print(\"SENTIMENT ANALYSIS\")\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    sentiment_train_loader, sentiment_val_loader, sentiment_train_dataset, sentiment_val_dataset = build_sentiment_dataset(\n",
    "        device, aspect_train_dataset, aspect_val_dataset, best_aspect_model, tokenizer, max_length=sequence_length\n",
    "    )\n",
    "    sentiment_model, sentiment_optimizer, sentiment_train_loader, sentiment_val_loader = accelerator.prepare(\n",
    "        sentiment_model, sentiment_optimizer, sentiment_train_loader, sentiment_val_loader\n",
    "    )\n",
    "    sentiment_result = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        sentiment_model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in sentiment_train_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            labels = batch['labels']\n",
    "        \n",
    "            sentiment_optimizer.zero_grad()\n",
    "            outputs = sentiment_model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            accelerator.backward(loss)\n",
    "            sentiment_optimizer.step()\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        sentiment_model.eval()\n",
    "        sentiment_val_outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in sentiment_val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "                \n",
    "                outputs = sentiment_model(**inputs)\n",
    "                preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "                for i in range(len(preds)):\n",
    "                    val_output = {\n",
    "                        'label': batch['labels'][i],\n",
    "                        'aspect': batch['aspect'][i],\n",
    "                        'ori_indices': batch['ori_indices'][i],\n",
    "                        'pred': np.argmax(preds[i].cpu().numpy()),\n",
    "                    }\n",
    "                    sentiment_val_outputs.append(val_output)\n",
    "\n",
    "        sentiment_val_outputs = accelerator.gather_for_metrics(sentiment_val_outputs)\n",
    "        unique_val_outputs = {(x['ori_indices'].item(), x['aspect']): x for x in sentiment_val_outputs}\n",
    "        sentiment_val_outputs = list(unique_val_outputs.values())\n",
    "\n",
    "        result = compute_metrics(\n",
    "            type('EvalOutput', (object,), {'predictions': [item['pred'] for item in sentiment_val_outputs], 'label_ids': [np.argmax(item['label'].cpu().numpy()) for item in sentiment_val_outputs]}),\n",
    "            [0, 1],\n",
    "            ['negative', 'positive']\n",
    "        )\n",
    "\n",
    "        if sentiment_result is None or result['f1_micro'] >= sentiment_result['f1_micro']:\n",
    "            accelerator.print(\"Higher F1 achieved, saving model\")\n",
    "            sentiment_result = result\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(sentiment_model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                 f'{filename}-sentiment-{trials + 1}-model-{model_num+1}',\n",
    "                is_main_process=accelerator.is_main_process,\n",
    "                save_function=accelerator.save,\n",
    "            )\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {round(epoch_loss / len(sentiment_train_loader), 4)}, Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    accelerator.print(f\"\\nModel {model_num+1} of sentiment analysis, accuracy: {round(sentiment_result['accuracy'], 4)}, F1 Micro: {round(sentiment_result['f1_micro'], 4)}, F1 Macro: {round(sentiment_result['f1_macro'], 4)}\")\n",
    "    accelerator.print(sentiment_result['report'])\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    best_sentiment_model = BertForSequenceClassification.from_pretrained( f'{filename}-sentiment-{trials + 1}-model-{model_num+1}')\n",
    "    best_sentiment_model = accelerator.prepare(best_sentiment_model)\n",
    "\n",
    "    # Compute overall metrics\n",
    "    aspect_labels = []\n",
    "    aspect_indices = []\n",
    "    aspect_preds = []\n",
    "\n",
    "    aspect_outputs = {}\n",
    "    sentiment_outputs = []\n",
    "    \n",
    "    best_aspect_model.eval()\n",
    "    best_sentiment_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in aspect_val_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            outputs = best_aspect_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "\n",
    "            aspect_indices.append(accelerator.gather(batch['ori_indices']))\n",
    "            aspect_labels.append(accelerator.gather(batch['ori_label']))\n",
    "            aspect_preds.append(accelerator.gather(preds))\n",
    "\n",
    "        aspect_indices = torch.cat(aspect_indices).cpu().numpy()\n",
    "        aspect_labels = torch.cat(aspect_labels).cpu().numpy()\n",
    "        aspect_preds = torch.cat(aspect_preds).cpu().numpy()\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        aspect_outputs = [\n",
    "            {'ori_indices': aspect_indices[i], \n",
    "             'ori_labels': aspect_labels[i], \n",
    "             'pred': aspect_preds[i]}\n",
    "            for i in range(len(aspect_preds))\n",
    "        ]\n",
    "        aspect_outputs = {x['ori_indices'].item(): x for x in aspect_outputs}\n",
    "    \n",
    "        for batch in sentiment_val_loader:\n",
    "            inputs = {key: val for key, val in batch.items() if key not in ignored_keys}\n",
    "            outputs = best_sentiment_model(**inputs)\n",
    "            preds = torch.sigmoid(outputs.logits).round()\n",
    "    \n",
    "            for i in range(len(preds)):\n",
    "                output = {\n",
    "                    'aspect': batch['aspect'][i],\n",
    "                    'ori_indices': batch['ori_indices'][i],\n",
    "                    'pred': np.argmax(preds[i].cpu().numpy()),\n",
    "                }\n",
    "                sentiment_outputs.append(output)\n",
    "\n",
    "        sentiment_outputs = accelerator.gather_for_metrics(sentiment_outputs)\n",
    "        sentiment_outputs = {(x['ori_indices'].item(), x['aspect']): x for x in sentiment_outputs}\n",
    "\n",
    "    # Replcae non neutral aspect to its predicted sentiment\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        i = -1\n",
    "        for (ori_index, aspect), value in sentiment_outputs.items():\n",
    "            aspect = aspect_mapping[aspect]\n",
    "            aspect_outputs[ori_index]['pred'][aspect] = 2 if value['pred'] == 1.0 else value['pred']\n",
    "\n",
    "        result = compute_metrics_overall(\n",
    "            type('EvalOutput', (object,), {'predictions': [output['pred'] for output in aspect_outputs.values()], 'label_ids': [output['ori_labels'] for output in aspect_outputs.values()]}),\n",
    "            ['negative', 'neutral', 'positive'],\n",
    "        )\n",
    "\n",
    "        accelerator.print(\"--------------------------------------------------\")\n",
    "        accelerator.print(f\"Model {model_num+1} - Iteration {current_train_size}: Accuracy: {round(result['accuracy'], 4)}, F1 Micro: {round(result['f1_micro'], 4)}, F1 Macro: {round(result['f1_macro'], 4)}\")\n",
    "        accelerator.print(\"--------------------------------------------------\")\n",
    "        for i in range(len(train_labels)):\n",
    "            accelerator.print(f\"Aspect {aspect_list[i]} report:\")\n",
    "            accelerator.print(result['reports'][i])\n",
    "       \n",
    "        aspect_metrics[0].append(aspect_result['accuracy'])\n",
    "        aspect_metrics[1].append(aspect_result['f1_micro'])\n",
    "        aspect_metrics[2].append(aspect_result['f1_macro'])\n",
    "        sentiment_metrics[0].append(sentiment_result['accuracy'])\n",
    "        sentiment_metrics[1].append(sentiment_result['f1_micro'])\n",
    "        sentiment_metrics[2].append(sentiment_result['f1_macro'])\n",
    "        metrics[0].append(result['accuracy'])\n",
    "        metrics[1].append(result['f1_micro'])\n",
    "        metrics[2].append(result['f1_macro'])\n",
    "        \n",
    "    accelerator.print(f\"Total train time: {duration} s\")\n",
    "    accelerator.end_training()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c187c5",
   "metadata": {
    "papermill": {
     "duration": 0.015683,
     "end_time": "2025-04-05T06:35:20.960852",
     "exception": false,
     "start_time": "2025-04-05T06:35:20.945169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PLOT THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "504df491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:20.993192Z",
     "iopub.status.busy": "2025-04-05T06:35:20.992866Z",
     "iopub.status.idle": "2025-04-05T06:35:20.998938Z",
     "shell.execute_reply": "2025-04-05T06:35:20.998102Z"
    },
    "papermill": {
     "duration": 0.023995,
     "end_time": "2025-04-05T06:35:21.000406",
     "exception": false,
     "start_time": "2025-04-05T06:35:20.976411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_result(data_used, accuracies, f1_micros, f1_macros):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(21, 5))\n",
    "    data_used = [round(data / total_data * 100, 1) for data in data_used]\n",
    "\n",
    "    # Plot for Accuracy\n",
    "    axs[0].plot(data_used, accuracies, label=\"Accuracy\", color=\"blue\")\n",
    "    axs[0].set_xlabel(\"Percentage of data used\")\n",
    "    axs[0].set_title(\"Accuracy\")\n",
    "    axs[0].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Micro\n",
    "    axs[1].plot(data_used, f1_micros, label=\"F1 Micro\", color=\"orange\")\n",
    "    axs[1].set_xlabel(\"Percentage of data used\")\n",
    "    axs[1].set_title(\"F1 Micro\")\n",
    "    axs[1].set_xticks(data_used)\n",
    "\n",
    "    # Plot for F1 Macro\n",
    "    axs[2].plot(data_used, f1_macros, label=\"F1 Macro\", color=\"green\")\n",
    "    axs[2].set_xlabel(\"Percentage of data used\")\n",
    "    axs[2].set_title(\"F1 Macro\")\n",
    "    axs[2].set_xticks(data_used)\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6be3d5",
   "metadata": {
    "papermill": {
     "duration": 0.015349,
     "end_time": "2025-04-05T06:35:21.031005",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.015656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QUERY STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c73c9ae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:21.063143Z",
     "iopub.status.busy": "2025-04-05T06:35:21.062838Z",
     "iopub.status.idle": "2025-04-05T06:35:21.069980Z",
     "shell.execute_reply": "2025-04-05T06:35:21.069195Z"
    },
    "papermill": {
     "duration": 0.024844,
     "end_time": "2025-04-05T06:35:21.071305",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.046461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beta_score(p, y, alpha=0.1, beta=3):\n",
    "    \"\"\"Calculates Beta score for a given probability p and label y.\"\"\"\n",
    "    \n",
    "    if y == 1:\n",
    "        return -betaln(alpha, beta + 1) + betaln(alpha + p, beta + 1 - p)\n",
    "    elif y == 0:\n",
    "        return -betaln(alpha + 1, beta) + betaln(alpha + 1 - p, beta + p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label: y must be 0 or 1.\")\n",
    "\n",
    "def bayesian_update(prior, likelihood, evidence, alpha=0.1, beta_param=3):\n",
    "    \"\"\" \n",
    "    Bayes' Theorem: P(y'|x') = P(x'|y') * P(y') / P(x')\n",
    "    P(y'|x') or likelihood = model probs\n",
    "    p(y') or prior = class probabilities\n",
    "    p(x') or evidence = 1 / number of data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the Beta score to simulate the posterior\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    \n",
    "    # We calculate the posterior using the Beta distribution\n",
    "    return posterior\n",
    "\n",
    "def compute_expected_score_change(predicted_prob, class_probs, label_probs, class_idx):\n",
    "    scores_before = []\n",
    "    scores_after = []\n",
    "\n",
    "    # Before data addition: calculate Beta score for predicted prob\n",
    "    scores_before.append(beta_score(predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    scores_before.append(beta_score(1-predicted_prob, int(1 if predicted_prob >= 0.5 else 0)))\n",
    "    \n",
    "    for k in range(2):\n",
    "        prior = predicted_prob\n",
    "        likelihood = class_probs[class_idx][k]  # Likelihood is the true label (0 or 1)\n",
    "        posterior = bayesian_update(prior, likelihood, 1)\n",
    "        scores_after.append(beta_score(posterior, int(1 if posterior >= 0.5 else 0)))\n",
    "\n",
    "    score_diff_0 = scores_after[0] - scores_before[0]\n",
    "    score_diff_1 = scores_after[1] - scores_before[1]\n",
    "    \n",
    "    return label_probs['0'] * score_diff_0 + label_probs['1'] * score_diff_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "378b480e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:21.105002Z",
     "iopub.status.busy": "2025-04-05T06:35:21.104610Z",
     "iopub.status.idle": "2025-04-05T06:35:21.131585Z",
     "shell.execute_reply": "2025-04-05T06:35:21.130764Z"
    },
    "papermill": {
     "duration": 0.045797,
     "end_time": "2025-04-05T06:35:21.133028",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.087231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def besra_sampling(aspect_models, sentiment_models, X_pool, train_indices, remaining_indices, tokenizer, sampling_dur, new_samples, trials, n_clusters=min_increment):\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    for aspect_model in aspect_models:\n",
    "        aspect_model.to(device)\n",
    "        aspect_model.eval()\n",
    "\n",
    "    for sentiment_model in sentiment_models:\n",
    "        sentiment_model.to(device)\n",
    "        sentiment_model.eval()\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    aspect_dataset = AspectDetectionDataset(\n",
    "        X_pool, \n",
    "        [['neutral' for i in range(len(train_labels))] for x in range(len(X_pool))], \n",
    "        label_mapping, \n",
    "        tokenizer, \n",
    "        max_length=sequence_length\n",
    "    )\n",
    "    aspect_loader = DataLoader(\n",
    "        aspect_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    current_train_size = len(train_indices)\n",
    "    current_X_train = [X_train[i] for i in train_indices]\n",
    "    current_y_train = [y_train[i] for i in train_indices]\n",
    "    labeled_aspect_dataset = AspectDetectionDataset(current_X_train, current_y_train, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    label_aspect_probs = labeled_aspect_dataset.get_global_probs()\n",
    "    class_aspect_probs = labeled_aspect_dataset.get_per_class_probs()\n",
    "\n",
    "    _, _, labeled_sentiment_dataset, _ = build_sentiment_dataset(\n",
    "        aspect_models[0].device, labeled_aspect_dataset, labeled_aspect_dataset, aspect_models[0], tokenizer, max_length=sequence_length\n",
    "    )\n",
    "    label_sentiment_probs = labeled_sentiment_dataset.get_global_probs()\n",
    "    class_sentiment_probs = labeled_sentiment_dataset.get_per_class_probs()\n",
    "    \n",
    "    aspect_outputs = {}\n",
    "    sentiment_outputs = {}\n",
    "\n",
    "    aspects = []\n",
    "    data = []\n",
    "    labels = []\n",
    "    indices = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    aspect_uncertainties = []\n",
    "\n",
    "    # Pass through aspect detction model\n",
    "    for batch in aspect_loader:\n",
    "        model_probs = []\n",
    "        score_changes = []\n",
    "        \n",
    "        for model in aspect_models:\n",
    "            token_type_ids = batch['token_type_ids'].to(device, non_blocking=True)\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.sigmoid(logits)  # Multi-label classification uses sigmoid\n",
    "                model_probs.append(probs.unsqueeze(0))  # Add batch dimension for averaging\n",
    "\n",
    "        # Stack all model predictions and compute the mean across models\n",
    "        model_probs = torch.cat(model_probs, dim=0)  # Concatenate predictions across models\n",
    "        mean_probs = model_probs.mean(dim=0)  # Take the mean along the model axis\n",
    "\n",
    "        # Calculate Beta scores before and after data addition\n",
    "        for i in range(len(mean_probs)):\n",
    "            score_diff = []\n",
    "            for class_idx in range(mean_probs.shape[1]):\n",
    "                predicted_prob = mean_probs[i, class_idx].item()\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_aspect_probs, label_aspect_probs, class_idx))\n",
    "\n",
    "                if int(mean_probs[i][class_idx].round()) != 1:\n",
    "                        aspects.append(aspect_list[class_idx])\n",
    "                        data.append(batch['ori_text'][i])\n",
    "                        labels.append(batch['ori_label'][i][class_idx])\n",
    "                        indices.append(batch['ori_indices'][i])\n",
    "\n",
    "            aspect_outputs[batch['ori_indices'][i].item()] = np.mean(score_diff)\n",
    "    \n",
    "    sentiment_dataset = SentimentAnalysisDataset(data, labels, aspects, indices, label_mapping, tokenizer, max_length=sequence_length)\n",
    "    sentiment_loader = torch.utils.data.DataLoader(\n",
    "        sentiment_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4,\n",
    "    )\n",
    "\n",
    "    # Pass through sentiment analysis model\n",
    "    for batch in sentiment_loader:\n",
    "        token_type_ids = batch['token_type_ids'].to(device, non_blocking=True)\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "\n",
    "        batch_probs = []\n",
    "        for sentiment_model in sentiment_models:\n",
    "            with torch.no_grad():\n",
    "                outputs = sentiment_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                preds = torch.sigmoid(outputs.logits)\n",
    "\n",
    "                for j in range(len(preds)):\n",
    "                    ori_index = batch['ori_indices'][j].item()\n",
    "                    if ori_index in sentiment_outputs.keys():\n",
    "                        sentiment_outputs[ori_index].append(preds[j].cpu().numpy())\n",
    "                    else:\n",
    "                        sentiment_outputs[ori_index] = [preds[j].cpu().numpy()]\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    for indices, probs in sentiment_outputs.items():\n",
    "        sentiment_outputs[indices] = [[probs[i], probs[i+1], probs[i+2]] for i in range(int(len(probs) / 3))]\n",
    "        mean_probs = np.mean(sentiment_outputs[indices], axis=1)\n",
    "\n",
    "        score_changes = []\n",
    "        for prob in mean_probs:\n",
    "            score_diff = []\n",
    "            for class_idx in range(len(prob)):\n",
    "                predicted_prob = prob[class_idx]\n",
    "                score_diff.append(compute_expected_score_change(predicted_prob, class_sentiment_probs, label_sentiment_probs, class_idx))\n",
    "\n",
    "            score_changes.append(np.mean(score_diff))\n",
    "        sentiment_outputs[indices] = np.mean(score_changes)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        aspect_outputs = dict(sorted(aspect_outputs.items()))\n",
    "        if len(data) > 0:\n",
    "            for key, val in sentiment_outputs.items():\n",
    "                aspect_outputs[key] = (val + aspect_outputs[key]) / 2\n",
    "\n",
    "        score_changes = np.array(list(aspect_outputs.values())).reshape(-1, 1)\n",
    "\n",
    "        collected_indices = set()\n",
    "        thresholds = []\n",
    "\n",
    "        target_samples = math.ceil(0.1 * len(X_pool))\n",
    "\n",
    "        # Check nearest checkpoint\n",
    "        nearest_cp = 0\n",
    "        arrived_at_cp = False\n",
    "        for cp in checkpoints:\n",
    "            if cp > current_train_size:\n",
    "                nearest_cp = cp\n",
    "                break\n",
    "\n",
    "        # Determine number of maximum samples to be acquired\n",
    "        if target_samples <= n_clusters and n_clusters < nearest_cp - current_train_size:\n",
    "            target_samples = n_clusters\n",
    "        elif target_samples > n_clusters and target_samples < nearest_cp - current_train_size:\n",
    "            target_samples = target_samples\n",
    "        else:\n",
    "            arrived_at_cp = True\n",
    "            target_samples = nearest_cp - current_train_size\n",
    "\n",
    "        # No clustering needed when there's little data left\n",
    "        if current_train_size >= checkpoints[len(checkpoints)-1] - min_increment:\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            temp = train_indices.copy()\n",
    "            temp.extend(remaining_indices)\n",
    "            \n",
    "            # Save acquired data up to checkpoint\n",
    "            acquired_data = pd.DataFrame({\n",
    "                'processed_text': [X_train[i] for i in temp],\n",
    "                'fuel': [y_train[i][0] for i in temp],\n",
    "                'machine': [y_train[i][1] for i in temp],\n",
    "                'others': [y_train[i][2] for i in temp],\n",
    "                'part': [y_train[i][3] for i in temp],\n",
    "                'price': [y_train[i][4] for i in temp],\n",
    "                'service': [y_train[i][5] for i in temp],\n",
    "            })\n",
    "            acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "\n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(remaining_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "            \n",
    "            sampling_dur.append(duration)\n",
    "            for i in remaining_indices:\n",
    "                new_samples.append(i)\n",
    "        else:\n",
    "            # Cluster the data based on its score changes\n",
    "            kmeans=KMeans(n_clusters=n_clusters, n_init=1)\n",
    "            kmeans.fit(score_changes)\n",
    "            \n",
    "            for cluster_id in range(n_clusters):\n",
    "                # Cluster center and indices of samples in the current cluster\n",
    "                cluster_center = kmeans.cluster_centers_[cluster_id]\n",
    "                cluster_indices = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "            \n",
    "                if cluster_indices.size == 0:\n",
    "                    # Skip clusters with no members\n",
    "                    print(f\"Cluster {cluster_id} has no members, skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                # Calculate distances of each point in the cluster from the cluster center\n",
    "                cluster_distances = np.linalg.norm(score_changes[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "                # Determine the local threshold (10th percentile of closest distances to cluster center)\n",
    "                local_threshold = np.percentile(cluster_distances, 10)\n",
    "                thresholds.append(local_threshold)\n",
    "            \n",
    "                below_threshold_indices = cluster_indices[cluster_distances <= local_threshold]\n",
    "                collected_indices.update(below_threshold_indices)\n",
    "\n",
    "            # Handle multiple points with same distance\n",
    "            if len(collected_indices) > target_samples:\n",
    "                collected_indices = np.array(list(collected_indices))\n",
    "                np.random.shuffle(collected_indices)\n",
    "                collected_indices = collected_indices[:target_samples]\n",
    "                \n",
    "            end_time = time.time() \n",
    "            duration = end_time - start_time \n",
    "    \n",
    "            if arrived_at_cp:\n",
    "                temp = train_indices.copy()\n",
    "                temp.extend(collected_indices)\n",
    "                \n",
    "                # Save acquired data up to checkpoint\n",
    "                acquired_data = pd.DataFrame({\n",
    "                    'processed_text': [X_train[i] for i in temp],\n",
    "                    'fuel': [y_train[i][0] for i in temp],\n",
    "                    'machine': [y_train[i][1] for i in temp],\n",
    "                    'others': [y_train[i][2] for i in temp],\n",
    "                    'part': [y_train[i][3] for i in temp],\n",
    "                    'price': [y_train[i][4] for i in temp],\n",
    "                    'service': [y_train[i][5] for i in temp],\n",
    "                })\n",
    "                acquired_data.to_csv(f'acquired_data/{filename}-{trials+1}-data-{nearest_cp}.csv', index=False)\n",
    "            \n",
    "            print(\"Nearest checkpoint:\", nearest_cp)\n",
    "            print(\"Acquired samples:\", len(collected_indices))\n",
    "            print(f\"Sampling duration: {duration} seconds\")\n",
    "        \n",
    "            sampling_dur.append(duration)\n",
    "            for i in collected_indices:\n",
    "                new_samples.append(remaining_indices[i])\n",
    "\n",
    "        threshold_data = pd.DataFrame({\n",
    "            'Threshold': thresholds\n",
    "        })\n",
    "        threshold_data.to_csv(f\"results/{filename}-thresholds-{trials+1}-{current_train_size}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c9a26",
   "metadata": {
    "papermill": {
     "duration": 0.015442,
     "end_time": "2025-04-05T06:35:21.164594",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.149152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MAIN LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cdde8b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:21.196854Z",
     "iopub.status.busy": "2025-04-05T06:35:21.196470Z",
     "iopub.status.idle": "2025-04-05T06:35:21.211841Z",
     "shell.execute_reply": "2025-04-05T06:35:21.211099Z"
    },
    "papermill": {
     "duration": 0.032899,
     "end_time": "2025-04-05T06:35:21.213161",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.180262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def active_learning(seed, i):\n",
    "    aspect_accuracies = manager.list()\n",
    "    aspect_f1_micros = manager.list()\n",
    "    aspect_f1_macros = manager.list()\n",
    "    sentiment_accuracies = manager.list()\n",
    "    sentiment_f1_micros = manager.list()\n",
    "    sentiment_f1_macros = manager.list()\n",
    "    accuracies = manager.list()\n",
    "    f1_micros = manager.list()\n",
    "    f1_macros = manager.list()\n",
    "    data_used = manager.list()\n",
    "    sampling_dur = manager.list()\n",
    "    new_samples = manager.list()\n",
    "    \n",
    "    print(\"===============================================\")\n",
    "    print(\"TRIAL {}\".format(i+1))\n",
    "    print(\"Random seed:\", seed)\n",
    "    \n",
    "    train_indices = np.random.choice(range(len(X_train)), initial_train_size, replace=False).tolist()\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(train_indices))\n",
    "    \n",
    "    current_train_size = initial_train_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while current_train_size < checkpoints[len(checkpoints) - 1]:\n",
    "        model_aspect_accuracies = manager.list()\n",
    "        model_aspect_f1_micros = manager.list()\n",
    "        model_aspect_f1_macros = manager.list()\n",
    "        model_sentiment_accuracies = manager.list()\n",
    "        model_sentiment_f1_micros = manager.list()\n",
    "        model_sentiment_f1_macros = manager.list()\n",
    "        model_accuracies = manager.list()\n",
    "        model_f1_micros = manager.list()\n",
    "        model_f1_macros = manager.list()\n",
    "\n",
    "        # Train the model\n",
    "        for j in range(3):\n",
    "            set_seed(seed[j])\n",
    "            args = (\n",
    "                current_train_size, \n",
    "                train_indices, \n",
    "                (model_aspect_accuracies, model_aspect_f1_micros, model_aspect_f1_macros), \n",
    "                (model_sentiment_accuracies, model_sentiment_f1_micros, model_sentiment_f1_macros),\n",
    "                (model_accuracies, model_f1_micros, model_f1_macros), \n",
    "                i,\n",
    "                j\n",
    "            )\n",
    "            notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "        data_used.append(current_train_size)\n",
    "        aspect_accuracies.append(np.mean(model_aspect_accuracies))\n",
    "        aspect_f1_micros.append(np.mean(model_aspect_f1_micros))\n",
    "        aspect_f1_macros.append(np.mean(model_aspect_f1_macros))\n",
    "        sentiment_accuracies.append(np.mean(model_sentiment_accuracies))\n",
    "        sentiment_f1_micros.append(np.mean(model_sentiment_f1_micros))\n",
    "        sentiment_f1_macros.append(np.mean(model_sentiment_f1_macros))\n",
    "        accuracies.append(np.mean(model_accuracies))\n",
    "        f1_micros.append(np.mean(model_f1_micros))\n",
    "        f1_macros.append(np.mean(model_f1_macros))\n",
    "        print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "\n",
    "        aspect_models = []\n",
    "        sentiment_models = []\n",
    "        for j in range(3):\n",
    "            aspect_model = BertForSequenceClassification.from_pretrained(f'{filename}-aspect-{i+1}-model-{j+1}')\n",
    "            sentiment_model = BertForSequenceClassification.from_pretrained(f'{filename}-sentiment-{i+1}-model-{j+1}')\n",
    "            \n",
    "            aspect_models.append(aspect_model)\n",
    "            sentiment_models.append(sentiment_model)\n",
    "    \n",
    "        # Perform query strategy to select new samples\n",
    "        new_samples = manager.list()\n",
    "        sampling_args = (\n",
    "            aspect_models, \n",
    "            sentiment_models, \n",
    "            [X_train[i] for i in remaining_indices], \n",
    "            train_indices, \n",
    "            remaining_indices, \n",
    "            tokenizer,\n",
    "            sampling_dur, \n",
    "            new_samples, \n",
    "            i\n",
    "        )\n",
    "        notebook_launcher(besra_sampling, sampling_args, num_processes=2)\n",
    "        new_samples = list(new_samples)\n",
    "        train_indices.extend(new_samples)\n",
    "        remaining_indices = list(set(remaining_indices) - set(new_samples))\n",
    "    \n",
    "        # Update current training size\n",
    "        current_train_size = len(train_indices)\n",
    "        print(\"New train size: {}\".format(current_train_size))\n",
    "    \n",
    "    # Train last epoch\n",
    "    for j in range(3):\n",
    "        set_seed(seed[j])\n",
    "        args = (\n",
    "            current_train_size, \n",
    "            train_indices, \n",
    "            (model_aspect_accuracies, model_aspect_f1_micros, model_aspect_f1_macros), \n",
    "            (model_sentiment_accuracies, model_sentiment_f1_micros, model_sentiment_f1_macros),\n",
    "            (model_accuracies, model_f1_micros, model_f1_macros), \n",
    "            i,\n",
    "            j\n",
    "        )\n",
    "        notebook_launcher(train_model, args, num_processes=2)\n",
    "\n",
    "    data_used.append(current_train_size)\n",
    "    aspect_accuracies.append(np.mean(model_aspect_accuracies))\n",
    "    aspect_f1_micros.append(np.mean(model_aspect_f1_micros))\n",
    "    aspect_f1_macros.append(np.mean(model_aspect_f1_macros))\n",
    "    sentiment_accuracies.append(np.mean(model_sentiment_accuracies))\n",
    "    sentiment_f1_micros.append(np.mean(model_sentiment_f1_micros))\n",
    "    sentiment_f1_macros.append(np.mean(model_sentiment_f1_macros))\n",
    "    accuracies.append(np.mean(model_accuracies))\n",
    "    f1_micros.append(np.mean(model_f1_micros))\n",
    "    f1_macros.append(np.mean(model_f1_macros))\n",
    "    print(f\"Averaged - Iteration {current_train_size}: Accuracy: {round(np.mean(accuracies), 4)}, F1 Micro: {round(np.mean(f1_micros), 4)}, F1 Macro: {round(np.mean(f1_macros), 4)}\")\n",
    "\n",
    "    aspect_accuracies, aspect_f1_micros, aspect_f1_macros = list(aspect_accuracies), list(aspect_f1_micros), list(aspect_f1_macros)\n",
    "    sentiment_accuracies, sentiment_f1_micros, sentiment_f1_macros = list(sentiment_accuracies), list(sentiment_f1_micros), list(sentiment_f1_macros)\n",
    "    data_used, accuracies, f1_micros, f1_macros, sampling_dur = list(data_used), list(accuracies), list(f1_micros), list(f1_macros), list(sampling_dur)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # print(f\"Total sampling time: {np.array(sampling_dur).sum().round(2)} seconds\")\n",
    "    print(f\"Total runtime: {duration} seconds\")\n",
    "    \n",
    "    plot_result(data_used, accuracies, f1_micros, f1_macros)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Data Used': data_used,\n",
    "        'Aspect Accuracy': aspect_accuracies,\n",
    "        'Aspect F1 Micro': aspect_f1_micros,\n",
    "        'Aspect F1 Macro': aspect_f1_macros,\n",
    "        'Sentiment Accuracy': sentiment_accuracies,\n",
    "        'Sentiment F1 Micro': sentiment_f1_micros,\n",
    "        'Sentiment F1 Macro': sentiment_f1_macros,\n",
    "        'Accuracy': accuracies,\n",
    "        'F1 Micro': f1_micros,\n",
    "        'F1 Macro': f1_macros,\n",
    "    })\n",
    "    \n",
    "    sampling_dur.insert(0, 0)\n",
    "    results['Sampling Duration'] = sampling_dur\n",
    "    results.to_csv(f'results/{filename}-{i+1}-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ee5dd89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:21.245660Z",
     "iopub.status.busy": "2025-04-05T06:35:21.245312Z",
     "iopub.status.idle": "2025-04-05T06:35:21.249550Z",
     "shell.execute_reply": "2025-04-05T06:35:21.248598Z"
    },
    "papermill": {
     "duration": 0.022642,
     "end_time": "2025-04-05T06:35:21.250924",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.228282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seeds = [[50, 67, 42], [81, 90, 11], [14, 61, 33], [3, 44, 85], [94, 21, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bccb9b6",
   "metadata": {
    "papermill": {
     "duration": 0.015652,
     "end_time": "2025-04-05T06:35:21.281825",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.266173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ec12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 1\n",
      "Random seed: [50, 67, 42]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9b3ea9ee40456598ef145195d1a4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6756, Accuracy: 0.7701, F1 Micro: 0.8685, F1 Macro: 0.8664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5949, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5663, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5413, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.485, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Epoch 6/10, Train Loss: 0.4934, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 7/10, Train Loss: 0.4861, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4478, Accuracy: 0.7924, F1 Micro: 0.8836, F1 Macro: 0.882\n",
      "Epoch 9/10, Train Loss: 0.4182, Accuracy: 0.7924, F1 Micro: 0.8826, F1 Macro: 0.8805\n",
      "Epoch 10/10, Train Loss: 0.3944, Accuracy: 0.7939, F1 Micro: 0.8833, F1 Macro: 0.881\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7924, F1 Micro: 0.8836, F1 Macro: 0.882\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.72      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7484, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5901, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4859, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.421, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3657, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2973, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2815, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2474, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2277, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2016, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         2\n",
      "    positive       0.75      1.00      0.86         6\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.38      0.50      0.43         8\n",
      "weighted avg       0.56      0.75      0.64         8\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7909, F1 Micro: 0.7909, F1 Macro: 0.303\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.50      0.10      0.16        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.41      0.36      0.33       216\n",
      "weighted avg       0.60      0.71      0.62       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 61.52419996261597 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6474, Accuracy: 0.7091, F1 Micro: 0.8196, F1 Macro: 0.7917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.589, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8815\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5429, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5279, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Epoch 5/10, Train Loss: 0.4875, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Epoch 6/10, Train Loss: 0.4868, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Epoch 7/10, Train Loss: 0.4839, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4515, Accuracy: 0.7932, F1 Micro: 0.8838, F1 Macro: 0.8822\n",
      "Epoch 9/10, Train Loss: 0.4287, Accuracy: 0.7932, F1 Micro: 0.8834, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.397, Accuracy: 0.7976, F1 Micro: 0.8856, F1 Macro: 0.8839\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7976, F1 Micro: 0.8856, F1 Macro: 0.8839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.80      0.98      0.88       175\n",
      "      others       0.74      0.97      0.84       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7455, Accuracy: 0.5385, F1 Micro: 0.5385, F1 Macro: 0.35\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6946, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5674, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.579, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.5553, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5072, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4154, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4522, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.5831, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3683, Accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8077, F1 Micro: 0.8077, F1 Macro: 0.4468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         5\n",
      "    positive       0.81      1.00      0.89        21\n",
      "\n",
      "    accuracy                           0.81        26\n",
      "   macro avg       0.40      0.50      0.45        26\n",
      "weighted avg       0.65      0.81      0.72        26\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7948, F1 Micro: 0.7948, F1 Macro: 0.3184\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.79      0.98      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.76       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.61      0.76      0.68       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.74      0.97      0.84       152\n",
      "    positive       0.76      0.25      0.38        52\n",
      "\n",
      "    accuracy                           0.75       216\n",
      "   macro avg       0.50      0.41      0.41       216\n",
      "weighted avg       0.71      0.75      0.68       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.05        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.69      0.71      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 62.98211979866028 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6607, Accuracy: 0.7649, F1 Micro: 0.864, F1 Macro: 0.8569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5793, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5503, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.517, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4778, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4767, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4783, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4419, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4097, Accuracy: 0.7939, F1 Micro: 0.8842, F1 Macro: 0.8826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3844, Accuracy: 0.7976, F1 Micro: 0.8852, F1 Macro: 0.8831\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7976, F1 Micro: 0.8852, F1 Macro: 0.8831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.72      0.99      0.84       158\n",
      "        part       0.75      0.93      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6786, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.5733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5919, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4489, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4731, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4406, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5754, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.6467, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3345, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.353, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.359, Accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.8182, F1 Micro: 0.8182, F1 Macro: 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         6\n",
      "    positive       0.82      1.00      0.90        27\n",
      "\n",
      "    accuracy                           0.82        33\n",
      "   macro avg       0.41      0.50      0.45        33\n",
      "weighted avg       0.67      0.82      0.74        33\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7948, F1 Micro: 0.7948, F1 Macro: 0.3236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.67      0.08      0.14        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.46      0.36      0.32       216\n",
      "weighted avg       0.67      0.72      0.62       216\n",
      "\n",
      "Aspect part report:              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.76      0.94      0.84       152\n",
      "    positive       0.48      0.32      0.38        41\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.41      0.42      0.41       216\n",
      "weighted avg       0.62      0.72      0.66       216\n",
      "\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 73.65723919868469 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7935, F1 Micro: 0.7935, F1 Macro: 0.315\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 8.62182331085205 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.637, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5378, Accuracy: 0.7939, F1 Micro: 0.8844, F1 Macro: 0.8829\n",
      "Epoch 3/10, Train Loss: 0.4902, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Epoch 4/10, Train Loss: 0.4661, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4441, Accuracy: 0.8125, F1 Micro: 0.8937, F1 Macro: 0.8928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.395, Accuracy: 0.8147, F1 Micro: 0.895, F1 Macro: 0.8938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3825, Accuracy: 0.8415, F1 Micro: 0.9084, F1 Macro: 0.9078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3416, Accuracy: 0.8661, F1 Micro: 0.9213, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3066, Accuracy: 0.8862, F1 Micro: 0.9314, F1 Macro: 0.93\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2519, Accuracy: 0.9003, F1 Micro: 0.9386, F1 Macro: 0.9361\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9003, F1 Micro: 0.9386, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.92      1.00      0.96       187\n",
      "     machine       0.90      0.90      0.90       175\n",
      "      others       0.84      0.90      0.87       158\n",
      "        part       0.89      1.00      0.94       158\n",
      "       price       0.95      1.00      0.97       192\n",
      "     service       0.97      0.99      0.98       191\n",
      "\n",
      "   micro avg       0.91      0.97      0.94      1061\n",
      "   macro avg       0.91      0.96      0.94      1061\n",
      "weighted avg       0.91      0.97      0.94      1061\n",
      " samples avg       0.92      0.97      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6221, Accuracy: 0.7109, F1 Micro: 0.7109, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5929, Accuracy: 0.7109, F1 Micro: 0.7109, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5364, Accuracy: 0.7109, F1 Micro: 0.7109, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3624, Accuracy: 0.8531, F1 Micro: 0.8531, F1 Macro: 0.7954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2449, Accuracy: 0.8673, F1 Micro: 0.8673, F1 Macro: 0.8401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1711, Accuracy: 0.8768, F1 Micro: 0.8768, F1 Macro: 0.8501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1413, Accuracy: 0.891, F1 Micro: 0.891, F1 Macro: 0.8668\n",
      "Epoch 8/10, Train Loss: 0.112, Accuracy: 0.872, F1 Micro: 0.872, F1 Macro: 0.831\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.872, F1 Micro: 0.872, F1 Macro: 0.831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0978, Accuracy: 0.9005, F1 Micro: 0.9005, F1 Macro: 0.8745\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9005, F1 Micro: 0.9005, F1 Macro: 0.8745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.77      0.82        61\n",
      "    positive       0.91      0.95      0.93       150\n",
      "\n",
      "    accuracy                           0.90       211\n",
      "   macro avg       0.89      0.86      0.87       211\n",
      "weighted avg       0.90      0.90      0.90       211\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8873, F1 Micro: 0.8873, F1 Macro: 0.7416\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.09      0.17        11\n",
      "     neutral       0.92      1.00      0.96       181\n",
      "    positive       0.83      0.62      0.71        24\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.57      0.61       216\n",
      "weighted avg       0.91      0.91      0.89       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.69      0.71        16\n",
      "     neutral       0.89      0.90      0.90       167\n",
      "    positive       0.55      0.55      0.55        33\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.72      0.71      0.72       216\n",
      "weighted avg       0.83      0.83      0.83       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.33      0.44        12\n",
      "     neutral       0.84      0.90      0.87       152\n",
      "    positive       0.66      0.60      0.63        52\n",
      "\n",
      "    accuracy                           0.80       216\n",
      "   macro avg       0.72      0.61      0.65       216\n",
      "weighted avg       0.79      0.80      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.61      0.74        23\n",
      "     neutral       0.88      1.00      0.94       152\n",
      "    positive       0.86      0.61      0.71        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.89      0.74      0.80       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.46      0.63        13\n",
      "     neutral       0.94      1.00      0.97       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.96      0.72      0.80       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      0.99      0.98       185\n",
      "    positive       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 85.24974489212036 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6278, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5295, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4957, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4676, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4446, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3967, Accuracy: 0.808, F1 Micro: 0.8913, F1 Macro: 0.8897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3745, Accuracy: 0.8423, F1 Micro: 0.9086, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3207, Accuracy: 0.8705, F1 Micro: 0.9227, F1 Macro: 0.9209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2813, Accuracy: 0.8862, F1 Micro: 0.9312, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2381, Accuracy: 0.8966, F1 Micro: 0.9366, F1 Macro: 0.9341\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8966, F1 Micro: 0.9366, F1 Macro: 0.9341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.92      1.00      0.96       187\n",
      "     machine       0.90      0.90      0.90       175\n",
      "      others       0.86      0.90      0.88       158\n",
      "        part       0.88      0.99      0.93       158\n",
      "       price       0.93      1.00      0.96       192\n",
      "     service       0.95      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.91      0.97      0.94      1061\n",
      "   macro avg       0.90      0.97      0.93      1061\n",
      "weighted avg       0.91      0.97      0.94      1061\n",
      " samples avg       0.91      0.97      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6094, Accuracy: 0.6995, F1 Micro: 0.6995, F1 Macro: 0.4116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5728, Accuracy: 0.6995, F1 Micro: 0.6995, F1 Macro: 0.4116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4926, Accuracy: 0.7488, F1 Micro: 0.7488, F1 Macro: 0.6067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3607, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.8241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2325, Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.8425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.179, Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.8411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1508, Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.8362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1048, Accuracy: 0.8768, F1 Micro: 0.8768, F1 Macro: 0.8514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.8768, F1 Micro: 0.8768, F1 Macro: 0.8467\n",
      "Epoch 10/10, Train Loss: 0.059, Accuracy: 0.8424, F1 Micro: 0.8424, F1 Macro: 0.7957\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8768, F1 Micro: 0.8768, F1 Macro: 0.8467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.72      0.78        61\n",
      "    positive       0.89      0.94      0.91       142\n",
      "\n",
      "    accuracy                           0.88       203\n",
      "   macro avg       0.87      0.83      0.85       203\n",
      "weighted avg       0.88      0.88      0.87       203\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8835, F1 Micro: 0.8835, F1 Macro: 0.727\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.18      0.31        11\n",
      "     neutral       0.92      1.00      0.96       181\n",
      "    positive       0.88      0.62      0.73        24\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.93      0.60      0.67       216\n",
      "weighted avg       0.92      0.92      0.90       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.62      0.67        16\n",
      "     neutral       0.89      0.90      0.90       167\n",
      "    positive       0.55      0.55      0.55        33\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.72      0.69      0.70       216\n",
      "weighted avg       0.83      0.83      0.83       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.25      0.32        12\n",
      "     neutral       0.86      0.90      0.88       152\n",
      "    positive       0.67      0.63      0.65        52\n",
      "\n",
      "    accuracy                           0.80       216\n",
      "   macro avg       0.65      0.60      0.62       216\n",
      "weighted avg       0.79      0.80      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.52      0.69        23\n",
      "     neutral       0.88      0.99      0.93       152\n",
      "    positive       0.81      0.63      0.71        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.90      0.72      0.78       216\n",
      "weighted avg       0.88      0.88      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.76        13\n",
      "     neutral       0.93      1.00      0.96       186\n",
      "    positive       0.88      0.41      0.56        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.94      0.68      0.76       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.95      1.00      0.97       185\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.96      0.76      0.84       216\n",
      "weighted avg       0.95      0.95      0.94       216\n",
      "\n",
      "Total train time: 89.47017669677734 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6128, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5201, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.491, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4562, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4393, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3943, Accuracy: 0.8177, F1 Micro: 0.8965, F1 Macro: 0.8955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3724, Accuracy: 0.8475, F1 Micro: 0.9114, F1 Macro: 0.9106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3193, Accuracy: 0.8705, F1 Micro: 0.9231, F1 Macro: 0.9217\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2902, Accuracy: 0.8958, F1 Micro: 0.9366, F1 Macro: 0.9346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2403, Accuracy: 0.9062, F1 Micro: 0.9422, F1 Macro: 0.9396\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9062, F1 Micro: 0.9422, F1 Macro: 0.9396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.94      1.00      0.97       187\n",
      "     machine       0.90      0.92      0.91       175\n",
      "      others       0.84      0.89      0.87       158\n",
      "        part       0.90      0.99      0.94       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.92      0.97      0.94      1061\n",
      "   macro avg       0.92      0.97      0.94      1061\n",
      "weighted avg       0.92      0.97      0.94      1061\n",
      " samples avg       0.92      0.97      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5943, Accuracy: 0.6866, F1 Micro: 0.6866, F1 Macro: 0.4071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5155, Accuracy: 0.6866, F1 Micro: 0.6866, F1 Macro: 0.4071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3721, Accuracy: 0.8111, F1 Micro: 0.8111, F1 Macro: 0.7355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2949, Accuracy: 0.8756, F1 Micro: 0.8756, F1 Macro: 0.8602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1955, Accuracy: 0.8848, F1 Micro: 0.8848, F1 Macro: 0.8633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.179, Accuracy: 0.9032, F1 Micro: 0.9032, F1 Macro: 0.8897\n",
      "Epoch 7/10, Train Loss: 0.1411, Accuracy: 0.8848, F1 Micro: 0.8848, F1 Macro: 0.8609\n",
      "Epoch 8/10, Train Loss: 0.1324, Accuracy: 0.894, F1 Micro: 0.894, F1 Macro: 0.8731\n",
      "Epoch 9/10, Train Loss: 0.1159, Accuracy: 0.8894, F1 Micro: 0.8894, F1 Macro: 0.867\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.8986, F1 Micro: 0.8986, F1 Macro: 0.8781\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9032, F1 Micro: 0.9032, F1 Macro: 0.8897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        68\n",
      "    positive       0.94      0.91      0.93       149\n",
      "\n",
      "    accuracy                           0.90       217\n",
      "   macro avg       0.88      0.90      0.89       217\n",
      "weighted avg       0.91      0.90      0.90       217\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8943, F1 Micro: 0.8943, F1 Macro: 0.7925\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.55      0.71        11\n",
      "     neutral       0.95      1.00      0.97       181\n",
      "    positive       0.95      0.75      0.84        24\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.97      0.77      0.84       216\n",
      "weighted avg       0.95      0.95      0.94       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.69      0.71        16\n",
      "     neutral       0.89      0.92      0.91       167\n",
      "    positive       0.57      0.52      0.54        33\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.73      0.71      0.72       216\n",
      "weighted avg       0.83      0.84      0.83       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.50      0.50        12\n",
      "     neutral       0.84      0.89      0.87       152\n",
      "    positive       0.60      0.48      0.53        52\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.64      0.63      0.63       216\n",
      "weighted avg       0.76      0.77      0.77       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.74      0.81        23\n",
      "     neutral       0.90      0.99      0.94       152\n",
      "    positive       0.87      0.63      0.73        41\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.89      0.79      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.62      0.73        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.77      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.65      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.99      0.83      0.90       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 85.41111660003662 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8409, F1 Micro: 0.8409, F1 Macro: 0.5343\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 14.833057403564453 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5958, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.512, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Epoch 3/10, Train Loss: 0.4934, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4457, Accuracy: 0.8103, F1 Micro: 0.8924, F1 Macro: 0.8914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4133, Accuracy: 0.8371, F1 Micro: 0.906, F1 Macro: 0.9055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3562, Accuracy: 0.8772, F1 Micro: 0.9273, F1 Macro: 0.9264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3023, Accuracy: 0.9055, F1 Micro: 0.9425, F1 Macro: 0.9402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2595, Accuracy: 0.9137, F1 Micro: 0.9461, F1 Macro: 0.9427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.218, Accuracy: 0.9249, F1 Micro: 0.9533, F1 Macro: 0.9502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1925, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.959\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.95      0.94       175\n",
      "      others       0.88      0.92      0.90       158\n",
      "        part       0.91      0.99      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6161, Accuracy: 0.678, F1 Micro: 0.678, F1 Macro: 0.404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.48, Accuracy: 0.678, F1 Micro: 0.678, F1 Macro: 0.404\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3759, Accuracy: 0.8983, F1 Micro: 0.8983, F1 Macro: 0.8843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2315, Accuracy: 0.9068, F1 Micro: 0.9068, F1 Macro: 0.8953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1696, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9338\n",
      "Epoch 7/10, Train Loss: 0.0762, Accuracy: 0.9322, F1 Micro: 0.9322, F1 Macro: 0.9229\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9364, F1 Micro: 0.9364, F1 Macro: 0.9275\n",
      "Epoch 9/10, Train Loss: 0.0613, Accuracy: 0.9322, F1 Micro: 0.9322, F1 Macro: 0.9229\n",
      "Epoch 10/10, Train Loss: 0.0332, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9127\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.91        76\n",
      "    positive       0.98      0.93      0.96       160\n",
      "\n",
      "    accuracy                           0.94       236\n",
      "   macro avg       0.92      0.95      0.93       236\n",
      "weighted avg       0.94      0.94      0.94       236\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.8695\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.92      0.95      0.94       167\n",
      "    positive       0.71      0.61      0.66        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.79      0.80       216\n",
      "weighted avg       0.88      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.89      0.93      0.91       152\n",
      "    positive       0.80      0.69      0.74        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.81      0.79      0.80       216\n",
      "weighted avg       0.86      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.90      0.99      0.94       152\n",
      "    positive       0.97      0.68      0.80        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 94.08809733390808 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.588, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.518, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Epoch 3/10, Train Loss: 0.4969, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4561, Accuracy: 0.7917, F1 Micro: 0.8829, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4144, Accuracy: 0.8289, F1 Micro: 0.9017, F1 Macro: 0.9001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3488, Accuracy: 0.8869, F1 Micro: 0.9314, F1 Macro: 0.9289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2845, Accuracy: 0.9122, F1 Micro: 0.9462, F1 Macro: 0.9435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2477, Accuracy: 0.9189, F1 Micro: 0.9496, F1 Macro: 0.947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2046, Accuracy: 0.9301, F1 Micro: 0.9565, F1 Macro: 0.9537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1794, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.9591\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.9591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.87      0.92      0.89       158\n",
      "        part       0.94      0.97      0.95       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.592, Accuracy: 0.6752, F1 Micro: 0.6752, F1 Macro: 0.4031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4844, Accuracy: 0.765, F1 Micro: 0.765, F1 Macro: 0.6668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3429, Accuracy: 0.8675, F1 Micro: 0.8675, F1 Macro: 0.8474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.209, Accuracy: 0.8974, F1 Micro: 0.8974, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1725, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0818, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Epoch 7/10, Train Loss: 0.0515, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8913\n",
      "Epoch 8/10, Train Loss: 0.0938, Accuracy: 0.9188, F1 Micro: 0.9188, F1 Macro: 0.9071\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9103, F1 Micro: 0.9103, F1 Macro: 0.9\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.9045\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.92      0.90        76\n",
      "    positive       0.96      0.94      0.95       158\n",
      "\n",
      "    accuracy                           0.93       234\n",
      "   macro avg       0.92      0.93      0.92       234\n",
      "weighted avg       0.93      0.93      0.93       234\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.929, F1 Micro: 0.929, F1 Macro: 0.8626\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.92      0.99      0.95       167\n",
      "    positive       0.88      0.64      0.74        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.79      0.84       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.87      0.92      0.89       152\n",
      "    positive       0.77      0.65      0.71        52\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.79      0.75      0.77       216\n",
      "weighted avg       0.84      0.84      0.84       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.87      0.83        23\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.85      0.71      0.77        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.80      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 93.14520573616028 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.581, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5032, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4906, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4399, Accuracy: 0.7999, F1 Micro: 0.8875, F1 Macro: 0.8861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4065, Accuracy: 0.8482, F1 Micro: 0.9119, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.346, Accuracy: 0.8936, F1 Micro: 0.9358, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2865, Accuracy: 0.9159, F1 Micro: 0.9486, F1 Macro: 0.9466\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2526, Accuracy: 0.9278, F1 Micro: 0.955, F1 Macro: 0.9522\n",
      "Epoch 9/10, Train Loss: 0.2057, Accuracy: 0.9226, F1 Micro: 0.9522, F1 Macro: 0.9491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1799, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9613\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.86      0.91      0.88       158\n",
      "        part       0.95      0.98      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5615, Accuracy: 0.6723, F1 Micro: 0.6723, F1 Macro: 0.402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4869, Accuracy: 0.8085, F1 Micro: 0.8085, F1 Macro: 0.7619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3037, Accuracy: 0.8851, F1 Micro: 0.8851, F1 Macro: 0.8732\n",
      "Epoch 4/10, Train Loss: 0.1865, Accuracy: 0.8723, F1 Micro: 0.8723, F1 Macro: 0.8498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1329, Accuracy: 0.8979, F1 Micro: 0.8979, F1 Macro: 0.8863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0723, Accuracy: 0.8979, F1 Micro: 0.8979, F1 Macro: 0.8799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0631, Accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.8996\n",
      "Epoch 8/10, Train Loss: 0.0667, Accuracy: 0.8894, F1 Micro: 0.8894, F1 Macro: 0.8698\n",
      "Epoch 9/10, Train Loss: 0.0732, Accuracy: 0.9064, F1 Micro: 0.9064, F1 Macro: 0.8981\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.9021, F1 Micro: 0.9021, F1 Macro: 0.8878\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.8996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.88      0.87        77\n",
      "    positive       0.94      0.92      0.93       158\n",
      "\n",
      "    accuracy                           0.91       235\n",
      "   macro avg       0.90      0.90      0.90       235\n",
      "weighted avg       0.91      0.91      0.91       235\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9282, F1 Micro: 0.9282, F1 Macro: 0.8606\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.62      0.74        16\n",
      "     neutral       0.92      0.99      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.76      0.82       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.86      0.91      0.88       152\n",
      "    positive       0.71      0.58      0.64        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.74      0.74      0.74       216\n",
      "weighted avg       0.81      0.82      0.81       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.78      0.78        23\n",
      "     neutral       0.94      0.98      0.96       152\n",
      "    positive       0.83      0.71      0.76        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.82      0.84       216\n",
      "weighted avg       0.90      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 98.15881180763245 s\n",
      "Averaged - Iteration 208: Accuracy: 0.8704, F1 Micro: 0.8704, F1 Macro: 0.6443\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 14.901065349578857 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5992, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Epoch 2/10, Train Loss: 0.5086, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4611, Accuracy: 0.8132, F1 Micro: 0.8941, F1 Macro: 0.8934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4069, Accuracy: 0.8289, F1 Micro: 0.902, F1 Macro: 0.9013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3698, Accuracy: 0.8929, F1 Micro: 0.9361, F1 Macro: 0.9352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.299, Accuracy: 0.9278, F1 Micro: 0.9556, F1 Macro: 0.9539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2628, Accuracy: 0.9323, F1 Micro: 0.9576, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2004, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9646\n",
      "Epoch 9/10, Train Loss: 0.1715, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1496, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.969\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.91      0.91       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6396, Accuracy: 0.6786, F1 Micro: 0.6786, F1 Macro: 0.4043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4382, Accuracy: 0.8532, F1 Micro: 0.8532, F1 Macro: 0.8162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2888, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1084, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.932\n",
      "Epoch 5/10, Train Loss: 0.0958, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "Epoch 7/10, Train Loss: 0.0966, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8861\n",
      "Epoch 8/10, Train Loss: 0.0517, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9248\n",
      "Epoch 9/10, Train Loss: 0.0332, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9219\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9256\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.92        81\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.94      0.94       252\n",
      "weighted avg       0.95      0.94      0.94       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9452, F1 Micro: 0.9452, F1 Macro: 0.8953\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.76      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.82      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.94      0.73      0.82        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 100.99076581001282 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.583, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.514, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.4706, Accuracy: 0.7902, F1 Micro: 0.8825, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4222, Accuracy: 0.8304, F1 Micro: 0.9023, F1 Macro: 0.9007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3625, Accuracy: 0.9033, F1 Micro: 0.941, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2887, Accuracy: 0.9286, F1 Micro: 0.9556, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2401, Accuracy: 0.9353, F1 Micro: 0.9596, F1 Macro: 0.9574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1861, Accuracy: 0.9368, F1 Micro: 0.9603, F1 Macro: 0.9568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1574, Accuracy: 0.9412, F1 Micro: 0.9631, F1 Macro: 0.9604\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1382, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9701\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.89      0.94      0.92       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6126, Accuracy: 0.6762, F1 Micro: 0.6762, F1 Macro: 0.4034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4568, Accuracy: 0.8402, F1 Micro: 0.8402, F1 Macro: 0.7965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2345, Accuracy: 0.9057, F1 Micro: 0.9057, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9349\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9207\n",
      "Epoch 6/10, Train Loss: 0.088, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0564, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9493\n",
      "Epoch 8/10, Train Loss: 0.0843, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Epoch 9/10, Train Loss: 0.0715, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 10/10, Train Loss: 0.0552, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.93\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        79\n",
      "    positive       0.98      0.95      0.97       165\n",
      "\n",
      "    accuracy                           0.95       244\n",
      "   macro avg       0.94      0.96      0.95       244\n",
      "weighted avg       0.96      0.95      0.96       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.9025\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.89      0.94      0.92       152\n",
      "    positive       0.82      0.69      0.75        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.82      0.79      0.81       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 102.66154599189758 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5885, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5035, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4567, Accuracy: 0.7954, F1 Micro: 0.8853, F1 Macro: 0.8838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.402, Accuracy: 0.8363, F1 Micro: 0.9055, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3533, Accuracy: 0.9055, F1 Micro: 0.9428, F1 Macro: 0.9412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.286, Accuracy: 0.9286, F1 Micro: 0.956, F1 Macro: 0.9537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2442, Accuracy: 0.9353, F1 Micro: 0.9595, F1 Macro: 0.957\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1887, Accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.156, Accuracy: 0.9442, F1 Micro: 0.9652, F1 Macro: 0.9628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1367, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9694\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.93      0.91       158\n",
      "        part       0.95      0.98      0.96       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.582, Accuracy: 0.6694, F1 Micro: 0.6694, F1 Macro: 0.401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4515, Accuracy: 0.8694, F1 Micro: 0.8694, F1 Macro: 0.8542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2238, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8913\n",
      "Epoch 4/10, Train Loss: 0.1153, Accuracy: 0.8939, F1 Micro: 0.8939, F1 Macro: 0.8858\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1089, Accuracy: 0.9061, F1 Micro: 0.9061, F1 Macro: 0.8891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0552, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9326\n",
      "Epoch 7/10, Train Loss: 0.1077, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9236\n",
      "Epoch 8/10, Train Loss: 0.0967, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9232\n",
      "Epoch 9/10, Train Loss: 0.0791, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0392, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9319\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        81\n",
      "    positive       0.97      0.94      0.95       164\n",
      "\n",
      "    accuracy                           0.94       245\n",
      "   macro avg       0.93      0.94      0.93       245\n",
      "weighted avg       0.94      0.94      0.94       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9437, F1 Micro: 0.9437, F1 Macro: 0.8917\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.90      0.93      0.91       152\n",
      "    positive       0.79      0.71      0.75        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.81      0.80      0.80       216\n",
      "weighted avg       0.86      0.87      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.94      0.98      0.96       152\n",
      "    positive       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 108.9228150844574 s\n",
      "Averaged - Iteration 274: Accuracy: 0.8891, F1 Micro: 0.8891, F1 Macro: 0.7074\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 13.97156834602356 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.583, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5066, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4656, Accuracy: 0.808, F1 Micro: 0.8916, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3902, Accuracy: 0.8876, F1 Micro: 0.9333, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.328, Accuracy: 0.9368, F1 Micro: 0.961, F1 Macro: 0.9596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2574, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1993, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1484, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1245, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1112, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.558, Accuracy: 0.6789, F1 Micro: 0.6789, F1 Macro: 0.4044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3798, Accuracy: 0.8862, F1 Micro: 0.8862, F1 Macro: 0.8712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1662, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1331, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.94\n",
      "Epoch 5/10, Train Loss: 0.1191, Accuracy: 0.9187, F1 Micro: 0.9187, F1 Macro: 0.9106\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9392\n",
      "Epoch 7/10, Train Loss: 0.0605, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8768\n",
      "Epoch 8/10, Train Loss: 0.0865, Accuracy: 0.9146, F1 Micro: 0.9146, F1 Macro: 0.9059\n",
      "Epoch 9/10, Train Loss: 0.06, Accuracy: 0.8902, F1 Micro: 0.8902, F1 Macro: 0.882\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9244\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.91      0.92        79\n",
      "    positive       0.96      0.96      0.96       167\n",
      "\n",
      "    accuracy                           0.95       246\n",
      "   macro avg       0.94      0.94      0.94       246\n",
      "weighted avg       0.95      0.95      0.95       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9012\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.58      0.74        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.93      0.78      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.91      0.76      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 113.52072095870972 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5616, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5073, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4696, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3917, Accuracy: 0.8884, F1 Micro: 0.9332, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3211, Accuracy: 0.9323, F1 Micro: 0.9584, F1 Macro: 0.9567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2468, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1921, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1433, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9705\n",
      "Epoch 9/10, Train Loss: 0.1179, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1073, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5896, Accuracy: 0.6892, F1 Micro: 0.6892, F1 Macro: 0.408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4689, Accuracy: 0.8805, F1 Micro: 0.8805, F1 Macro: 0.8574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2309, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.164, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9218\n",
      "Epoch 5/10, Train Loss: 0.1112, Accuracy: 0.8964, F1 Micro: 0.8964, F1 Macro: 0.8701\n",
      "Epoch 6/10, Train Loss: 0.0728, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9069\n",
      "Epoch 7/10, Train Loss: 0.0582, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9151\n",
      "Epoch 8/10, Train Loss: 0.0804, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.898\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8968\n",
      "Epoch 10/10, Train Loss: 0.068, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.8944\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        78\n",
      "    positive       0.96      0.94      0.95       173\n",
      "\n",
      "    accuracy                           0.93       251\n",
      "   macro avg       0.92      0.93      0.92       251\n",
      "weighted avg       0.93      0.93      0.93       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8914\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.67      0.67        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.79      0.80       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 108.34325289726257 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5611, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4988, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4554, Accuracy: 0.8021, F1 Micro: 0.8886, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3852, Accuracy: 0.8876, F1 Micro: 0.9324, F1 Macro: 0.9305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3178, Accuracy: 0.9308, F1 Micro: 0.9576, F1 Macro: 0.9561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2504, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1924, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1464, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9744\n",
      "Epoch 9/10, Train Loss: 0.1241, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9711\n",
      "Epoch 10/10, Train Loss: 0.1095, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9729\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5751, Accuracy: 0.6707, F1 Micro: 0.6707, F1 Macro: 0.4015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4408, Accuracy: 0.874, F1 Micro: 0.874, F1 Macro: 0.8618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.246, Accuracy: 0.9106, F1 Micro: 0.9106, F1 Macro: 0.9032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1359, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9364\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.9268, F1 Micro: 0.9268, F1 Macro: 0.9143\n",
      "Epoch 6/10, Train Loss: 0.1645, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9162\n",
      "Epoch 7/10, Train Loss: 0.103, Accuracy: 0.9187, F1 Micro: 0.9187, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0888, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9458\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9264\n",
      "Epoch 10/10, Train Loss: 0.0662, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9312\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9458\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        81\n",
      "    positive       0.98      0.95      0.96       165\n",
      "\n",
      "    accuracy                           0.95       246\n",
      "   macro avg       0.94      0.95      0.95       246\n",
      "weighted avg       0.95      0.95      0.95       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9127\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 106.73005032539368 s\n",
      "Averaged - Iteration 333: Accuracy: 0.9016, F1 Micro: 0.9016, F1 Macro: 0.7462\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 12.21908974647522 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5584, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4881, Accuracy: 0.8013, F1 Micro: 0.8882, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4163, Accuracy: 0.8668, F1 Micro: 0.9215, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3339, Accuracy: 0.9323, F1 Micro: 0.9585, F1 Macro: 0.9573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2624, Accuracy: 0.9435, F1 Micro: 0.965, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2109, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9707\n",
      "Epoch 7/10, Train Loss: 0.1585, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1255, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.108, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Epoch 10/10, Train Loss: 0.089, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9709\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.96      0.93       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6444, Accuracy: 0.6774, F1 Micro: 0.6774, F1 Macro: 0.4038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.41, Accuracy: 0.879, F1 Micro: 0.879, F1 Macro: 0.8679\n",
      "Epoch 3/10, Train Loss: 0.2305, Accuracy: 0.8427, F1 Micro: 0.8427, F1 Macro: 0.8358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1905, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.911\n",
      "Epoch 5/10, Train Loss: 0.1453, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9276\n",
      "Epoch 7/10, Train Loss: 0.0478, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9241\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.919\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9142\n",
      "Epoch 10/10, Train Loss: 0.036, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9142\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.90        80\n",
      "    positive       0.97      0.93      0.95       168\n",
      "\n",
      "    accuracy                           0.94       248\n",
      "   macro avg       0.92      0.94      0.93       248\n",
      "weighted avg       0.94      0.94      0.94       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9036\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.86      0.73      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.81      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.83      0.79        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 109.26648473739624 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5568, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4979, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4314, Accuracy: 0.8371, F1 Micro: 0.906, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.341, Accuracy: 0.9286, F1 Micro: 0.956, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2636, Accuracy: 0.9464, F1 Micro: 0.9667, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.206, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Epoch 7/10, Train Loss: 0.1524, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.969\n",
      "Epoch 8/10, Train Loss: 0.1182, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1013, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Epoch 10/10, Train Loss: 0.0862, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6201, Accuracy: 0.6708, F1 Micro: 0.6708, F1 Macro: 0.4015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4048, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.8994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2354, Accuracy: 0.9136, F1 Micro: 0.9136, F1 Macro: 0.9053\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1437, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.908\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.8999\n",
      "Epoch 6/10, Train Loss: 0.0612, Accuracy: 0.9053, F1 Micro: 0.9053, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0455, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9225\n",
      "Epoch 9/10, Train Loss: 0.0757, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.908\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.9114\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        80\n",
      "    positive       0.97      0.93      0.95       163\n",
      "\n",
      "    accuracy                           0.93       243\n",
      "   macro avg       0.92      0.93      0.92       243\n",
      "weighted avg       0.93      0.93      0.93       243\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8985\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.82      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.93      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.81      0.82       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.89      0.78      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.86      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 111.80507612228394 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5571, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4888, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4176, Accuracy: 0.869, F1 Micro: 0.9227, F1 Macro: 0.9211\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3374, Accuracy: 0.9278, F1 Micro: 0.9558, F1 Macro: 0.9544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2651, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2104, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9704\n",
      "Epoch 7/10, Train Loss: 0.1564, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1258, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1051, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "Epoch 10/10, Train Loss: 0.0896, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9706\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.89      0.97      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5773, Accuracy: 0.6762, F1 Micro: 0.6762, F1 Macro: 0.4034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3839, Accuracy: 0.8975, F1 Micro: 0.8975, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2301, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9226\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1125, Accuracy: 0.9303, F1 Micro: 0.9303, F1 Macro: 0.9226\n",
      "Epoch 5/10, Train Loss: 0.1132, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1051, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9361\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.9149\n",
      "Epoch 8/10, Train Loss: 0.0715, Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.9089\n",
      "Epoch 9/10, Train Loss: 0.0395, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.93\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9102\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        79\n",
      "    positive       0.98      0.93      0.96       165\n",
      "\n",
      "    accuracy                           0.94       244\n",
      "   macro avg       0.93      0.95      0.94       244\n",
      "weighted avg       0.95      0.94      0.94       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8994\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.80      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.88      0.69      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.78      0.80       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 112.01847457885742 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9097, F1 Micro: 0.9097, F1 Macro: 0.7719\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 11.567233085632324 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5661, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4172, Accuracy: 0.8839, F1 Micro: 0.9308, F1 Macro: 0.9303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3155, Accuracy: 0.933, F1 Micro: 0.9586, F1 Macro: 0.9571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2513, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1891, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1486, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9728\n",
      "Epoch 8/10, Train Loss: 0.122, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Epoch 9/10, Train Loss: 0.1029, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0817, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.91      0.92       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5709, Accuracy: 0.8647, F1 Micro: 0.8647, F1 Macro: 0.8355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3228, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.892\n",
      "Epoch 3/10, Train Loss: 0.1703, Accuracy: 0.8647, F1 Micro: 0.8647, F1 Macro: 0.854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1663, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9263\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.8992\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9089\n",
      "Epoch 8/10, Train Loss: 0.0889, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9009\n",
      "Epoch 9/10, Train Loss: 0.0463, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9156\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9222\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        81\n",
      "    positive       0.97      0.94      0.95       185\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.92      0.94      0.93       266\n",
      "weighted avg       0.94      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9057\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.94      0.91      0.92       152\n",
      "    positive       0.79      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.83      0.82       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.87      0.82        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.89      0.76      0.82        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.87      0.87       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 118.98066544532776 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5609, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4863, Accuracy: 0.7932, F1 Micro: 0.884, F1 Macro: 0.8824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4285, Accuracy: 0.8698, F1 Micro: 0.9228, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3181, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2384, Accuracy: 0.9368, F1 Micro: 0.9606, F1 Macro: 0.9593\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1849, Accuracy: 0.9464, F1 Micro: 0.9664, F1 Macro: 0.9641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.143, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9701\n",
      "Epoch 8/10, Train Loss: 0.1111, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9698\n",
      "Epoch 9/10, Train Loss: 0.0978, Accuracy: 0.9487, F1 Micro: 0.9676, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0807, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5814, Accuracy: 0.8462, F1 Micro: 0.8462, F1 Macro: 0.8095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2922, Accuracy: 0.8731, F1 Micro: 0.8731, F1 Macro: 0.864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2159, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1251, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9218\n",
      "Epoch 5/10, Train Loss: 0.0875, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9177\n",
      "Epoch 6/10, Train Loss: 0.0818, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9121\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Epoch 8/10, Train Loss: 0.0562, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9033\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9182\n",
      "Epoch 10/10, Train Loss: 0.0488, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.911\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.99      0.90        81\n",
      "    positive       0.99      0.91      0.95       179\n",
      "\n",
      "    accuracy                           0.93       260\n",
      "   macro avg       0.91      0.95      0.92       260\n",
      "weighted avg       0.94      0.93      0.93       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8932\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.94      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.75      0.64        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.77      0.82      0.79       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.87      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 122.13246202468872 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5546, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4796, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4172, Accuracy: 0.8981, F1 Micro: 0.9384, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3141, Accuracy: 0.9278, F1 Micro: 0.9558, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2441, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1897, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1487, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "Epoch 8/10, Train Loss: 0.1192, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "Epoch 9/10, Train Loss: 0.0995, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9727\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.96      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.563, Accuracy: 0.7808, F1 Micro: 0.7808, F1 Macro: 0.6998\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3814, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8935\n",
      "Epoch 3/10, Train Loss: 0.1832, Accuracy: 0.8731, F1 Micro: 0.8731, F1 Macro: 0.8646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1804, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8868\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1318, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0926, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9227\n",
      "Epoch 7/10, Train Loss: 0.1031, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0822, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9313\n",
      "Epoch 10/10, Train Loss: 0.06, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9121\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9313\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.98      0.91        82\n",
      "    positive       0.99      0.92      0.95       178\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.92      0.95      0.93       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.904\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.93      0.92      0.93       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.78      0.82      0.80       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.96      0.97      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 123.98728609085083 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.7904\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 10.474074125289917 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5562, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4828, Accuracy: 0.808, F1 Micro: 0.8915, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3979, Accuracy: 0.9107, F1 Micro: 0.946, F1 Macro: 0.9451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.295, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2336, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9701\n",
      "Epoch 6/10, Train Loss: 0.1736, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.144, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Epoch 8/10, Train Loss: 0.1114, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "Epoch 9/10, Train Loss: 0.0902, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9716\n",
      "Epoch 10/10, Train Loss: 0.0767, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9703\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.91      0.92       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6002, Accuracy: 0.6846, F1 Micro: 0.6846, F1 Macro: 0.4064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.338, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.898\n",
      "Epoch 3/10, Train Loss: 0.2187, Accuracy: 0.8962, F1 Micro: 0.8962, F1 Macro: 0.8794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1519, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.078, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0832, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9347\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Epoch 9/10, Train Loss: 0.0725, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9195\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9227\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9347\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.91        82\n",
      "    positive       0.98      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.94      0.93       260\n",
      "weighted avg       0.95      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9088\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.83      0.83       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 121.13966608047485 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5538, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4862, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.404, Accuracy: 0.8958, F1 Micro: 0.9371, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2964, Accuracy: 0.9397, F1 Micro: 0.9624, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2245, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9674\n",
      "Epoch 6/10, Train Loss: 0.1645, Accuracy: 0.9479, F1 Micro: 0.9673, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1326, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1062, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "Epoch 9/10, Train Loss: 0.0836, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9717\n",
      "Epoch 10/10, Train Loss: 0.073, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.97\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.92      0.92       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5329, Accuracy: 0.7821, F1 Micro: 0.7821, F1 Macro: 0.6883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3187, Accuracy: 0.8949, F1 Micro: 0.8949, F1 Macro: 0.8692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2352, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1475, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1095, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1122, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0545, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9279\n",
      "Epoch 9/10, Train Loss: 0.0645, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9246\n",
      "Epoch 10/10, Train Loss: 0.0571, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9167\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9279\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.91      0.90        80\n",
      "    positive       0.96      0.95      0.95       177\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.93      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.8993\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.85      0.88        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.93      0.92       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.82      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.85      0.80      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 126.74548578262329 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.549, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4752, Accuracy: 0.8006, F1 Micro: 0.8879, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3923, Accuracy: 0.9033, F1 Micro: 0.9417, F1 Macro: 0.9405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2881, Accuracy: 0.9353, F1 Micro: 0.96, F1 Macro: 0.9581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2291, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1688, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1371, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "Epoch 8/10, Train Loss: 0.1083, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "Epoch 9/10, Train Loss: 0.0863, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9726\n",
      "Epoch 10/10, Train Loss: 0.0782, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9706\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5757, Accuracy: 0.7671, F1 Micro: 0.7671, F1 Macro: 0.6852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3247, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9289\n",
      "Epoch 3/10, Train Loss: 0.1917, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1547, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9467\n",
      "Epoch 5/10, Train Loss: 0.1011, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9421\n",
      "Epoch 6/10, Train Loss: 0.0885, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9421\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9339\n",
      "Epoch 8/10, Train Loss: 0.0653, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9263\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9268\n",
      "Epoch 10/10, Train Loss: 0.0421, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9225\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        82\n",
      "    positive       0.99      0.94      0.96       167\n",
      "\n",
      "    accuracy                           0.95       249\n",
      "   macro avg       0.94      0.96      0.95       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9103\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 119.01229071617126 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9199, F1 Micro: 0.9199, F1 Macro: 0.8048\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 9.331108808517456 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5487, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.467, Accuracy: 0.8199, F1 Micro: 0.897, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3726, Accuracy: 0.9152, F1 Micro: 0.9485, F1 Macro: 0.947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2669, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1928, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "Epoch 6/10, Train Loss: 0.1444, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1178, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Epoch 8/10, Train Loss: 0.0954, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0685, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6084, Accuracy: 0.8605, F1 Micro: 0.8605, F1 Macro: 0.8251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2855, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9265\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.8996\n",
      "Epoch 4/10, Train Loss: 0.1362, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9115\n",
      "Epoch 5/10, Train Loss: 0.114, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9151\n",
      "Epoch 6/10, Train Loss: 0.123, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9023\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9066\n",
      "Epoch 8/10, Train Loss: 0.0793, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0743, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9106\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.91        83\n",
      "    positive       0.98      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.93      0.95      0.94       258\n",
      "weighted avg       0.95      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9159\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.83      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.188805103302 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5379, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.808, F1 Micro: 0.8915, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3792, Accuracy: 0.9159, F1 Micro: 0.9485, F1 Macro: 0.9467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2638, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1866, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1401, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1101, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9728\n",
      "Epoch 8/10, Train Loss: 0.0913, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0634, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5746, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2865, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2069, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9138\n",
      "Epoch 4/10, Train Loss: 0.1487, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.926\n",
      "Epoch 6/10, Train Loss: 0.0904, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9108\n",
      "Epoch 7/10, Train Loss: 0.0695, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9068\n",
      "Epoch 8/10, Train Loss: 0.0596, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9209\n",
      "Epoch 9/10, Train Loss: 0.052, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9148\n",
      "Epoch 10/10, Train Loss: 0.0617, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9145\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        80\n",
      "    positive       0.98      0.93      0.95       184\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.92      0.94      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9007\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.83      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.90      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 134.0220377445221 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5348, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4636, Accuracy: 0.8192, F1 Micro: 0.8971, F1 Macro: 0.896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.365, Accuracy: 0.9234, F1 Micro: 0.953, F1 Macro: 0.9515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2677, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1902, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1458, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1206, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 8/10, Train Loss: 0.0948, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9748\n",
      "Epoch 9/10, Train Loss: 0.078, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9749\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6024, Accuracy: 0.689, F1 Micro: 0.689, F1 Macro: 0.4522\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3269, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1731, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9339\n",
      "Epoch 4/10, Train Loss: 0.1357, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9297\n",
      "Epoch 5/10, Train Loss: 0.1245, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9251\n",
      "Epoch 6/10, Train Loss: 0.1067, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9216\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9063\n",
      "Epoch 8/10, Train Loss: 0.0558, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9172\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9255\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9222\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        83\n",
      "    positive       0.97      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.93      0.94      0.93       254\n",
      "weighted avg       0.94      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9141\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 125.6157603263855 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.8165\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 8.541018009185791 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5464, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.8185, F1 Micro: 0.8967, F1 Macro: 0.8959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3672, Accuracy: 0.9211, F1 Micro: 0.9512, F1 Macro: 0.949\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2748, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1963, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Epoch 6/10, Train Loss: 0.158, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1256, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0987, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9733\n",
      "Epoch 9/10, Train Loss: 0.0802, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.94      0.90      0.92       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5513, Accuracy: 0.7799, F1 Micro: 0.7799, F1 Macro: 0.6648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.288, Accuracy: 0.8657, F1 Micro: 0.8657, F1 Macro: 0.857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2049, Accuracy: 0.8881, F1 Micro: 0.8881, F1 Macro: 0.8794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1334, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.117, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9408\n",
      "Epoch 6/10, Train Loss: 0.0992, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9179\n",
      "Epoch 7/10, Train Loss: 0.0936, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9225\n",
      "Epoch 8/10, Train Loss: 0.0665, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9265\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9251\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9283\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        84\n",
      "    positive       0.98      0.94      0.96       184\n",
      "\n",
      "    accuracy                           0.95       268\n",
      "   macro avg       0.93      0.95      0.94       268\n",
      "weighted avg       0.95      0.95      0.95       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9182\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.93      0.82      0.87        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.89      0.92       152\n",
      "    positive       0.76      0.85      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.86      0.83       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 136.692608833313 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5453, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4857, Accuracy: 0.8073, F1 Micro: 0.8909, F1 Macro: 0.8896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3728, Accuracy: 0.9278, F1 Micro: 0.9554, F1 Macro: 0.9537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2721, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1915, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1494, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1189, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0929, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9729\n",
      "Epoch 10/10, Train Loss: 0.0643, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5629, Accuracy: 0.7719, F1 Micro: 0.7719, F1 Macro: 0.664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3031, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 3/10, Train Loss: 0.2185, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.8968\n",
      "Epoch 4/10, Train Loss: 0.134, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.137, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0886, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0663, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.935\n",
      "Epoch 8/10, Train Loss: 0.0643, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9309\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        82\n",
      "    positive       0.98      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9135\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 136.57480025291443 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5413, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4716, Accuracy: 0.817, F1 Micro: 0.8959, F1 Macro: 0.8948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.363, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2694, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1918, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9716\n",
      "Epoch 6/10, Train Loss: 0.1539, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1227, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "Epoch 8/10, Train Loss: 0.0983, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "Epoch 9/10, Train Loss: 0.0805, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9726\n",
      "Epoch 10/10, Train Loss: 0.072, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9702\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      1.00      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5506, Accuracy: 0.8394, F1 Micro: 0.8394, F1 Macro: 0.8193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3101, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9187\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1959, Accuracy: 0.9277, F1 Micro: 0.9277, F1 Macro: 0.9196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1392, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9407\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9324\n",
      "Epoch 7/10, Train Loss: 0.0706, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9163\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9289\n",
      "Epoch 9/10, Train Loss: 0.0552, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9293\n",
      "Epoch 10/10, Train Loss: 0.0694, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9336\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        81\n",
      "    positive       0.96      0.96      0.96       168\n",
      "\n",
      "    accuracy                           0.95       249\n",
      "   macro avg       0.94      0.94      0.94       249\n",
      "weighted avg       0.95      0.95      0.95       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9112\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.93      1.00      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.79      0.85       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.93       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.88      0.85      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 128.55720353126526 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9268, F1 Micro: 0.9268, F1 Macro: 0.8263\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 8.125666379928589 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5414, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4663, Accuracy: 0.84, F1 Micro: 0.9078, F1 Macro: 0.9072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3445, Accuracy: 0.9301, F1 Micro: 0.9568, F1 Macro: 0.955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2526, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1882, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1467, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1138, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0758, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5829, Accuracy: 0.8315, F1 Micro: 0.8315, F1 Macro: 0.77\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.281, Accuracy: 0.8727, F1 Micro: 0.8727, F1 Macro: 0.8641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1822, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Epoch 4/10, Train Loss: 0.1813, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9322\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9242\n",
      "Epoch 6/10, Train Loss: 0.1328, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0935, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1102, Accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9476\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9451\n",
      "Epoch 10/10, Train Loss: 0.0713, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9373\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.92      0.93        84\n",
      "    positive       0.96      0.97      0.97       183\n",
      "\n",
      "    accuracy                           0.96       267\n",
      "   macro avg       0.95      0.94      0.95       267\n",
      "weighted avg       0.95      0.96      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.925\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.79      0.87      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.92      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 139.89748001098633 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5432, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4731, Accuracy: 0.814, F1 Micro: 0.8941, F1 Macro: 0.8931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3519, Accuracy: 0.9234, F1 Micro: 0.9524, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2519, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.9673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1808, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1356, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1093, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Epoch 9/10, Train Loss: 0.0762, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5467, Accuracy: 0.8517, F1 Micro: 0.8517, F1 Macro: 0.8203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3094, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2252, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9395\n",
      "Epoch 4/10, Train Loss: 0.1629, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "Epoch 5/10, Train Loss: 0.1481, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Epoch 6/10, Train Loss: 0.1157, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 7/10, Train Loss: 0.0961, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0868, Accuracy: 0.962, F1 Micro: 0.962, F1 Macro: 0.956\n",
      "Epoch 9/10, Train Loss: 0.1015, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9395\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.962, F1 Micro: 0.962, F1 Macro: 0.956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        82\n",
      "    positive       0.98      0.97      0.97       181\n",
      "\n",
      "    accuracy                           0.96       263\n",
      "   macro avg       0.95      0.96      0.96       263\n",
      "weighted avg       0.96      0.96      0.96       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9221\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.84      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 140.4442961215973 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5357, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4597, Accuracy: 0.8207, F1 Micro: 0.8976, F1 Macro: 0.896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3419, Accuracy: 0.9278, F1 Micro: 0.9554, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2553, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1883, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1473, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1102, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0931, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5528, Accuracy: 0.8238, F1 Micro: 0.8238, F1 Macro: 0.7794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2753, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2255, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9273\n",
      "Epoch 4/10, Train Loss: 0.1654, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.8994\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 6/10, Train Loss: 0.1076, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 7/10, Train Loss: 0.0911, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0789, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9355\n",
      "Epoch 9/10, Train Loss: 0.0671, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9314\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9141\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        85\n",
      "    positive       0.97      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.94      0.94       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9151\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 140.99226474761963 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.8349\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 7.5045387744903564 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4586, Accuracy: 0.8519, F1 Micro: 0.9134, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3378, Accuracy: 0.9315, F1 Micro: 0.9573, F1 Macro: 0.9552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2355, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1753, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1407, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1069, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0729, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.96      0.92      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5781, Accuracy: 0.717, F1 Micro: 0.717, F1 Macro: 0.5191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2893, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1782, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9371\n",
      "Epoch 4/10, Train Loss: 0.1327, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9327\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9137\n",
      "Epoch 6/10, Train Loss: 0.154, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9207\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9327\n",
      "Epoch 8/10, Train Loss: 0.0658, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0867, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0643, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9364\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        85\n",
      "    positive       0.98      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.93      0.95      0.94       265\n",
      "weighted avg       0.95      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9186\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.97      0.92      0.94       152\n",
      "    positive       0.78      0.88      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.88      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.91      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.4632797241211 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5503, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4767, Accuracy: 0.8222, F1 Micro: 0.8987, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3574, Accuracy: 0.9226, F1 Micro: 0.9522, F1 Macro: 0.9508\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.245, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1788, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1383, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1057, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 8/10, Train Loss: 0.0882, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9722\n",
      "Epoch 9/10, Train Loss: 0.0741, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Epoch 10/10, Train Loss: 0.0653, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5431, Accuracy: 0.8864, F1 Micro: 0.8864, F1 Macro: 0.8603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.293, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1591, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1186, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1139, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1113, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9339\n",
      "Epoch 7/10, Train Loss: 0.1093, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9241\n",
      "Epoch 8/10, Train Loss: 0.0752, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9241\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0543, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.94\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        84\n",
      "    positive       0.98      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.93      0.95      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9213\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.83      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.87      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.24633121490479 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5454, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4551, Accuracy: 0.8519, F1 Micro: 0.9137, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3382, Accuracy: 0.9308, F1 Micro: 0.9573, F1 Macro: 0.9556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2373, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1815, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1436, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1078, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 8/10, Train Loss: 0.0889, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0742, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9721\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5212, Accuracy: 0.8594, F1 Micro: 0.8594, F1 Macro: 0.8275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2697, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9166\n",
      "Epoch 3/10, Train Loss: 0.1692, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1445, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9307\n",
      "Epoch 5/10, Train Loss: 0.1647, Accuracy: 0.8828, F1 Micro: 0.8828, F1 Macro: 0.8755\n",
      "Epoch 6/10, Train Loss: 0.1524, Accuracy: 0.8867, F1 Micro: 0.8867, F1 Macro: 0.8794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0852, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0879, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0741, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.938\n",
      "Epoch 10/10, Train Loss: 0.05, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9188\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.938\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        83\n",
      "    positive       0.96      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.94      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9173\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.90      0.88      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.1521120071411 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.8419\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 6.852073907852173 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5404, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4495, Accuracy: 0.8527, F1 Micro: 0.9144, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3207, Accuracy: 0.936, F1 Micro: 0.9602, F1 Macro: 0.9582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2265, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9721\n",
      "Epoch 5/10, Train Loss: 0.1605, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1277, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9751\n",
      "Epoch 7/10, Train Loss: 0.1061, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9743\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0683, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9809\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.95      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.526, Accuracy: 0.8969, F1 Micro: 0.8969, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2242, Accuracy: 0.9046, F1 Micro: 0.9046, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1795, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.937\n",
      "Epoch 4/10, Train Loss: 0.181, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 5/10, Train Loss: 0.1405, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1255, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9487\n",
      "Epoch 7/10, Train Loss: 0.0942, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9357\n",
      "Epoch 8/10, Train Loss: 0.0985, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9379\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.928\n",
      "Epoch 10/10, Train Loss: 0.0594, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9323\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        85\n",
      "    positive       0.98      0.95      0.97       177\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.96      0.95       262\n",
      "weighted avg       0.96      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9282\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.85      0.87      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 146.37313294410706 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5446, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4584, Accuracy: 0.8423, F1 Micro: 0.9091, F1 Macro: 0.9078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3327, Accuracy: 0.936, F1 Micro: 0.9601, F1 Macro: 0.9586\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2302, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.9674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1625, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Epoch 6/10, Train Loss: 0.1297, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Epoch 7/10, Train Loss: 0.1025, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0813, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.0667, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9739\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9745\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5697, Accuracy: 0.8745, F1 Micro: 0.8745, F1 Macro: 0.8455\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.282, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1644, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "Epoch 4/10, Train Loss: 0.1547, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1331, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "Epoch 6/10, Train Loss: 0.0908, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9204\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1127, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9399\n",
      "Epoch 8/10, Train Loss: 0.0713, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.92\n",
      "Epoch 10/10, Train Loss: 0.0562, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.916\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9399\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        84\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9159\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.83988571166992 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5373, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4422, Accuracy: 0.8594, F1 Micro: 0.9179, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3232, Accuracy: 0.9368, F1 Micro: 0.9607, F1 Macro: 0.9591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2339, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1653, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1301, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1081, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0843, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Epoch 9/10, Train Loss: 0.0666, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5408, Accuracy: 0.8726, F1 Micro: 0.8726, F1 Macro: 0.847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2635, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1859, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "Epoch 4/10, Train Loss: 0.1342, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "Epoch 5/10, Train Loss: 0.1051, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Epoch 6/10, Train Loss: 0.1318, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.952\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9274\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        83\n",
      "    positive       0.98      0.95      0.97       176\n",
      "\n",
      "    accuracy                           0.96       259\n",
      "   macro avg       0.95      0.96      0.95       259\n",
      "weighted avg       0.96      0.96      0.96       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9213\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.88      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.84      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 150.71491527557373 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9342, F1 Micro: 0.9342, F1 Macro: 0.8481\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 6.214140892028809 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4427, Accuracy: 0.8705, F1 Micro: 0.924, F1 Macro: 0.9233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3122, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.9628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2182, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1662, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1189, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9737\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0641, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "Epoch 10/10, Train Loss: 0.0544, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9729\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.91      0.92       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5602, Accuracy: 0.8856, F1 Micro: 0.8856, F1 Macro: 0.861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2724, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1913, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9299\n",
      "Epoch 4/10, Train Loss: 0.1353, Accuracy: 0.9114, F1 Micro: 0.9114, F1 Macro: 0.9028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1078, Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.9407\n",
      "Epoch 6/10, Train Loss: 0.0971, Accuracy: 0.9041, F1 Micro: 0.9041, F1 Macro: 0.8951\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9327\n",
      "Epoch 8/10, Train Loss: 0.0794, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9255\n",
      "Epoch 9/10, Train Loss: 0.0521, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9331\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9299, F1 Micro: 0.9299, F1 Macro: 0.9216\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.9407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        85\n",
      "    positive       0.97      0.95      0.96       186\n",
      "\n",
      "    accuracy                           0.95       271\n",
      "   macro avg       0.94      0.95      0.94       271\n",
      "weighted avg       0.95      0.95      0.95       271\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9206\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.91      0.92       152\n",
      "    positive       0.74      0.81      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 154.07537841796875 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5443, Accuracy: 0.7939, F1 Micro: 0.8844, F1 Macro: 0.8829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4443, Accuracy: 0.8467, F1 Micro: 0.911, F1 Macro: 0.9093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3152, Accuracy: 0.9382, F1 Micro: 0.9616, F1 Macro: 0.96\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2173, Accuracy: 0.9494, F1 Micro: 0.9682, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1665, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1158, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Epoch 7/10, Train Loss: 0.1005, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9722\n",
      "Epoch 8/10, Train Loss: 0.0771, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0656, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0533, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5599, Accuracy: 0.8848, F1 Micro: 0.8848, F1 Macro: 0.8636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2514, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.201, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9372\n",
      "Epoch 4/10, Train Loss: 0.1315, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9182\n",
      "Epoch 5/10, Train Loss: 0.1468, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9312\n",
      "Epoch 6/10, Train Loss: 0.1202, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9096\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9332\n",
      "Epoch 8/10, Train Loss: 0.0818, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9253\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9372\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       184\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.95      0.94       269\n",
      "weighted avg       0.95      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9173\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.78      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.83      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 149.12850880622864 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5429, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4342, Accuracy: 0.875, F1 Micro: 0.9261, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3149, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2205, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1197, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 7/10, Train Loss: 0.1037, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0781, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9744\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5426, Accuracy: 0.877, F1 Micro: 0.877, F1 Macro: 0.862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2623, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9122\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1875, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1768, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1389, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9414\n",
      "Epoch 6/10, Train Loss: 0.1027, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1176, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.951\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.926\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9421\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9127\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.97       170\n",
      "\n",
      "    accuracy                           0.96       252\n",
      "   macro avg       0.94      0.96      0.95       252\n",
      "weighted avg       0.96      0.96      0.96       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9226\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.88      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 154.4587938785553 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.8532\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.854115962982178 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4451, Accuracy: 0.8854, F1 Micro: 0.9319, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3113, Accuracy: 0.9345, F1 Micro: 0.9593, F1 Macro: 0.9573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2189, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1607, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1139, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0745, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 9/10, Train Loss: 0.0687, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9598, F1 Micro: 0.9745, F1 Macro: 0.9722\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5149, Accuracy: 0.8674, F1 Micro: 0.8674, F1 Macro: 0.8336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2422, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9269\n",
      "Epoch 3/10, Train Loss: 0.206, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9245\n",
      "Epoch 4/10, Train Loss: 0.1725, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9229\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.163, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9326\n",
      "Epoch 6/10, Train Loss: 0.1136, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Epoch 7/10, Train Loss: 0.0882, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.8999\n",
      "Epoch 8/10, Train Loss: 0.1161, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0801, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "Epoch 10/10, Train Loss: 0.0924, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       179\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.93      0.95      0.94       264\n",
      "weighted avg       0.95      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9222\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.88      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 153.99517846107483 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5427, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4495, Accuracy: 0.8787, F1 Micro: 0.9275, F1 Macro: 0.9255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3119, Accuracy: 0.9345, F1 Micro: 0.9592, F1 Macro: 0.9574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2131, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1552, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9759\n",
      "Epoch 6/10, Train Loss: 0.1137, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Epoch 7/10, Train Loss: 0.0869, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0529, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.91      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5273, Accuracy: 0.8839, F1 Micro: 0.8839, F1 Macro: 0.865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2643, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1915, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9373\n",
      "Epoch 4/10, Train Loss: 0.151, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9178\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.8952\n",
      "Epoch 6/10, Train Loss: 0.1494, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9254\n",
      "Epoch 7/10, Train Loss: 0.0813, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.925\n",
      "Epoch 8/10, Train Loss: 0.0989, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9293\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9269\n",
      "Epoch 10/10, Train Loss: 0.0671, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9136\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9373\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.97      0.92        86\n",
      "    positive       0.98      0.93      0.96       181\n",
      "\n",
      "    accuracy                           0.94       267\n",
      "   macro avg       0.93      0.95      0.94       267\n",
      "weighted avg       0.95      0.94      0.94       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9201\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.77      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.86      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.7949936389923 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5362, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4388, Accuracy: 0.9062, F1 Micro: 0.9436, F1 Macro: 0.9422\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3092, Accuracy: 0.9435, F1 Micro: 0.9651, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2167, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1595, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1152, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9751\n",
      "Epoch 8/10, Train Loss: 0.0774, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9754\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0533, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5388, Accuracy: 0.868, F1 Micro: 0.868, F1 Macro: 0.8341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2495, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9459\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Epoch 4/10, Train Loss: 0.1384, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9169\n",
      "Epoch 5/10, Train Loss: 0.0851, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0812, Accuracy: 0.96, F1 Micro: 0.96, F1 Macro: 0.9552\n",
      "Epoch 7/10, Train Loss: 0.1018, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.938\n",
      "Epoch 8/10, Train Loss: 0.0893, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.96, F1 Micro: 0.96, F1 Macro: 0.9552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94        81\n",
      "    positive       0.99      0.95      0.97       169\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.95      0.96      0.96       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9218\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 147.4828507900238 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.8578\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.416698694229126 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5392, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4489, Accuracy: 0.872, F1 Micro: 0.9243, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3198, Accuracy: 0.942, F1 Micro: 0.964, F1 Macro: 0.9627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2115, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9755\n",
      "Epoch 6/10, Train Loss: 0.1181, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.095, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.082, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.0566, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9751\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5566, Accuracy: 0.8669, F1 Micro: 0.8669, F1 Macro: 0.8319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2586, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1799, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9204\n",
      "Epoch 6/10, Train Loss: 0.1392, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9268\n",
      "Epoch 7/10, Train Loss: 0.1177, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 8/10, Train Loss: 0.0951, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9195\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8922\n",
      "Epoch 10/10, Train Loss: 0.0744, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9223\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9204\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 156.6621813774109 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5384, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4576, Accuracy: 0.8616, F1 Micro: 0.9186, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3252, Accuracy: 0.9427, F1 Micro: 0.9643, F1 Macro: 0.9629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.208, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1534, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Epoch 6/10, Train Loss: 0.1172, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9752\n",
      "Epoch 7/10, Train Loss: 0.0937, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0808, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0621, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 10/10, Train Loss: 0.0544, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.535, Accuracy: 0.8876, F1 Micro: 0.8876, F1 Macro: 0.8729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2798, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.185, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9152\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1479, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9214\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9179\n",
      "Epoch 7/10, Train Loss: 0.1147, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9218\n",
      "Epoch 9/10, Train Loss: 0.0701, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9175\n",
      "Epoch 10/10, Train Loss: 0.0555, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9113\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.87      0.89        86\n",
      "    positive       0.94      0.96      0.95       181\n",
      "\n",
      "    accuracy                           0.93       267\n",
      "   macro avg       0.93      0.92      0.92       267\n",
      "weighted avg       0.93      0.93      0.93       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9053\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.78      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.75194382667542 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5331, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4312, Accuracy: 0.8914, F1 Micro: 0.9352, F1 Macro: 0.9341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3083, Accuracy: 0.9472, F1 Micro: 0.9673, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2089, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1517, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Epoch 6/10, Train Loss: 0.1164, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.097, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0813, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 9/10, Train Loss: 0.0653, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 10/10, Train Loss: 0.0576, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5201, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9028\n",
      "Epoch 2/10, Train Loss: 0.2521, Accuracy: 0.8774, F1 Micro: 0.8774, F1 Macro: 0.8698\n",
      "Epoch 3/10, Train Loss: 0.2023, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.8926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1636, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1465, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1167, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9269\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9179\n",
      "Epoch 8/10, Train Loss: 0.0777, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.922\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9224\n",
      "Epoch 10/10, Train Loss: 0.0567, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9112\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9269\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.93      0.90        85\n",
      "    positive       0.96      0.94      0.95       176\n",
      "\n",
      "    accuracy                           0.93       261\n",
      "   macro avg       0.92      0.93      0.93       261\n",
      "weighted avg       0.94      0.93      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9078\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 155.53294920921326 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9384, F1 Micro: 0.9384, F1 Macro: 0.8611\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.689525365829468 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5405, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4328, Accuracy: 0.9107, F1 Micro: 0.9461, F1 Macro: 0.9453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2987, Accuracy: 0.9457, F1 Micro: 0.9664, F1 Macro: 0.965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2012, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1196, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0937, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.0749, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 9/10, Train Loss: 0.0606, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9745\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5417, Accuracy: 0.8986, F1 Micro: 0.8986, F1 Macro: 0.8852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2598, Accuracy: 0.942, F1 Micro: 0.942, F1 Macro: 0.9344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2054, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1297, Accuracy: 0.9493, F1 Micro: 0.9493, F1 Macro: 0.9423\n",
      "Epoch 5/10, Train Loss: 0.144, Accuracy: 0.942, F1 Micro: 0.942, F1 Macro: 0.9348\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9384, F1 Micro: 0.9384, F1 Macro: 0.9284\n",
      "Epoch 7/10, Train Loss: 0.1065, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0612, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9502\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9205\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.942, F1 Micro: 0.942, F1 Macro: 0.9315\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9502\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.97       190\n",
      "\n",
      "    accuracy                           0.96       276\n",
      "   macro avg       0.94      0.96      0.95       276\n",
      "weighted avg       0.96      0.96      0.96       276\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9288\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.96      0.91      0.93       152\n",
      "    positive       0.75      0.88      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.88      0.87       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 168.3139934539795 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5419, Accuracy: 0.7932, F1 Micro: 0.8836, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4356, Accuracy: 0.9062, F1 Micro: 0.9427, F1 Macro: 0.941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2957, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1948, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1383, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1149, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0574, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 10/10, Train Loss: 0.0544, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5153, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2501, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1811, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9258\n",
      "Epoch 4/10, Train Loss: 0.1554, Accuracy: 0.9148, F1 Micro: 0.9148, F1 Macro: 0.9064\n",
      "Epoch 5/10, Train Loss: 0.125, Accuracy: 0.9148, F1 Micro: 0.9148, F1 Macro: 0.9022\n",
      "Epoch 6/10, Train Loss: 0.128, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0806, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.061, Accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.9403\n",
      "Epoch 10/10, Train Loss: 0.0579, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.929\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9481, F1 Micro: 0.9481, F1 Macro: 0.9403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        86\n",
      "    positive       0.96      0.96      0.96       184\n",
      "\n",
      "    accuracy                           0.95       270\n",
      "   macro avg       0.94      0.94      0.94       270\n",
      "weighted avg       0.95      0.95      0.95       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9231\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.80      0.85      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 169.8799602985382 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.535, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4279, Accuracy: 0.8988, F1 Micro: 0.9392, F1 Macro: 0.9377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3011, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2003, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1412, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1176, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0959, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0739, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0615, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.498, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2526, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9343\n",
      "Epoch 3/10, Train Loss: 0.2247, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1657, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.944\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1144, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9443\n",
      "Epoch 6/10, Train Loss: 0.125, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9238\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Epoch 8/10, Train Loss: 0.0811, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9392\n",
      "Epoch 9/10, Train Loss: 0.0523, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9381\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9234\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        85\n",
      "    positive       0.99      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.96      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9241\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.19535398483276 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.8649\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.219687461853027 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5408, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4286, Accuracy: 0.8876, F1 Micro: 0.9335, F1 Macro: 0.933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2825, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1952, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1391, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 6/10, Train Loss: 0.1059, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5005, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.9018\n",
      "Epoch 2/10, Train Loss: 0.2501, Accuracy: 0.8937, F1 Micro: 0.8937, F1 Macro: 0.8867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2163, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1624, Accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9518\n",
      "Epoch 5/10, Train Loss: 0.1092, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9424\n",
      "Epoch 6/10, Train Loss: 0.095, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9185\n",
      "Epoch 7/10, Train Loss: 0.094, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9327\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9346\n",
      "Epoch 9/10, Train Loss: 0.0601, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9222\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9466\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        84\n",
      "    positive       0.98      0.95      0.97       170\n",
      "\n",
      "    accuracy                           0.96       254\n",
      "   macro avg       0.95      0.96      0.95       254\n",
      "weighted avg       0.96      0.96      0.96       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9284\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 165.07943272590637 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5302, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.442, Accuracy: 0.9115, F1 Micro: 0.946, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2837, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1915, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1357, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1012, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0799, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.068, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9784\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.512, Accuracy: 0.8779, F1 Micro: 0.8779, F1 Macro: 0.8696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2321, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1772, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9404\n",
      "Epoch 4/10, Train Loss: 0.1418, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9054\n",
      "Epoch 5/10, Train Loss: 0.1727, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "Epoch 6/10, Train Loss: 0.103, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9349\n",
      "Epoch 7/10, Train Loss: 0.0859, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 9/10, Train Loss: 0.0636, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9226\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.93      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9266\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 168.48591446876526 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5281, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4256, Accuracy: 0.9048, F1 Micro: 0.9428, F1 Macro: 0.9414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2787, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1962, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1373, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1059, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 7/10, Train Loss: 0.0844, Accuracy: 0.9665, F1 Micro: 0.9787, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0736, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.98\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Epoch 10/10, Train Loss: 0.0521, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9793\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5144, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2692, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2131, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1455, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9298\n",
      "Epoch 5/10, Train Loss: 0.1535, Accuracy: 0.9127, F1 Micro: 0.9127, F1 Macro: 0.9067\n",
      "Epoch 6/10, Train Loss: 0.1232, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9127\n",
      "Epoch 7/10, Train Loss: 0.1006, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8973\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9256\n",
      "Epoch 9/10, Train Loss: 0.0722, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9131\n",
      "Epoch 10/10, Train Loss: 0.069, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9095\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        84\n",
      "    positive       0.97      0.93      0.95       168\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.92      0.94      0.93       252\n",
      "weighted avg       0.94      0.94      0.94       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9152\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 163.3968005180359 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9408, F1 Micro: 0.9408, F1 Macro: 0.8681\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.5572972297668457 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5405, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4179, Accuracy: 0.9241, F1 Micro: 0.9535, F1 Macro: 0.9523\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2749, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1885, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9759\n",
      "Epoch 6/10, Train Loss: 0.1055, Accuracy: 0.9576, F1 Micro: 0.973, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0857, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0517, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5063, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9068\n",
      "Epoch 2/10, Train Loss: 0.2519, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.8938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2063, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9218\n",
      "Epoch 4/10, Train Loss: 0.1501, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1729, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Epoch 6/10, Train Loss: 0.1251, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9286\n",
      "Epoch 7/10, Train Loss: 0.0802, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9326\n",
      "Epoch 8/10, Train Loss: 0.0668, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9088\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9278\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9162\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       182\n",
      "\n",
      "    accuracy                           0.94       267\n",
      "   macro avg       0.93      0.95      0.94       267\n",
      "weighted avg       0.95      0.94      0.94       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9223\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.77      0.79      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.82      0.82       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 163.1666224002838 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5452, Accuracy: 0.7902, F1 Micro: 0.8823, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4355, Accuracy: 0.9085, F1 Micro: 0.9439, F1 Macro: 0.9419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2848, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.186, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1374, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9776\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 8/10, Train Loss: 0.0687, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.053, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9805\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.489, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9156\n",
      "Epoch 2/10, Train Loss: 0.2306, Accuracy: 0.9108, F1 Micro: 0.9108, F1 Macro: 0.9028\n",
      "Epoch 3/10, Train Loss: 0.1994, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1409, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9257\n",
      "Epoch 5/10, Train Loss: 0.1094, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9144\n",
      "Epoch 7/10, Train Loss: 0.088, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.924\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9144\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9174\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.91      0.90        87\n",
      "    positive       0.96      0.95      0.95       182\n",
      "\n",
      "    accuracy                           0.93       269\n",
      "   macro avg       0.92      0.93      0.92       269\n",
      "weighted avg       0.93      0.93      0.93       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9161\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.85      0.96      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.83      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 168.58436751365662 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.537, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4172, Accuracy: 0.9152, F1 Micro: 0.948, F1 Macro: 0.9459\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2772, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1876, Accuracy: 0.9591, F1 Micro: 0.9746, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1419, Accuracy: 0.9643, F1 Micro: 0.9778, F1 Macro: 0.9766\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9748\n",
      "Epoch 7/10, Train Loss: 0.085, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0726, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9746\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.92      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4941, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9372\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9169\n",
      "Epoch 3/10, Train Loss: 0.1588, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Epoch 4/10, Train Loss: 0.1609, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.113, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9399\n",
      "Epoch 6/10, Train Loss: 0.107, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9284\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9209\n",
      "Epoch 8/10, Train Loss: 0.095, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9272\n",
      "Epoch 9/10, Train Loss: 0.08, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9284\n",
      "Epoch 10/10, Train Loss: 0.0598, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9365\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9399\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        85\n",
      "    positive       0.97      0.96      0.96       181\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.94      0.94      0.94       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9208\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.96      0.93      0.94       152\n",
      "    positive       0.79      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.87      0.85       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 165.89534831047058 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9418, F1 Micro: 0.9418, F1 Macro: 0.8708\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.3838837146759033 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5447, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4101, Accuracy: 0.9144, F1 Micro: 0.9481, F1 Macro: 0.9469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2817, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.187, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9743\n",
      "Epoch 5/10, Train Loss: 0.138, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.089, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0735, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9794\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5036, Accuracy: 0.8621, F1 Micro: 0.8621, F1 Macro: 0.8546\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2338, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9165\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2018, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9277\n",
      "Epoch 4/10, Train Loss: 0.1613, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1148, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Epoch 6/10, Train Loss: 0.1278, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1161, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.098, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Epoch 9/10, Train Loss: 0.0921, Accuracy: 0.9004, F1 Micro: 0.9004, F1 Macro: 0.8859\n",
      "Epoch 10/10, Train Loss: 0.0548, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9229\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        86\n",
      "    positive       0.96      0.95      0.95       175\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.93      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9176\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 172.1355812549591 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.54, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4173, Accuracy: 0.9174, F1 Micro: 0.9494, F1 Macro: 0.9476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2776, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1806, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1331, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1015, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9751\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5042, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2313, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9277\n",
      "Epoch 3/10, Train Loss: 0.1634, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1543, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.927\n",
      "Epoch 5/10, Train Loss: 0.1076, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9098\n",
      "Epoch 6/10, Train Loss: 0.1029, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9073\n",
      "Epoch 7/10, Train Loss: 0.0849, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9225\n",
      "Epoch 10/10, Train Loss: 0.0545, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9111\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        85\n",
      "    positive       0.98      0.92      0.95       171\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.92      0.94      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9135\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 168.30369925498962 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5351, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3967, Accuracy: 0.9219, F1 Micro: 0.9525, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.273, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1795, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 7/10, Train Loss: 0.0887, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 8/10, Train Loss: 0.0704, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Epoch 9/10, Train Loss: 0.0555, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4623, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2391, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1683, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 4/10, Train Loss: 0.1389, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Epoch 5/10, Train Loss: 0.1081, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9225\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0746, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        83\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.93      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9151\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 170.9658763408661 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.8731\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.904515504837036 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5293, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4081, Accuracy: 0.9301, F1 Micro: 0.9571, F1 Macro: 0.9558\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2625, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1797, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1309, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9787\n",
      "Epoch 6/10, Train Loss: 0.0969, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5001, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2604, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Epoch 3/10, Train Loss: 0.1718, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 4/10, Train Loss: 0.1698, Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.8989\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1368, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9392\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.935\n",
      "Epoch 8/10, Train Loss: 0.0809, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9264\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9181\n",
      "Epoch 10/10, Train Loss: 0.0787, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9214\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        84\n",
      "    positive       0.98      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.93      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9212\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.8321602344513 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5279, Accuracy: 0.7924, F1 Micro: 0.8831, F1 Macro: 0.8813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.415, Accuracy: 0.9219, F1 Micro: 0.9521, F1 Macro: 0.9503\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2565, Accuracy: 0.9479, F1 Micro: 0.9675, F1 Macro: 0.9663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1746, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1274, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Epoch 6/10, Train Loss: 0.0951, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0729, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.979\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4751, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2267, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1839, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "Epoch 4/10, Train Loss: 0.165, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9113\n",
      "Epoch 5/10, Train Loss: 0.1204, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9198\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9136\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9572, F1 Micro: 0.9572, F1 Macro: 0.9523\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9572, F1 Micro: 0.9572, F1 Macro: 0.9523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        85\n",
      "    positive       0.98      0.95      0.97       172\n",
      "\n",
      "    accuracy                           0.96       257\n",
      "   macro avg       0.95      0.96      0.95       257\n",
      "weighted avg       0.96      0.96      0.96       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9334\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.98      0.95       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 175.96209955215454 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5164, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3942, Accuracy: 0.9286, F1 Micro: 0.9564, F1 Macro: 0.955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2518, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1758, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 5/10, Train Loss: 0.131, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9805\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9799\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.94      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4917, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9132\n",
      "Epoch 2/10, Train Loss: 0.2398, Accuracy: 0.9057, F1 Micro: 0.9057, F1 Macro: 0.8981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1803, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.919\n",
      "Epoch 4/10, Train Loss: 0.1911, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9137\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9203\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9134\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9266\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9323\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9132\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        87\n",
      "    positive       0.97      0.94      0.95       178\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.93      0.94      0.93       265\n",
      "weighted avg       0.94      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9205\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.96      0.94      0.95       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.89      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 180.6589207649231 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.8755\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.2320027351379395 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5278, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4116, Accuracy: 0.9323, F1 Micro: 0.9584, F1 Macro: 0.9574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2708, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1255, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       1.00      0.99      1.00       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4739, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8983\n",
      "Epoch 2/10, Train Loss: 0.2424, Accuracy: 0.8955, F1 Micro: 0.8955, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1957, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9291\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9103\n",
      "Epoch 5/10, Train Loss: 0.1148, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0962, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0917, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9371\n",
      "Epoch 9/10, Train Loss: 0.0711, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9327\n",
      "Epoch 10/10, Train Loss: 0.0672, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8988\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       182\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.93      0.95      0.94       268\n",
      "weighted avg       0.95      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9197\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       1.00      0.99      1.00       181\n",
      "    positive       0.92      1.00      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.97      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.75      0.81      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.85      0.85       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 183.82770228385925 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5284, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4223, Accuracy: 0.9278, F1 Micro: 0.9555, F1 Macro: 0.9536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2644, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0955, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.9613, F1 Micro: 0.9754, F1 Macro: 0.9733\n",
      "Epoch 8/10, Train Loss: 0.0639, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9729\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5087, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.217, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.186, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9369\n",
      "Epoch 4/10, Train Loss: 0.1318, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.916\n",
      "Epoch 5/10, Train Loss: 0.1374, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9135\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "Epoch 9/10, Train Loss: 0.0399, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.913\n",
      "Epoch 10/10, Train Loss: 0.0348, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9209\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        85\n",
      "    positive       0.98      0.94      0.96       184\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.95      0.94       269\n",
      "weighted avg       0.95      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9262\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.81      0.83      0.82       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 175.74274349212646 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5223, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4034, Accuracy: 0.9241, F1 Micro: 0.9533, F1 Macro: 0.9517\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2679, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.178, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.128, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1052, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9788\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4864, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.9165\n",
      "Epoch 2/10, Train Loss: 0.2492, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.9062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1982, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9332\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1829, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9379\n",
      "Epoch 5/10, Train Loss: 0.0957, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9339\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9328\n",
      "Epoch 7/10, Train Loss: 0.1098, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.928\n",
      "Epoch 8/10, Train Loss: 0.0855, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0706, Accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.945\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9213\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9517, F1 Micro: 0.9517, F1 Macro: 0.945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.96      0.96       183\n",
      "\n",
      "    accuracy                           0.95       269\n",
      "   macro avg       0.94      0.95      0.94       269\n",
      "weighted avg       0.95      0.95      0.95       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9268\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.78      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 184.97280073165894 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9443, F1 Micro: 0.9443, F1 Macro: 0.8778\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 1.8106894493103027 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5323, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3968, Accuracy: 0.9241, F1 Micro: 0.9535, F1 Macro: 0.9519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2523, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1719, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1319, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0972, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9752\n",
      "Epoch 7/10, Train Loss: 0.0794, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0611, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0571, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4471, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9183\n",
      "Epoch 2/10, Train Loss: 0.2132, Accuracy: 0.8826, F1 Micro: 0.8826, F1 Macro: 0.8745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1693, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9314\n",
      "Epoch 4/10, Train Loss: 0.1733, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Epoch 5/10, Train Loss: 0.1136, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9166\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.917\n",
      "Epoch 7/10, Train Loss: 0.0787, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0665, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9326\n",
      "Epoch 9/10, Train Loss: 0.0656, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9201\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9282\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9326\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        85\n",
      "    positive       0.98      0.93      0.95       179\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.92      0.95      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9222\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 181.84529638290405 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5294, Accuracy: 0.7917, F1 Micro: 0.8828, F1 Macro: 0.881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3908, Accuracy: 0.9375, F1 Micro: 0.9614, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2411, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1631, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Epoch 5/10, Train Loss: 0.1217, Accuracy: 0.9583, F1 Micro: 0.9736, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0899, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9621, F1 Micro: 0.9758, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.058, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.053, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "Epoch 10/10, Train Loss: 0.0433, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9762\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4867, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.224, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9257\n",
      "Epoch 3/10, Train Loss: 0.1546, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9139\n",
      "Epoch 4/10, Train Loss: 0.1669, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9148\n",
      "Epoch 5/10, Train Loss: 0.1097, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9135\n",
      "Epoch 6/10, Train Loss: 0.0979, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9092\n",
      "Epoch 7/10, Train Loss: 0.1008, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9174\n",
      "Epoch 8/10, Train Loss: 0.0765, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0619, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "Epoch 10/10, Train Loss: 0.0515, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9068\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        87\n",
      "    positive       0.97      0.93      0.95       177\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.92      0.94      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9191\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.95       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 179.80938816070557 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5214, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3782, Accuracy: 0.9271, F1 Micro: 0.955, F1 Macro: 0.9533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2458, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1721, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9753\n",
      "Epoch 5/10, Train Loss: 0.1286, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0806, Accuracy: 0.9665, F1 Micro: 0.9787, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0603, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0565, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4858, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2532, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9366\n",
      "Epoch 3/10, Train Loss: 0.1568, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 4/10, Train Loss: 0.136, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1235, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9355\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.102, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9441\n",
      "Epoch 8/10, Train Loss: 0.0739, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9318\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9237\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9233\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.92      0.92        86\n",
      "    positive       0.96      0.97      0.96       181\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.95      0.94      0.94       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9238\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.80      0.85      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 180.8679645061493 s\n",
      "Averaged - Iteration 856: Accuracy: 0.945, F1 Micro: 0.945, F1 Macro: 0.8797\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 1.2126030921936035 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5177, Accuracy: 0.7999, F1 Micro: 0.8875, F1 Macro: 0.8861\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3826, Accuracy: 0.9249, F1 Micro: 0.953, F1 Macro: 0.9491\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2401, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1672, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1254, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 6/10, Train Loss: 0.0974, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Epoch 9/10, Train Loss: 0.05, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4888, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9264\n",
      "Epoch 2/10, Train Loss: 0.2284, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9113\n",
      "Epoch 3/10, Train Loss: 0.1558, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1266, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9316\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Epoch 6/10, Train Loss: 0.1084, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9268\n",
      "Epoch 7/10, Train Loss: 0.0779, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0717, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "Epoch 9/10, Train Loss: 0.0469, Accuracy: 0.9027, F1 Micro: 0.9027, F1 Macro: 0.8954\n",
      "Epoch 10/10, Train Loss: 0.0561, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9218\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        84\n",
      "    positive       0.98      0.93      0.95       173\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.92      0.94      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9214\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.98      0.95       152\n",
      "    positive       0.93      0.75      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 177.06642723083496 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5221, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3982, Accuracy: 0.9174, F1 Micro: 0.948, F1 Macro: 0.9436\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2413, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1661, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1204, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.965, F1 Micro: 0.9777, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.974, F1 Micro: 0.9836, F1 Macro: 0.9828\n",
      "Epoch 9/10, Train Loss: 0.0478, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 10/10, Train Loss: 0.0474, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.974, F1 Micro: 0.9836, F1 Macro: 0.9828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.98      0.96       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4927, Accuracy: 0.8969, F1 Micro: 0.8969, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2351, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1575, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "Epoch 4/10, Train Loss: 0.133, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9154\n",
      "Epoch 5/10, Train Loss: 0.1065, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0781, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9242\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Epoch 8/10, Train Loss: 0.0815, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9009\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.91      0.90        86\n",
      "    positive       0.95      0.95      0.95       176\n",
      "\n",
      "    accuracy                           0.94       262\n",
      "   macro avg       0.93      0.93      0.93       262\n",
      "weighted avg       0.94      0.94      0.94       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9194\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.79      0.82      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.84       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.98      0.96       152\n",
      "    positive       0.91      0.81      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 178.8998510837555 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.513, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3754, Accuracy: 0.9167, F1 Micro: 0.9474, F1 Macro: 0.9411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2377, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1662, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1051, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4824, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2015, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "Epoch 3/10, Train Loss: 0.1625, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9188\n",
      "Epoch 4/10, Train Loss: 0.1266, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1058, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Epoch 6/10, Train Loss: 0.1056, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9139\n",
      "Epoch 7/10, Train Loss: 0.0711, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8833\n",
      "Epoch 8/10, Train Loss: 0.0803, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9282\n",
      "Epoch 10/10, Train Loss: 0.0688, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9187\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        84\n",
      "    positive       0.94      0.97      0.95       172\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.93      0.92      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9178\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.77        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.81      0.85       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 185.79119753837585 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.8814\n",
      "Total runtime: 10913.551169395447 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43384d68",
   "metadata": {
    "papermill": {
     "duration": 0.016279,
     "end_time": "2025-04-05T06:35:21.351552",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.335273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a1639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 2\n",
      "Random seed: [81, 90, 11]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6696, Accuracy: 0.6935, F1 Micro: 0.8002, F1 Macro: 0.707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5881, Accuracy: 0.7775, F1 Micro: 0.8718, F1 Macro: 0.8663\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5506, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8808\n",
      "Epoch 4/10, Train Loss: 0.528, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4866, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4714, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.48, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4383, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 9/10, Train Loss: 0.4116, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3836, Accuracy: 0.7946, F1 Micro: 0.8847, F1 Macro: 0.8832\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7946, F1 Micro: 0.8847, F1 Macro: 0.8832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.79      0.99      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.80      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6963, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5281, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5145, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4748, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4531, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.5539, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.5383, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4717, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3387, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.3529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3776, Accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.4762\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.5455, F1 Micro: 0.5455, F1 Macro: 0.4762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.20      0.29         5\n",
      "    positive       0.56      0.83      0.67         6\n",
      "\n",
      "    accuracy                           0.55        11\n",
      "   macro avg       0.53      0.52      0.48        11\n",
      "weighted avg       0.53      0.55      0.49        11\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3111\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.06      0.11        16\n",
      "     neutral       0.79      0.99      0.88       167\n",
      "    positive       0.50      0.06      0.11        33\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.60      0.37      0.36       216\n",
      "weighted avg       0.72      0.78      0.70       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       0.50      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.40      0.34      0.29       216\n",
      "weighted avg       0.62      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       0.33      0.02      0.05        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.35      0.34      0.29       216\n",
      "weighted avg       0.57      0.71      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 63.60741996765137 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6698, Accuracy: 0.7924, F1 Micro: 0.8815, F1 Macro: 0.8788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5861, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Epoch 3/10, Train Loss: 0.5481, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.5332, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 5/10, Train Loss: 0.4877, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 6/10, Train Loss: 0.4812, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4841, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 8/10, Train Loss: 0.4614, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 9/10, Train Loss: 0.4247, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4075, Accuracy: 0.7932, F1 Micro: 0.884, F1 Macro: 0.8825\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7932, F1 Micro: 0.884, F1 Macro: 0.8825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      0.99      0.87       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.72      1.00      0.84       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.812, Accuracy: 0.375, F1 Micro: 0.375, F1 Macro: 0.3651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6454, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3987, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3302, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2917, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2519, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.216, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1941, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0953, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         2\n",
      "    positive       0.75      1.00      0.86         6\n",
      "\n",
      "    accuracy                           0.75         8\n",
      "   macro avg       0.38      0.50      0.43         8\n",
      "weighted avg       0.56      0.75      0.64         8\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7924, F1 Micro: 0.7924, F1 Macro: 0.3069\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.78      0.99      0.87       167\n",
      "    positive       0.50      0.03      0.06        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.43      0.34      0.31       216\n",
      "weighted avg       0.68      0.77      0.68       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      1.00      0.84       152\n",
      "    positive       0.67      0.10      0.17        41\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.46      0.37      0.34       216\n",
      "weighted avg       0.64      0.72      0.62       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 54.105098724365234 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6693, Accuracy: 0.782, F1 Micro: 0.8767, F1 Macro: 0.875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5726, Accuracy: 0.7894, F1 Micro: 0.8816, F1 Macro: 0.8798\n",
      "Epoch 3/10, Train Loss: 0.5386, Accuracy: 0.7879, F1 Micro: 0.8813, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5272, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4812, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4653, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4676, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Epoch 8/10, Train Loss: 0.4355, Accuracy: 0.7902, F1 Micro: 0.8816, F1 Macro: 0.8795\n",
      "Epoch 9/10, Train Loss: 0.4096, Accuracy: 0.7879, F1 Micro: 0.8795, F1 Macro: 0.8767\n",
      "Epoch 10/10, Train Loss: 0.3739, Accuracy: 0.7879, F1 Micro: 0.8786, F1 Macro: 0.8755\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      0.99      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.88      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6057, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4393, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4544, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4236, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3367, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Epoch 6/10, Train Loss: 0.3349, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 7/10, Train Loss: 0.2332, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 8/10, Train Loss: 0.2243, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 9/10, Train Loss: 0.1859, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Epoch 10/10, Train Loss: 0.1717, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         4\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         4\n",
      "   macro avg       0.50      0.50      0.50         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7909, F1 Micro: 0.7909, F1 Macro: 0.2997\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      0.99      0.83       152\n",
      "    positive       0.75      0.06      0.11        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.49      0.35      0.31       216\n",
      "weighted avg       0.68      0.71      0.61       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 50.87819528579712 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3059\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 9.428232192993164 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6103, Accuracy: 0.7924, F1 Micro: 0.8826, F1 Macro: 0.8805\n",
      "Epoch 2/10, Train Loss: 0.5351, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.5061, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.4865, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4228, Accuracy: 0.8051, F1 Micro: 0.8898, F1 Macro: 0.8882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3957, Accuracy: 0.8423, F1 Micro: 0.9078, F1 Macro: 0.906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3723, Accuracy: 0.8757, F1 Micro: 0.9253, F1 Macro: 0.9233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2934, Accuracy: 0.8943, F1 Micro: 0.9357, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2682, Accuracy: 0.9137, F1 Micro: 0.9473, F1 Macro: 0.9453\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2392, Accuracy: 0.9182, F1 Micro: 0.9497, F1 Macro: 0.9478\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9182, F1 Micro: 0.9497, F1 Macro: 0.9478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.96      0.99      0.98       187\n",
      "     machine       0.93      0.94      0.94       175\n",
      "      others       0.85      0.94      0.89       158\n",
      "        part       0.88      0.99      0.93       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.94      0.99      0.97       191\n",
      "\n",
      "   micro avg       0.92      0.98      0.95      1061\n",
      "   macro avg       0.92      0.98      0.95      1061\n",
      "weighted avg       0.92      0.98      0.95      1061\n",
      " samples avg       0.93      0.98      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5783, Accuracy: 0.7095, F1 Micro: 0.7095, F1 Macro: 0.415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5272, Accuracy: 0.7095, F1 Micro: 0.7095, F1 Macro: 0.415\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4312, Accuracy: 0.819, F1 Micro: 0.819, F1 Macro: 0.7602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2808, Accuracy: 0.8524, F1 Micro: 0.8524, F1 Macro: 0.8295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.181, Accuracy: 0.8714, F1 Micro: 0.8714, F1 Macro: 0.8476\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1375, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0753, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.122, Accuracy: 0.8952, F1 Micro: 0.8952, F1 Macro: 0.8753\n",
      "Epoch 9/10, Train Loss: 0.069, Accuracy: 0.881, F1 Micro: 0.881, F1 Macro: 0.8625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0667, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8781\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.82      0.83        61\n",
      "    positive       0.93      0.93      0.93       149\n",
      "\n",
      "    accuracy                           0.90       210\n",
      "   macro avg       0.88      0.88      0.88       210\n",
      "weighted avg       0.90      0.90      0.90       210\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.9043, F1 Micro: 0.9043, F1 Macro: 0.7842\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.97      0.99      0.98       181\n",
      "    positive       0.95      0.79      0.86        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.90      0.93       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.93      0.95      0.94       167\n",
      "    positive       0.68      0.64      0.66        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.80      0.81       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.33      0.38        12\n",
      "     neutral       0.85      0.95      0.89       152\n",
      "    positive       0.78      0.56      0.65        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.69      0.61      0.64       216\n",
      "weighted avg       0.81      0.82      0.81       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.39      0.55        23\n",
      "     neutral       0.88      0.99      0.93       152\n",
      "    positive       0.80      0.68      0.74        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.86      0.69      0.74       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.54      0.70        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.79      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.50      0.61        14\n",
      "     neutral       0.94      0.99      0.97       185\n",
      "    positive       0.82      0.53      0.64        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.84      0.67      0.74       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Total train time: 82.08249068260193 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6123, Accuracy: 0.7842, F1 Micro: 0.879, F1 Macro: 0.877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5339, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5128, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.494, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4308, Accuracy: 0.8058, F1 Micro: 0.8895, F1 Macro: 0.8874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4163, Accuracy: 0.8155, F1 Micro: 0.8926, F1 Macro: 0.8891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3849, Accuracy: 0.846, F1 Micro: 0.9085, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3026, Accuracy: 0.8832, F1 Micro: 0.9291, F1 Macro: 0.9257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2723, Accuracy: 0.8951, F1 Micro: 0.9348, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.24, Accuracy: 0.9092, F1 Micro: 0.9437, F1 Macro: 0.9404\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9092, F1 Micro: 0.9437, F1 Macro: 0.9404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      0.99      0.98       187\n",
      "     machine       0.92      0.95      0.94       175\n",
      "      others       0.88      0.84      0.86       158\n",
      "        part       0.86      0.99      0.92       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.94      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.92      0.96      0.94      1061\n",
      "   macro avg       0.92      0.96      0.94      1061\n",
      "weighted avg       0.93      0.96      0.94      1061\n",
      " samples avg       0.93      0.96      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6077, Accuracy: 0.7249, F1 Micro: 0.7249, F1 Macro: 0.4203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4862, Accuracy: 0.7249, F1 Micro: 0.7249, F1 Macro: 0.4203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3571, Accuracy: 0.8646, F1 Micro: 0.8646, F1 Macro: 0.8277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2323, Accuracy: 0.8908, F1 Micro: 0.8908, F1 Macro: 0.8638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1443, Accuracy: 0.9039, F1 Micro: 0.9039, F1 Macro: 0.885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9083, F1 Micro: 0.9083, F1 Macro: 0.8808\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9039, F1 Micro: 0.9039, F1 Macro: 0.8784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9083, F1 Micro: 0.9083, F1 Macro: 0.8845\n",
      "Epoch 9/10, Train Loss: 0.0261, Accuracy: 0.9039, F1 Micro: 0.9039, F1 Macro: 0.884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0827, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.8944\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.8944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.83      0.85        63\n",
      "    positive       0.93      0.95      0.94       166\n",
      "\n",
      "    accuracy                           0.92       229\n",
      "   macro avg       0.90      0.89      0.89       229\n",
      "weighted avg       0.92      0.92      0.92       229\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8974, F1 Micro: 0.8974, F1 Macro: 0.7853\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.91      0.96      0.94       167\n",
      "    positive       0.73      0.58      0.64        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.78      0.81       216\n",
      "weighted avg       0.88      0.89      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.42      0.48        12\n",
      "     neutral       0.87      0.84      0.86       152\n",
      "    positive       0.62      0.71      0.66        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.68      0.66      0.66       216\n",
      "weighted avg       0.79      0.79      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.43      0.61        23\n",
      "     neutral       0.86      0.99      0.92       152\n",
      "    positive       0.83      0.61      0.70        41\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.90      0.68      0.74       216\n",
      "weighted avg       0.87      0.86      0.85       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.62      0.73        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.70      0.82      0.76        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.85      0.81      0.82       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.43      0.57        14\n",
      "     neutral       0.93      1.00      0.97       185\n",
      "    positive       0.82      0.53      0.64        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.65      0.73       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Total train time: 85.76399350166321 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.632, Accuracy: 0.7746, F1 Micro: 0.8716, F1 Macro: 0.867\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5308, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5115, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Epoch 4/10, Train Loss: 0.4978, Accuracy: 0.7917, F1 Micro: 0.8815, F1 Macro: 0.8787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4303, Accuracy: 0.7984, F1 Micro: 0.8852, F1 Macro: 0.8829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.425, Accuracy: 0.8043, F1 Micro: 0.887, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3886, Accuracy: 0.8251, F1 Micro: 0.8967, F1 Macro: 0.8931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3222, Accuracy: 0.8393, F1 Micro: 0.9048, F1 Macro: 0.9025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2855, Accuracy: 0.8624, F1 Micro: 0.9156, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2606, Accuracy: 0.8765, F1 Micro: 0.9245, F1 Macro: 0.9216\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.8765, F1 Micro: 0.9245, F1 Macro: 0.9216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.94      1.00      0.97       187\n",
      "     machine       0.91      0.90      0.90       175\n",
      "      others       0.87      0.87      0.87       158\n",
      "        part       0.83      0.97      0.89       158\n",
      "       price       0.89      0.99      0.94       192\n",
      "     service       0.91      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.89      0.96      0.92      1061\n",
      "   macro avg       0.89      0.96      0.92      1061\n",
      "weighted avg       0.89      0.96      0.92      1061\n",
      " samples avg       0.90      0.96      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5646, Accuracy: 0.7716, F1 Micro: 0.7716, F1 Macro: 0.4355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4925, Accuracy: 0.797, F1 Micro: 0.797, F1 Macro: 0.6172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3932, Accuracy: 0.8122, F1 Micro: 0.8122, F1 Macro: 0.7434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2567, Accuracy: 0.8325, F1 Micro: 0.8325, F1 Macro: 0.7883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1116, Accuracy: 0.868, F1 Micro: 0.868, F1 Macro: 0.8184\n",
      "Epoch 6/10, Train Loss: 0.1442, Accuracy: 0.8579, F1 Micro: 0.8579, F1 Macro: 0.8214\n",
      "Epoch 7/10, Train Loss: 0.0568, Accuracy: 0.8528, F1 Micro: 0.8528, F1 Macro: 0.7928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1056, Accuracy: 0.8832, F1 Micro: 0.8832, F1 Macro: 0.8469\n",
      "Epoch 9/10, Train Loss: 0.0953, Accuracy: 0.8376, F1 Micro: 0.8376, F1 Macro: 0.8083\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.8477, F1 Micro: 0.8477, F1 Macro: 0.8167\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.8832, F1 Micro: 0.8832, F1 Macro: 0.8469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.87      0.77        45\n",
      "    positive       0.96      0.89      0.92       152\n",
      "\n",
      "    accuracy                           0.88       197\n",
      "   macro avg       0.83      0.88      0.85       197\n",
      "weighted avg       0.90      0.88      0.89       197\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8681, F1 Micro: 0.8681, F1 Macro: 0.6809\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.95      1.00      0.97       181\n",
      "    positive       1.00      0.75      0.86        24\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.98      0.80      0.87       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        16\n",
      "     neutral       0.90      0.90      0.90       167\n",
      "    positive       0.63      0.58      0.60        33\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.71      0.74      0.72       216\n",
      "weighted avg       0.84      0.84      0.84       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.42      0.42      0.42        12\n",
      "     neutral       0.86      0.87      0.87       152\n",
      "    positive       0.69      0.67      0.68        52\n",
      "\n",
      "    accuracy                           0.80       216\n",
      "   macro avg       0.66      0.65      0.65       216\n",
      "weighted avg       0.80      0.80      0.80       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.48      0.63        23\n",
      "     neutral       0.83      0.97      0.89       152\n",
      "    positive       0.80      0.49      0.61        41\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.85      0.65      0.71       216\n",
      "weighted avg       0.83      0.83      0.81       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.08      0.14        13\n",
      "     neutral       0.89      0.99      0.94       186\n",
      "    positive       0.62      0.29      0.40        17\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.46      0.49       216\n",
      "weighted avg       0.88      0.88      0.85       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.21      0.33        14\n",
      "     neutral       0.91      1.00      0.95       185\n",
      "    positive       0.89      0.47      0.62        17\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.56      0.63       216\n",
      "weighted avg       0.90      0.91      0.89       216\n",
      "\n",
      "Total train time: 78.4302966594696 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8408, F1 Micro: 0.8408, F1 Macro: 0.528\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 14.838332653045654 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6065, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5258, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4947, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4543, Accuracy: 0.8155, F1 Micro: 0.8953, F1 Macro: 0.8942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4127, Accuracy: 0.8616, F1 Micro: 0.9184, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3369, Accuracy: 0.907, F1 Micro: 0.9438, F1 Macro: 0.9424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2978, Accuracy: 0.9286, F1 Micro: 0.9561, F1 Macro: 0.9545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2564, Accuracy: 0.9353, F1 Micro: 0.9598, F1 Macro: 0.9583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2128, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1778, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9674\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9674\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.96       175\n",
      "      others       0.90      0.94      0.92       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.98      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6033, Accuracy: 0.6793, F1 Micro: 0.6793, F1 Macro: 0.4045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4369, Accuracy: 0.7173, F1 Micro: 0.7173, F1 Macro: 0.5198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3126, Accuracy: 0.903, F1 Micro: 0.903, F1 Macro: 0.8918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1357, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9063\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0685, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.922\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.9241, F1 Micro: 0.9241, F1 Macro: 0.9146\n",
      "Epoch 8/10, Train Loss: 0.0768, Accuracy: 0.9156, F1 Micro: 0.9156, F1 Macro: 0.9073\n",
      "Epoch 9/10, Train Loss: 0.0653, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9214\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.87      0.89        76\n",
      "    positive       0.94      0.96      0.95       161\n",
      "\n",
      "    accuracy                           0.93       237\n",
      "   macro avg       0.93      0.92      0.92       237\n",
      "weighted avg       0.93      0.93      0.93       237\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9406, F1 Micro: 0.9406, F1 Macro: 0.8791\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.90      0.95      0.92       152\n",
      "    positive       0.82      0.71      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.61      0.70        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.86      0.78      0.82        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.79      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.86      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.83      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 89.62171721458435 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6033, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5222, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4953, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4551, Accuracy: 0.8043, F1 Micro: 0.8897, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.421, Accuracy: 0.8564, F1 Micro: 0.9163, F1 Macro: 0.9151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.354, Accuracy: 0.901, F1 Micro: 0.9407, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3038, Accuracy: 0.9241, F1 Micro: 0.953, F1 Macro: 0.9507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2582, Accuracy: 0.9315, F1 Micro: 0.9576, F1 Macro: 0.9557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.212, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9617\n",
      "Epoch 10/10, Train Loss: 0.1809, Accuracy: 0.9405, F1 Micro: 0.9628, F1 Macro: 0.9608\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.92      0.97      0.95       175\n",
      "      others       0.91      0.92      0.91       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.563, Accuracy: 0.6864, F1 Micro: 0.6864, F1 Macro: 0.407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3978, Accuracy: 0.7839, F1 Micro: 0.7839, F1 Macro: 0.6833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2988, Accuracy: 0.8602, F1 Micro: 0.8602, F1 Macro: 0.8288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1879, Accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1399, Accuracy: 0.9068, F1 Micro: 0.9068, F1 Macro: 0.8953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9054\n",
      "Epoch 7/10, Train Loss: 0.067, Accuracy: 0.8983, F1 Micro: 0.8983, F1 Macro: 0.8827\n",
      "Epoch 8/10, Train Loss: 0.06, Accuracy: 0.9153, F1 Micro: 0.9153, F1 Macro: 0.9049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.072, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9149\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.911, F1 Micro: 0.911, F1 Macro: 0.8978\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.96      0.89        74\n",
      "    positive       0.98      0.91      0.94       162\n",
      "\n",
      "    accuracy                           0.92       236\n",
      "   macro avg       0.90      0.93      0.91       236\n",
      "weighted avg       0.93      0.92      0.93       236\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.8633\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.92      0.98      0.95       167\n",
      "    positive       0.91      0.61      0.73        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.82      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.83      0.69        12\n",
      "     neutral       0.91      0.92      0.92       152\n",
      "    positive       0.76      0.65      0.70        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.75      0.80      0.77       216\n",
      "weighted avg       0.85      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.74      0.77        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.96      0.81      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 88.16145467758179 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6018, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Epoch 2/10, Train Loss: 0.5339, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.501, Accuracy: 0.7932, F1 Micro: 0.8841, F1 Macro: 0.8825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4667, Accuracy: 0.8095, F1 Micro: 0.8909, F1 Macro: 0.8886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4342, Accuracy: 0.8333, F1 Micro: 0.9039, F1 Macro: 0.9025\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3749, Accuracy: 0.8653, F1 Micro: 0.9192, F1 Macro: 0.9171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3253, Accuracy: 0.8973, F1 Micro: 0.9372, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2747, Accuracy: 0.9167, F1 Micro: 0.9489, F1 Macro: 0.947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2254, Accuracy: 0.9249, F1 Micro: 0.9531, F1 Macro: 0.9501\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1886, Accuracy: 0.9412, F1 Micro: 0.9636, F1 Macro: 0.962\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9412, F1 Micro: 0.9636, F1 Macro: 0.962\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.98       187\n",
      "     machine       0.93      0.98      0.95       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.91      0.98      0.95       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5872, Accuracy: 0.677, F1 Micro: 0.677, F1 Macro: 0.4037\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4279, Accuracy: 0.6903, F1 Micro: 0.6903, F1 Macro: 0.4464\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3657, Accuracy: 0.8717, F1 Micro: 0.8717, F1 Macro: 0.8481\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1752, Accuracy: 0.8982, F1 Micro: 0.8982, F1 Macro: 0.8878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.9002\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0458, Accuracy: 0.9292, F1 Micro: 0.9292, F1 Macro: 0.9226\n",
      "Epoch 7/10, Train Loss: 0.0776, Accuracy: 0.9159, F1 Micro: 0.9159, F1 Macro: 0.9049\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9071, F1 Micro: 0.9071, F1 Macro: 0.8969\n",
      "Epoch 9/10, Train Loss: 0.1184, Accuracy: 0.9204, F1 Micro: 0.9204, F1 Macro: 0.9134\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9159, F1 Micro: 0.9159, F1 Macro: 0.9035\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9292, F1 Micro: 0.9292, F1 Macro: 0.9226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.99      0.90        73\n",
      "    positive       0.99      0.90      0.95       153\n",
      "\n",
      "    accuracy                           0.93       226\n",
      "   macro avg       0.91      0.94      0.92       226\n",
      "weighted avg       0.94      0.93      0.93       226\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.861\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.97      1.00      0.99       181\n",
      "    positive       1.00      0.83      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.95      0.64      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.75      0.64        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.80      0.63      0.71        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.76      0.78      0.76       216\n",
      "weighted avg       0.86      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.78      0.80        23\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.93      0.68      0.79        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.80      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.85      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 87.89974665641785 s\n",
      "Averaged - Iteration 208: Accuracy: 0.872, F1 Micro: 0.872, F1 Macro: 0.6413\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 14.396318197250366 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5934, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5164, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4675, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3959, Accuracy: 0.8661, F1 Micro: 0.9208, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3357, Accuracy: 0.9174, F1 Micro: 0.9498, F1 Macro: 0.9488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2822, Accuracy: 0.936, F1 Micro: 0.9605, F1 Macro: 0.9592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2362, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1908, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9684\n",
      "Epoch 9/10, Train Loss: 0.159, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1238, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9708\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.88      0.97      0.92       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5329, Accuracy: 0.6777, F1 Micro: 0.6777, F1 Macro: 0.4039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4667, Accuracy: 0.876, F1 Micro: 0.876, F1 Macro: 0.8551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2205, Accuracy: 0.905, F1 Micro: 0.905, F1 Macro: 0.8909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1218, Accuracy: 0.9215, F1 Micro: 0.9215, F1 Macro: 0.9131\n",
      "Epoch 5/10, Train Loss: 0.1076, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9072\n",
      "Epoch 6/10, Train Loss: 0.0767, Accuracy: 0.905, F1 Micro: 0.905, F1 Macro: 0.8901\n",
      "Epoch 7/10, Train Loss: 0.0979, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9008\n",
      "Epoch 8/10, Train Loss: 0.0986, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8887\n",
      "Epoch 9/10, Train Loss: 0.0744, Accuracy: 0.8967, F1 Micro: 0.8967, F1 Macro: 0.8837\n",
      "Epoch 10/10, Train Loss: 0.0558, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9088\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9215, F1 Micro: 0.9215, F1 Macro: 0.9131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.95      0.89        78\n",
      "    positive       0.97      0.91      0.94       164\n",
      "\n",
      "    accuracy                           0.92       242\n",
      "   macro avg       0.90      0.93      0.91       242\n",
      "weighted avg       0.93      0.92      0.92       242\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.8814\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.67      0.64        12\n",
      "     neutral       0.88      0.98      0.93       152\n",
      "    positive       0.91      0.60      0.72        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.80      0.75      0.76       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 92.81863141059875 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5852, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.516, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4752, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4237, Accuracy: 0.8497, F1 Micro: 0.9127, F1 Macro: 0.9115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3583, Accuracy: 0.9107, F1 Micro: 0.9462, F1 Macro: 0.9447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2967, Accuracy: 0.9368, F1 Micro: 0.9608, F1 Macro: 0.9594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2367, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1898, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9677\n",
      "Epoch 9/10, Train Loss: 0.154, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1238, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9691\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.88      0.97      0.92       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5223, Accuracy: 0.6609, F1 Micro: 0.6609, F1 Macro: 0.3979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4109, Accuracy: 0.8627, F1 Micro: 0.8627, F1 Macro: 0.8416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.248, Accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9211\n",
      "Epoch 4/10, Train Loss: 0.1418, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9063\n",
      "Epoch 5/10, Train Loss: 0.1024, Accuracy: 0.8841, F1 Micro: 0.8841, F1 Macro: 0.8785\n",
      "Epoch 6/10, Train Loss: 0.0853, Accuracy: 0.897, F1 Micro: 0.897, F1 Macro: 0.8785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0881, Accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9207\n",
      "Epoch 8/10, Train Loss: 0.0749, Accuracy: 0.8927, F1 Micro: 0.8927, F1 Macro: 0.8784\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9158\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9207\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.927, F1 Micro: 0.927, F1 Macro: 0.9207\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        79\n",
      "    positive       0.97      0.92      0.94       154\n",
      "\n",
      "    accuracy                           0.93       233\n",
      "   macro avg       0.91      0.93      0.92       233\n",
      "weighted avg       0.93      0.93      0.93       233\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.8841\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.94      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.88      0.98      0.93       152\n",
      "    positive       0.91      0.62      0.74        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.78      0.80       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 96.8340196609497 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.582, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5256, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4847, Accuracy: 0.7939, F1 Micro: 0.8844, F1 Macro: 0.8829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4336, Accuracy: 0.8266, F1 Micro: 0.9006, F1 Macro: 0.8993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3837, Accuracy: 0.8832, F1 Micro: 0.9306, F1 Macro: 0.9294\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3163, Accuracy: 0.9144, F1 Micro: 0.9474, F1 Macro: 0.9458\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2557, Accuracy: 0.9412, F1 Micro: 0.9635, F1 Macro: 0.9623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2007, Accuracy: 0.9457, F1 Micro: 0.9664, F1 Macro: 0.9649\n",
      "Epoch 9/10, Train Loss: 0.1603, Accuracy: 0.9457, F1 Micro: 0.966, F1 Macro: 0.9639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1269, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9713\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9713\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.88      0.98      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5263, Accuracy: 0.6552, F1 Micro: 0.6552, F1 Macro: 0.3958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.406, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.228, Accuracy: 0.9353, F1 Micro: 0.9353, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1395, Accuracy: 0.9397, F1 Micro: 0.9397, F1 Macro: 0.9343\n",
      "Epoch 5/10, Train Loss: 0.1225, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.9041\n",
      "Epoch 6/10, Train Loss: 0.0745, Accuracy: 0.9181, F1 Micro: 0.9181, F1 Macro: 0.9067\n",
      "Epoch 7/10, Train Loss: 0.1009, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.9045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1092, Accuracy: 0.9397, F1 Micro: 0.9397, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0636, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9389\n",
      "Epoch 10/10, Train Loss: 0.0404, Accuracy: 0.9397, F1 Micro: 0.9397, F1 Macro: 0.934\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        80\n",
      "    positive       0.97      0.94      0.96       152\n",
      "\n",
      "    accuracy                           0.94       232\n",
      "   macro avg       0.93      0.95      0.94       232\n",
      "weighted avg       0.95      0.94      0.94       232\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8971\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.88      0.98      0.93       152\n",
      "    positive       0.91      0.62      0.74        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.87      0.78      0.81       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.92      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 94.9171028137207 s\n",
      "Averaged - Iteration 274: Accuracy: 0.8902, F1 Micro: 0.8902, F1 Macro: 0.7028\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 13.666977643966675 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5754, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5022, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4414, Accuracy: 0.8311, F1 Micro: 0.9033, F1 Macro: 0.902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.362, Accuracy: 0.904, F1 Micro: 0.9414, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2981, Accuracy: 0.933, F1 Micro: 0.9588, F1 Macro: 0.9575\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2241, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1815, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.969\n",
      "Epoch 8/10, Train Loss: 0.1401, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.9674\n",
      "Epoch 9/10, Train Loss: 0.1217, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9679\n",
      "Epoch 10/10, Train Loss: 0.1015, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9677\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.88      0.98      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.576, Accuracy: 0.6681, F1 Micro: 0.6681, F1 Macro: 0.4005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3928, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2507, Accuracy: 0.9009, F1 Micro: 0.9009, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.2026, Accuracy: 0.8621, F1 Micro: 0.8621, F1 Macro: 0.8559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.178, Accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.916\n",
      "Epoch 6/10, Train Loss: 0.0855, Accuracy: 0.9181, F1 Micro: 0.9181, F1 Macro: 0.9111\n",
      "Epoch 7/10, Train Loss: 0.0945, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9141\n",
      "Epoch 8/10, Train Loss: 0.0855, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.9018\n",
      "Epoch 9/10, Train Loss: 0.055, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9131\n",
      "Epoch 10/10, Train Loss: 0.0559, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9151\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9267, F1 Micro: 0.9267, F1 Macro: 0.916\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        77\n",
      "    positive       0.93      0.96      0.95       155\n",
      "\n",
      "    accuracy                           0.93       232\n",
      "   macro avg       0.92      0.91      0.92       232\n",
      "weighted avg       0.93      0.93      0.93       232\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.8735\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.78      0.64      0.70        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.79      0.82       216\n",
      "weighted avg       0.90      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.88      0.99      0.93       152\n",
      "    positive       0.94      0.65      0.77        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.90      0.77      0.82       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.74      0.81        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 96.65173292160034 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5678, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5037, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4516, Accuracy: 0.811, F1 Micro: 0.8931, F1 Macro: 0.8917\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3797, Accuracy: 0.9107, F1 Micro: 0.946, F1 Macro: 0.945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3006, Accuracy: 0.9368, F1 Micro: 0.9609, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2245, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9669\n",
      "Epoch 7/10, Train Loss: 0.179, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.14, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9715\n",
      "Epoch 9/10, Train Loss: 0.1184, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.969\n",
      "Epoch 10/10, Train Loss: 0.1059, Accuracy: 0.9494, F1 Micro: 0.9681, F1 Macro: 0.9658\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5376, Accuracy: 0.6787, F1 Micro: 0.6787, F1 Macro: 0.4043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3707, Accuracy: 0.8956, F1 Micro: 0.8956, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2059, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9159\n",
      "Epoch 4/10, Train Loss: 0.1547, Accuracy: 0.9197, F1 Micro: 0.9197, F1 Macro: 0.9107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9277\n",
      "Epoch 6/10, Train Loss: 0.0884, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9163\n",
      "Epoch 7/10, Train Loss: 0.0649, Accuracy: 0.9116, F1 Micro: 0.9116, F1 Macro: 0.8966\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9247\n",
      "Epoch 9/10, Train Loss: 0.0402, Accuracy: 0.9197, F1 Micro: 0.9197, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0631, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9336\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.99      0.91        80\n",
      "    positive       0.99      0.92      0.95       169\n",
      "\n",
      "    accuracy                           0.94       249\n",
      "   macro avg       0.92      0.95      0.93       249\n",
      "weighted avg       0.95      0.94      0.94       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8996\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.94      0.86        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.93       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.82      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.87      0.83        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.97      0.68      0.80        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 104.25851345062256 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5762, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5117, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4731, Accuracy: 0.7954, F1 Micro: 0.8852, F1 Macro: 0.8836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4171, Accuracy: 0.8631, F1 Micro: 0.9197, F1 Macro: 0.919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3326, Accuracy: 0.9211, F1 Micro: 0.9514, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2499, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.9628\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1981, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1528, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9699\n",
      "Epoch 9/10, Train Loss: 0.1298, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.11, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9706\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5407, Accuracy: 0.6917, F1 Micro: 0.6917, F1 Macro: 0.4433\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3537, Accuracy: 0.8696, F1 Micro: 0.8696, F1 Macro: 0.8543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1791, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1579, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9119\n",
      "Epoch 5/10, Train Loss: 0.1191, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8966\n",
      "Epoch 6/10, Train Loss: 0.0957, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9078\n",
      "Epoch 7/10, Train Loss: 0.1024, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8984\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9031\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8966\n",
      "Epoch 10/10, Train Loss: 0.039, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9008\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.94      0.88        81\n",
      "    positive       0.97      0.91      0.94       172\n",
      "\n",
      "    accuracy                           0.92       253\n",
      "   macro avg       0.90      0.93      0.91       253\n",
      "weighted avg       0.93      0.92      0.92       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.887\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.83      0.69        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.80      0.67      0.73        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.77      0.81      0.78       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.91      0.76      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 104.08017778396606 s\n",
      "Averaged - Iteration 333: Accuracy: 0.901, F1 Micro: 0.901, F1 Macro: 0.7396\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 12.519469261169434 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5667, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4958, Accuracy: 0.8021, F1 Micro: 0.8886, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.418, Accuracy: 0.8862, F1 Micro: 0.932, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3198, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9515\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2483, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1896, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1559, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.113, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.104, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9714\n",
      "Epoch 10/10, Train Loss: 0.0828, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9693\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5635, Accuracy: 0.684, F1 Micro: 0.684, F1 Macro: 0.4062\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.348, Accuracy: 0.912, F1 Micro: 0.912, F1 Macro: 0.903\n",
      "Epoch 3/10, Train Loss: 0.2011, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.8978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1061, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9304\n",
      "Epoch 5/10, Train Loss: 0.1484, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9193\n",
      "Epoch 6/10, Train Loss: 0.0825, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9211\n",
      "Epoch 7/10, Train Loss: 0.0844, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9155\n",
      "Epoch 8/10, Train Loss: 0.1035, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.8901\n",
      "Epoch 9/10, Train Loss: 0.095, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9077\n",
      "Epoch 10/10, Train Loss: 0.0758, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9146\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.90      0.90        79\n",
      "    positive       0.95      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.93      0.93      0.93       250\n",
      "weighted avg       0.94      0.94      0.94       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8917\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.85      0.70      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.81      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.79      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.87      0.85        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 108.07517910003662 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5642, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5042, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4441, Accuracy: 0.8571, F1 Micro: 0.916, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3468, Accuracy: 0.9256, F1 Micro: 0.9543, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.263, Accuracy: 0.9449, F1 Micro: 0.9661, F1 Macro: 0.965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1957, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9691\n",
      "Epoch 7/10, Train Loss: 0.1607, Accuracy: 0.9509, F1 Micro: 0.9693, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1194, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9726\n",
      "Epoch 9/10, Train Loss: 0.1052, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "Epoch 10/10, Train Loss: 0.0858, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9715\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5857, Accuracy: 0.7148, F1 Micro: 0.7148, F1 Macro: 0.4835\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3461, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.8903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1945, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1696, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9259\n",
      "Epoch 5/10, Train Loss: 0.168, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1307, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "Epoch 7/10, Train Loss: 0.1183, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.882\n",
      "Epoch 8/10, Train Loss: 0.1013, Accuracy: 0.8935, F1 Micro: 0.8935, F1 Macro: 0.883\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9079\n",
      "Epoch 10/10, Train Loss: 0.0891, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.9015\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.90        81\n",
      "    positive       0.98      0.93      0.95       182\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.92      0.94      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8965\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.96      0.85        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.89      0.78      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.90      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 111.95077896118164 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5647, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5174, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4611, Accuracy: 0.8028, F1 Micro: 0.8889, F1 Macro: 0.8874\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3756, Accuracy: 0.9137, F1 Micro: 0.9473, F1 Macro: 0.946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2804, Accuracy: 0.9427, F1 Micro: 0.9645, F1 Macro: 0.9632\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2085, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1675, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9711\n",
      "Epoch 8/10, Train Loss: 0.1193, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1073, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Epoch 10/10, Train Loss: 0.0862, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9733\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5808, Accuracy: 0.6902, F1 Micro: 0.6902, F1 Macro: 0.4424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3317, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9118\n",
      "Epoch 3/10, Train Loss: 0.1943, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1293, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1344, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0907, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9435\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.8984\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "Epoch 10/10, Train Loss: 0.0688, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9146\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.99      0.93        82\n",
      "    positive       0.99      0.93      0.96       173\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.96      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9123\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.96      0.76      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 113.46409487724304 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.7664\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 12.428317785263062 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5671, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4952, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4192, Accuracy: 0.8973, F1 Micro: 0.9382, F1 Macro: 0.9375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3063, Accuracy: 0.936, F1 Micro: 0.9607, F1 Macro: 0.9594\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2298, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.968\n",
      "Epoch 6/10, Train Loss: 0.1774, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.14, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9714\n",
      "Epoch 8/10, Train Loss: 0.1095, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0991, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9716\n",
      "Epoch 10/10, Train Loss: 0.0773, Accuracy: 0.9524, F1 Micro: 0.97, F1 Macro: 0.968\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.90      0.95      0.92       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5258, Accuracy: 0.6887, F1 Micro: 0.6887, F1 Macro: 0.4078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2951, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9136\n",
      "Epoch 3/10, Train Loss: 0.194, Accuracy: 0.8949, F1 Micro: 0.8949, F1 Macro: 0.886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1328, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9231\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1074, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9259\n",
      "Epoch 7/10, Train Loss: 0.1193, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9214\n",
      "Epoch 8/10, Train Loss: 0.07, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0693, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9293\n",
      "Epoch 10/10, Train Loss: 0.0595, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9014\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9293\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.90        80\n",
      "    positive       0.98      0.93      0.95       177\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.92      0.94      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.8921\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.67      0.73        12\n",
      "     neutral       0.90      0.95      0.92       152\n",
      "    positive       0.83      0.73      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.78      0.81       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 116.14135432243347 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5616, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5061, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4356, Accuracy: 0.8735, F1 Micro: 0.9256, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.327, Accuracy: 0.9375, F1 Micro: 0.9615, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2381, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9676\n",
      "Epoch 6/10, Train Loss: 0.1786, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.139, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1145, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.101, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0771, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9707\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5296, Accuracy: 0.7308, F1 Micro: 0.7308, F1 Macro: 0.5667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2971, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9115\n",
      "Epoch 3/10, Train Loss: 0.186, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1349, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 5/10, Train Loss: 0.1121, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.8993\n",
      "Epoch 6/10, Train Loss: 0.1224, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9145\n",
      "Epoch 7/10, Train Loss: 0.1223, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.9035\n",
      "Epoch 8/10, Train Loss: 0.0776, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9182\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.8972\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9235\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9157\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 113.95492243766785 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5617, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5187, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.467, Accuracy: 0.8348, F1 Micro: 0.9041, F1 Macro: 0.9032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3588, Accuracy: 0.9219, F1 Micro: 0.952, F1 Macro: 0.9504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2617, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1898, Accuracy: 0.9539, F1 Micro: 0.9715, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1483, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Epoch 8/10, Train Loss: 0.1161, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0983, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Epoch 10/10, Train Loss: 0.0793, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9733\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5351, Accuracy: 0.7043, F1 Micro: 0.7043, F1 Macro: 0.4969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2924, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9038\n",
      "Epoch 3/10, Train Loss: 0.1845, Accuracy: 0.9027, F1 Micro: 0.9027, F1 Macro: 0.8959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1467, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1209, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9339\n",
      "Epoch 6/10, Train Loss: 0.1317, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9073\n",
      "Epoch 7/10, Train Loss: 0.1286, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9194\n",
      "Epoch 8/10, Train Loss: 0.0893, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0905, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.935\n",
      "Epoch 10/10, Train Loss: 0.0659, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.91        84\n",
      "    positive       0.98      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.94      0.94       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9059\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 118.63199758529663 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.7861\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 10.630324363708496 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5619, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4848, Accuracy: 0.8013, F1 Micro: 0.8882, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3934, Accuracy: 0.91, F1 Micro: 0.9449, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2971, Accuracy: 0.939, F1 Micro: 0.9624, F1 Macro: 0.961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2166, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1663, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1367, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1096, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0899, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.0752, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9713\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.89      0.97      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5561, Accuracy: 0.6855, F1 Micro: 0.6855, F1 Macro: 0.4302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3016, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1531, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1501, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.928\n",
      "Epoch 5/10, Train Loss: 0.1536, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1216, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9247\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0945, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.928\n",
      "Epoch 9/10, Train Loss: 0.1049, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.9051\n",
      "Epoch 10/10, Train Loss: 0.0752, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.9029\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.90        80\n",
      "    positive       0.97      0.93      0.95       168\n",
      "\n",
      "    accuracy                           0.94       248\n",
      "   macro avg       0.92      0.94      0.93       248\n",
      "weighted avg       0.94      0.94      0.94       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.8975\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.67      0.70        12\n",
      "     neutral       0.89      0.97      0.93       152\n",
      "    positive       0.88      0.67      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.77      0.79       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.79      0.85        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 123.36099910736084 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5533, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4864, Accuracy: 0.7976, F1 Micro: 0.8864, F1 Macro: 0.8849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4048, Accuracy: 0.9055, F1 Micro: 0.9429, F1 Macro: 0.9421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2961, Accuracy: 0.9397, F1 Micro: 0.9628, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2169, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1643, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9719\n",
      "Epoch 7/10, Train Loss: 0.133, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1076, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0926, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9742\n",
      "Epoch 10/10, Train Loss: 0.0801, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9731\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5387, Accuracy: 0.8353, F1 Micro: 0.8353, F1 Macro: 0.7932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2874, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1752, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1003, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9414\n",
      "Epoch 5/10, Train Loss: 0.1091, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9324\n",
      "Epoch 6/10, Train Loss: 0.1223, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 7/10, Train Loss: 0.0936, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9328\n",
      "Epoch 8/10, Train Loss: 0.1117, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9332\n",
      "Epoch 9/10, Train Loss: 0.0765, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9178\n",
      "Epoch 10/10, Train Loss: 0.0563, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9248\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        81\n",
      "    positive       0.97      0.96      0.96       174\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.94      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9063\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.84      0.73      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 121.15520143508911 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5601, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5013, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4369, Accuracy: 0.869, F1 Micro: 0.9229, F1 Macro: 0.9222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3252, Accuracy: 0.9427, F1 Micro: 0.9647, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2331, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.171, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1374, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1121, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Epoch 9/10, Train Loss: 0.096, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Epoch 10/10, Train Loss: 0.079, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9705\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.88      0.97      0.92       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5569, Accuracy: 0.6763, F1 Micro: 0.6763, F1 Macro: 0.4155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3648, Accuracy: 0.9461, F1 Micro: 0.9461, F1 Macro: 0.9397\n",
      "Epoch 3/10, Train Loss: 0.1962, Accuracy: 0.9046, F1 Micro: 0.9046, F1 Macro: 0.8973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1505, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0799, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9585, F1 Micro: 0.9585, F1 Macro: 0.9532\n",
      "Epoch 7/10, Train Loss: 0.1052, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9269\n",
      "Epoch 8/10, Train Loss: 0.0684, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9353\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9295, F1 Micro: 0.9295, F1 Macro: 0.9221\n",
      "Epoch 10/10, Train Loss: 0.0471, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9269\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9585, F1 Micro: 0.9585, F1 Macro: 0.9532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        79\n",
      "    positive       0.97      0.96      0.97       162\n",
      "\n",
      "    accuracy                           0.96       241\n",
      "   macro avg       0.95      0.96      0.95       241\n",
      "weighted avg       0.96      0.96      0.96       241\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9061\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.88      0.97      0.93       152\n",
      "    positive       0.90      0.67      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.89      0.77      0.82       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.69      0.82        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 122.16586804389954 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9197, F1 Micro: 0.9197, F1 Macro: 0.8008\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 9.763554573059082 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5519, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4591, Accuracy: 0.8415, F1 Micro: 0.9088, F1 Macro: 0.9081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3403, Accuracy: 0.9219, F1 Micro: 0.9519, F1 Macro: 0.9506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.261, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1855, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1412, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1122, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Epoch 8/10, Train Loss: 0.0951, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9734\n",
      "Epoch 9/10, Train Loss: 0.0722, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9706\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.88      0.96      0.92       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5554, Accuracy: 0.836, F1 Micro: 0.836, F1 Macro: 0.7813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2621, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9104\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.163, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1276, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9361\n",
      "Epoch 5/10, Train Loss: 0.0975, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0654, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9348\n",
      "Epoch 7/10, Train Loss: 0.0555, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.924\n",
      "Epoch 8/10, Train Loss: 0.0571, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9265\n",
      "Epoch 9/10, Train Loss: 0.0696, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9183\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9072\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.90      0.91        79\n",
      "    positive       0.95      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.94      0.93      0.93       250\n",
      "weighted avg       0.94      0.94      0.94       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9022\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.88      0.96      0.92       152\n",
      "    positive       0.88      0.71      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.88      0.78      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.90      0.88      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 125.40600991249084 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5499, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.48, Accuracy: 0.808, F1 Micro: 0.8916, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3629, Accuracy: 0.9286, F1 Micro: 0.956, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2751, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1906, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1444, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Epoch 7/10, Train Loss: 0.1145, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Epoch 8/10, Train Loss: 0.0949, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.972\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9726\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5851, Accuracy: 0.7649, F1 Micro: 0.7649, F1 Macro: 0.6447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3231, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2122, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9402\n",
      "Epoch 4/10, Train Loss: 0.1496, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.931\n",
      "Epoch 5/10, Train Loss: 0.1342, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.937\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9335\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9125\n",
      "Epoch 8/10, Train Loss: 0.1097, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9323\n",
      "Epoch 9/10, Train Loss: 0.0717, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.897\n",
      "Epoch 10/10, Train Loss: 0.085, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9319\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.91      0.92        80\n",
      "    positive       0.96      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.94      0.94       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9063\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 121.17171049118042 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5534, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.4953, Accuracy: 0.7887, F1 Micro: 0.8818, F1 Macro: 0.8802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4009, Accuracy: 0.9033, F1 Micro: 0.9413, F1 Macro: 0.9396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3024, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1983, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1501, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1176, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Epoch 8/10, Train Loss: 0.0985, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9734\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.89      0.96      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5346, Accuracy: 0.7912, F1 Micro: 0.7912, F1 Macro: 0.7005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2767, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9498\n",
      "Epoch 3/10, Train Loss: 0.1843, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9421\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.9518, F1 Micro: 0.9518, F1 Macro: 0.9458\n",
      "Epoch 5/10, Train Loss: 0.1241, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9418\n",
      "Epoch 6/10, Train Loss: 0.1085, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0996, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0471, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0438, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9504\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9378\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.93        81\n",
      "    positive       0.98      0.95      0.97       168\n",
      "\n",
      "    accuracy                           0.96       249\n",
      "   macro avg       0.94      0.96      0.95       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9116\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.90      0.96      0.93       152\n",
      "    positive       0.86      0.71      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.89      0.81      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 124.55962443351746 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.8125\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 8.87943434715271 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5407, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4651, Accuracy: 0.8408, F1 Micro: 0.908, F1 Macro: 0.9078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3469, Accuracy: 0.9293, F1 Micro: 0.9566, F1 Macro: 0.9549\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.257, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1854, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1395, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9708\n",
      "Epoch 7/10, Train Loss: 0.1224, Accuracy: 0.9531, F1 Micro: 0.9703, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Epoch 10/10, Train Loss: 0.0686, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9703\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.89      0.96      0.92       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5101, Accuracy: 0.856, F1 Micro: 0.856, F1 Macro: 0.8129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2148, Accuracy: 0.904, F1 Micro: 0.904, F1 Macro: 0.8958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2172, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.171, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9244\n",
      "Epoch 5/10, Train Loss: 0.1329, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0969, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9401\n",
      "Epoch 7/10, Train Loss: 0.0805, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9265\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9287\n",
      "Epoch 9/10, Train Loss: 0.0697, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9167\n",
      "Epoch 10/10, Train Loss: 0.0844, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9123\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.91      0.92        80\n",
      "    positive       0.96      0.96      0.96       170\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.94      0.94      0.94       250\n",
      "weighted avg       0.95      0.95      0.95       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9049\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.89      0.97      0.93       152\n",
      "    positive       0.88      0.71      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.89      0.78      0.83       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.89366817474365 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5373, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4811, Accuracy: 0.8333, F1 Micro: 0.9044, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3638, Accuracy: 0.9286, F1 Micro: 0.9564, F1 Macro: 0.9552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2672, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9731\n",
      "Epoch 5/10, Train Loss: 0.1871, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9725\n",
      "Epoch 6/10, Train Loss: 0.1412, Accuracy: 0.9531, F1 Micro: 0.9705, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.122, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Epoch 8/10, Train Loss: 0.1007, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9736\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5297, Accuracy: 0.8566, F1 Micro: 0.8566, F1 Macro: 0.8232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2744, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.194, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9327\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1384, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9409\n",
      "Epoch 5/10, Train Loss: 0.1398, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9254\n",
      "Epoch 6/10, Train Loss: 0.1398, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9323\n",
      "Epoch 7/10, Train Loss: 0.1165, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9207\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0864, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9413\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9319\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        81\n",
      "    positive       0.97      0.95      0.96       170\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.95      0.94       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9079\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 127.37730312347412 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4978, Accuracy: 0.8162, F1 Micro: 0.8955, F1 Macro: 0.8941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4045, Accuracy: 0.8988, F1 Micro: 0.9393, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2974, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2029, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9708\n",
      "Epoch 6/10, Train Loss: 0.1511, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.129, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1021, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0848, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Epoch 10/10, Train Loss: 0.0704, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9747\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.89      0.98      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4787, Accuracy: 0.9024, F1 Micro: 0.9024, F1 Macro: 0.8896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2471, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9455\n",
      "Epoch 3/10, Train Loss: 0.1755, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9411\n",
      "Epoch 4/10, Train Loss: 0.1367, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.153, Accuracy: 0.9634, F1 Micro: 0.9634, F1 Macro: 0.9587\n",
      "Epoch 6/10, Train Loss: 0.0774, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9498\n",
      "Epoch 7/10, Train Loss: 0.0657, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9455\n",
      "Epoch 8/10, Train Loss: 0.0491, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9451\n",
      "Epoch 9/10, Train Loss: 0.0472, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9404\n",
      "Epoch 10/10, Train Loss: 0.0646, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9486\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9634, F1 Micro: 0.9634, F1 Macro: 0.9587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.96      0.94        80\n",
      "    positive       0.98      0.96      0.97       166\n",
      "\n",
      "    accuracy                           0.96       246\n",
      "   macro avg       0.95      0.96      0.96       246\n",
      "weighted avg       0.96      0.96      0.96       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9177\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.89      0.98      0.93       152\n",
      "    positive       0.93      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.79      0.83       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 133.83977627754211 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.8223\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 8.190579414367676 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5433, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.451, Accuracy: 0.8452, F1 Micro: 0.9107, F1 Macro: 0.9099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3238, Accuracy: 0.9308, F1 Micro: 0.9572, F1 Macro: 0.9556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2383, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1772, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Epoch 6/10, Train Loss: 0.1371, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.106, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Epoch 8/10, Train Loss: 0.0954, Accuracy: 0.9561, F1 Micro: 0.9722, F1 Macro: 0.97\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9704\n",
      "Epoch 10/10, Train Loss: 0.0629, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.97\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5161, Accuracy: 0.8462, F1 Micro: 0.8462, F1 Macro: 0.7954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2523, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9278\n",
      "Epoch 3/10, Train Loss: 0.1837, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.9076\n",
      "Epoch 4/10, Train Loss: 0.1344, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9231\n",
      "Epoch 5/10, Train Loss: 0.1052, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9231\n",
      "Epoch 6/10, Train Loss: 0.0902, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1473, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9317\n",
      "Epoch 8/10, Train Loss: 0.0879, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.9034\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9401\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        80\n",
      "    positive       0.96      0.96      0.96       167\n",
      "\n",
      "    accuracy                           0.95       247\n",
      "   macro avg       0.94      0.94      0.94       247\n",
      "weighted avg       0.95      0.95      0.95       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9042\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.79      0.83       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 130.09674167633057 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5456, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4733, Accuracy: 0.808, F1 Micro: 0.8916, F1 Macro: 0.8902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3478, Accuracy: 0.9301, F1 Micro: 0.9568, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2477, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1826, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.973\n",
      "Epoch 6/10, Train Loss: 0.1408, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Epoch 7/10, Train Loss: 0.1036, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0936, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0633, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5369, Accuracy: 0.864, F1 Micro: 0.864, F1 Macro: 0.8381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.24, Accuracy: 0.912, F1 Micro: 0.912, F1 Macro: 0.9035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2045, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1553, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9505\n",
      "Epoch 5/10, Train Loss: 0.146, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1244, Accuracy: 0.96, F1 Micro: 0.96, F1 Macro: 0.9549\n",
      "Epoch 7/10, Train Loss: 0.1134, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9462\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9304\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9462\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9288\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.96, F1 Micro: 0.96, F1 Macro: 0.9549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.97      0.94        80\n",
      "    positive       0.99      0.95      0.97       170\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.95      0.96      0.95       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9164\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 133.03003025054932 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.546, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4854, Accuracy: 0.8043, F1 Micro: 0.8894, F1 Macro: 0.8879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3833, Accuracy: 0.9249, F1 Micro: 0.954, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2663, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1919, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1455, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1067, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9748\n",
      "Epoch 8/10, Train Loss: 0.0964, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0749, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 10/10, Train Loss: 0.066, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.99      0.97      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.504, Accuracy: 0.8919, F1 Micro: 0.8919, F1 Macro: 0.8743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2255, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9468\n",
      "Epoch 3/10, Train Loss: 0.1727, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9217\n",
      "Epoch 4/10, Train Loss: 0.1632, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.93\n",
      "Epoch 5/10, Train Loss: 0.1386, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "Epoch 6/10, Train Loss: 0.0815, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9185\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "Epoch 9/10, Train Loss: 0.0822, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9387\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        81\n",
      "    positive       0.98      0.96      0.97       178\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.95      0.95       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9158\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.81      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.99      0.97      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.95      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 132.26962995529175 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9292, F1 Micro: 0.9292, F1 Macro: 0.8305\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 7.6456193923950195 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5372, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4456, Accuracy: 0.8884, F1 Micro: 0.9329, F1 Macro: 0.9315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3104, Accuracy: 0.9412, F1 Micro: 0.9636, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2327, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1733, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1355, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Epoch 8/10, Train Loss: 0.0899, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.972\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5106, Accuracy: 0.8919, F1 Micro: 0.8919, F1 Macro: 0.8808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2435, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1954, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 4/10, Train Loss: 0.1222, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9295\n",
      "Epoch 5/10, Train Loss: 0.093, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9099\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9185\n",
      "Epoch 8/10, Train Loss: 0.0588, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9189\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.93\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8929\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        82\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.93      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.911\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.84      0.73      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.81      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 137.19292545318604 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5416, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4712, Accuracy: 0.8557, F1 Micro: 0.9159, F1 Macro: 0.9147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3316, Accuracy: 0.936, F1 Micro: 0.9602, F1 Macro: 0.9589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2382, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1722, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9716\n",
      "Epoch 6/10, Train Loss: 0.1401, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1079, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9747\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.9561, F1 Micro: 0.9722, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "Epoch 10/10, Train Loss: 0.0628, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9764\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.99      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4894, Accuracy: 0.8911, F1 Micro: 0.8911, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2471, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1845, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "Epoch 4/10, Train Loss: 0.128, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9024\n",
      "Epoch 5/10, Train Loss: 0.1034, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9255\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9385\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9293\n",
      "Epoch 8/10, Train Loss: 0.0739, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9194\n",
      "Epoch 9/10, Train Loss: 0.0416, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0346, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9388\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        82\n",
      "    positive       0.98      0.94      0.96       175\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.93      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9165\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.99      0.98      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 139.4258861541748 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5431, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4826, Accuracy: 0.8229, F1 Micro: 0.8977, F1 Macro: 0.8959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3589, Accuracy: 0.9286, F1 Micro: 0.9558, F1 Macro: 0.9545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2572, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1849, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9708\n",
      "Epoch 6/10, Train Loss: 0.1412, Accuracy: 0.9524, F1 Micro: 0.97, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0929, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 10/10, Train Loss: 0.0617, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.483, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.8907\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2497, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9386\n",
      "Epoch 3/10, Train Loss: 0.1226, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9141\n",
      "Epoch 4/10, Train Loss: 0.1528, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1077, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "Epoch 6/10, Train Loss: 0.0937, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9319\n",
      "Epoch 7/10, Train Loss: 0.1192, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 8/10, Train Loss: 0.0885, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9132\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9113\n",
      "Epoch 10/10, Train Loss: 0.0625, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.907\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        82\n",
      "    positive       0.97      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.94      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9135\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 138.17120265960693 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.8374\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 6.977647304534912 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.54, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4325, Accuracy: 0.8988, F1 Micro: 0.9387, F1 Macro: 0.9375\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3125, Accuracy: 0.9405, F1 Micro: 0.9632, F1 Macro: 0.9618\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2217, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9726\n",
      "Epoch 5/10, Train Loss: 0.1617, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1224, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Epoch 7/10, Train Loss: 0.0988, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0821, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 10/10, Train Loss: 0.061, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.479, Accuracy: 0.8923, F1 Micro: 0.8923, F1 Macro: 0.8745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2567, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9297\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1684, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "Epoch 4/10, Train Loss: 0.1371, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.944\n",
      "Epoch 5/10, Train Loss: 0.1325, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9381\n",
      "Epoch 7/10, Train Loss: 0.0999, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9235\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0774, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "Epoch 9/10, Train Loss: 0.0673, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "Epoch 10/10, Train Loss: 0.0575, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.95      0.95       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9204\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.95895504951477 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5355, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4478, Accuracy: 0.8728, F1 Micro: 0.925, F1 Macro: 0.9238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3318, Accuracy: 0.9405, F1 Micro: 0.9634, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.228, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9715\n",
      "Epoch 5/10, Train Loss: 0.1634, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1272, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0995, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9734\n",
      "Epoch 8/10, Train Loss: 0.0818, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9754\n",
      "Epoch 9/10, Train Loss: 0.0728, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Epoch 10/10, Train Loss: 0.0635, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5535, Accuracy: 0.8699, F1 Micro: 0.8699, F1 Macro: 0.8374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2536, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9404\n",
      "Epoch 3/10, Train Loss: 0.1787, Accuracy: 0.9309, F1 Micro: 0.9309, F1 Macro: 0.9242\n",
      "Epoch 4/10, Train Loss: 0.1347, Accuracy: 0.9146, F1 Micro: 0.9146, F1 Macro: 0.9078\n",
      "Epoch 5/10, Train Loss: 0.1561, Accuracy: 0.935, F1 Micro: 0.935, F1 Macro: 0.9289\n",
      "Epoch 6/10, Train Loss: 0.0851, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9367\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9328\n",
      "Epoch 8/10, Train Loss: 0.1077, Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.9371\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0658, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9498\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9451\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        81\n",
      "    positive       0.98      0.96      0.97       165\n",
      "\n",
      "    accuracy                           0.96       246\n",
      "   macro avg       0.95      0.95      0.95       246\n",
      "weighted avg       0.96      0.96      0.96       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9155\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.82      0.85       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      1.00      0.99       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 136.6632423400879 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5401, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4711, Accuracy: 0.8207, F1 Micro: 0.8976, F1 Macro: 0.8964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3708, Accuracy: 0.9189, F1 Micro: 0.9498, F1 Macro: 0.9472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2504, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1734, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1045, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0848, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9752\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0638, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.94      0.92       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4832, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.8854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2492, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 3/10, Train Loss: 0.1722, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "Epoch 4/10, Train Loss: 0.1797, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1262, Accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9525\n",
      "Epoch 6/10, Train Loss: 0.1274, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8959\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9244\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "Epoch 10/10, Train Loss: 0.0338, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9005\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9575, F1 Micro: 0.9575, F1 Macro: 0.9525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        84\n",
      "    positive       0.99      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.96       259\n",
      "   macro avg       0.94      0.96      0.95       259\n",
      "weighted avg       0.96      0.96      0.96       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9213\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.82      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 147.800518989563 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9334, F1 Micro: 0.9334, F1 Macro: 0.8437\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 6.3132476806640625 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5432, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4284, Accuracy: 0.9077, F1 Micro: 0.9437, F1 Macro: 0.9425\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2962, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9662\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2024, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1548, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Epoch 6/10, Train Loss: 0.1168, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0926, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0772, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 10/10, Train Loss: 0.0543, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5484, Accuracy: 0.8863, F1 Micro: 0.8863, F1 Macro: 0.8701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2719, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Epoch 3/10, Train Loss: 0.1687, Accuracy: 0.9059, F1 Micro: 0.9059, F1 Macro: 0.8975\n",
      "Epoch 4/10, Train Loss: 0.1628, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9206\n",
      "Epoch 5/10, Train Loss: 0.1367, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9201\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9302\n",
      "Epoch 7/10, Train Loss: 0.1243, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9215\n",
      "Epoch 8/10, Train Loss: 0.0947, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9302\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9228\n",
      "Epoch 10/10, Train Loss: 0.0509, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9118\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        82\n",
      "    positive       0.99      0.93      0.96       173\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9176\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.82      0.82       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.94558811187744 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5397, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4449, Accuracy: 0.8936, F1 Micro: 0.9366, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.313, Accuracy: 0.9435, F1 Micro: 0.965, F1 Macro: 0.9633\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.205, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1212, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0944, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "Epoch 8/10, Train Loss: 0.0781, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9792\n",
      "Epoch 10/10, Train Loss: 0.0535, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9788\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.94      0.94       158\n",
      "        part       0.99      0.99      0.99       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5469, Accuracy: 0.8815, F1 Micro: 0.8815, F1 Macro: 0.8643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2624, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1885, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "Epoch 4/10, Train Loss: 0.1493, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9056\n",
      "Epoch 5/10, Train Loss: 0.1422, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9273\n",
      "Epoch 6/10, Train Loss: 0.1107, Accuracy: 0.9185, F1 Micro: 0.9185, F1 Macro: 0.9062\n",
      "Epoch 7/10, Train Loss: 0.0938, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0692, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9366\n",
      "Epoch 9/10, Train Loss: 0.0751, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9326\n",
      "Epoch 10/10, Train Loss: 0.0581, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.929\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        86\n",
      "    positive       0.97      0.95      0.96       184\n",
      "\n",
      "    accuracy                           0.94       270\n",
      "   macro avg       0.93      0.94      0.94       270\n",
      "weighted avg       0.95      0.94      0.94       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9218\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.94      0.95       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.89      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.99      0.99      0.99       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.84      0.94      0.89        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.95      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 150.9273808002472 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5458, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4789, Accuracy: 0.8251, F1 Micro: 0.9, F1 Macro: 0.8989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3512, Accuracy: 0.9286, F1 Micro: 0.9556, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2265, Accuracy: 0.9405, F1 Micro: 0.9626, F1 Macro: 0.9591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1664, Accuracy: 0.9635, F1 Micro: 0.9773, F1 Macro: 0.9763\n",
      "Epoch 6/10, Train Loss: 0.1222, Accuracy: 0.9568, F1 Micro: 0.9726, F1 Macro: 0.9698\n",
      "Epoch 7/10, Train Loss: 0.0954, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0803, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 9/10, Train Loss: 0.0655, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4903, Accuracy: 0.891, F1 Micro: 0.891, F1 Macro: 0.8726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2188, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9325\n",
      "Epoch 3/10, Train Loss: 0.1728, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9037\n",
      "Epoch 4/10, Train Loss: 0.1398, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1663, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9304\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9236\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Epoch 8/10, Train Loss: 0.0904, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0767, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0564, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9361\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        86\n",
      "    positive       0.97      0.95      0.96       180\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.93      0.94      0.94       266\n",
      "weighted avg       0.94      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9183\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 149.540869474411 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.8491\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.92437481880188 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5347, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4194, Accuracy: 0.9003, F1 Micro: 0.9385, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2776, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1926, Accuracy: 0.9606, F1 Micro: 0.9755, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.143, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.0554, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5242, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8647\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2172, Accuracy: 0.9015, F1 Micro: 0.9015, F1 Macro: 0.8941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1835, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9333\n",
      "Epoch 4/10, Train Loss: 0.1505, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9363\n",
      "Epoch 6/10, Train Loss: 0.1268, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "Epoch 7/10, Train Loss: 0.1018, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.08, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9373\n",
      "Epoch 9/10, Train Loss: 0.0586, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0454, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9407\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.93      0.95      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9228\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.701003074646 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5386, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4393, Accuracy: 0.9077, F1 Micro: 0.9442, F1 Macro: 0.9426\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.295, Accuracy: 0.9501, F1 Micro: 0.9692, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1981, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1434, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1135, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9764\n",
      "Epoch 7/10, Train Loss: 0.09, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "Epoch 9/10, Train Loss: 0.062, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0553, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.92      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5303, Accuracy: 0.8926, F1 Micro: 0.8926, F1 Macro: 0.8795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2579, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1812, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1676, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9326\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1317, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1165, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1082, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0708, Accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.945\n",
      "Epoch 9/10, Train Loss: 0.066, Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.9454\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9519, F1 Micro: 0.9519, F1 Macro: 0.9454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.96      0.96       183\n",
      "\n",
      "    accuracy                           0.95       270\n",
      "   macro avg       0.94      0.95      0.95       270\n",
      "weighted avg       0.95      0.95      0.95       270\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9273\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.92      0.94       152\n",
      "    positive       0.77      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.87      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 159.72581577301025 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.545, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4716, Accuracy: 0.8512, F1 Micro: 0.9134, F1 Macro: 0.9127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3294, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9631\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2146, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1518, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1173, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 9/10, Train Loss: 0.0651, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9759\n",
      "Epoch 10/10, Train Loss: 0.0582, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4973, Accuracy: 0.8992, F1 Micro: 0.8992, F1 Macro: 0.8838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2666, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1804, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9403\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1041, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1091, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0594, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        85\n",
      "    positive       0.98      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.96      0.95       258\n",
      "weighted avg       0.96      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.926\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 153.99832153320312 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.8542\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.496435880661011 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5351, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.408, Accuracy: 0.9025, F1 Micro: 0.9399, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2889, Accuracy: 0.9449, F1 Micro: 0.9659, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2017, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9755\n",
      "Epoch 5/10, Train Loss: 0.1482, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9719\n",
      "Epoch 6/10, Train Loss: 0.1141, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Epoch 8/10, Train Loss: 0.0787, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.974\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.9583, F1 Micro: 0.9735, F1 Macro: 0.9706\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9728\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.502, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2613, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9239\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1741, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1404, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9382\n",
      "Epoch 5/10, Train Loss: 0.1538, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9136\n",
      "Epoch 6/10, Train Loss: 0.0936, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9256\n",
      "Epoch 10/10, Train Loss: 0.0405, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.934\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        84\n",
      "    positive       0.97      0.95      0.96       168\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.94      0.95      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9183\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.2189393043518 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5381, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4278, Accuracy: 0.9107, F1 Micro: 0.9458, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3002, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2068, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9753\n",
      "Epoch 5/10, Train Loss: 0.1526, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9713\n",
      "Epoch 6/10, Train Loss: 0.118, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0959, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 9/10, Train Loss: 0.0648, Accuracy: 0.9635, F1 Micro: 0.9768, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5175, Accuracy: 0.8872, F1 Micro: 0.8872, F1 Macro: 0.8772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2539, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.182, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1397, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9204\n",
      "Epoch 5/10, Train Loss: 0.1466, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.8996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1172, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9244\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1196, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9248\n",
      "Epoch 8/10, Train Loss: 0.0779, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9165\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9095\n",
      "Epoch 10/10, Train Loss: 0.0709, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9173\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        86\n",
      "    positive       0.98      0.92      0.95       180\n",
      "\n",
      "    accuracy                           0.93       266\n",
      "   macro avg       0.92      0.94      0.92       266\n",
      "weighted avg       0.94      0.93      0.93       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9142\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.81      0.83      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 156.47761988639832 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5386, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4603, Accuracy: 0.8467, F1 Micro: 0.9108, F1 Macro: 0.9101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3341, Accuracy: 0.9412, F1 Micro: 0.9635, F1 Macro: 0.9615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2269, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1597, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "Epoch 6/10, Train Loss: 0.1237, Accuracy: 0.9568, F1 Micro: 0.9726, F1 Macro: 0.97\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0984, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0835, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0627, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.97      0.91      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5085, Accuracy: 0.9036, F1 Micro: 0.9036, F1 Macro: 0.8922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.9143, F1 Micro: 0.9143, F1 Macro: 0.9058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1909, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9352\n",
      "Epoch 4/10, Train Loss: 0.1277, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.917\n",
      "Epoch 5/10, Train Loss: 0.1339, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.931\n",
      "Epoch 6/10, Train Loss: 0.1393, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9294\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9233\n",
      "Epoch 8/10, Train Loss: 0.0631, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9199\n",
      "Epoch 9/10, Train Loss: 0.0485, Accuracy: 0.9214, F1 Micro: 0.9214, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9333\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.9333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.90      0.91        88\n",
      "    positive       0.95      0.96      0.96       192\n",
      "\n",
      "    accuracy                           0.94       280\n",
      "   macro avg       0.94      0.93      0.93       280\n",
      "weighted avg       0.94      0.94      0.94       280\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9206\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.62      0.74        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.75      0.82      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.81      0.83       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.97      0.91      0.94       152\n",
      "    positive       0.77      0.90      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.92      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.60641503334045 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9381, F1 Micro: 0.9381, F1 Macro: 0.8581\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.72658371925354 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5322, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4002, Accuracy: 0.907, F1 Micro: 0.942, F1 Macro: 0.9399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2841, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1947, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1491, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1131, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0702, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5172, Accuracy: 0.8851, F1 Micro: 0.8851, F1 Macro: 0.8753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2255, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1455, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9102\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1265, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1087, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9224\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9086\n",
      "Epoch 7/10, Train Loss: 0.0845, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9205\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0893, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Epoch 10/10, Train Loss: 0.0957, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9273\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        85\n",
      "    positive       0.97      0.94      0.95       176\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.93      0.94      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9144\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 165.07732248306274 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4222, Accuracy: 0.9137, F1 Micro: 0.9466, F1 Macro: 0.9447\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2912, Accuracy: 0.9464, F1 Micro: 0.9669, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.197, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1473, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 6/10, Train Loss: 0.1068, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0872, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0714, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0606, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.0525, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5431, Accuracy: 0.8664, F1 Micro: 0.8664, F1 Macro: 0.8359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2841, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "Epoch 3/10, Train Loss: 0.1741, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.164, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9308\n",
      "Epoch 6/10, Train Loss: 0.1039, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9154\n",
      "Epoch 7/10, Train Loss: 0.0913, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9194\n",
      "Epoch 8/10, Train Loss: 0.0946, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9262\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9194\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.077, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        85\n",
      "    positive       0.97      0.94      0.95       177\n",
      "\n",
      "    accuracy                           0.94       262\n",
      "   macro avg       0.93      0.94      0.93       262\n",
      "weighted avg       0.94      0.94      0.94       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9164\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 161.4758014678955 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5426, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4523, Accuracy: 0.8698, F1 Micro: 0.9229, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3294, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2142, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9661\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1536, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 7/10, Train Loss: 0.0956, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0622, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.0525, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4579, Accuracy: 0.896, F1 Micro: 0.896, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.243, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1485, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Epoch 4/10, Train Loss: 0.1427, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9326\n",
      "Epoch 5/10, Train Loss: 0.0971, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1114, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9462\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.938\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.938\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9265\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.97       168\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.94      0.96      0.95       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9219\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 153.1806583404541 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.8616\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.189330101013184 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.526, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3975, Accuracy: 0.9196, F1 Micro: 0.95, F1 Macro: 0.9472\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.264, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1892, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1358, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.078, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.98\n",
      "Epoch 8/10, Train Loss: 0.0678, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9764\n",
      "Epoch 9/10, Train Loss: 0.0575, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9765\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.97      1.00      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.505, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2356, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1467, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "Epoch 4/10, Train Loss: 0.1502, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "Epoch 6/10, Train Loss: 0.1191, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9398\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 10/10, Train Loss: 0.0591, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        85\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.93      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9191\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.83      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      1.00      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 157.3951427936554 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5254, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4079, Accuracy: 0.9204, F1 Micro: 0.951, F1 Macro: 0.9488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2705, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1891, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1327, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1113, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9796\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0502, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.94      0.97      0.96       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4746, Accuracy: 0.9019, F1 Micro: 0.9019, F1 Macro: 0.8948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.235, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2191, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.938\n",
      "Epoch 4/10, Train Loss: 0.1542, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1089, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9374\n",
      "Epoch 6/10, Train Loss: 0.097, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9294\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9291\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9219\n",
      "Epoch 9/10, Train Loss: 0.0694, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9215\n",
      "Epoch 10/10, Train Loss: 0.0548, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9294\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        88\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.93      0.95      0.94       265\n",
      "weighted avg       0.95      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9294\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.97      0.96       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.90      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 167.9260733127594 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5267, Accuracy: 0.7887, F1 Micro: 0.8819, F1 Macro: 0.8803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4417, Accuracy: 0.8876, F1 Micro: 0.9326, F1 Macro: 0.9306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2981, Accuracy: 0.9494, F1 Micro: 0.9687, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2049, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1362, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1184, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0809, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "Epoch 8/10, Train Loss: 0.067, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 9/10, Train Loss: 0.0593, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0484, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.979\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4736, Accuracy: 0.8538, F1 Micro: 0.8538, F1 Macro: 0.8478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2448, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1686, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "Epoch 4/10, Train Loss: 0.1395, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9243\n",
      "Epoch 5/10, Train Loss: 0.1427, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9199\n",
      "Epoch 6/10, Train Loss: 0.1079, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Epoch 7/10, Train Loss: 0.0765, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0622, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "Epoch 9/10, Train Loss: 0.0391, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9243\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9358\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9256\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.95      0.95       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 157.382493019104 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9406, F1 Micro: 0.9406, F1 Macro: 0.8651\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.9252140522003174 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5329, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4044, Accuracy: 0.9211, F1 Micro: 0.9511, F1 Macro: 0.9488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2759, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1867, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1359, Accuracy: 0.9702, F1 Micro: 0.9814, F1 Macro: 0.9804\n",
      "Epoch 6/10, Train Loss: 0.1121, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0862, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.9643, F1 Micro: 0.9773, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.049, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5028, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2427, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "Epoch 3/10, Train Loss: 0.1591, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9356\n",
      "Epoch 4/10, Train Loss: 0.1529, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9119\n",
      "Epoch 5/10, Train Loss: 0.139, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1098, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.086, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Epoch 8/10, Train Loss: 0.0942, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.94\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9114\n",
      "Epoch 10/10, Train Loss: 0.068, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9236\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       170\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.928\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.88      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 156.4058141708374 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5335, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4145, Accuracy: 0.9144, F1 Micro: 0.9478, F1 Macro: 0.9463\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.279, Accuracy: 0.9561, F1 Micro: 0.9728, F1 Macro: 0.9717\n",
      "Epoch 4/10, Train Loss: 0.1847, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.14, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9777\n",
      "Epoch 6/10, Train Loss: 0.1146, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9761\n",
      "Epoch 7/10, Train Loss: 0.0834, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.98\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4837, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.9035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2239, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Epoch 3/10, Train Loss: 0.1655, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1469, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.138, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "Epoch 6/10, Train Loss: 0.1382, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9391\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9067\n",
      "Epoch 8/10, Train Loss: 0.0695, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.938\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9311\n",
      "Epoch 10/10, Train Loss: 0.0563, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9124\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.924\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 159.61302399635315 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5391, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4514, Accuracy: 0.8743, F1 Micro: 0.9247, F1 Macro: 0.9236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3165, Accuracy: 0.9494, F1 Micro: 0.9687, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1978, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "Epoch 8/10, Train Loss: 0.0698, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "Epoch 10/10, Train Loss: 0.0496, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5261, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.8953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2556, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1836, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1298, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.138, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.944\n",
      "Epoch 6/10, Train Loss: 0.0931, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Epoch 7/10, Train Loss: 0.0669, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9172\n",
      "Epoch 8/10, Train Loss: 0.0799, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9251\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 10/10, Train Loss: 0.0587, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        86\n",
      "    positive       0.97      0.96      0.96       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9274\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.93      0.82      0.87        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.90      0.83      0.86        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.88      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 166.91364216804504 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9418, F1 Micro: 0.9418, F1 Macro: 0.8684\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.298839569091797 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5319, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.394, Accuracy: 0.9211, F1 Micro: 0.9518, F1 Macro: 0.9502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2689, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1841, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1354, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9759\n",
      "Epoch 7/10, Train Loss: 0.0791, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0489, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4889, Accuracy: 0.875, F1 Micro: 0.875, F1 Macro: 0.8672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2597, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.168, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9368\n",
      "Epoch 4/10, Train Loss: 0.1219, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9174\n",
      "Epoch 5/10, Train Loss: 0.1229, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.908\n",
      "Epoch 6/10, Train Loss: 0.1134, Accuracy: 0.9081, F1 Micro: 0.9081, F1 Macro: 0.8996\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.9338, F1 Micro: 0.9338, F1 Macro: 0.9257\n",
      "Epoch 8/10, Train Loss: 0.0837, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.9059\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "Epoch 10/10, Train Loss: 0.0816, Accuracy: 0.8897, F1 Micro: 0.8897, F1 Macro: 0.8798\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9368\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        87\n",
      "    positive       0.96      0.96      0.96       185\n",
      "\n",
      "    accuracy                           0.94       272\n",
      "   macro avg       0.94      0.94      0.94       272\n",
      "weighted avg       0.95      0.94      0.94       272\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9261\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.81      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 165.90848064422607 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5326, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4075, Accuracy: 0.9278, F1 Micro: 0.9559, F1 Macro: 0.9545\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2701, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1809, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1363, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1011, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9814\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5207, Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.8974\n",
      "Epoch 2/10, Train Loss: 0.237, Accuracy: 0.8988, F1 Micro: 0.8988, F1 Macro: 0.8924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2057, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9116\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1143, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0954, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9385\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Epoch 7/10, Train Loss: 0.1095, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9194\n",
      "Epoch 9/10, Train Loss: 0.0452, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Epoch 10/10, Train Loss: 0.0737, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9113\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.91      0.92        86\n",
      "    positive       0.95      0.96      0.96       171\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.94      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9244\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.88      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 173.94951629638672 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.7999, F1 Micro: 0.8872, F1 Macro: 0.8857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4405, Accuracy: 0.881, F1 Micro: 0.929, F1 Macro: 0.9273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2976, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1968, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1424, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1047, Accuracy: 0.971, F1 Micro: 0.9816, F1 Macro: 0.9803\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0798, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "Epoch 9/10, Train Loss: 0.054, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Epoch 10/10, Train Loss: 0.0462, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9807\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4753, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8843\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2423, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1603, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.166, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1145, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "Epoch 7/10, Train Loss: 0.0836, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9196\n",
      "Epoch 8/10, Train Loss: 0.0794, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.94\n",
      "Epoch 9/10, Train Loss: 0.0582, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9321\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9281\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.95      0.94       256\n",
      "weighted avg       0.95      0.95      0.95       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9288\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 170.7301163673401 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9428, F1 Micro: 0.9428, F1 Macro: 0.8713\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.920041561126709 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5211, Accuracy: 0.8058, F1 Micro: 0.8905, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3844, Accuracy: 0.9323, F1 Micro: 0.9582, F1 Macro: 0.9563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2542, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1758, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.128, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 6/10, Train Loss: 0.096, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 9/10, Train Loss: 0.0501, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0428, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4867, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2448, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9322\n",
      "Epoch 3/10, Train Loss: 0.1691, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9127\n",
      "Epoch 4/10, Train Loss: 0.1216, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9217\n",
      "Epoch 5/10, Train Loss: 0.1174, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9213\n",
      "Epoch 6/10, Train Loss: 0.1064, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9265\n",
      "Epoch 7/10, Train Loss: 0.1124, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Epoch 8/10, Train Loss: 0.0886, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9057\n",
      "Epoch 10/10, Train Loss: 0.0631, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9178\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9322\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        87\n",
      "    positive       0.97      0.94      0.95       177\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.93      0.94      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9221\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 166.00302505493164 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5185, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3943, Accuracy: 0.9278, F1 Micro: 0.9554, F1 Macro: 0.9531\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2519, Accuracy: 0.9591, F1 Micro: 0.9746, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1742, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "Epoch 5/10, Train Loss: 0.1295, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9754, F1 Micro: 0.9845, F1 Macro: 0.9836\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9754, F1 Micro: 0.9845, F1 Macro: 0.9836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.98      0.98      0.98       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4871, Accuracy: 0.906, F1 Micro: 0.906, F1 Macro: 0.8984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.213, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9339\n",
      "Epoch 3/10, Train Loss: 0.1647, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Epoch 4/10, Train Loss: 0.1445, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9015\n",
      "Epoch 5/10, Train Loss: 0.1245, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9078\n",
      "Epoch 6/10, Train Loss: 0.1311, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0951, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0845, Accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9369\n",
      "Epoch 9/10, Train Loss: 0.0922, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Epoch 10/10, Train Loss: 0.0572, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9436, F1 Micro: 0.9436, F1 Macro: 0.9369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.92        87\n",
      "    positive       0.97      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.94       266\n",
      "   macro avg       0.93      0.94      0.94       266\n",
      "weighted avg       0.95      0.94      0.94       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.966, F1 Micro: 0.966, F1 Macro: 0.9336\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 167.0070903301239 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5259, Accuracy: 0.7909, F1 Micro: 0.883, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4353, Accuracy: 0.8914, F1 Micro: 0.9346, F1 Macro: 0.9333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2891, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.194, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1357, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9792\n",
      "Epoch 6/10, Train Loss: 0.1014, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Epoch 7/10, Train Loss: 0.0789, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0683, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4777, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2428, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9367\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1816, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9442\n",
      "Epoch 4/10, Train Loss: 0.1374, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9279\n",
      "Epoch 5/10, Train Loss: 0.091, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Epoch 9/10, Train Loss: 0.0809, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9349\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9442\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9271\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 167.8305425643921 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9437, F1 Micro: 0.9437, F1 Macro: 0.874\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.5919525623321533 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5287, Accuracy: 0.8013, F1 Micro: 0.8882, F1 Macro: 0.8869\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3834, Accuracy: 0.9092, F1 Micro: 0.9425, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2516, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1691, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 5/10, Train Loss: 0.1255, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.977\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.974, F1 Micro: 0.9836, F1 Macro: 0.9828\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 10/10, Train Loss: 0.0444, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.974, F1 Micro: 0.9836, F1 Macro: 0.9828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.98      0.95       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.504, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2168, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1771, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 4/10, Train Loss: 0.1549, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Epoch 5/10, Train Loss: 0.1133, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Epoch 6/10, Train Loss: 0.0821, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9115\n",
      "Epoch 7/10, Train Loss: 0.1051, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9068\n",
      "Epoch 8/10, Train Loss: 0.0706, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9121\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9119\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        86\n",
      "    positive       0.97      0.94      0.95       174\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.94      0.93       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9254\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.98      0.96       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.83      0.85       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 167.44202613830566 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5267, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4023, Accuracy: 0.9219, F1 Micro: 0.9511, F1 Macro: 0.9486\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2571, Accuracy: 0.9561, F1 Micro: 0.9728, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.17, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.099, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9769\n",
      "Epoch 7/10, Train Loss: 0.0778, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "Epoch 9/10, Train Loss: 0.0514, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4994, Accuracy: 0.8973, F1 Micro: 0.8973, F1 Macro: 0.89\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2447, Accuracy: 0.8973, F1 Micro: 0.8973, F1 Macro: 0.89\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2135, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1381, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0897, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0749, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Epoch 9/10, Train Loss: 0.0579, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Epoch 10/10, Train Loss: 0.0599, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9223\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 179.4477195739746 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5335, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4354, Accuracy: 0.901, F1 Micro: 0.9392, F1 Macro: 0.9374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2846, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1841, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1339, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9787\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.98      0.98      0.98       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4631, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2052, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1453, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1176, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1124, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9494\n",
      "Epoch 7/10, Train Loss: 0.0747, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.941\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9491\n",
      "Epoch 10/10, Train Loss: 0.0548, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.902\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9491\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.97       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.94      0.96      0.95       264\n",
      "weighted avg       0.96      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.932\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.85      0.88      0.87        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 177.11301827430725 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9446, F1 Micro: 0.9446, F1 Macro: 0.8763\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 1.9590578079223633 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5223, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3669, Accuracy: 0.9308, F1 Micro: 0.9573, F1 Macro: 0.9552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2383, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.158, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1152, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.077, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "Epoch 9/10, Train Loss: 0.0499, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 10/10, Train Loss: 0.0442, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.976\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4356, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2355, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1664, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "Epoch 4/10, Train Loss: 0.1443, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9322\n",
      "Epoch 5/10, Train Loss: 0.1038, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.923\n",
      "Epoch 6/10, Train Loss: 0.0787, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9073\n",
      "Epoch 7/10, Train Loss: 0.1072, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9234\n",
      "Epoch 8/10, Train Loss: 0.1039, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9185\n",
      "Epoch 9/10, Train Loss: 0.0796, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9274\n",
      "Epoch 10/10, Train Loss: 0.06, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9234\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        85\n",
      "    positive       0.96      0.95      0.96       174\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9216\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.84      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 174.29638576507568 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5248, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.378, Accuracy: 0.9301, F1 Micro: 0.9567, F1 Macro: 0.9547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.24, Accuracy: 0.9561, F1 Micro: 0.9728, F1 Macro: 0.9719\n",
      "Epoch 4/10, Train Loss: 0.1568, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1176, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0974, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "Epoch 8/10, Train Loss: 0.0629, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9665, F1 Micro: 0.9787, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0447, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.975\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.487, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2518, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1819, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9248\n",
      "Epoch 4/10, Train Loss: 0.1327, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0989, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9406\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 7/10, Train Loss: 0.0924, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9322\n",
      "Epoch 8/10, Train Loss: 0.0822, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9174\n",
      "Epoch 9/10, Train Loss: 0.0788, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9265\n",
      "Epoch 10/10, Train Loss: 0.0619, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9086\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       175\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.93      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9291\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       1.00      0.96      0.98        24\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.99      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.84      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 173.64429354667664 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5303, Accuracy: 0.7946, F1 Micro: 0.8848, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4266, Accuracy: 0.9129, F1 Micro: 0.9473, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2753, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.173, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1228, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1024, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0792, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 8/10, Train Loss: 0.0632, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4586, Accuracy: 0.9101, F1 Micro: 0.9101, F1 Macro: 0.8937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2215, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9451\n",
      "Epoch 3/10, Train Loss: 0.1929, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.933\n",
      "Epoch 4/10, Train Loss: 0.1436, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9407\n",
      "Epoch 5/10, Train Loss: 0.1415, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9293\n",
      "Epoch 6/10, Train Loss: 0.1058, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9258\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9333\n",
      "Epoch 8/10, Train Loss: 0.084, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9333\n",
      "Epoch 9/10, Train Loss: 0.0509, Accuracy: 0.9101, F1 Micro: 0.9101, F1 Macro: 0.9024\n",
      "Epoch 10/10, Train Loss: 0.0657, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9326\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.96      0.96       180\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.94      0.95      0.95       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.93\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.90      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 175.1175091266632 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9454, F1 Micro: 0.9454, F1 Macro: 0.8785\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 1.3298730850219727 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5203, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3683, Accuracy: 0.9315, F1 Micro: 0.9578, F1 Macro: 0.9555\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2296, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1604, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9743\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1185, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0918, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Epoch 7/10, Train Loss: 0.0724, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 8/10, Train Loss: 0.0626, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Epoch 9/10, Train Loss: 0.0482, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4731, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1403, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1139, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9487\n",
      "Epoch 5/10, Train Loss: 0.1166, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9292\n",
      "Epoch 6/10, Train Loss: 0.0965, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 7/10, Train Loss: 0.0678, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9198\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 10/10, Train Loss: 0.065, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.8957\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.96      0.95       260\n",
      "weighted avg       0.96      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9304\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.21812081336975 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5209, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3772, Accuracy: 0.9353, F1 Micro: 0.9598, F1 Macro: 0.9577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2318, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1648, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1192, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0883, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "Epoch 7/10, Train Loss: 0.0724, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9802\n",
      "Epoch 9/10, Train Loss: 0.0488, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.98\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4426, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Epoch 2/10, Train Loss: 0.2063, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1665, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9329\n",
      "Epoch 4/10, Train Loss: 0.1289, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.107, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0614, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9407\n",
      "Epoch 8/10, Train Loss: 0.074, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9269\n",
      "Epoch 9/10, Train Loss: 0.051, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0372, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9438\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.91      0.92        87\n",
      "    positive       0.96      0.97      0.96       177\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.95      0.94      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.95      0.95       152\n",
      "    positive       0.84      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 175.58872771263123 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5273, Accuracy: 0.7984, F1 Micro: 0.8866, F1 Macro: 0.8851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4232, Accuracy: 0.9241, F1 Micro: 0.9534, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2555, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1725, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 7/10, Train Loss: 0.074, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "Epoch 9/10, Train Loss: 0.0483, Accuracy: 0.9673, F1 Micro: 0.9792, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.9717, F1 Micro: 0.9821, F1 Macro: 0.981\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9821, F1 Macro: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4687, Accuracy: 0.893, F1 Micro: 0.893, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2188, Accuracy: 0.9188, F1 Micro: 0.9188, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.147, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9345\n",
      "Epoch 4/10, Train Loss: 0.1238, Accuracy: 0.9188, F1 Micro: 0.9188, F1 Macro: 0.9117\n",
      "Epoch 5/10, Train Loss: 0.1246, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1068, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0884, Accuracy: 0.9446, F1 Micro: 0.9446, F1 Macro: 0.9385\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9345\n",
      "Epoch 9/10, Train Loss: 0.0592, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9143\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9345\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9446, F1 Micro: 0.9446, F1 Macro: 0.9385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        88\n",
      "    positive       0.98      0.93      0.96       183\n",
      "\n",
      "    accuracy                           0.94       271\n",
      "   macro avg       0.93      0.95      0.94       271\n",
      "weighted avg       0.95      0.94      0.95       271\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9331\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.88      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 177.00558805465698 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9461, F1 Micro: 0.9461, F1 Macro: 0.8806\n",
      "Total runtime: 10507.516779661179 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACCoAAAHqCAYAAADInz0eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADN4UlEQVR4nOzdeXSU9d3+8fdkD4QE2RKWsIgKuIFFCCjuKC6PrRaXaiuKW7ViW2mroChaq2jtj2IVxVrXChWtS221VkWxWgUUUEQFZQ0EgoQlgUDWmd8fkwQCYQkJzCR5v865T+65557JZzhPn3OZXPl+A6FQKIQkSZIkSZIkSZIkSdIBEBPpASRJkiRJkiRJkiRJUtNhUUGSJEmSJEmSJEmSJB0wFhUkSZIkSZIkSZIkSdIBY1FBkiRJkiRJkiRJkiQdMBYVJEmSJEmSJEmSJEnSAWNRQZIkSZIkSZIkSZIkHTAWFSRJkiRJkiRJkiRJ0gFjUUGSJEmSJEmSJEmSJB0wFhUkSZIkSZIkSZIkSdIBY1FBkiRJkiQ1OFdccQVdu3aN9BiSJEmSJGkfWFSQpHr0yCOPEAgEyMrKivQokiRJUp08/fTTBAKBGo9Ro0ZV3ffWW29x1VVXceSRRxIbG1vr8kDle1599dU1Pn/bbbdV3ZOXl1eXjyRJkqQmxDwrSdEtLtIDSFJjMnnyZLp27cqsWbNYtGgRhxxySKRHkiRJkurkt7/9Ld26dat27cgjj6w6nzJlClOnTuV73/seHTp02KfvkZSUxEsvvcQjjzxCQkJCtef+9re/kZSURFFRUbXrjz/+OMFgcJ++nyRJkpqOaM2zktTUuaKCJNWTpUuX8tFHHzF+/Hjatm3L5MmTIz1SjQoLCyM9giRJkhqQs846i5/85CfVjj59+lQ9f++991JQUMD//vc/evfuvU/f48wzz6SgoIB///vf1a5/9NFHLF26lHPOOWen18THx5OYmLhP3297wWDQHxpLkiQ1YtGaZ/c3fw4sKdpZVJCkejJ58mQOOuggzjnnHC644IIaiwobN27kpptuomvXriQmJtKpUyeGDRtWbcmvoqIi7rzzTg477DCSkpJo3749P/zhD1m8eDEA06dPJxAIMH369GrvvWzZMgKBAE8//XTVtSuuuIKUlBQWL17M2WefTYsWLfjxj38MwAcffMCFF15I586dSUxMJDMzk5tuuomtW7fuNPeCBQu46KKLaNu2LcnJyfTo0YPbbrsNgPfee49AIMArr7yy0+umTJlCIBDg448/rvW/pyRJkhqGDh06EB8fX6f36NixIyeeeCJTpkypdn3y5MkcddRR1f7irdIVV1yx07K8wWCQBx98kKOOOoqkpCTatm3LmWeeyaefflp1TyAQYMSIEUyePJkjjjiCxMRE3nzzTQDmzp3LWWedRWpqKikpKZx22mnMmDGjTp9NkiRJ0S1Seba+fj4LcOeddxIIBPjqq6+49NJLOeiggxg0aBAAZWVl3H333XTv3p3ExES6du3KrbfeSnFxcZ0+syTVlVs/SFI9mTx5Mj/84Q9JSEjgkksu4dFHH+WTTz6hX79+AGzevJkTTjiBr7/+miuvvJLvfe975OXl8dprr7Fy5UratGlDeXk5//d//8e0adP40Y9+xC9+8Qs2bdrE22+/zfz58+nevXut5yorK2PIkCEMGjSIP/zhDzRr1gyAF198kS1btnD99dfTunVrZs2axUMPPcTKlSt58cUXq14/b948TjjhBOLj47n22mvp2rUrixcv5p///Cf33HMPJ598MpmZmUyePJnzzz9/p3+T7t27M3DgwDr8y0qSJCmS8vPzd9pLt02bNvX+fS699FJ+8YtfsHnzZlJSUigrK+PFF19k5MiRe73iwVVXXcXTTz/NWWedxdVXX01ZWRkffPABM2bM4Nhjj62679133+WFF15gxIgRtGnThq5du/Lll19ywgknkJqays0330x8fDyPPfYYJ598Mu+//z5ZWVn1/pklSZK0/0Vrnq2vn89u78ILL+TQQw/l3nvvJRQKAXD11VfzzDPPcMEFF/CrX/2KmTNnMm7cOL7++usa//hMkg4UiwqSVA9mz57NggULeOihhwAYNGgQnTp1YvLkyVVFhQceeID58+fz8ssvV/uF/pgxY6pC47PPPsu0adMYP348N910U9U9o0aNqrqntoqLi7nwwgsZN25ctev3338/ycnJVY+vvfZaDjnkEG699Vays7Pp3LkzADfeeCOhUIg5c+ZUXQO47777gPBfpP3kJz9h/Pjx5Ofnk5aWBsDatWt56623qjV7JUmS1PAMHjx4p2v7mk1354ILLmDEiBG8+uqr/OQnP+Gtt94iLy+PSy65hKeeemqPr3/vvfd4+umn+fnPf86DDz5Ydf1Xv/rVTvMuXLiQL774gsMPP7zq2vnnn09paSkffvghBx98MADDhg2jR48e3Hzzzbz//vv19EklSZJ0IEVrnq2vn89ur3fv3tVWdfj888955plnuPrqq3n88ccB+NnPfka7du34wx/+wHvvvccpp5xSb/8GklQbbv0gSfVg8uTJpKenV4W6QCDAxRdfzPPPP095eTkAL730Er17995p1YHK+yvvadOmDTfeeOMu79kX119//U7Xtg/BhYWF5OXlcdxxxxEKhZg7dy4QLhv897//5corr6wWgnecZ9iwYRQXF/P3v/+96trUqVMpKyvjJz/5yT7PLUmSpMibOHEib7/9drVjfzjooIM488wz+dvf/gaEtxE77rjj6NKly169/qWXXiIQCDB27NidntsxS5900knVSgrl5eW89dZbnHfeeVUlBYD27dtz6aWX8uGHH1JQULAvH0uSJEkRFq15tj5/Plvpuuuuq/b4jTfeAGDkyJHVrv/qV78C4PXXX6/NR5SkeuWKCpJUR+Xl5Tz//POccsopLF26tOp6VlYW/+///T+mTZvGGWecweLFixk6dOhu32vx4sX06NGDuLj6+3/PcXFxdOrUaafr2dnZ3HHHHbz22mts2LCh2nP5+fkALFmyBKDGPdS217NnT/r168fkyZO56qqrgHB5Y8CAARxyyCH18TEkSZIUIf3796+2bcL+dOmll3LZZZeRnZ3Nq6++yu9///u9fu3ixYvp0KEDrVq12uO93bp1q/Z47dq1bNmyhR49eux0b69evQgGg6xYsYIjjjhir+eRJElSdIjWPFufP5+ttGPOXb58OTExMTv9jDYjI4OWLVuyfPnyvXpfSdofLCpIUh29++67rF69mueff57nn39+p+cnT57MGWecUW/fb1crK1Su3LCjxMREYmJidrr39NNPZ/369dxyyy307NmT5s2bk5OTwxVXXEEwGKz1XMOGDeMXv/gFK1eupLi4mBkzZvDwww/X+n0kSZLUdH3/+98nMTGRyy+/nOLiYi666KL98n22/+s1SZIkqb7sbZ7dHz+fhV3n3Lqs1itJ+4tFBUmqo8mTJ9OuXTsmTpy403Mvv/wyr7zyCpMmTaJ79+7Mnz9/t+/VvXt3Zs6cSWlpKfHx8TXec9BBBwGwcePGatdr03794osv+Oabb3jmmWcYNmxY1fUdlz2rXPZ2T3MD/OhHP2LkyJH87W9/Y+vWrcTHx3PxxRfv9UySJElScnIy5513Hs899xxnnXUWbdq02evXdu/enf/85z+sX79+r1ZV2F7btm1p1qwZCxcu3Om5BQsWEBMTQ2ZmZq3eU5IkSU3P3ubZ/fHz2Zp06dKFYDDIt99+S69evaqur1mzho0bN+71NmuStD/E7PkWSdKubN26lZdffpn/+7//44ILLtjpGDFiBJs2beK1115j6NChfP7557zyyis7vU8oFAJg6NCh5OXl1bgSQeU9Xbp0ITY2lv/+97/Vnn/kkUf2eu7Y2Nhq71l5/uCDD1a7r23btpx44ok8+eSTZGdn1zhPpTZt2nDWWWfx3HPPMXnyZM4888xa/WBZkiRJAvj1r3/N2LFjuf3222v1uqFDhxIKhbjrrrt2em7H7Lqj2NhYzjjjDP7xj3+wbNmyqutr1qxhypQpDBo0iNTU1FrNI0mSpKZpb/Ls/vj5bE3OPvtsACZMmFDt+vjx4wE455xz9vgekrS/uKKCJNXBa6+9xqZNm/j+979f4/MDBgygbdu2TJ48mSlTpvD3v/+dCy+8kCuvvJK+ffuyfv16XnvtNSZNmkTv3r0ZNmwYzz77LCNHjmTWrFmccMIJFBYW8s477/Czn/2MH/zgB6SlpXHhhRfy0EMPEQgE6N69O//617/47rvv9nrunj170r17d37961+Tk5NDamoqL7300k57oQH86U9/YtCgQXzve9/j2muvpVu3bixbtozXX3+dzz77rNq9w4YN44ILLgDg7rvv3vt/SEmSJDVY8+bN47XXXgNg0aJF5Ofn87vf/Q6A3r17c+6559bq/Xr37k3v3r1rPccpp5zCZZddxp/+9Ce+/fZbzjzzTILBIB988AGnnHIKI0aM2O3rf/e73/H2228zaNAgfvaznxEXF8djjz1GcXHxbvcWliRJUsMWiTy7v34+W9Msl19+OX/+85/ZuHEjJ510ErNmzeKZZ57hvPPO45RTTqnVZ5Ok+mRRQZLqYPLkySQlJXH66afX+HxMTAznnHMOkydPpri4mA8++ICxY8fyyiuv8Mwzz9CuXTtOO+00OnXqBISbtG+88Qb33HMPU6ZM4aWXXqJ169YMGjSIo446qup9H3roIUpLS5k0aRKJiYlcdNFFPPDAAxx55JF7NXd8fDz//Oc/+fnPf864ceNISkri/PPPZ8SIETuF6N69ezNjxgxuv/12Hn30UYqKiujSpUuN+6ude+65HHTQQQSDwV2WNyRJktS4zJkzZ6e/Fqt8fPnll9f6B7t18dRTT3H00UfzxBNP8Jvf/Ia0tDSOPfZYjjvuuD2+9ogjjuCDDz5g9OjRjBs3jmAwSFZWFs899xxZWVkHYHpJkiRFQiTy7P76+WxN/vKXv3DwwQfz9NNP88orr5CRkcHo0aMZO3ZsvX8uSaqNQGhv1oaRJGkvlJWV0aFDB84991yeeOKJSI8jSZIkSZIkSZKkKBQT6QEkSY3Hq6++ytq1axk2bFikR5EkSZIkSZIkSVKUckUFSVKdzZw5k3nz5nH33XfTpk0b5syZE+mRJEmSJEmSJEmSFKVcUUGSVGePPvoo119/Pe3atePZZ5+N9DiSJEmSJEmSJEmKYq6oIEmSJEmSJEmSJEmSDhhXVJAkSZIkSZIkSZIkSQeMRQVJkiRJkiRJkiRJknTAxEV6gAMlGAyyatUqWrRoQSAQiPQ4kiRJqoNQKMSmTZvo0KEDMTFNr3trtpUkSWo8zLZmW0mSpMaiNtm2yRQVVq1aRWZmZqTHkCRJUj1asWIFnTp1ivQYB5zZVpIkqfEx20qSJKmx2Jts22SKCi1atADC/yipqakRnkaSJEl1UVBQQGZmZlXGa2rMtpIkSY2H2dZsK0mS1FjUJts2maJC5bJhqampBl5JkqRGoqkuDWu2lSRJanzMtmZbSZKkxmJvsm3T2/RMkiRJkiRJkiRJkiRFjEUFSZIkSZIkSZIkSZJ0wFhUkCRJkiRJkiRJkiRJB4xFBUmSJEmSJEmSJEmSdMBYVJAkSZIkSZIkSZIkSQeMRQVJkiRJkiRJkiRJknTAWFSQJEmSJEmSJEmSJEkHjEUFSZIkSZIkSZIkSZJ0wFhUkCRJkiRJkiRJkiRJB4xFBUmSJEmSJEmSJEmSdMBYVJAkSZIkSZIkSZIkSQeMRQVJkiRJkiRJkiRJknTAWFSQJEmSJEmSJEmSJEkHjEUFSZIkSZIkSZIkSZJ0wMRFegBJkiTtrLgYsrNh2TIoKYGMjPDRrh3Ex0d6OkmSJKkWyouhMBsKl0GwBJIzICkDktpBjOFWkiRJDUdZsIytpVvZUrqFYChIcnwyyXHJJMQmEAgEIj0eAKXlpazatIrs/Gyy87M5ocsJdE7rHOmxdmJRQZIkqcKqVfDaa/Dvf0NR0bZywI5H+/aQlgZ1yZ3FxbBiBSxdGi4j7HisWlXz6wIBaNNm17NtP2PLlnuesagI8vNh48bw1+3Pa7qWnw+hELRoAamp4a+Vx/e/D8ccs+//JpIkSapHW1ZBzmuw6t9QXrStHLDT1/YQX8dwW14MW1bA5qXhMkLhMti8bNv51l2EWwKQ2Ga74kJN81XO2HLPM5YXQUk+lG6s+Lr9+XbXSjZWPJcPhCCuBcSnQnyLivMW0PH70MpwK0mSFA2Ky4r5cu2XfJ77OQXFBXV+v2AoyNaycNmg8tjxcdX10urXS4OlNb5ngABJcUlVxYXKr7u8VvE4KS4JgPJgOeWhcoKh4K7PQ+WUB3d9vn7repbnL2fVplUEQ8Gq2Sb/cDKXHnVpnf/d6ptFBUmS1KR99RW8+ir84x8wa9bevy4xcc9lgbQ0yMnZdREhFNr992jWDLp1g4QEWLMmfJSXw9q14eOLL3b/+oSEbbO0bRsuR+xYRCgp2fvPvCddulhUkCRJiqj8r2Dlq7DyH7CuFuE2JnHngsCOZYH4NNiaU72AUFlI2LoK2EO4jW0GKd0gJgGK1oSPUDkUrw0f7CHcxiRsmyWxLQSLtxUQKssHwXoMt827WFSQJElRa9WmVXyS8wmfrPqE2atnExcTxyldT+HUbqdydPrRxARiIj3iPispL2HO6jnMWDmDublz+Sz3M75a+xVlwbJIj1ZNgACBQKCqEBAixNayrWwt2xrhycISYhPITM2kc1pnUhNTIz1OjSwqSJKkJqeoCF58ER5+eOdyQlYW/OAH0KED5ObWfGzcGP6l//Ll4WNfNWsGXbvu+mjTpvofjQWDkJe367m2PzZsCJcQsrPDx+4EAuHVEVq2DJcr0tJ2fV65ksSmTeGjoGDb+eGH7/u/hSRJkvZReRFkvwjfPLxzOaF1FnT6ASR3gKJc2Jq789fSjeFf+hcuDx/7KrYZpHSF5tsd2z9O3CHchoJQnFfzTEU7nJdsCJcQtmSHj90KhFdHSGgZLlfEp9V8nlDxmACUbYLSTVBasO081XArSZKiw7ot6/h01ad8sipcTPh01aes2rTzilX/+uZfALRKblVVWji126n0aN0jarYkqMn6rev5eMXH/G/F//jfiv8xK2cWRWVFO913UNJB9MnoQ3pKep2/Z4AAzeKbkRyXTLP4Znt1JMdXvzcxNhGA0mApW0vDBYWtpVspKiuqOq/pa1FZ0U73BwIBYgIxxAZiiY2JrXYeG6h4XMP5jvemJabRpWUXOqd1pl3zdlFfWLGoIEmSmozsbJg0CR5/PPwLfwivOjB4cLiccO654S0T9qSoKLy6werVuy8LbNwYLjzsqojQtm3tVtiNiYF27cLH0Ufv3YyVs6xdC8nJNZcPWrQIv7ckSZIakMJs+HYSLH48/At/CK86kDE4XE7oeG54y4Q9KS8Kr26wdfUeygIbw4WHHQsIlY8TaxluAzGQ1C58sIdwWzVj5VxrITZ5W9lg+yJCfIvwe0uSJDVAm4o3MWf1nKpCwierPmHJhiU73RcTiOHwtofTr0M/+nXox9ayrby79F3eX/4+67eu56WvX+Klr18CoH1K+6rSwqndTqVry64H+FNtEwqFWLxhMf/L/l9VMeGrtV/tdF/r5NYcl3kcx3Y4lj4ZfeiT0YfM1MyoLFwkxCaQEJtAGmmRHqXBCYRCe1p0uHEoKCggLS2N/Px8UlOjc3kLSZJU/0IhePfd8OoJr70WXpUAIDMTrrsOrr46/It/NSxNPds19c8vSVKTFQrBmnfDqyfkvBZelQCgWSYceh10v7riF/9qSJp6tmvqn1+S1DiEQiHyi/MpC5YRDAX3+igoLmD2qtlVqyV8vfZrQjVsqXVIq0OqSgn9OvbjmIxjaJ7QfKf7SstL+XTVp7y79F3eXfYu/8v+H8XlxdXu6dayW1Vp4ZSup9C+xV6UW/dR5TYOlcWEj1Z8xJrCNTvdd1jrwzg+8/jw0fn4qF8FQrtWm2xnUUGSJDVKBQXw7LMwcSIsWLDt+mmnwQ03hFdPiHNtqQarqWe7pv75JUlqckoLYMmz8O1EKNgu3KafBofdEF49IcZw21A19WzX1D+/JKlhCYVC5G7O5cu1X/LV2q/48rsvq843FG2ol+/RKbVTtVJC3/Z9OSj5oH16r6KyIj5e8XFVcWFWzizKgmXV7unVphendjuVk7qcRKvkVgQCAQIEdvoaE4jZ5XPbf121aVVVMeGTVZ/stI1DQmwCx3Y4tqqYcFzmcbRt3naf/70UXSwq1MDAK0lS0/DVV+FywrPPwubN4WspKXD55fCzn8HhbjXbKDT1bNfUP78kSU1G/lfwzURY+iyUVYTbuBTodjkc9jNIM9w2Bk092zX1zy9Jik7bFxK+/K6ilFCLQkJMIGavjsTYRI5OP5pjOxxbVUzISMnYb59rU/EmPsz+sKq4MHf13BpXcKhPrZNbc3zn46uKCX079CUpLmm/fk9FTm2ynVVrSZLU4JWVhbd1mDgxvM1DpZ49YcQIuOwy8OddkiRJahCCZeFtHb6ZGN7moVJqTzhsBHS7DOINt5IkSfUhFAqxevPqnVZH+HLtl2ws2ljja2ICMRzS6hCOaHsEh7c9vOrroa0PJSkuiZhAzIH9ELXQIrEFZx16FmcdehYA67eu5/1l7/Pu0neZkTODorIiQqEQIUI7fQ2Ggrt8bvuvqYmpDOw0sKqccFjrw9zGQTWyqCBJkmoUCkFeHixbtu2IjQ3/8r9nT+jSJfw4kr77Dh5/HCZNgpUrw9diYuAHPwhv73DqqWAGliRJEqEQFOdB4bLwsXkZBGLDv/xP6wnNukBMhMNt0Xew6HFYNAm2VITbQAx0/EF4e4d0w60kSVJ5sJzC0kI2l2ymsKRwt+eFJRWPS3d939ota/eqkFBVSmh3BIe1PqzRrAjQKrkV5/c6n/N7nR/pUdQEWVSQJKmJCoVg3brqRYQdj8LCXb8+KQkOO2xbcaHyOOwwaN68/mfdsAFyc7cd//43vPAClJSE72nTBq65Bq67Djp3rt/vL0mSpCgXCkHxum1FhMoywvaPy3YTbmOToMVh4eJCteMwiNsP4bZkAxTlwtbc8NdV/4bsFyBYEW4T20D3a+DQ66C54VaSJDVMwVCQLaVb2FyymU3Fm8JfS8Jfd3ltD88XlRXV+5yxgVgOaXVItdURjmh3BD1a9yAxLrHev5+kMIsKkiQ1YqWl8M03sGDBtvLB0qV7V0So1KEDdO0aXkGhrCz8Xt98A0VFMG9e+NhR587Vywu9eoW/pqdX/yOwrVurlw92d1QWEnbUv394e4cLLwyXJyRJktRIBUuh4BsoWLBdGWHp3hURKiV3gOZdoXkXCJWF36vgGygvgo3zwseOmnXeVlxI6wmpvcLnSTuE27Kt1csH25/v+DW4i3Dbun94e4fOF4bLE5IkSVGouKyY7Pxslm1cxtKNS1m2cVnVed6WvKpSQWFJISFC+2WGmEAMzeObk5KQQvOE5jWfxzenecLuz1smteTQVodaSJAiwKKCJEmNQHk5LF4M8+fDl1+Gj/nzw4WC0tLdv7Z9+3ARofLo1m3beWZmzb/8Ly8PFx0WLNj5yMuD7Ozw8dZb1V+Xlgbdu8PmzeHyQUFB7T7nQQdBRkb4OOwwuOoq6Nevdu8hSZKkKBcsh82LIX8+bPwS8r8Mn2/6JlxW2J3k9hVFhIojpdt2jzNr/uV/sDxcdChYsPNRnAdbssNH7g7hNj4NUrpD2eZw+aC0luE24SBIyoDkjPBqDt2vgtaGW0mSFHllwTJW5K+oVkSo+rphKas2rapVASFAgJSEFFoktiAlISV8ntBi27X4lN0/X3Ft+yMxNpGA22JJDZpFBUmSGpDycli+vHoZ4csv4euvobi45tekpMDhh4cLAtsXErp2Da98sC+rEMTGht+ve3c455zqz+XlwcKF1csLX38dXskhPx/mzKl+f2JiuCxRWUCo6WjfHtq1c8UESZKkRiVYDluWVy8j5H8J+V9DcBfhNi4F0g4PFwSad4WUrtsVETrv2yoEMbHQonv46LhDuC3Kg00LtxUX8hdAwddQuBRK82HDDuE2JjFclqgsINT4tT0ktXPFBEmSdMAUlxWzbus61m1ZV/V1/db11a9VnK8sWMnKgpWUh8p3+57Jccl0O6gbXVt2pVvLbV/bNW9Hi8QWVUWDlIQUmsU3s1QgaScWFSRJijLBIKxcCd9+Gz4WLdp2vnjxrrdASE4OFxKOOAKOPHLb18zM6ivS7m9t2oSP44+vfr2oKPxZliyB1NRtJYS0tAM7nyRJkg6gUBC2rIRN31Yci7adb1686y0QYpPDhYS0IyDtyPDXlkdCswMcbpPahI+2O4Tb8qLwZ9m8BOJTtxUR4g23avgmTpzIAw88QG5uLr179+ahhx6if//+Nd5bWlrKuHHjeOaZZ8jJyaFHjx7cf//9nHnmmQd4akmqu1AoxKaSTeRtyav65f3256FQiITYBBJiE0iMSwx/jU3cq2vbX9/+WlxM7X5NVx4sZ2PRxl2XDbasY33R+mqP121dx5bSLbX+90iITaBry647FRG6tuxKt4O60bZZW8sHkurEooIkSREQCsGqVdsKCNsfixeHf6m/KwkJ0LNn9TLCEUeEt2yIiTlwn6G2kpLCsx55ZKQnkSRJUr0KhWDrqu3KCN9WLyOU7ybcxiRAas9wGaHldqWElG4QiOJwG5sULk60NNyqcZk6dSojR45k0qRJZGVlMWHCBIYMGcLChQtp167dTvePGTOG5557jscff5yePXvyn//8h/PPP5+PPvqIY445JgKfQJLCgqEg+UX54aLBDoWDyvO8rRVfK+5Zt2UdpXvaZqqexQRi9lh4CAQCrN+6nvVb17Nh64ZabbmwvdhALK2SW9EquRWtm7WmdXJrWjdrTauk6o/bp7Sn20HdyEjJICaa85ikBi8QCoX27f+jNTAFBQWkpaWRn59PampqpMeRJNWzYBAKC2Hz5j0fe7qvsDC8tUFiYvhISNh2vuOxq+d2vF5Ssm1lhEWLwseW3RSZ4+Lg4IPh0EOrH4ccEt6uITb2wP3bStGoqWe7pv75JanRCwWhrBDKNkPp5vDX7Y9q1wp3cX275wOx4S0JYhPDxYCq8+2+xiRCbMJ254m7fk2wBDYv2m6FhEVQvptwG4iDlIOhxaE7HIdAs87hrRekJiyasl1WVhb9+vXj4YcfBiAYDJKZmcmNN97IqFGjdrq/Q4cO3Hbbbdxwww1V14YOHUpycjLPPffcXn3PaPr8kqJTebC82soBO5YPqooGOxQSgqHgPn2/pLgk2jRrQ5tmbWid3Jo2zdrQKrkVsYFYSspLKC4vpqS8pNp5cdnO13Z1vT6kJqbSOrl19dLBjo+bVb+Wmphq8UDSflebbOeKCpKkBiMYhLfegr/8Bb75pnrpYHe/9I9WsbHhVRAOOWTnQkKXLuGygiRJkhqpUBBWvwWL/wKbvgmXDMoLK742wHAbiIXm3cLlgx0LCc27QC2XNZZ04JWUlDB79mxGjx5ddS0mJobBgwfz8ccf1/ia4uJikpKSql1LTk7mww8/3OX3KS4upri4uOpxQUFBHSeX1FiEQiGWbVzGJ6s+YVbOLD5Z9Qlffvcl67eu3+dVBFISUqrKBq2bta5WPtj++vbnzeKb1fMn2yYUClEWLKuxzLCrgkMwFOSg5IOqSgetklsRHxu/32aUpAPF/0qUJEW9vDx46imYNAmWLNn9vTExkJKy+6N5890/36wZlJeHV0EoLq5+7Hhtb+4pLg7PteMKCV27Qrz/TSFJktS0FOXBkqdg0STYvIdwG4iBuJTqR/wOj+Oa7+J6xbXYZhAqD6+CECyG8uLtvu5wLViyw/O7uEbMziskpHSFGMOt1JDl5eVRXl5Oenp6tevp6eksWLCgxtcMGTKE8ePHc+KJJ9K9e3emTZvGyy+/THl5+S6/z7hx47jrrrvqdXZJDdN3hd/xSc62UsInqz4hb0veLu9PS0zbbdmgpvJBYlziAfxEexYIBIiPjbdoIElYVJAkRalQCGbMgEcegRdfDP+yHyAtDa64As4+G1JTdy4ZJCZCIBDR0SVJkqTqQiHImwHfPgrZL1T8sh+IT4ODr4AOZ0N86s4lgxjDraTo9uCDD3LNNdfQs2dPAoEA3bt3Z/jw4Tz55JO7fM3o0aMZOXJk1eOCggIyMzMPxLiSImhT8SZmr55dVUqYlTOL7Pzsne6Lj4nn6PSj6d+xP/069OOY9sfQPqW9qwhIUiNkUUGSFFU2b4YpU8IFhc8/33b9e9+Dn/0MfvSj8IoIkiRJUtQr3QzLp4QLChs+23b9oO/BYT+DLj8Kr4ggSVGgTZs2xMbGsmbNmmrX16xZQ0ZGRo2vadu2La+++ipFRUWsW7eODh06MGrUKA4++OBdfp/ExEQSE6PrL5wl1a/ismLmrZlXrZSwIG/BTts3BAjQs01P+nXsR78O/ejfsT9Hpx9NUlzSLt5ZktSYWFSQJEWFr76CRx+FZ56BTZvC15KSwsWE66+Hfv38YzJJkiQ1EPlfhcsJS5+F0oq912OTwsWEQ66H1oZbSdEnISGBvn37Mm3aNM477zwAgsEg06ZNY8SIEbt9bVJSEh07dqS0tJSXXnqJiy666ABMLCkalAfLWZC3ILx1Q84nzFo1i89zP6c0WLrTvZ3TOlcVEvp16EffDn1JTUyNwNSSpGhgUUGSFDElJfDKK+HVE/77323XDzkkXE644gpo1Spi40mSJEl7r7wEVr4K3z4C372/7XrKIXDo9eEtHhINt5Ki28iRI7n88ss59thj6d+/PxMmTKCwsJDhw4cDMGzYMDp27Mi4ceMAmDlzJjk5OfTp04ecnBzuvPNOgsEgN998cyQ/hqT9JBQKkZ2fXW2lhNmrZ7O5ZPNO97ZObk2/jv3o36F/1YoJ6SnpEZhakhStLCpIkva78nLYuhW2bIHCQsjPh7//Hf7yF6hcUTImBr7//fD2DqedFn4sSZIkRZ1QEMq3QtkWKCuE0nzI/jssfhyKKsJtIAY6fh8O/RlknBZ+LEkNwMUXX8zatWu54447yM3NpU+fPrz55pukp4d/uZidnU3Mdv/BXlRUxJgxY1iyZAkpKSmcffbZ/PWvf6Vly5YR+gSS6tPawrXVVkr4JOcT1m5Zu9N9zeKb0bd936qVEvp17Ee3lt0IuIKUJGk3AqFQKLTn2xq+goIC0tLSyM/PJzXVpYQkqVJliWDr1nCJYMuWbcf2j/f1uS1boKho198/IwOuuSZ8ZGYeuM8tqWFr6tmuqX9+SdqlYHm4RFC+NVwiKN8SLhSUV5QKqs631PB8TddqerybcJuUAYdcA92vgeaGW0l7p6lnu6b++dV4hUIhlmxYwn+X/5eNRRuJjYklNhBLbEwscTFxVec1fY2Lidun5/b03pXPxQRidioRbCrexJzVc6pWS/hk1Scs27hsp88VFxPH0elHV62U0L9jf3q16UVsTOwB+peVJEWz2mQ7V1SQpCgTDIZ/sV9ZHtiyZdt5TY/rek/pztvF7VfJyeGjTx+47jo47zyIjz+wM0iSJOkACQXDv9ivKg9s2XZe4+Ot4ULAbh9vd23HxzXshbxfxSaHj4P6wKHXQafzIMZwK0lSU1RZTJi+bDrTl09n+rLprCxYGemxdikmEFOtyLCldAshdv671p5teoZXSegQLiX0zuhNUlxSBCaWJDU2+1RUmDhxIg888AC5ubn07t2bhx56iP79+9d4b2lpKePGjeOZZ54hJyeHHj16cP/993PmmWdW3XPnnXdy1113VXtdjx49WLBgQdXjoqIifvWrX/H8889TXFzMkCFDeOSRR6qWHZOkhmLTJli6dNuxbFn1802bIjdbcjI0a7btaN68+uOartXmcXKyWzpIij5mW0mqg9JNsHkpFC6t+Lpsu8fLoCyC4TY2GeKaQWyziq/Nd3jcDOJ2uBbXvBbPJ7ulgyRJTVgoFGLpxqW8t/S9XRYT4mPiyeqURee0zpQHyykPle/0tSxYVuNzZcGyOt0fDAV3O38wFCQYClK6XdEzMzUzvEpCxWoJfdv3JS0pbb/8+0mSVOuiwtSpUxk5ciSTJk0iKyuLCRMmMGTIEBYuXEi7du12un/MmDE899xzPP744/Ts2ZP//Oc/nH/++Xz00Uccc8wxVfcdccQRvPPOO9sGi6s+2k033cTrr7/Oiy++SFpaGiNGjOCHP/wh//vf/2r7ESRpvyoqguXLq5cRti8krFu39+8VH79tBYLKEsH2j2u6VtvHldeSkiwRSGp6zLaStAflRVC4vHoZobKQULgUimsRbmPit61AUFUiSN79tdo+rrqWZIlAkiTVq8piwvRl06uOFQUrqt0THxPPgE4DOLnryZzc9WQGdBpAs/hmEZs3GArudbmhRWIL2jXf+b+DJUnaXwKhUGjntXx2Iysri379+vHwww8DEAwGyczM5MYbb2TUqFE73d+hQwduu+02brjhhqprQ4cOJTk5meeeew4I/9XZq6++ymeffVbj98zPz6dt27ZMmTKFCy64AIAFCxbQq1cvPv74YwYMGLDHud3rTFJ9KSuDFStqXhFh6VJYvXrP79GqFXTrFj66dq1+3rr1tgJBnBv0SFKN6ivbmW0lNXnBMtiyovoqCFWrIyyFrXsRbhNaQUo3aN4NUrpWfO0GzbtCYuttBYIYw60k1aSpZ7um/vkVvfa2mJDVKYuTu4SLCQMzB0asmCBJUjSoTbar1U8JSkpKmD17NqNHj666FhMTw+DBg/n4449rfE1xcTFJSdX3K0pOTubDDz+sdu3bb7+lQ4cOJCUlMXDgQMaNG0fnzp0BmD17NqWlpQwePLjq/p49e9K5c+dd/jC3uLiY4uLiqscFBQW1+aiSmrBgEHJzd14RobKUsGIFlJfv/j1SUnYuIWz/2P/ulqTIM9tKahJCQdiau8NqCNutirBlBYT2EG7jUrYVDypLCJWPU7pBvOFWkiQ1fKFQiGUbl4VLCRVbOWTnZ1e7x2KCJEn1p1ZFhby8PMrLy3faOzc9Pb3anrvbGzJkCOPHj+fEE0+ke/fuTJs2jZdffpny7X7Ll5WVxdNPP02PHj1YvXo1d911FyeccALz58+nRYsW5ObmkpCQQMuWLXf6vrm5uTV+33Hjxu20N7Ak7SgvD/7zH/jf/2DJknAZYfly2O53QTVKSNh1CaFbt/CqCIHAgfgEkqR9ZbaV1OgU5cHq/0De/2DzkooywnII7iHcxiRsKx0036GE0LxbeFUEw60kSWqElm5YusdiQv+O/au2chjYaSDNE5pHaFpJkhqX/b7u4oMPPsg111xDz549CQQCdO/eneHDh/Pkk09W3XPWWWdVnR999NFkZWXRpUsXXnjhBa666qp9+r6jR49m5MiRVY8LCgrIzMzc9w8iqVEIBmHuXHjjjfAxcybUtAFObCxkZtZcQujWDTIyIMYtbyWpyTHbSooqoSBsmAs5b8CqN2DdTKCGcBuIhWaZ24oHlSWEysfJGRAw3EqSpMavasWEimN5/vJqz8fFxJHVMctigiRJB0Ctigpt2rQhNjaWNWvWVLu+Zs0aMjIyanxN27ZtefXVVykqKmLdunV06NCBUaNGcfDBB+/y+7Rs2ZLDDjuMRYsWAZCRkUFJSQkbN26s9pdnu/u+iYmJJCYm1ubjSWqkNmyAt98OFxP+/W/47rvqzx99NJxxBvTqta2QkJkJcW6hK0mNmtlWUoNUsgFWvx0uJqz+NxTtEG5bHg3tz4DUXtsKCc0yIcZwK0mSmp69KSb079ifk7uczCndTrGYIEnSAVSrn1QkJCTQt29fpk2bxnnnnQdAMBhk2rRpjBgxYrevTUpKomPHjpSWlvLSSy9x0UUX7fLezZs3s3jxYi677DIA+vbtS3x8PNOmTWPo0KEALFy4kOzsbAYOHFibjyCpCQiFYN68bcWEjz6C7VbkJiUFTj8dzj4bzjwTOnWK3KySpMgx20pqEEIh2DgvXExY9W/I+whC24XbuBTIOB06nA0dzoRmhltJktR0Ld+4vNpWDss2Lqv2/PbFhJO7nsxxmcdZTJAkKUJq/ScVI0eO5PLLL+fYY4+lf//+TJgwgcLCQoYPHw7AsGHD6NixI+PGjQNg5syZ5OTk0KdPH3JycrjzzjsJBoPcfPPNVe/561//mnPPPZcuXbqwatUqxo4dS2xsLJdccgkAaWlpXHXVVYwcOZJWrVqRmprKjTfeyMCBAxkwYEB9/DtIauAKCmDatG3lhJyc6s8ffni4mHDWWTBoECQkRGZOSVJ0MdtKikqlBZA7bVs5YesO4Tbt8HAxof1Z0HYQxBpuJUlS07Q3xYR+HfpxcteTOaXrKRYTJEmKIrUuKlx88cWsXbuWO+64g9zcXPr06cObb75Jeno6ANnZ2cRst3F7UVERY8aMYcmSJaSkpHD22Wfz17/+tdoytytXruSSSy5h3bp1tG3blkGDBjFjxgzatm1bdc8f//hHYmJiGDp0KMXFxQwZMoRHHnmkDh9dUkMWCsHXX4eLCW+8AR98AGVl255v1gxOPXVbOaFr14iNKkmKYmZbSVEhFIKCryuKCW/Adx9AaLtwG9sM0k+FjhXlhJSuERtVkiQpkrLzs5m+bDrvLXtvj8WEyhUTUhJSIjOsJEnarUAoFApFeogDoaCggLS0NPLz80lNTY30OJL2QWEhvPvutlUTllffUo5DDw0XE84+G048EZKSIjOnJGn/a+rZrql/fqlRKCuE3HfDxYTV/4bCHcJti0MrtnM4G9qdCLGGW0lqrJp6tmvqn19756m5T3H3f+9m6cal1a5bTJAkKbrUJtvVekUFSTqQvv1226oJ06dDScm25xIT4ZRTtq2acMghERtTkiRJ2rOCb7dbNWE6BLcLtzGJkH5KRTnhLGhhuJUkSQJ4b+l7XPXaVYQIERuIpV/HfpzcJVxMOL7z8RYTJElqoCwqSIoqW7fC++9vWzVh0aLqz3ftum3VhFNOCW/xIEmSJEWlsq3w3fsV5YR/w+Ydwm3zrttWTUg/BeIMt5IkSdtbs3kNl758KSFCXN77ch466yFaJLaI9FiSJKkeWFSQGonVqyEvD5o3D//yvvKIawD/K1+6NFxKeOON8NYOW7duey4+PryNQ2U5oUcPCAQiN6skSZIOgK2roTgP4ppDbLPwL/Bjm0FMAwi3m5eGSwmr3oA170L5duE2Jh7anritnJBquJUkSdqV8mA5P3nlJ+RuzuXIdkfyyDmP0CzeYqckSY1FA/gpj6TdmT8f7r0Xpk6FYHDn5xMStpUWdiwx1OXx9uexsbWbubgYPvxw25YOCxZUf75Tp23FhFNPhRaWpCVJkpqGjfPhy3sheyqEagi3MQnbigs7lhjimm93vhfPx1Y83v48thnE1DLclhfD2g+3belQsEO4bdZpu1UTToV4w60kSdLeGPfhON5Z8g7N4pvxwgUvWFKQJKmRsaggNVCzZ8M998Arr2y71qZNeDWCLVsgFApfKykJHxs37r9ZEhL2vuSQnQ3TpsHmzdteHxsLgwbBWWeFywlHHukflkmSJDUp62fD/Htg5XbhNrFNeDWCsi1ARbgNloSP0o37b5aYhL0vQWzJhtxpULZduA3EQttB0OGscDkhzXArSZJUW+8ve5+x08cC8Og5j9Krba8ITyRJkuqbRQWpgfnoI/jd78JbJUD4Z54XXAC33gp9+oSvhUJQVBQuLFQehYU1n9flcaXKMsSGDXv/OTIythUTBg+Gli3r619IkiRJDcbaj2D+72B1RbglAJ0vgCNuhYP6hC+FQlBeBOVbwqWF8i1QVriL8y1QXvF4++d3+drtnq8UrAi31CLcJmVsKyZkDIaElvXz7yNJktQEfVf4HZe8dAnBUJAr+lzBsN7DIj2SJEnaDywqSA1AKATvvRcuKLz3XvhabCxceimMHg29digUBwKQnBw+WrfefzNVliFqU4JISYEzzgiXKmJi9s9skiRJimKhEKx5D778XfgrhFch6HIpHDEa0moIt3HJ4SNxP4bbqjLEXpQgKh/HpUD7M8KlioDhVpIkqa6CoSCXvXIZqzev5vC2h/PwWQ9HeiRJkrSfWFSQolgoFF454Xe/g48/Dl+Lj4crroBbboHu3SM324EoQ0iSJKkRCYVg1b/DBYW8inAbEw/droDDb4EWEQ63+7sMIUmSpD2678P7eGvxWyTHJfPCBS/QPKF5pEeSJEn7iUUFKQoFg/Dqq+GCwty54WtJSXDNNfCb30BmZkTHkyRJkvZeKAgrXw1v8bChItzGJkH3a6DXb6C54VaSJEnwwfIPuP292wGYePZEjmh3RIQnkiRJ+5NFBSmKlJXBCy/APffAV1+FrzVvDj/7GYwcCRkZkZ1PkiRJ2mvBMsh+Ab68B/Irwm1cczj0Z9BzJCQbbiVJkhS2tnAtP3rpRwRDQYb1HsYVfa6I9EiSJGk/s6ggRYGSEnjuORg3DhYtCl9LS4Of/xx+8Qu3VpAkSVIDUl4Cy56DL8fB5opwG58GPX4OPX7h1gqSJEmqJhgKMuzVYazatIqebXoy8eyJBAKBSI8lSZL2M4sKUgQVFcGTT8L990N2dvha69Zw001www3QsmVEx5MkSZL2XnkRLH4SvroftlSE28TW0OMmOOwGSGgZ0fEkSZIUnR743wO8uehNkuKSeOGCF0hJSIn0SJIk6QCwqCBFQGEhPPYY/OEPsHp1+FpGBvz61/DTn0KKWVySJEkNRVkhfPsYLPgDbK0It0kZ0OvXcMhPId5wK0mSpJp9mP0ht717GwAPn/UwR6UfFeGJJEnSgWJRQTqA8vNh4kT44x8hLy98LTMTbrkFrrwSkpMjO58kSZK010ry4duJsOCPUFwRbptlwuG3wMFXQpzhVpIkSbuWtyWPS166hPJQOT8+6sdcecyVkR5JkiQdQBYVpANg3Tp48EH405/CZQWA7t1h9Gi47DJISIjsfJIkSdJeK14HCx+EhX+C0opwm9IdjhgNXS+DWMOtJEmSdi8YCnL5q5ezsmAlPVr3YNL/TSIQCER6LEmSdABZVJD2o9xcGD8eHn0UNm8OX+vVC267DS6+GOL8X6AkSZIaiq25sGA8fPsolFWE29RecMRt0OViiDHcSpIkae/8v4/+H298+wZJcUm8cOELpCS4XZgkSU2NP0mS9oOVK+H3v4fHH4eiovC1Pn1gzBg4/3yIiYnoeJIkSdLe27ISvvo9LH4cyivC7UF94IgxkHk+BAy3kiRJ2nsfrfiI0dNGA/DgmQ9ydPrREZ5IkiRFgkUFqR4tWQL33w9PPQWlpeFrWVlw++1w9tng6mWSJElqMDYvga/uhyVPQbAi3LbOgiNvhw6GW0mSJNXeui3r+NHff0R5qJxLjryEa753TaRHkiRJEWJRQaoHCxbAvffClClQXh6+dtJJ4YLCqaf6M1xJkiQ1IPkL4Mt7YfkUCFWE23YnhQsK6YZbSZIk7ZtQKMQV/7iCFQUrOLTVoTz2f48RMFtKktRkWVSQ6mDePLjnHnjxRQiFwtfOPBNuuw0GDYrsbJIkSVKtbJgHX94D2S8CFeG2/ZlwxG3QznArSZKkuhn/8Xj+9c2/SIxN5IULX6BFYotIjyRJkiLIooK0D2bNChcUXntt27Uf/ADGjIFjj43cXJIkSVKt5c0KFxRytgu3nX4AR4yB1oZbSZIk1d2MlTMYNW0UABPOnECfjD6RHUiSJEWcRQWpFj74AH73O3jrrfDjQAAuughuvRWOPjqys0mSJEm18t0HMP93kFsRbglA54vgiFvhIMOtJEmS6sf6reu5+O8XUxYs46IjLuKnfX8a6ZEkSVIUsKgg7UEoBO+8Ey4o/Pe/4WuxsfCTn8Do0dCjR2TnkyRJkvZaKAS578CXv4PvKsJtIBa6/gSOGA2phltJkiTVn1AoxPB/DCc7P5vuB3Xn8XMfJxAIRHosSZIUBSwqSLsQCsG//hUuKMyaFb6WkADDh8Mtt0C3bpGdT5IkSdproRDk/CtcUFhXEW5jEuDg4XD4LZBiuJUkSVL9e3Dmg7y28DUSYhN48cIXSU1MjfRIkiQpSlhUkHZQXg4vvwz33AOffx6+lpQEP/0p/PrX0KlTZOeTJEmS9lqwHFa+DPPvgY0V4TY2CQ75KfT6NTQz3EqSJGn/mJUzi5vfvhmAPw75I8e0PybCE0mSpGhiUUHaznvvwc9+BgsWhB+npMANN8BNN0F6emRnkyRJkmplzXvwyc+goCLcxqXAYTdAj5sg2XArSZKk/WfD1g1c/PeLKQ2WcsHhF3D9sddHeiRJkhRlLCpIFTZsgKFDw19btoRf/AJ+/nNo1SrSk0mSJEm1VLIBPhga/hrfEnr8Anr8HBINt5IkSdq/QqEQV752Jcs2LuPggw7mL+f+hUAgEOmxJElSlLGoIFUYNy5cUjj8cPj4Y0h1uzRJkiQ1VF+OC5cU0g6HMz6GeMOtJEmSDoyHZj3EqwteJSE2gRcueIG0pLRIjyRJkqJQTKQHkKJBdjb86U/h8/vvt6QgSZKkBqwwGxZWhNs+91tSkCRJ0gHz6apP+fVbvwbgD6f/gb4d+kZ4IkmSFK0sKkjA7bdDcTGcdBKcc06kp5EkSZLqYN7tECyGdidBB8OtJEmSDoyNRRu56MWLKA2W8sNeP2RE/xGRHkmSJEUxiwpq8j7/HP761/D5738PbpcmSZKkBmvD57C0Itz2MdxKkiTpwAiFQlz92tUs3biUri278sT3nyBgFpUkSbthUUFN3i23QCgEF10E/ftHehpJkiSpDj67BQhB54ugjeFWkiRJB8bETyby0tcvER8TzwsXvEDLpJaRHkmSJEU5iwpq0t55B/7zH4iPh3vvjfQ0kiRJUh3kvgOr/wMx8dDbcCtJkqQDY87qOfzqrV8B8MDpD9CvY78ITyRJkhoCiwpqsoJBuPnm8Pl110H37pGdR5IkSdpnoSDMrQi3h1wHLQy3kiRJ2v/yi/K56MWLKCkv4bye5/HzrJ9HeiRJktRAWFRQk/W3v8HcudCiBdx+e6SnkSRJkupg2d9gw1yIawFHGm4lSZK0/4VCIa755zUs3rCYLmldePL7TxIIBCI9liRJaiAsKqhJKi6G224Ln48aBW3bRnYeSZIkaZ+VF8O8inB7xChIMtxKkiRp/5v06SRe/OpF4mLimHrBVA5KPijSI0mSpAbEooKapIkTYfly6NABfvnLSE8jSZIk1cE3E6FwOSR3gB6/jPQ0kiSpgZs4cSJdu3YlKSmJrKwsZs2atdv7J0yYQI8ePUhOTiYzM5ObbrqJoqKiAzStImXu6rn88j+/BOD+wfeT1SkrsgNJkqQGx6KCmpwNG+B3vwuf//a30KxZZOeRJEmS9lnJBviyItwe/VuIM9xKkqR9N3XqVEaOHMnYsWOZM2cOvXv3ZsiQIXz33Xc13j9lyhRGjRrF2LFj+frrr3niiSeYOnUqt9566wGeXAdSQXEBF/39IkrKSzj3sHO5acBNkR5JkiQ1QBYV1OTcd1+4rHD44XD55ZGeRpIkSaqDL+8LlxXSDoduhltJklQ348eP55prrmH48OEcfvjhTJo0iWbNmvHkk0/WeP9HH33E8ccfz6WXXkrXrl0544wzuOSSS/a4CoMarlAoxLX/vJZF6xfROa0zT5/3NIFAINJjSZKkBsiigpqU7Gx48MHw+f33Q1xcZOeRJEmS9llhNiysCLd97ocYw60kSdp3JSUlzJ49m8GDB1ddi4mJYfDgwXz88cc1vua4445j9uzZVcWEJUuW8MYbb3D22Wfv8vsUFxdTUFBQ7VDD8efZf2bql1OJi4nj+aHP0yq5VaRHkiRJDZQ/yVKTcscdUFwMJ50E55wT6WkkSZKkOph3BwSLod1J0MFwK0mS6iYvL4/y8nLS09OrXU9PT2fBggU1vubSSy8lLy+PQYMGEQqFKCsr47rrrtvt1g/jxo3jrrvuqtfZdWB8nvs5v3jzFwCMO20cAzMHRngiSZLUkLmigpqMzz+HZ58Nn//+9+CKZJIkSWqwNnwOSyvCbR/DrSRJiozp06dz77338sgjjzBnzhxefvllXn/9de6+++5dvmb06NHk5+dXHStWrDiAE2tfbSrexIUvXkhxeTHnHHoOIweOjPRIkiSpgdunosLEiRPp2rUrSUlJZGVl7XbPsdLSUn7729/SvXt3kpKS6N27N2+++Wa1e8aNG0e/fv1o0aIF7dq147zzzmPhwoXV7jn55JMJBALVjuuuu25fxlcTNWoUhEJw0UXQv3+kp5EkSdHCbKsG6bNRQAg6XwRtDLeSJKnu2rRpQ2xsLGvWrKl2fc2aNWRkZNT4mttvv53LLruMq6++mqOOOorzzz+fe++9l3HjxhEMBmt8TWJiIqmpqdUORbdQKMR1r1/Ht+u/pVNqJ5457xliAv4NpCRJqptap4mpU6cycuRIxo4dy5w5c+jduzdDhgzhu+++q/H+MWPG8Nhjj/HQQw/x1Vdfcd1113H++eczd+7cqnvef/99brjhBmbMmMHbb79NaWkpZ5xxBoWFhdXe65prrmH16tVVx+9///vajq8m6p134M03IS4O7rkn0tNIkqRoYbZVg5T7Dqx+EwJx0NtwK0mS6kdCQgJ9+/Zl2rRpVdeCwSDTpk1j4MCal/jfsmULMTHVf8QcGxsLhH+5rcbhiblPMOWLKcQGYpl6wVRaN2sd6ZEkSVIjEAjVMjFmZWXRr18/Hn74YSAcVjMzM7nxxhsZNWrUTvd36NCB2267jRtuuKHq2tChQ0lOTua5556r8XusXbuWdu3a8f7773PiiScC4b8669OnDxMmTKjNuFUKCgpIS0sjPz/flm4TEwzCscfC3Llw443wpz9FeiJJklRX9ZXtzLZqcEJBePNY2DAXDrsRjjXcSpLU0EVTtps6dSqXX345jz32GP3792fChAm88MILLFiwgPT0dIYNG0bHjh0ZN24cAHfeeSfjx4/nz3/+M1lZWSxatIjrr7+evn37MnXq1L36ntH0+bWzeWvmkfWXLIrKirjvtPu4ZdAtkR5JkiRFsdpku1qtqFBSUsLs2bMZPHjwtjeIiWHw4MF8/PHHNb6muLiYpKSkateSk5P58MMPd/l98vPzAWjVqlW165MnT6ZNmzYceeSRjB49mi1bttRmfDVRzz8fLim0aAG33x7paSRJUrQw26pBWv58uKQQ1wKONNxKkqT6dfHFF/OHP/yBO+64gz59+vDZZ5/x5ptvkp6eDkB2djarV6+uun/MmDH86le/YsyYMRx++OFcddVVDBkyhMceeyxSH0H1aHPJZi568SKKyoo465Cz+M3xv4n0SJIkqRGJq83NeXl5lJeXVwXTSunp6SxYsKDG1wwZMoTx48dz4okn0r17d6ZNm8bLL79MeXl5jfcHg0F++ctfcvzxx3PkkUdWXb/00kvp0qULHTp0YN68edxyyy0sXLiQl19+ucb3KS4upri4uOpxQUFBbT6qGoniYrjttvD5LbdA27aRnUeSJEUPs60anPJi+Lwi3B5+CyQZbiVJUv0bMWIEI0aMqPG56dOnV3scFxfH2LFjGTt27AGYTAdSKBTi+tevZ+G6hXRs0ZFnz3+WmECtd5KWJEnapVoVFfbFgw8+yDXXXEPPnj0JBAJ0796d4cOH8+STT9Z4/w033MD8+fN3+qu0a6+9tur8qKOOon379px22mksXryY7t277/Q+48aN46677qrfD6MG55FHYNky6NABbrop0tNIkqSGzmyriPr2EShcBskdoKfhVpIkSfvPU589xXPzniM2EMvzFzxPm2ZtIj2SJElqZGpVgWzTpg2xsbGsWbOm2vU1a9aQkZFR42vatm3Lq6++SmFhIcuXL2fBggWkpKRw8MEH73TviBEj+Ne//sV7771Hp06ddjtLVlYWAIsWLarx+dGjR5Ofn191rFixYm8+ohqRjRvhd78Ln991FzRrFtFxJElSlDHbqkEp2QjzK8LtUXdBnOFWkiRJ+8f87+Yz4o3wqhp3n3I3gzoPivBEkiSpMapVUSEhIYG+ffsybdq0qmvBYJBp06YxcODA3b42KSmJjh07UlZWxksvvcQPfvCDqudCoRAjRozglVde4d1336Vbt257nOWzzz4DoH379jU+n5iYSGpqarVDTct998H69XD44XDFFZGeRpIkRRuzrRqUr+6DkvWQdjgcfEWkp5EkSVIjVVhSyEUvXsTWsq0M6T6EWwbdEumRJElSI1XrrR9GjhzJ5ZdfzrHHHkv//v2ZMGEChYWFDB8+HIBhw4bRsWNHxo0bB8DMmTPJycmhT58+5OTkcOeddxIMBrn55pur3vOGG25gypQp/OMf/6BFixbk5uYCkJaWRnJyMosXL2bKlCmcffbZtG7dmnnz5nHTTTdx4okncvTRR9fHv4MamexsmDAhfH7ffRC33zc5kSRJDZHZVg1CYTYsmBA+730fxBhuJUmStH/c8MYNfJ33NR1adODZ858lJlCrv3WUJEnaa7X+CdfFF1/M2rVrueOOO8jNzaVPnz68+eabpKenA5CdnU1MzLbwUlRUxJgxY1iyZAkpKSmcffbZ/PWvf6Vly5ZV9zz66KMAnHzyydW+11NPPcUVV1xBQkIC77zzTtUPjjMzMxk6dChjxozZh4+spuCOO6C4GE48Ef7v/yI9jSRJilZmWzUI8+6AYDG0OxE6Gm4lSZK0fzz92dM88/kzxARi+NvQv9GuebtIjyRJkhqxQCgUCkV6iAOhoKCAtLQ08vPzXSq3kZs3D/r0gVAIZs6E/v0jPZEkSapvTT3bNfXP36RsmAf/7gOE4IyZ0MZwK0lSY9PUs11T//zR4qu1X9Hv8X5sKd3C3afczZgTLVJLkqTaq022c90mNTq33BIuKVx4oSUFSZIkNXCf3QKEoPOFlhQkSZK0XxSWFHLhixeypXQLpx98OqMHjY70SJIkqQmwqKBGZdo0ePNNiIuDe++N9DSSJElSHeROg9VvQiAOehtuJUmStH/c+O8b+WrtV2SkZPDX8/9KbExspEeSJElNgEUFNRrBINx8c/j8uuvgkEMiO48kSZK0z0JBmFsRbg+9DloYbiVJklT/nv38WZ767CliAjFM+eEU0lPSIz2SJElqIiwqqNGYOhXmzIEWLeD22yM9jSRJklQHy6fChjkQ1wKONNxKkiSp/n299muuf/16AMaeNJZTup0S4YkkSVJTYlFBjUJxMdx6a/j8llugXbvIziNJkiTts/Ji+Lwi3B5+CyQZbiVJklS/tpRu4aK/X8SW0i2c2u1UbjvhtkiPJEmSmhiLCmoUHnkEli2D9u3hl7+M9DSSJElSHXz7CBQug+T20POXkZ5GkiRJjdAv/v0L5n83n/Tm6Uz+4WRiY2IjPZIkSWpiLCqowdu4EX73u/D5b38LzZtHdBxJkiRp35VshPkV4fao30Kc4VaSJEn1a/K8yfxl7l8IEGDyDyeTkZIR6ZEkSVITZFFBDd5998H69dCrF1xxRaSnkSRJkurgq/ugZD2k9oKDr4j0NJIkSWpkFuYt5Kf/+ikAd5x0B6cdfFqEJ5IkSU2VRQU1aCtWwIMPhs/vvx/i4iI7jyRJkrTPClfAwopw2+d+iDHcSpIkqf5sLd3KhS9eSGFpIad0PYXbT7w90iNJkqQmzKKCGrQ77oCiIjjhBPi//4v0NJIkSVIdfHEHlBdB2xOgo+FWkiRJ9euXb/6SL777gnbN2zH5h5OJjYmN9EiSJKkJs6igBmvePHjmmfD5Aw9AIBDZeSRJkqR9tmEeLKkIt8cYbiVJklS//vbF3/jznD8TIMBz5z9H+xbtIz2SJElq4iwqqMEaNQpCIbjwQsjKivQ0kiRJUh18NgoIQecLoY3hVpIkSfXnm3XfcO2/rgXgthNu4/Tup0d4IkmSJIsKaqDefRf+/W+Ii4N77430NJIkSVId5L4Lq/8NgTjobbiVJElS/SkqK+KiFy9ic8lmTupyEmNPHhvpkSRJkgCLCmqAgkG4+ebw+XXXwSGHRHYeSZIkaZ+FgvBZRbg99DpoYbiVJElS/bnpzZv4fM3ntG3WlilDpxAXExfpkSRJkgCLCmqApk6F2bMhJQVuvz3S00iSJEl1sHwqrJ8NcSlwpOFWkiRJ9eeFL19g0uxJAPz1/L/SoUWHCE8kSZK0jUUFNSjFxXDbbeHzW26Bdu0iO48kSZK0z8qL4fOKcHv4LZBkuJUkSVL9WLR+EVe/djUAtw66lSGHDInwRJIkSdVZVFCD8uijsHQptG8PN90U6WkkSZKkOvj2UShcCsntoafhVpIkSfXnxn/fyKaSTZzQ+QTuOuWuSI8jSZK0E4sKajA2boS77w6f33UXNG8e0XEkSZKkfVeyEeZXhNuj7oI4w60kSZLqR2l5KdOXTQfgkXMeIS4mLrIDSZIk1cCighqM+++H9euhVy8YPjzS00iSJEl18NX9ULIeUnvBwYZbSZIk1Z95a+ZRVFbEQUkHcXjbwyM9jiRJUo0sKqhBWLECJkwIn993H8RZApYkSVJDVbgCFk4In/e5D/wLN0mSJNWjGStnAJDVKYuYgL8CkCRJ0cmUogZh7FgoKoITToBzz430NJIkSVIdfDEWyoug7QnQ0XArSZKk+jUjJ1xUGNBxQIQnkSRJ2jWLCop6X3wBTz8dPn/gAQgEIjqOJEmStO82fgFLng6fH2O4lSRJUv2rXFFhQCeLCpIkKXpZVFDUGzUKQiG44ALIyor0NJIkSVIdfDYKCEHmBdDGcCtJkqT6lbclj0XrFwHQv2P/CE8jSZK0axYVFNXefRfeeAPi4uDeeyM9jSRJklQHue/CqjcgEAe9DbeSJEmqfzNXzgSgZ5ueHJR8UISnkSRJ2jWLCopawSDcfHP4/Kc/hUMPjew8kiRJ0j4LBeGzinB7yE8h1XArSZKk+le57UNWR1fvkiRJ0c2igqLWCy/A7NmQkgJ33BHpaSRJkqQ6WP4CrJ8NcSlwlOFWkiRJ+8eMnHBRYUCnARGeRJIkafcsKigqFRfDrbeGz2+5Bdq1i+w8kiRJ0j4rL4bPK8Lt4bdAkuFWkiRJ9S8YCjIrZxZgUUGSJEU/iwqKSpMmwdKl0L493HRTpKeRJEmS6uDbSVC4FJLbQ0/DrSRJkvaPBXkLKCguoFl8M45sd2Skx5EkSdotiwqKOhs3wt13h8/vuguaN4/oOJIkSdK+K9kIX1aE26PugjjDrSRJkvaPGSvD2z7069CPuJi4CE8jSZK0exYVFHXuvx/WrYOePWH48EhPI0mSJNXBV/dD8TpI7QkHG24lSZK0/1QWFdz2QZIkNQQWFRRVVq6ECRPC5/ffD3EWfyVJktRQbVkJCyeEz/vcD/5VmyRJkvYjiwqSJKkhsaigqHLHHVBUBIMGwbnnRnoaSZIkqQ7m3QHlRdB2EHQ03EqSJGn/2VS8ifnfzQcgq2NWhKeRJEnaM4sKihpffAHPPBM+f+ABCAQiO48kSZK0zzZ+AUsrwu0xhltJkiTtX5+s+oQQIbqkdaF9i/aRHkeSJGmPLCooaowaBcEgXHABDHB1MkmSJDVkn42CUBAyL4A2hltJkiTtX277IEmSGhqLCooK770Hb7wBcXFw772RnkaSJEmqgzXvwao3IBAHvQ23kiRJ2v8sKkiSpIbGooIiLhiEm28On//0p3DooZGdR5IkSdpnoSDMrQi3h/wUUg23kiRJ2r9CoZBFBUmS1OBYVFDEvfACfPoppKTAHXdEehpJkiSpDpa/AOs/hbgUOMpwK0mSpP1v6calrN2ylviYePpk9In0OJIkSXvFooIiqrgYbr01fH7zzdCuXWTnkSRJkvZZeTF8XhFue90MSYZbSZIk7X+Vqykc0/4YkuKSIjyNJEnS3rGooIiaNAmWLoWMDBg5MtLTSJIkSXXw7SQoXApJGdDLcCtJkqQDo2rbh45u+yBJkhoOiwqKmPx8uPvu8Pldd0Hz5pGdR5IkSdpnJfnwZUW4PfouiDPcSpIk6cCYmTMTgAGdLCpIkqSGw6KCIub++2HdOujZE668MtLTSJIkSXXw1f1QvA5Se8LBhltJkiQdGEVlRcxdPRewqCBJkhoWiwqKiJUr4Y9/DJ/fdx/ExUV2HkmSJGmfbVkJCyvCbZ/7IMZwK0mSpANj7uq5lAZLade8HV1bdo30OJIkSXvNooIiYuxYKCqCQYPg+9+P9DSSJElSHcwbC+VF0HYQdDTcSpKkhm3ixIl07dqVpKQksrKymDVr1i7vPfnkkwkEAjsd55xzzgGcuGmbsXIGEF5NIRAIRHgaSZKkvbdPRYXahNXS0lJ++9vf0r17d5KSkujduzdvvvlmrd+zqKiIG264gdatW5OSksLQoUNZs2bNvoyvCJs/H55+Onz+wANgfpYkSZFktlWdbJwPS58Onx9juJUkSQ3b1KlTGTlyJGPHjmXOnDn07t2bIUOG8N1339V4/8svv8zq1aurjvnz5xMbG8uFF154gCdvumbkVBQVOrrtgyRJalhqXVSobVgdM2YMjz32GA899BBfffUV1113Heeffz5z586t1XvedNNN/POf/+TFF1/k/fffZ9WqVfzwhz/ch4+sSBs1CoJBGDoUBpifJUlSBJltVWefjYJQEDKHQhvDrSRJatjGjx/PNddcw/Dhwzn88MOZNGkSzZo148knn6zx/latWpGRkVF1vP322zRr1syiwgG0/YoKkiRJDUkgFAqFavOCrKws+vXrx8MPPwxAMBgkMzOTG2+8kVGjRu10f4cOHbjtttu44YYbqq4NHTqU5ORknnvuub16z/z8fNq2bcuUKVO44IILAFiwYAG9evXi448/ZsBe/La7oKCAtLQ08vPzSU1Nrc1HVj2aPh1OOQXi4uDLL+GwwyI9kSRJaojqK9uZbVUna6bDtFMgEAfnfAmphltJklR70ZLtSkpKaNasGX//+98577zzqq5ffvnlbNy4kX/84x97fI+jjjqKgQMH8uc//3mX9xQXF1NcXFz1uKCggMzMzIh//oZo1aZVdBzfkZhADBtv2UiLxBaRHkmSJDVxtcm2tVpRoaSkhNmzZzN48OBtbxATw+DBg/n4449rfE1xcTFJSUnVriUnJ/Phhx/u9XvOnj2b0tLSavf07NmTzp077/b7FhQUVDsUWcEg/OY34fNrr7WkIEmSIstsqzoJBWFuRbg95FpLCpIkqcHLy8ujvLyc9PT0atfT09PJzc3d4+tnzZrF/Pnzufrqq3d737hx40hLS6s6MjMz6zR3UzZz5UwAjmx3pCUFSZLU4NSqqLAvYXXIkCGMHz+eb7/9lmAwyNtvv121d9nevmdubi4JCQm0bNlyr7+vgTf6vPgifPoppKTAHXdEehpJktTUmW1VJ9kvwvpPIS4FjjTcSpIkPfHEExx11FH0799/t/eNHj2a/Pz8qmPFihUHaMLGp2rbh45u+yBJkhqeWhUV9sWDDz7IoYceSs+ePUlISGDEiBEMHz6cmJj9+60NvNGlpARuvTV8fvPNsMPP7iVJkhoEs60AKC+BzyvCba+bIdlwK0mSGr42bdoQGxvLmjVrql1fs2YNGRkZu31tYWEhzz//PFddddUev09iYiKpqanVDu2bGTkVRYVOFhUkSVLDU6ufqO5LWG3bti2vvvoqhYWFLF++nAULFpCSksLBBx+81++ZkZFBSUkJGzdu3Ovva+CNLpMmwZIlkJEBI0dGehpJkiSzrepg0STYvASSMqCX4VaSJDUOCQkJ9O3bl2nTplVdCwaDTJs2jYEDB+72tS+++CLFxcX85Cc/2d9jqkJZsIxPcj4BIKtTVoSnkSRJqr1aFRXqElaTkpLo2LEjZWVlvPTSS/zgBz/Y6/fs27cv8fHx1e5ZuHAh2dnZe/y+irz8fPjtb8Pnd90FzZtHdh5JkiQw22ofleTD/Ipwe/RdEGe4lSRJjcfIkSN5/PHHeeaZZ/j666+5/vrrKSwsZPjw4QAMGzaM0aNH7/S6J554gvPOO4/WrVsf6JGbrC/WfMHWsq2kJqbSs03PSI8jSZJUa3G1fcHIkSO5/PLLOfbYY+nfvz8TJkzYKax27NiRcePGATBz5kxycnLo06cPOTk53HnnnQSDQW6++ea9fs+0tDSuuuoqRo4cSatWrUhNTeXGG29k4MCBDBjgslbR7ve/h3XroGdPuPLKSE8jSZK0jdlWtfb176F4HaT2hIMNt5IkqXG5+OKLWbt2LXfccQe5ubn06dOHN998k/SKfVyzs7N32vZs4cKFfPjhh7z11luRGLnJmpkzE4CsjlnEBPb7Ds+SJEn1rtZFhdqG1aKiIsaMGcOSJUtISUnh7LPP5q9//SstW7bc6/cE+OMf/0hMTAxDhw6luLiYIUOG8Mgjj9Tho+tAyMmBP/4xfH7ffRBX6/+LkyRJ2n/MtqqVLTmwoCLc9rkPYgy3kiSp8RkxYgQjRoyo8bnp06fvdK1Hjx6EQqH9PJV2NGPlDAAGdLLsLEmSGqZAqImkyIKCAtLS0sjPz3dP3wPo6qvhiSfg+OPhgw8gEIj0RJIkqTFo6tmuqX/+iJl5NSx+AtoeD4MNt5IkqX409WzX1D//vur5cE8WrlvI65e+ztmHnh3pcSRJkoDaZTvXhNJ+M38+PPVU+PyBB/w5riRJkhqwjfNhSUW47WO4lSRJUuSs37qehesWAuGtHyRJkhoiiwrab0aNgmAQhg6FgQMjPY0kSZJUB5+NglAQModCW8OtJEmSImdWziwADm11KK2btY7wNJIkSfvGooL2i+nT4fXXITYW7r030tNIkiRJdbBmOqx6HQKx0NtwK0mSpMiasXIGAAM6DYjwJJIkSfvOooLqXSgEN98cPv/pT+GwwyI7jyRJkrTPQiGYWxFuD/kppBpuJUmSFFkWFSRJUmNgUUH17sUX4ZNPICUF7rgj0tNIkiRJdZD9Iqz/BOJS4EjDrSRJkiIrGAoyM2cmYFFBkiQ1bBYVVK9KSmD06PD5b34D6emRnUeSJEnaZ+Ul8HlFuO31G0g23EqSJCmyvln3DRuLNpIcl8xR7Y6K9DiSJEn7zKKC6tVjj8GSJZCRASNHRnoaSZIkqQ4WPQabl0BSBvQ03EqSJCnyKrd9OLbDscTHxkd4GkmSpH1nUUH1Jj8ffvvb8Pmdd4a3fpAkSZIapJJ8mF8Rbo+6E+INt5IkSYq8yqJCVsesCE8iSZJUNxYVVG9+/3vIy4MePeCqqyI9jSRJklQHX/8eivMgtQd0N9xKkiQpOlQWFQZ0GhDhSSRJkurGooLqRU4O/PGP4fP77oO4uMjOI0mSJO2zLTmwoCLc9r4PYgy3kiRJirzCkkK++O4LwKKCJElq+CwqqF6MHQtbt8Lxx8MPfhDpaSRJkqQ6+GIslG+FtsdDJ8OtJEmSosOnqz4lGArSKbUTHVM7RnocSZKkOrGooDr78kt46qnw+QMPQCAQ2XkkSZKkfbbxS1hSEW77GG4lSZIUPdz2QZIkNSYWFVRno0ZBMAg//CEMHBjpaSRJkqQ6+GwUhIKQ+UNoa7iVJElS9JiRU1FU6GhRQZIkNXwWFVQn778P//oXxMbCuHGRnkaSJEmqgzXvw6p/QSAWehtuJUmSFD1CoZArKkiSpEbFooLq5K67wl+vvRYOOyyys0iSJEl1Mr8i3B5yLaQabiVJkhQ9svOzyd2cS1xMHN9r/71IjyNJklRnFhW0z/Lz4b//DZ//+teRnUWSJEmqk5J8+K4i3PYy3EqSJCm6VK6m0CejD8nxyRGeRpIkqe4sKmifTZ8O5eVw6KFw8MGRnkaSJEmqg++mQ6gcWhwKKYZbSZIkRZeqbR86uu2DJElqHCwqaJ+99Vb46xlnRHYOSZIkqc5WV4TbDMOtJEmSos+MnIqiQieLCpIkqXGwqKB9VllUOP30yM4hSZIk1VllUaG94VaSJEnRpbismDmr5wAWFSRJUuNhUUH7ZOlSWLQIYmPhlFMiPY0kSZJUB5uXwuZFEIiFdMOtJEmSostnuZ9RUl5Cm2ZtOPggtymTJEmNg0UF7ZO33w5/HTgQUlMjO4skSZJUJ7kV4bbNQIg33EqSJCm6zFgZ3vYhq2MWgUAgwtNIkiTVD4sK2idu+yBJkqRGo3LbhwzDrSRJkqLPjJxwUcFtHyRJUmNiUUG1VlYG06aFz884I7KzSJIkSXUSLIPcinDb3nArSZKk6DNz5UzAooIkSWpcLCqo1j79FDZuhJYt4dhjIz2NJEmSVAfrP4XSjRDfEloZbiVJkhRd1mxew9KNSwkQoF+HfpEeR5Ikqd5YVFCtvV2xhe9pp0FcXGRnkSRJkupkdUW4zTgNYgy3kiRJii4zc8KrKRze9nDSktIiPI0kSVL9saigWnurYgvf093CV5IkSQ1dbkW4zTDcSpIkKfrMWDkDcNsHSZLU+FhUUK0UFMDHH4fPz3ALX0mSJDVkpQWQVxFu2xtuJUmSFH0sKkiSpMbKooJqZfp0KC+HQw6Bbt0iPY0kSZJUB2umQ6gcUg6BFMOtJEmSokt5sJxZObMAiwqSJKnxsaigWqnc9sHVFCRJktTgra4It66mIEmSpCj05dovKSwtpEVCC3q16RXpcSRJkuqVRQXVSmVR4XS38JUkSVJDl1sRbjMMt5IkSYo+lds+9O/Yn9iY2AhPI0mSVL8sKmivLVsG334LsbFwyimRnkaSJEmqg83LYNO3EIiFdMOtJEmSok9lUcFtHyRJUmNkUUF77e23w18HDIC0tMjOIkmSJNVJbkW4bTMAEgy3kiRJij4WFSRJUmNmUUF7zW0fJEmS1GisdtsHSZIkRa+NRRv5Ou9rALI6ZkV4GkmSpPpnUUF7pbwcpk0Ln59xRmRnkSRJkuokWA5rKsJthuFWkiRJ0WdWziwADj7oYNo2bxvhaSRJkuqfRQXtldmzYcOG8JYP/fpFehpJkiSpDtbPhpINEJ8GrQ23kiRJij4zV84E3PZBkiQ1XhYVtFcqt3047TSIi4vsLJIkSVKd5FZu+3AaxBhuJUmSFH1m5MwAYEBHiwqSJKlxsqigvVJZVDjdLXwlSZLU0K2uLCoYbiVJkhR9QqEQM1ZWFBVcUUGSJDVSFhW0R5s2wccfh8/PcAtfSZIkNWSlmyCvIty2N9xKkiQp+ixav4j1W9eTGJtI74zekR5HkiRpv7CooD2aPh3KyqB7dzj44EhPI0mSJNXBmukQKoOU7pBiuJUkSVL0qVxNoW+HviTEJkR4GkmSpP3DooL2qHLbB1dTkCRJUoOXWxFuXU1BkiRJUapq24eObvsgSZIaL4sK2qPKosLpbuErSZKkhm51RbjNMNxKkiQpOs3IqSgqdLKoIEmSGi+LCtqt5cvhm28gNhZOOSXS00iSJEl1ULgcNn0DgVhIN9xKkiQp+mwp3cLnuZ8DFhUkSVLjtk9FhYkTJ9K1a1eSkpLIyspi1qxZu71/woQJ9OjRg+TkZDIzM7npppsoKiqqer5r164EAoGdjhtuuKHqnpNPPnmn56+77rp9GV+18Pbb4a9ZWdCyZURHkSRJ2i/Mtk3I6opw2zoLElpGdBRJkiSpJrNXzaY8VE6HFh3olNop0uNIkiTtN3G1fcHUqVMZOXIkkyZNIisriwkTJjBkyBAWLlxIu3btdrp/ypQpjBo1iieffJLjjjuOb775hiuuuIJAIMD48eMB+OSTTygvL696zfz58zn99NO58MILq73XNddcw29/+9uqx82aNavt+Kolt32QJEmNmdm2icl12wdJkiRFtxkrt237EAgEIjyNJEnS/lProsL48eO55pprGD58OACTJk3i9ddf58knn2TUqFE73f/RRx9x/PHHc+mllwLhvzC75JJLmDlzZtU9bdu2rfaa++67j+7du3PSSSdVu96sWTMyMjJqO7L2UXk5vPNO+PyMMyI7iyRJ0v5gtm1CguWQWxFu2xtuJUmSFJ1m5ISLClkdsyI8iSRJ0v5Vq60fSkpKmD17NoMHD972BjExDB48mI8//rjG1xx33HHMnj27agndJUuW8MYbb3D22Wfv8ns899xzXHnllTs1RidPnkybNm048sgjGT16NFu2bKnN+KqlOXNgwwZITYX+/SM9jSRJUv0y2zYxG+ZAyQaIT4XWhltJkiRFp+1XVJAkSWrMarWiQl5eHuXl5aSnp1e7np6ezoIFC2p8zaWXXkpeXh6DBg0iFApRVlbGddddx6233lrj/a+++iobN27kiiuu2Ol9unTpQocOHZg3bx633HILCxcu5OWXX67xfYqLiykuLq56XFBQUItPKti27cNpp0FcrdfekCRJim5m2yZmdUW4TT8NYgy3kiRJij4rC1ayatMqYgOx9G3fN9LjSJIk7Ve1WlFhX0yfPp17772XRx55hDlz5vDyyy/z+uuvc/fdd9d4/xNPPMFZZ51Fhw4dql2/9tprGTJkCEcddRQ//vGPefbZZ3nllVdYvHhxje8zbtw40tLSqo7MzMx6/2yNXWVR4XS38JUkSQLMtg1abkW4bW+4lSRJ2p2JEyfStWtXkpKSyMrKqlpNbFc2btzIDTfcQPv27UlMTOSwww7jjTfeOEDTNi6VqykcnX40zROaR3gaSZKk/atWRYU2bdoQGxvLmjVrql1fs2bNLvfXvf3227nsssu4+uqrOeqoozj//PO59957GTduHMFgsNq9y5cv55133uHqq6/e4yxZWeE9uhYtWlTj86NHjyY/P7/qWLFixd58RFXYtAk++ih8foZb+EqSpEbIbNuElG6CtRXhNsNwK0mStCtTp05l5MiRjB07ljlz5tC7d2+GDBnCd999V+P9JSUlnH766Sxbtoy///3vLFy4kMcff5yOHTse4MkbB7d9kCRJTUmtigoJCQn07duXadOmVV0LBoNMmzaNgQMH1viaLVu2EBNT/dvExsYCEAqFql1/6qmnaNeuHeecc84eZ/nss88AaN++fY3PJyYmkpqaWu3Q3nv/fSgrg4MPhu7dIz2NJElS/TPbNiHfvQ+hMkg5GFoYbiVJknZl/PjxXHPNNQwfPpzDDz+cSZMm0axZM5588ska73/yySdZv349r776Kscffzxdu3blpJNOonfv3gd48sbBooIkSWpKar31w8iRI3n88cd55pln+Prrr7n++uspLCxk+PDhAAwbNozRo0dX3X/uuefy6KOP8vzzz7N06VLefvttbr/9ds4999yqH+pC+IfCTz31FJdffjlxcdX3jF28eDF33303s2fPZtmyZbz22msMGzaME088kaOPPnpfP7t2w20fJElSU2C2bSJWV4TbDMOtJEnSrpSUlDB79mwGDx5cdS0mJobBgwfz8ccf1/ia1157jYEDB3LDDTeQnp7OkUceyb333kt5efmBGrvRKCkvYfbq2YBFBUmS1DTE7fmW6i6++GLWrl3LHXfcQW5uLn369OHNN98kPT0dgOzs7Gp/ZTZmzBgCgQBjxowhJyeHtm3bcu6553LPPfdUe9933nmH7Oxsrrzyyp2+Z0JCAu+88w4TJkygsLCQzMxMhg4dypgxY2o7vvZSZVHBbR8kSVJjZrZtInIrwm17w60kSdKu5OXlUV5eXpWFK6Wnp7NgwYIaX7NkyRLeffddfvzjH/PGG2+waNEifvazn1FaWsrYsWNrfE1xcTHFxcVVjwsKCurvQzRg89bMo6isiIOSDuLQVodGehxJkqT9LhDacY3aRqqgoIC0tDTy8/NdKncPsrOhSxeIiYF166Bly0hPJEmSVF1Tz3ZN/fPXSmE2/KMLBGJg6DpIaBnpiSRJkqqJlmy3atUqOnbsyEcffVRtK7Sbb76Z999/n5kzZ+70msMOO4yioiKWLl1atcLY+PHjeeCBB1i9enWN3+fOO+/krrvu2ul6pD9/pD0862Fu/PeNnHXIWbzx4zciPY4kSdI+qU22rfXWD2r83n47/DUry5KCJEmSGrjcinDbOsuSgiRJ0m60adOG2NhY1qxZU+36mjVryMjIqPE17du357DDDqu2DVqvXr3Izc2lpKSkxteMHj2a/Pz8qmPFihX19yEasBkrZwBu+yBJkpoOiwraSeW2D6e7ha8kSZIautUV4TbDcCtJkrQ7CQkJ9O3bl2nTplVdCwaDTJs2rdoKC9s7/vjjWbRoEcFgsOraN998Q/v27UlISKjxNYmJiaSmplY7ZFFBkiQ1PRYVVE15ObzzTvj8DLfwlSRJUkMWLIfcinDb3nArSZK0JyNHjuTxxx/nmWee4euvv+b666+nsLCQ4cOHAzBs2DBGjx5ddf/111/P+vXr+cUvfsE333zD66+/zr333ssNN9wQqY/QIK0tXMviDYsB6N+xf4SnkSRJOjDiIj2AosvcubB+PaSmQn8zsSRJkhqyDXOhZD3Ep0Jrw60kSdKeXHzxxaxdu5Y77riD3Nxc+vTpw5tvvkl6ejoA2dnZxMRs+9u3zMxM/vOf/3DTTTdx9NFH07FjR37xi19wyy23ROojNEgzc2YC0KtNL1omtYzsMJIkSQeIRQVVU7ntw6mnQnx8ZGeRJEmS6iS3ItymnwoxhltJkqS9MWLECEaMGFHjc9OnT9/p2sCBA5kxY8Z+nqpxq9z2IatTVoQnkSRJOnDc+kHVVBYVTncLX0mSJDV0qyvCbYbhVpIkSdGrsqgwoOOACE8iSZJ04FhUUJXNm+Gjj8LnZ7iFryRJkhqy0s2QVxFu2xtuJUmSFJ3Kg+XMypkFwIBOFhUkSVLTYVFBVd5/H0pLoVs36N490tNIkiRJdfDd+xAshebdIMVwK0mSpOi0IG8Bm0o20Ty+OUe0OyLS40iSJB0wFhVUZfttHwKByM4iSZIk1Unltg/tDbeSJEmKXpXbPvTr2I+4mLgITyNJknTgWFRQlcqigts+SJIkqcHLrQi3GYZbSZIkRa/KosKAjm77IEmSmhaLCgJgxQpYsABiYuDUUyM9jSRJklQHhSugYAEEYiDDcCtJkqToNSOnoqjQyaKCJElqWiwqCIC33w5/7d8fDjoosrNIkiRJdZJbEW5b9YcEw60kSZKiU0FxAV9+9yUAWZ2yIjyNJEnSgWVRQcC2bR9OPz2yc0iSJEl1troi3LY33EqSJCl6fZLzCSFCdG3ZlYyUjEiPI0mSdEBZVBDBILzzTvj8DLfwlSRJUkMWCsKainCbYbiVJElS9Jqx0m0fJElS02VRQcydC+vWQYsWkOUKY5IkSWrINsyF4nUQ1wLaGG4lSZIUvWbkVBQVOlpUkCRJTY9FBVVt+3DqqRAfH9lZJEmSpDqp3PYh41SIMdxKkiQpOoVCIVdUkCRJTZpFBVUVFU53C19JkiQ1dFVFBcOtJEmSoteSDUvI25JHQmwCfTL6RHocSZKkA86iQhO3eTP873/h8zPcwleSJEkNWelmyKsItxmGW0mSJEWvytUUjsk4hsS4xAhPI0mSdOBZVGji/vtfKC2Frl3hkEMiPY0kSZJUB9/9F4Kl0LwrtDDcSpIkKXrNzJkJuO2DJElquiwqNHHbb/sQCER2FkmSJKlOcrfb9sFwK0mSpChWuaKCRQVJktRUWVRo4iqLCm77IEmSpAZvdUW4bW+4lSRJUvTaWrqVublzAYsKkiSp6bKo0IStXAlffw0xMXDqqZGeRpIkSaqDLSuh4GsIxEC64VaSJEnRa27uXMqCZaQ3T6dLWpdIjyNJkhQRFhWasLffDn/t1w9atYrsLJIkSVKdrK4It636QaLhVpIkSdFr+20fAm5ZJkmSmiiLCk1Y5bYPp58e2TkkSZKkOsutCLcZhltJkiRFt+2LCpIkSU2VRYUmKhiEd94Jn5/hFr6SJElqyEJByK0It+0Nt5IkSYpuFhUkSZIsKjRZn30GeXmQkgIDzMOSJElqyDZ8BsV5EJcCbQy3kiRJil45BTmsKFhBTCCGYzscG+lxJEmSIsaiQhNVue3DKadAfHxkZ5EkSZLqZHVFuE0/BWIMt5IkSYpeM3NmAnBUu6NISUiJ8DSSJEmRY1GhiaosKrjtgyRJkhq83Ipwm2G4lSRJUnRz2wdJkqQwiwpNUGEhfPhh+NyigiRJkhq0skJYWxFu2xtuJUmSFN0sKkiSJIVZVGiC/vtfKC2FLl3g0EMjPY0kSZJUB9/9F4Kl0LwLtDDcSpIkKXqVlpfy6apPAcjqmBXhaSRJkiLLokITVLntw+mnQyAQ2VkkSZKkOlldue2D4VaSJEnRbf5389latpW0xDR6tOkR6XEkSZIiyqJCE1RZVHDbB0mSJDV4uRXh1m0fJEmSFOUqt33I6pRFTMAfzUuSpKbNNNTE5OTAV1+F/9jstNMiPY0kSZJUB1tyIP8rIADphltJkiRFtxk54aLCgI4DIjyJJElS5FlUaGLefjv8tV8/aNUqsrNIkiRJdZJbEW5b94NEw60kSZKiW+WKCgM6WVSQJEmyqNDEVG77cPrpkZ1DkiRJqrPVFeE2w3ArSZKk6LZuyzq+WfcNAP079o/wNJIkSZFnUaEJCQa3rahwhlv4SpIkqSELBbetqNDecCtJkqToNitnFgCHtT6M1s1aR3gaSZKkyLOo0IR8/jnk5UFKCgxwdTFJkiQ1ZBs+h+I8iEuB1oZbSZIkRTe3fZAkSarOokITUrntw8knQ0JCREeRJEmS6ia3Ity2OxliDbeSJEmKbjNyKooKHS0qSJIkgUWFJqWyqOC2D5IkSWrwVleEW7d9kCRJUpQLhoLMXDkTcEUFSZKkShYVmogtW+DDD8PnFhUkSZLUoJVtgbUV4daigiRJkqLcwryF5BfnkxyXzFHpR0V6HEmSpKhgUaGJ+O9/oaQEOneGww6L9DSSJElSHXz3XwiWQLPO0MJwK0mSpOg2Y2V424d+HfsRFxMX4WkkSZKig0WFJqJy24fTT4dAILKzSJIkSXVSte2D4VaSJEnRr7KokNUxK8KTSJIkRQ+LCk1EZVHBbR8kSZLU4OVWhNsMw60kSZKi38ycmQAM6DQgwpNIkiRFj30qKkycOJGuXbuSlJREVlYWs2bN2u39EyZMoEePHiQnJ5OZmclNN91EUVFR1fN33nkngUCg2tGzZ89q71FUVMQNN9xA69atSUlJYejQoaxZs2Zfxm9yVq2CL78M/7HZaadFehpJkqToYrZtYLasgvwvgQBkGG4lSZIU3TaXbOaL774ALCpIkiRtr9ZFhalTpzJy5EjGjh3LnDlz6N27N0OGDOG7776r8f4pU6YwatQoxo4dy9dff80TTzzB1KlTufXWW6vdd8QRR7B69eqq48MPP6z2/E033cQ///lPXnzxRd5//31WrVrFD3/4w9qO3yS9/Xb467HHQuvWkZ1FkiQpmphtG6DcinDb6lhINNxKkiQpun266lOCoSCZqZl0aNEh0uNIkiRFjbjavmD8+PFcc801DB8+HIBJkybx+uuv8+STTzJq1Kid7v/oo484/vjjufTSSwHo2rUrl1xyCTNnzqw+SFwcGRkZNX7P/Px8nnjiCaZMmcKpp54KwFNPPUWvXr2YMWMGAwbYRN2dym0fTj89snNIkiRFG7NtA7S6Ity2N9xKkiQp+s1YOQNwNQVJkqQd1WpFhZKSEmbPns3gwYO3vUFMDIMHD+bjjz+u8TXHHXccs2fPrlpCd8mSJbzxxhucffbZ1e779ttv6dChAwcffDA//vGPyc7Ornpu9uzZlJaWVvu+PXv2pHPnzrv8vsXFxRQUFFQ7mqJgcNuKCme4ha8kSVIVs20DFApuW1Ehw3ArSZKk6GdRQZIkqWa1WlEhLy+P8vJy0tPTq11PT09nwYIFNb7m0ksvJS8vj0GDBhEKhSgrK+O6666rtjxuVlYWTz/9ND169GD16tXcddddnHDCCcyfP58WLVqQm5tLQkICLVu23On75ubm1vh9x40bx1133VWbj9cozZsHa9dC8+YwcGCkp5EkSYoeZtsGaOM8KF4Lcc2hjeFWkiRJ0S0UCllUkCRJ2oVaraiwL6ZPn869997LI488wpw5c3j55Zd5/fXXufvuu6vuOeuss7jwwgs5+uijGTJkCG+88QYbN27khRde2OfvO3r0aPLz86uOFStW1MfHaXAqt304+WRISIjoKJIkSQ2e2TbCKrd9aHcyxBpuJUmSFN2W5y9nTeEa4mPiOSbjmEiPI0mSFFVqtaJCmzZtiI2NZc2aNdWur1mzZpd78N5+++1cdtllXH311QAcddRRFBYWcu2113LbbbcRE7NzV6Jly5YcdthhLFq0CICMjAxKSkrYuHFjtb882933TUxMJDExsTYfr1GqLCq47YMkSVJ1ZtsGqLKo0N5wK0mSpOhXuZpCn4w+JMcnR3gaSZKk6FKrFRUSEhLo27cv06ZNq7oWDAaZNm0aA3exr8CWLVt2+oFtbGwsEF76qiabN29m8eLFtG/fHoC+ffsSHx9f7fsuXLiQ7OzsXX5fwZYt8OGH4XOLCpIkSdWZbRuYsi2wtiLcZhhuJUmSFP3c9kGSJGnXarWiAsDIkSO5/PLLOfbYY+nfvz8TJkygsLCQ4cOHAzBs2DA6duzIuHHjADj33HMZP348xxxzDFlZWSxatIjbb7+dc889t+qHur/+9a8599xz6dKlC6tWrWLs2LHExsZyySWXAJCWlsZVV13FyJEjadWqFampqdx4440MHDiQAQMMebvywQdQXAyZmdCjR6SnkSRJij5m2wbkuw8gWAzNMiHVcCtJkqToZ1FBkiRp12pdVLj44otZu3Ytd9xxB7m5ufTp04c333yT9PR0ALKzs6v9ldmYMWMIBAKMGTOGnJwc2rZty7nnnss999xTdc/KlSu55JJLWLduHW3btmXQoEHMmDGDtm3bVt3zxz/+kZiYGIYOHUpxcTFDhgzhkUceqctnb/Qqt304/XQIBCI7iyRJUjQy2zYguRXhNsNwK0mSpOhXXFbM3Ny5gEUFSZKkmgRCu1qjtpEpKCggLS2N/Px8UlNTIz3OAXHUUTB/Pjz/PFx8caSnkSRJqj9NMdttr0l+/tePgvz5cPzz0MVwK0mSGo9oy3YTJ07kgQceIDc3l969e/PQQw/Rv3//Gu99+umnq1Yjq5SYmEhRUdFef79o+/z1ZcbKGQx8YiBtm7Vlza/XELBsK0mSmoDaZLuY3T6rBmv16nBJIRCA006L9DSSJOn/t3ff4VGV+fvH75mUSYGEAOkJRZAqvcSAgEJoahRwkRUXFBVwhbWgu4KCoP4WrIirKOpXwC66ouKCYIjCqvTQFUNEikAKSE2ABJLn90cyswwpJKRMJrxf15WLyZk5z/mck3NObvHDeQCUw+nU/CYFWaRQwi0AAEBlWbBggSZMmKCpU6dq48aNateunfr376+MjIxi1wkICFBqaqrja+/evVVYcfVln/YhJiqGJgUAAIAi0KhQQyUk5P/ZsaNUv75rawEAAADKJbUg3NbtKPkQbgEAACrLzJkzNXr0aI0aNUqtWrXSnDlz5Ofnp7lz5xa7jsViUVhYmOPLPo3a5W7tgbWSpKsjmfYBAACgKDQq1FDfFEzh26+fa+sAAAAAyi2tINyGEW4BAAAqS05OjpKSkhQXF+dYZrVaFRcXp9WrVxe7XmZmpho2bKjo6GjdfPPN+umnn6qi3GrP/kSFq6NoVAAAACgKjQo1UF7e/56oQKMCAAAA3JrJk9IKwm044RYAAKCyHD58WLm5uYWeiBAaGqq0tLQi12nevLnmzp2rL7/8Uu+//77y8vLUrVs37d+/v9jtZGdn68SJE05fNU1aZpr2HNsjiyzqEtnF1eUAAABUSzQq1EDbtkkZGZK/vxQb6+pqAAAAgHI4tk06kyF5+kv1CbcAAADVSWxsrEaOHKn27durV69eWrhwoYKDg/XGG28Uu86MGTMUGBjo+IqOjq7CiqvG2v350z60DmmtAFuAi6sBAAConmhUqIHs0z706iXZbK6tBQAAACiX1IJwG9JL8iDcAgAAVJb69evLw8ND6enpTsvT09MVFhZWqjG8vLzUoUMH/frrr8V+ZtKkSTp+/Ljj6/fffy9X3dWRY9qHSKZ9AAAAKA6NCjWQvVGBaR8AAADg9tIKwm0Y4RYAAKAyeXt7q1OnTkpMTHQsy8vLU2JiomJL+djW3Nxcbdu2TeHh4cV+xmazKSAgwOmrpllzoKBRIYpGBQAAgOJ4uroAVKzTp6Xvv89/TaMCAAAA3Nq501JGQbgNJ9wCAABUtgkTJuiOO+5Q586d1bVrV82aNUtZWVkaNWqUJGnkyJGKjIzUjBkzJElPPfWUrr76ajVt2lTHjh3T888/r7179+qee+5x5W641Lm8c1p/YL0kGhUAAABKQqNCDfP991J2thQVJbVo4epqAAAAgHI49L2Uly35RUkBhFsAAIDKNmzYMB06dEhPPPGE0tLS1L59ey1dulShoaGSpH379slq/d9Deo8eParRo0crLS1NQUFB6tSpk1atWqVWrVq5ahdc7qeMn5R1NksBtgC1DG7p6nIAAACqLRoVahj7tA99+0oWi2trAQAAAMol1T7tA+EWAACgqowfP17jx48v8r0VK1Y4ff/SSy/ppZdeqoKq3Mea/fnTPnSN7CqrhZmXAQAAikNSqmHsjQpM+wAAAAC3l2ZvVCDcAgAAwD2sOZDfqHB1JNM+AAAAlIRGhRokNVXati3/H5vFxbm6GgAAAKAcTqdKx7ZJskhhhFsAAAC4B/sTFa6OolEBAACgJDQq1CDLl+f/2aGDVL++a2sBAAAAyiWtINwGdZB8CLcAAACo/o6ePqpfDv8iKX/qBwAAABSPRoUahGkfAAAAUGOkFoTbcMItAAAA3MP6g+slSU2CmijYP9jF1QAAAFRvNCrUEMZICQn5r2lUAAAAgFszRkorCLc0KgAAAMBNMO0DAABA6dGoUENs2yalp0t+flK3bq6uBgAAACiHY9ukM+mSh59Un3ALAAAA90CjAgAAQOnRqFBD2Kd96NVLstlcWwsAAABQLmkF4Takl+RBuAUAAED1Z4yhUQEAAKAMaFSoIeyNCkz7AAAAALeXWhBumfYBAAAAbiLlSIqOnjkqH08ftQ1t6+pyAAAAqj0aFWqA06el77/Pf02jAgAAANzaudPSoYJwS6MCAAAA3IT9aQqdwjvJ28PbxdUAAABUfzQq1AA//CCdOSNFRkotW7q6GgAAAKAcDv0g5Z6RfCOlAMItAAAA3APTPgAAAJQNjQo1gH3ah759JYvFtbUAAAAA5ZJmn/aBcAsAAAD3QaMCAABA2dCoUAPYGxWY9gEAAABuL7Ug3IYRbgEAAOAesnKytDV9qyQaFQAAAEqLRgU3l5Ymbc3PwIqLc20tAAAAQLmcTpOOFYTbMMItAAAA3ENSapJyTa4ia0cqKiDK1eUAAAC4BRoV3Nzy5fl/duggBQe7thYAAACgXNIKwm1QB8mHcAsAAAD3wLQPAAAAZUejgptj2gcAAADUGPZpH8IJtwAAAHAfNCoAAACUHY0KbswYKSEh/zWNCgAAAHBrxkhpBeE2jHALAAAA92CMcTQqxETGuLgaAAAA90Gjghvbvl1KS5N8faXu3V1dDQAAAFAOx7dLZ9IkD18pmHALAAAA97D/xH6lZqbKw+KhThGdXF0OAACA26BRwY3Zp33o1Uuy2VxbCwAAAFAu9mkfQnpJHoRbAAAAuAf70xTahbWTn5efi6sBAABwHzQquDF7owLTPgAAAMDt2RsVwgm3AAAAcB/2RoWrI692cSUAAADuhUYFN3XmjPTf/+a/plEBAAAAbi33jHSoINyGEW4BAADgPtYcKGhUiKJRAQAAoCxoVHBTP/yQ36wQESG1auXqagAAAIByOPRDfrOCb4QUSLgFAACAe8jJzVHSwSRJNCoAAACUFY0Kbso+7UPfvpLF4tpaAAAAgHKxT/sQRrgFAACA+9iStkXZudmq61tXTes2dXU5AAAAboVGBTdlb1Rg2gcAAAC4PXujQjjhFgAAAO5jzf7/TftgoeEWAACgTGhUcEPp6dKWLfmv4+JcWwsAAABQLqfTpWMF4TaMcAsAAAD3seZAQaNCJNM+AAAAlBWNCm5o+fL8P9u3l0JCXFoKAAAAUD5pBeE2qL3kQ7gFAACA+zj/iQoAAAAoGxoV3BDTPgAAAKDGSCsIt2GEWwAAALiPjKwM/Xb0N1lkUdfIrq4uBwAAwO3QqOBmjJESEvJf06gAAAAAt2aMlFYQbsMJtwAAAHAfa/evlSS1DG6pQJ9AF1cDAADgfmhUcDM//SSlpkq+vlL37q6uBgAAACiH4z9Jp1MlD18pmHALAAAA92Gf9iEmMsbFlQAAALgnGhXcjH3ah549JR8f19YCAAAAlEtqQbgN6Sl5EG4BAADgPtYeyH+iwtVRV7u4EgAAAPdEo4KbsTcqMO0DAAAA3F5aQbgNI9wCAADAfeTm5WrdgXWSaFQAAAC4VDQquJEzZ6T//jf/NY0KAAAAcGu5Z6SMgnAbTrgFAACA+9hxeIdO5pyUv5e/Wge3dnU5AAAAbolGBTfy44/S6dNSeLjUmvwLAAAAd3boRyn3tOQbLgUSbgEAAOA+1uxfI0nqGtlVHlYPF1cDAADgni6pUWH27Nlq1KiRfHx8FBMTo3Xr1pX4+VmzZql58+by9fVVdHS0HnroIZ05c8bx/owZM9SlSxfVrl1bISEhGjRokJKTk53GuPbaa2WxWJy+7r333ksp323Zp33o21eyWFxbCwAAQE1BtnWRVPu0D4RbAAAAuBd7owLTPgAAAFy6MjcqLFiwQBMmTNDUqVO1ceNGtWvXTv3791dGRkaRn//www81ceJETZ06VTt27NDbb7+tBQsW6LHHHnN8ZuXKlRo3bpzWrFmjhIQEnT17Vv369VNWVpbTWKNHj1Zqaqrj67nnnitr+W7N3qjAtA8AAAAVg2zrQmn2RgXCLQAAANwLjQoAAADl51nWFWbOnKnRo0dr1KhRkqQ5c+Zo8eLFmjt3riZOnFjo86tWrVL37t01fPhwSVKjRo102223ae3atY7PLF261Gmd+fPnKyQkRElJSerZs6djuZ+fn8LCwspaco2QkSFt3pz/Oi7OpaUAAADUGGRbFzmTIR3dnP86jHALAAAA93H8zHH9fOhnSVJMZIyLqwEAAHBfZXqiQk5OjpKSkhR33v8pt1qtiouL0+rVq4tcp1u3bkpKSnI8Qve3337TkiVLdP311xe7nePHj0uS6tat67T8gw8+UP369XXVVVdp0qRJOnXqVFnKd2vLl+f/2a6dFBrq2loAAABqArKtC6UVhNs67SRfwi0AAADcx/qD62Vk1LhOY4XWIssCAABcqjI9UeHw4cPKzc1V6AX/pzw0NFS//PJLkesMHz5chw8f1jXXXCNjjM6dO6d7773X6fG458vLy9ODDz6o7t2766qrrnIap2HDhoqIiNDWrVv16KOPKjk5WQsXLixynOzsbGVnZzu+P3HiRFl2tdph2gcAAICKRbZ1odSCcBtOuAUAAIB7YdoHAACAilHmqR/KasWKFZo+fbpee+01xcTE6Ndff9UDDzygp59+WlOmTCn0+XHjxmn79u364YcfnJaPGTPG8bpNmzYKDw9Xnz59tGvXLjVp0qTQODNmzNCTTz5Z8TvkAsZICQn5r2lUAAAAcB2ybQUwRkorCLc0KgAAAMDN0KgAAABQMco09UP9+vXl4eGh9PR0p+Xp6enFzq87ZcoUjRgxQvfcc4/atGmjwYMHa/r06ZoxY4by8vKcPjt+/Hj95z//0XfffaeoqKgSa4mJyZ//69dffy3y/UmTJun48eOOr99//720u1nt/PyzdPCg5OMjXXONq6sBAACoGci2LnL8Z+n0QcnDRwom3AIAAMB9GGNoVAAAAKggZWpU8Pb2VqdOnZSYmOhYlpeXp8TERMXGxha5zqlTp2S1Om/Gw8NDUn6ws/85fvx4ff755/r222/VuHHji9ayefNmSVJ4eHiR79tsNgUEBDh9uSv7tA89e+Y3KwAAAKD8yLYuklYQboN75jcrAAAAAG5i19Fd+uP0H/L28Fa70HauLgcAAMCtlXnqhwkTJuiOO+5Q586d1bVrV82aNUtZWVkaNWqUJGnkyJGKjIzUjBkzJEnx8fGaOXOmOnTo4Hg87pQpUxQfH+/4S91x48bpww8/1JdffqnatWsrLS1NkhQYGChfX1/t2rVLH374oa6//nrVq1dPW7du1UMPPaSePXuqbdu2FXUsqi17owLTPgAAAFQssq0LpBaEW6Z9AAAAgJtZu3+tJKljeEfZPG0urgYAAMC9lblRYdiwYTp06JCeeOIJpaWlqX379lq6dKlCQ0MlSfv27XP6V2aTJ0+WxWLR5MmTdeDAAQUHBys+Pl7//Oc/HZ95/fXXJUnXXnut07bmzZunO++8U97e3lq+fLnjL46jo6N1yy23aPLkyZeyz24lO1tauTL/NY0KAAAAFYtsW8Vys6WMgnBLowIAAADcjGPah0imfQAAACgvi7E/o7aGO3HihAIDA3X8+HG3elTut99KffpIYWHSwYOSxeLqigAAAFzPXbNdRXHb/U/7Vvq2j+QTJg0m3AIAAEhunO0qiDvtf5e3umjDwQ36+JaPNeyqYa4uBwAAoNopS7azlvguXM4+7UPfvvw9LgAAANxcWkG4DSPcAgAAwL2cPntam9M2S5KujuKJCgAAAOVFo0I1Z29UYNoHAAAAuL3UgnDLtA8AAABwMxtTN+pc3jmF1QpTg8AGri4HAADA7dGoUI0dOiRt2pT/Oi7OtbUAAAAA5XLmkHS0INyGEW4BAADgXtbsXyMp/2kKFp4OBgAAUG40KlRjy5fn/9m2rRQW5tpaAAAAgHJJKwi3ddpKvoRbAAAAuJc1BwoaFSKZ9gEAAKAi0KhQjTHtAwAAAGqMNKZ9AAAAgPs6/4kKAAAAKD8aFaopY6SEhPzXNCoAAADArRkjpRaE2zDCLQAAQHU2e/ZsNWrUSD4+PoqJidG6detKtd7HH38si8WiQYMGVW6BLrD/xH7tP7FfVotVnSM6u7ocAACAGoFGhWpqxw7pwAHJZpOuucbV1QAAAADlcGKHdPqAZLVJwYRbAACA6mrBggWaMGGCpk6dqo0bN6pdu3bq37+/MjIySlxvz549euSRR9SjR48qqrRqrd2/VpLUNrSt/L39XVwNAABAzUCjQjVln/ahZ0/J19e1tQAAAADlkloQbkN6Sp6EWwAAgOpq5syZGj16tEaNGqVWrVppzpw58vPz09y5c4tdJzc3V7fffruefPJJXXHFFVVYbdVxTPsQybQPAAAAFYVGhWrK3qjAtA8AAABwe/ZGhXDCLQAAQHWVk5OjpKQkxcXFOZZZrVbFxcVp9erVxa731FNPKSQkRHfffXdVlOkSaw4UNCpE0agAAABQUTxdXQAKy86WVq7Mf02jAgAAANxabraUURBuwwi3AAAA1dXhw4eVm5ur0NBQp+WhoaH65Zdfilznhx9+0Ntvv63NmzeXejvZ2dnKzs52fH/ixIlLqreqnM09q6SDSZKkmKgYF1cDAABQc/BEhWpo1Srp1CkpNFRq08bV1QAAAADlcHiVlHtK8gmV6hBuAQAAaoqTJ09qxIgReuutt1S/fv1SrzdjxgwFBgY6vqKjoyuxyvLblrFNp8+dVh2fOmpWr5mrywEAAKgxeKJCNWSf9qFvX8licW0tAAAAQLnYp30II9wCAABUZ/Xr15eHh4fS09OdlqenpyssLKzQ53ft2qU9e/YoPj7esSwvL0+S5OnpqeTkZDVp0qTQepMmTdKECRMc3584caJaNyus2Z8/7UNMZIysFv7dHwAAQEWhUaEaSkjI/5NpHwAAAOD20grCbTjhFgAAoDrz9vZWp06dlJiYqEGDBknKbzxITEzU+PHjC32+RYsW2rZtm9OyyZMn6+TJk3r55ZeLbT6w2Wyy2WwVXn9lsTcqXB11tYsrAQAAqFloVKhmDh2SNm7Mfx0X59paAAAAgHI5c0g6UhBuwwi3AAAA1d2ECRN0xx13qHPnzuratatmzZqlrKwsjRo1SpI0cuRIRUZGasaMGfLx8dFVV13ltH6dOnUkqdByd0ajAgAAQOWgUaGaSUyUjJHatJHCw11dDQAAAFAOaYmSjFSnjeRLuAUAAKjuhg0bpkOHDumJJ55QWlqa2rdvr6VLlyo0NFSStG/fPlmtl8/0B3+c+kMpR1IkSV0ju7q4GgAAgJqFRoVq5puCKXyZ9gEAAABuL60g3IYRbgEAANzF+PHji5zqQZJWrFhR4rrz58+v+IJcaO2BtZKk5vWaq65vXRdXAwAAULNcPu2vbsAYKaFgCl8aFQAAAODWjJHSCsJtOOEWAAAA7odpHwAAACoPjQrVyC+/SPv3Szab1KOHq6sBAAAAyuHEL9Kp/ZLVJgUTbgEAAOB+aFQAAACoPDQqVCP2aR969JB8fV1bCwAAAFAuqQXhNqSH5Em4BQAAgHvJM3mOqR9oVAAAAKh4NCpUI/ZGBaZ9AAAAgNtLKwi3YYRbAAAAuJ9fDv+iE9kn5Oflp6tCrnJ1OQAAADUOjQrVRHa2tGJF/msaFQAAAODWcrOl9BX5r8MJtwAAAHA/9mkfukR0kafV08XVAAAA1Dw0KlQTq1dLp05JISFSmzaurgYAAAAoh8OrpdxTkk+IVIdwCwAAAPdjb1SIiYxxcSUAAAA1E40K1YR92oe+fSUrPxUAAAC4s1T7tA99JQvhFgAAAO5n7YG1kqSro652cSUAAAA1E39rWE0kJOT/ybQPAAAAcHtpBeE2jHALAAAA93My+6S2Z2yXJMVE8UQFAACAykCjQjVw+LCUlJT/um9f19YCAAAAlMuZw9KRgnAbTrgFAACA+9lwcIPyTJ4aBDZQRO0IV5cDAABQI9GoUA0kJkrGSFddJYWHu7oaAAAAoBzSEyUZKfAqyZdwCwAAAPezZv8aSUz7AAAAUJloVKgGvimYwpdpHwAAAOD2UgvCbTjhFgAAAO5pzYGCRoVIGhUAAAAqC40KLmaMlFAwhS+NCgAAAHBrxkhpBeE2jHALAAAA92OM4YkKAAAAVYBGBRdLTpZ+/13y9pZ69HB1NQAAAEA5nEiWTv0uWb2lEMItAAAA3M+eY3uUkZUhL6uXOoR3cHU5AAAANRaNCi5mn/ahRw/Jz8+1tQAAAADlklYQboN7SJ6EWwAAALgf+9MUOoR3kI+nj4urAQAAqLloVHAxe6MC0z4AAADA7aUWhNtwwi0AAADck2Pah0imfQAAAKhMNCq4UE6OtGJF/msaFQAAAODWcnOkjBX5r2lUAAAAgJtac6CgUSGKRgUAAIDKRKOCC61eLWVlScHBUtu2rq4GAAAAKIfDq6VzWZItWKpDuAUAAID7OXPujDalbpJEowIAAEBlo1HBhezTPvTtK1n5SQAAAMCdpRWE27C+koVwCwAAAPezKXWTzuadVYh/iBrVaeTqcgAAAGo0/gbRhRIS8v9k2gcAAAC4vdSCcMu0DwAAAHBTaw+slSTFRMbIYrG4uBoAAICajUYFF/njD2nDhvzXcXGurQUAAAAol+w/pCMF4TaMcAsAAAD3tGb/GklM+wAAAFAVaFRwkcREyRipdWspMtLV1QAAAADlkJYoyUiBrSU/wi0AAADcE40KAAAAVYdGBRf5pmAKX6Z9AAAAgNtLKwi3YYRbAAAAuKfUk6nae3yvLLKoS0QXV5cDAABQ49Go4ALGSAkFU/jSqAAAAAC3ZoyUWhBuwwm3AAAAcE9rD6yVJF0VcpVq22q7uBoAAICaj0YFF9i5U9q3T/L2lnr2dHU1AAAAQDmc3Cmd2idZvaUQwi0AAADcE9M+AAAAVC0aFVzAPu3DNddIfn6urQUAAAAol9SCcBt8jeRJuAUAAIB7olEBAACgatGo4AJM+wAAAIAaI41pHwAAAODezuWd0/qD6yXRqAAAAFBVaFSoYjk50nff5b+mUQEAAABuLTdHSi8It2GEWwAAALin7RnbdersKQXYAtSifgtXlwMAAHBZuKRGhdmzZ6tRo0by8fFRTEyM1q1bV+LnZ82apebNm8vX11fR0dF66KGHdObMmTKNeebMGY0bN0716tVTrVq1dMsttyg9Pf1SynepNWukzEwpOFhq187V1QAAAIBsWw5/rJHOZUq2YCmIcAsAAAD3ZJ/2ISYyRlYL/7YPAACgKpQ5dS1YsEATJkzQ1KlTtXHjRrVr1079+/dXRkZGkZ//8MMPNXHiRE2dOlU7duzQ22+/rQULFuixxx4r05gPPfSQvvrqK3366adauXKlDh48qCFDhlzCLrvWNwVT+MbFSVYyLwAAgEuRbcsptSDchsVJ/IUuAAAA3JS9UYFpHwAAAKpOmf82cebMmRo9erRGjRqlVq1aac6cOfLz89PcuXOL/PyqVavUvXt3DR8+XI0aNVK/fv102223Of2rsouNefz4cb399tuaOXOmevfurU6dOmnevHlatWqV1qxZc4m77hoJBVP4Mu0DAACA65FtyymtINyGE24BAADgvmhUAAAAqHplalTIyclRUlKS4uLi/jeA1aq4uDitXr26yHW6deumpKQkx1/e/vbbb1qyZImuv/76Uo+ZlJSks2fPOn2mRYsWatCgQbHbzc7O1okTJ5y+XO3IEWn9+vzXffu6thYAAIDLHdm2nLKPSH8UhNswwi0AAADc09HTR5X8R7IkqWtkVxdXAwAAcPnwLMuHDx8+rNzcXIWGhjotDw0N1S+//FLkOsOHD9fhw4d1zTXXyBijc+fO6d5773U8Hrc0Y6alpcnb21t16tQp9Jm0tLQitztjxgw9+eSTZdm9SpeYKBkjtWolRUa6uhoAAIDLG9m2nNITJRkpsJXkR7gFAACAe1p3IL8JuWndpqrvV9/F1QAAAFw+Kn0i2RUrVmj69Ol67bXXtHHjRi1cuFCLFy/W008/XanbnTRpko4fP+74+v333yt1e6XxTcEUvkz7AAAA4J7ItudJLQi3YYRbAAAAuC+mfQAAAHCNMj1RoX79+vLw8FB6errT8vT0dIWFhRW5zpQpUzRixAjdc889kqQ2bdooKytLY8aM0eOPP16qMcPCwpSTk6Njx445/cuzkrZrs9lks9nKsnuVyhgpoWAKXxoVAAAAXI9sWw7GSGkF4TaccAsAAAD3teZAQaNCJI0KAAAAValMT1Tw9vZWp06dlJiY6FiWl5enxMRExcbGFrnOqVOnZLU6b8bDw0OSZIwp1ZidOnWSl5eX02eSk5O1b9++Yrdb3aSkSHv3St7eUs+erq4GAAAAZNtyOJkiZe2VrN5SCOEWAAAA7inP5Gnt/rWSeKICAABAVSvTExUkacKECbrjjjvUuXNnde3aVbNmzVJWVpZGjRolSRo5cqQiIyM1Y8YMSVJ8fLxmzpypDh06KCYmRr/++qumTJmi+Ph4x1/qXmzMwMBA3X333ZowYYLq1q2rgIAA/e1vf1NsbKyuvto9AqR92ofu3SV/f9fWAgAAgHxk20tkn/YhuLvkSbgFAACAe0r5I0VHzxyVj6eP2oa2dXU5AAAAl5UyNyoMGzZMhw4d0hNPPKG0tDS1b99eS5cuVWhoqCRp3759Tv/KbPLkybJYLJo8ebIOHDig4OBgxcfH65///Gepx5Skl156SVarVbfccouys7PVv39/vfbaa+XZ9yrFtA8AAADVD9n2EtmnfQgj3AIAAMB9rdmfP+1D54jO8vLwcnE1AAAAlxeLMca4uoiqcOLECQUGBur48eMKCAio0m2fPSvVrStlZkpJSVLHjlW6eQAAgBrHldmuOnDp/uedlf5dVzqXKQ1IkuoSbgEAAMqDbOu6/f/rf/6qOUlz9EjsI3q+3/NVum0AAICaqCzZzlriu6gQa9bkNynUry+1b+/qagAAAIByOLwmv0nBVl8Kau/qagAAAIBLtuZA/hMVro5ykynYAAAAahAaFarANwVT+MbFSVaOOAAAANxZakG4DYuTLIRbAAAAuKesnCxtTd8qiUYFAAAAV+BvFqtAQsEUvv2YwhcAAADuLq0g3IYRbgEAAOC+NhzcoDyTp6iAKEUGRLq6HAAAgMsOjQqV7MgRaf36/Nd9+7q2FgAAAKBcso9IRwrCbTjhFgAAAO5rzX6mfQAAAHAlGhUq2bffSnl5UsuWUlSUq6sBAAAAyiH9W8nkSQEtJT/CLQAAANzXmgMFjQqRNCoAAAC4Ao0Kleybgil8mfYBAAAAbi+1INyGE24BAADgvowxjicqxETFuLgaAACAyxONCpXIGBoVAAAAUEMYI6UVhNswwi0AAADc1+8nfldaZpo8rZ7qGN7R1eUAAABclmhUqES//irt3St5eUm9erm6GgAAAKAcTv4qZe2VrF5SKOEWAAAA7sv+NIV2oe3k5+Xn4moAAAAuTzQqVCL70xS6d5f8/V1bCwAAAFAu9qcp1O8ueRJuAQAA4L7sjQpXR13t4koAAAAuXzQqVKKEhPw/mfYBAAAAbi+tINyGE24BAADg3mhUAAAAcD0aFSrJ2bPSt9/mv+7b17W1AAAAAOWSd1ZKKwi3YYRbAACAmmz27Nlq1KiRfHx8FBMTo3Xr1hX72YULF6pz586qU6eO/P391b59e7333ntVWG3ZZZ/L1sbUjZJoVAAAAHAlGhUqydq10smTUr16UocOrq4GAAAAKIfDa6VzJyVbPSmIcAsAAFBTLViwQBMmTNDUqVO1ceNGtWvXTv3791dGRkaRn69bt64ef/xxrV69Wlu3btWoUaM0atQoLVu2rIorL70t6VuUnZuter711CSoiavLAQAAuGx5urqAmqpdO+nzz6U//pA8PFxdDQAAAFAOQe2kHp9LOX9IVsItAABATTVz5kyNHj1ao0aNkiTNmTNHixcv1ty5czVx4sRCn7/22mudvn/ggQf0zjvv6IcfflD//v2rouQya16vuT679TMdPX1UFovF1eUAAABctmhUqCS1a0uDBrm6CgAAAKACeNWWoge5ugoAAABUopycHCUlJWnSpEmOZVarVXFxcVq9evVF1zfG6Ntvv1VycrKeffbZyiy1XAJ9AjWk5RBXlwEAAHDZo1EBAAAAAAAAAC5zhw8fVm5urkJDQ52Wh4aG6pdffil2vePHjysyMlLZ2dny8PDQa6+9pr59+xb7+ezsbGVnZzu+P3HiRPmLBwAAgNuhUQEAAAAAAAAAcElq166tzZs3KzMzU4mJiZowYYKuuOKKQtNC2M2YMUNPPvlk1RYJAACAaodGBQAAAAAAAAC4zNWvX18eHh5KT093Wp6enq6wsLBi17NarWratKkkqX379tqxY4dmzJhRbKPCpEmTNGHCBMf3J06cUHR0dPl3AAAAAG7F6uoCAAAAAAAAAACu5e3trU6dOikxMdGxLC8vT4mJiYqNjS31OHl5eU5TO1zIZrMpICDA6QsAAACXH56oAAAAAAAAAADQhAkTdMcdd6hz587q2rWrZs2apaysLI0aNUqSNHLkSEVGRmrGjBmS8qdx6Ny5s5o0aaLs7GwtWbJE7733nl5//XVX7gYAAADcAI0KAAAAAAAAAAANGzZMhw4d0hNPPKG0tDS1b99eS5cuVWhoqCRp3759slr/95DerKws3Xfffdq/f798fX3VokULvf/++xo2bJirdgEAAABuwmKMMa4uoiqcOHFCgYGBOn78OI8TAwAAcHOXe7a73PcfAACgJrncs93lvv8AAAA1SVmynbXEdwEAAAAAAAAAAAAAACoQjQoAAAAAAAAAAAAAAKDK0KgAAAAAAAAAAAAAAACqDI0KAAAAAAAAAAAAAACgytCoAAAAAAAAAAAAAAAAqgyNCgAAAAAAAAAAAAAAoMrQqAAAAAAAAAAAAAAAAKoMjQoAAAAAAAAAAAAAAKDKeLq6gKpijJEknThxwsWVAAAAoLzsmc6e8S43ZFsAAICag2xLtgUAAKgpypJtL5tGhZMnT0qSoqOjXVwJAAAAKsrJkycVGBjo6jKqHNkWAACg5iHbkm0BAABqitJkW4u5TFp18/LydPDgQdWuXVsWi6VKtnnixAlFR0fr999/V0BAQJVss6rVtH105/1xh9qra43VqS5X1VLV2y3v9iq73ooevyLHu5SxKmr71Wmcyj6m1alGdxjHFfcuY4xOnjypiIgIWa2X32xmZNvKUdP20Z33xx1qr641Vqe6yLZVs35Vj0+2rfhxyLbVaxyybdUj21aOmraP7rw/7lB7da2xOtVFtq2a9at6fLJtxY9Dtq1e41T3bHvZPFHBarUqKirKJdsOCAhw+S/RylbT9tGd98cdaq+uNVanulxVS1Vvt7zbq+x6K3r8ihzvUsaqqO1Xp3Eq+5hWpxrdYZyqvodcjv/azI5sW7lq2j668/64Q+3VtcbqVBfZtmrWr+rxybYVPw7ZtnqNQ7atOmTbylXT9tGd98cdaq+uNVanusi2VbN+VY9Ptq34cci21Wuc6pptL78WXQAAAAAAAAAAAAAA4DI0KgAAAAAAAAAAAAAAgCpDo0Ilstlsmjp1qmw2m6tLqTQ1bR/deX/cofbqWmN1qstVtVT1dsu7vcqut6LHr8jxLmWsitp+dRqnso9pdarRHcapTvdRVJ7L4edc0/bRnffHHWqvrjVWp7rItlWzflWPT7at+HHIttVrnOp0H0XluRx+zjVtH915f9yh9upaY3Wqi2xbNetX9fhk24ofh2xbvcapTvfRoliMMcbVRQAAAAAAAAAAAAAAgMsDT1QAAAAAAAAAAAAAAABVhkYFAAAAAAAAAAAAAABQZWhUAAAAAAAAAAAAAAAAVYZGhUs0bdo0WSwWp68WLVqUuM6nn36qFi1ayMfHR23atNGSJUuqqNrS+e9//6v4+HhFRETIYrHoiy++cLx39uxZPfroo2rTpo38/f0VERGhkSNH6uDBgyWOeSnHqaKUtD+SlJ6erjvvvFMRERHy8/PTgAEDlJKSUuKYCxcuVOfOnVWnTh35+/urffv2eu+99yq89hkzZqhLly6qXbu2QkJCNGjQICUnJzt95tprry10bO+9995Sb+Pee++VxWLRrFmzLqnG119/XW3btlVAQIACAgIUGxurr7/+2vH+mTNnNG7cONWrV0+1atXSLbfcovT09BLHzMzM1Pjx4xUVFSVfX1+1atVKc+bMqdC6LuW4VURdzzzzjCwWix588EHHsks5RtOmTVOLFi3k7++voKAgxcXFae3atWXetp0xRgMHDizyGrmUbV+4rT179hQ63vavTz/91DHuhe9deeWVjuvT19dXDRo0UFBQUKmPkzFGTzzxhGrVqlXiPWjs2LFq0qSJfH19FRwcrJtvvlm//PJLiWMPGzasxDHLco4Vte9Wq9VxjqWlpWnEiBEKCwuTv7+/OnbsqM8++0wHDhzQX/7yF9WrV0++vr5q06aNNmzYICn/GmjTpo1sNpusVqusVqs6dOhQ5P3twnEiIiIUHh4uHx8fdenSRSNHjrzoff/CMSIjI9W0adMir8GS7jsXjtOiRQsNHDjQaR8//fRT3XTTTQoMDJS/v7+6dOmiffv2lThOaGioPD09izwHPT09NWDAAG3fvr3Ea3HhwoWy2WxFjuHv7y8fHx9FR0friiuucJyv999/v44fP15oPxs1alTkODabzemaKunaLG6Mxo0bO45Ny5Yt1a1bN/n7+ysgIEA9e/bU6dOnS11PrVq1FBERIR8fH/n7+8vf31+1a9fWrbfeqvT0dMc1Fh4eLl9fX8XFxTnOsZLuw7Nnz1ajRo3k4+OjmJgYrVu3rlBNcA2yLdmWbEu2LQuyLdm2uGNKti16HLIt2RZVi2xLtiXbkm3LgmxLti3umJJtix6HbEu2rUg0KpRD69atlZqa6vj64Ycfiv3sqlWrdNttt+nuu+/Wpk2bNGjQIA0aNEjbt2+vwopLlpWVpXbt2mn27NmF3jt16pQ2btyoKVOmaOPGjVq4cKGSk5N10003XXTcshynilTS/hhjNGjQIP3222/68ssvtWnTJjVs2FBxcXHKysoqdsy6devq8ccf1+rVq7V161aNGjVKo0aN0rJlyyq09pUrV2rcuHFas2aNEhISdPbsWfXr169QbaNHj3Y6ts8991ypxv/888+1Zs0aRUREXHKNUVFReuaZZ5SUlKQNGzaod+/euvnmm/XTTz9Jkh566CF99dVX+vTTT7Vy5UodPHhQQ4YMKXHMCRMmaOnSpXr//fe1Y8cOPfjggxo/frwWLVpUYXVJZT9u5a1r/fr1euONN9S2bVun5ZdyjJo1a6ZXX31V27Zt0w8//KBGjRqpX79+OnToUJm2bTdr1ixZLJZS7cfFtl3UtqKjo52OdWpqqp588knVqlVLAwcOdHzu/PvEwYMHFRgY6Lg+Bw0apCNHjsjb21tLly4t1XF67rnn9K9//Us33nijmjRpon79+ik6Olq7d+92ugd16tRJ8+bN044dO7Rs2TIZY9SvXz/l5uYWO3ZOTo5CQkL0wgsvSJISEhIK3dfKco61bt1at99+uxo2bKjPPvtMGzZscJxjAwcOVHJyshYtWqRt27ZpyJAhGjp0qLp06SIvLy99/fXX+vnnn/Xiiy8qKChIUv410LlzZ9lsNr366qu6++67tWXLFvXu3VtnzpxxbPfo0aPq3r27Y5znnntOhw4d0oMPPqiNGzeqdevW+uijj3T//fcXe9+/cIyff/5ZY8eO1aRJkwpdgy+//HKx950Lx1m9erWOHj0qPz8/x7gPP/ywxowZoxYtWmjFihXaunWrpkyZIh8fn2LHGTlypM6dO6cXXnhBa9as0fTp0yVJTZo0kSTNnTtXDRs2VGxsrBYtWlTstVi3bl298cYbWrlypVavXq2nnnrK8d6kSZP0wQcfKDc3V6dOnVJSUpLmz5+vpUuX6u677y60r+vXr3ecF7Nnz9azzz4rSZozZ47TNVXStXn+GKmpqXrnnXckSTExMVqxYoXmz5+vffv2qXfv3lq3bp3Wr1+v8ePHy2otHPvsY8XHx6tZs2Z68cUXJUnnzp3TsWPHVL9+fV111VWSpHHjxiknJ0fx8fF69tln9a9//Utz5szR2rVr5e/vr/79++vMmTPF3odfeOEFTZgwQVOnTtXGjRvVrl079e/fXxkZGUXuJ6oe2ZZsS7Yl25YG2ZZsS7Yl29qRbcm21RnZlmxLtiXblgbZlmxLtiXb2pFtXZRtDS7J1KlTTbt27Ur9+VtvvdXccMMNTstiYmLM2LFjK7iyiiHJfP755yV+Zt26dUaS2bt3b7GfKetxqiwX7k9ycrKRZLZv3+5Ylpuba4KDg81bb71VprE7dOhgJk+eXFGlFikjI8NIMitXrnQs69Wrl3nggQfKPNb+/ftNZGSk2b59u2nYsKF56aWXKqzOoKAg83//93/m2LFjxsvLy3z66aeO93bs2GEkmdWrVxe7fuvWrc1TTz3ltKxjx47m8ccfr5C6jLm041aeuk6ePGmuvPJKk5CQ4LTtSz1GFzp+/LiRZJYvX17qbdtt2rTJREZGmtTU1FJd8yVt+2LbOl/79u3NXXfd5fj+wvvE+den/TgtWLDAcX1e7Djl5eWZsLAw8/zzzzvGPnbsmLHZbOajjz4qcZ+2bNliJJlff/212M/Yx9y9e7eRZDZt2uT0flnOMftYxZ1jXl5e5t1333Va7uPjY5o2bVrsmOfvv12dOnWMp6en0/4/+uij5pprrnF837VrVzNu3DjH97m5uSYiIsLMmDHDsezC+/6FYxQnMDDQBAUFFXvfuXCcosYdNmyY+ctf/lLidi5cLzw83Lz66quO7+3nVqNGjUyTJk1MXl6eOXLkiJFk7r33XsfnSnOOWSwW4+vra/Ly8owxptA59sknnxhvb29z9uzZEmt+4IEHHLXYr6k5c+aU6dq88sorTa1atRy1xMTElOn30qlTp4yHh4f5z3/+Yx544AHj5+dnRo0aZZo2bWosFos5fvy4GTJkiLn99tvNsWPHjCRTt25dp3PsYtdYUFCQady48UXPMbgO2ZZsa0e2/R+ybWFk28LItoXHItuSbcm2cDWyLdnWjmz7P2Tbwsi2hZFtC49FtiXbkm0rF09UKIeUlBRFREToiiuu0O23317oMSbnW716teLi4pyW9e/fX6tXr67sMivN8ePHZbFYVKdOnRI/V5bjVFWys7Mlyamjy2q1ymazlbpz2BijxMREJScnq2fPnpVSp539MTR169Z1Wv7BBx84uqYmTZqkU6dOlThOXl6eRowYob///e9q3bp1hdWXm5urjz/+WFlZWYqNjVVSUpLOnj3rdM63aNFCDRo0KPGc79atmxYtWqQDBw7IGKPvvvtOO3fuVL9+/SqkLruyHrfy1DVu3DjdcMMNha7/Sz1G58vJydGbb76pwMBAtWvXrtTblvK77YcPH67Zs2crLCysVNsradslbet8SUlJ2rx5c6GOxfPvEw899JCk/OvTfpz69evnuD4vdpx2796ttLQ0Ry0pKSlq2bKlLBaLpk2bVuw9KCsrS/PmzVPjxo0VHR1d4n6kpKQoJiZGkvTYY48VGrMs51hKSop2796t//f//p8GDx6svXv3Os6xdu3aacGCBTpy5Ijy8vL08ccfKzs7W9dcc42GDh2qkJAQdejQQW+99VaR+2+/Bk6dOqX27ds7HbNFixapc+fOjnHWrVunvLw8x/tWq1VxcXFO61x4379wjAtryc3N1YcffqgTJ05o7Nixxd53Lhxn1qxZstlsju/bt2+vL774Qs2aNVP//v0VEhKimJiYQo/WunCcjIwMp0dU2e/9+/bt01133SWLxaJNmzY59s2upHPMGKP58+fLGKO+ffs6umcDAwMVExPjWOf48eMKCAiQp6dnkfss5V9H77//vu666y6dPXtWb775pgICAjRz5sxSX5tnzpxxnI8DBgxQ/fr1tXbtWqWlpalbt24KDQ1Vr169Svzddu7cOeXm5srDw0Pvv/++unfvrm+//VZ5eXkyxig5OVk//PCDBg4cKB8fH1mtVh05csTper9w/+3s52BmZqb27dvntE5R5xhci2xLtiXb5iPbFo9s64xsW/RYZFuyLdkW1QHZlmxLts1Hti0e2dYZ2bbosci2ZFuybSWr9FaIGmrJkiXmk08+MVu2bDFLly41sbGxpkGDBubEiRNFft7Ly8t8+OGHTstmz55tQkJCqqLcMtNFOoFOnz5tOnbsaIYPH17iOGU9TpXlwv3JyckxDRo0MEOHDjVHjhwx2dnZ5plnnjGSTL9+/Uoc69ixY8bf3994enoam81m3n777UqtPTc319xwww2me/fuTsvfeOMNs3TpUrN161bz/vvvm8jISDN48OASx5o+fbrp27evo3urvJ25W7duNf7+/sbDw8MEBgaaxYsXG2OM+eCDD4y3t3ehz3fp0sX84x//KHa8M2fOmJEjRxpJxtPT03h7e5t33nmnwuoy5tKO26XW9dFHH5mrrrrKnD592hjj3LF5qcfIGGO++uor4+/vbywWi4mIiDDr1q0r07aNMWbMmDHm7rvvdnx/sWu+pG1fbFvn++tf/2patmzptOzC+8TVV19tPDw8zKBBg8ybb75pvL29C12fJR2nH3/80UgyBw8edBq7R48epl69eoXuQbNnzzb+/v5GkmnevHmJXbnn17tkyRIjybRt29ZpzLKcY/ax1q9fb/r06WMkGUnGy8vLvPPOO+bo0aOmX79+jnMvICDAeHl5GZvNZiZNmmQ2btxo3njjDePj42Pmz5/vtP++vr5O18DQoUPNrbfe6ti2zWZzjLNs2TIjyXh7ezvGMcaYv//976Zr167GmKLv++ePcX4tTz/9tOMatNlspkOHDiXedy4cx9PT00gyN9xwg9m4caN57rnnHPXNnDnTbNq0ycyYMcNYLBazYsWKYsfp0qWLsVgs5plnnjG5ubmOn5kk89NPP5ns7Gzz5z//uch7/4Xn2Pn3fg8PDyPJbNy40Wkd+zE+dOiQadCggXnsscdKPJcWLFhgrFar8fX1dVxTgwcPLtO1+cYbbxhJxsfHx8ycOdO88847jn189NFHzcaNG82DDz5ovL29zc6dO4sdJzY21rRs2dJ4eHiYPXv2mBtvvNExjiQzbdo0k5mZacaPH+9YdvDgwSL335jC9+F3333XSDKrVq1yWuf8cwyuRbYl25JtybYXQ7YtjGxb9FhkW7It2RauRrYl25JtybYXQ7YtjGxb9FhkW7It2bZy0ahQQY4ePWoCAgIcjym6UE0KvDk5OSY+Pt506NDBHD9+vEzjXuw4VZai9mfDhg2mXbt2RpLx8PAw/fv3NwMHDjQDBgwocazc3FyTkpJiNm3aZF544QUTGBhovvvuu0qr/d577zUNGzY0v//+e4mfS0xMLPHRRxs2bDChoaHmwIEDjmXlDbzZ2dkmJSXFbNiwwUycONHUr1/f/PTTT5cc5p5//nnTrFkzs2jRIrNlyxbzyiuvmFq1apmEhIQKqasoFztul1rXvn37TEhIiNmyZYtjWUUF3szMTJOSkmJWr15t7rrrLtOoUSOTnp5e6m1/+eWXpmnTpubkyZOO90sbeC/cdlRUlKlfv36x2zrfqVOnTGBgoHnhhRdK3MbRo0eNv7+/iYqKcvxivfD6LG3gPd/QoUPNoEGDCt2Djh07Znbu3GlWrlxp4uPjTceOHR3hvST2R4j997//LfG+VpZz7MMPPzS1atUyw4cPN7Vq1TI333yz6dq1q1m+fLnZvHmzmTZtmpFU6NGMf/vb38zVV1/ttP8//vij0zXQv39/p8Dr5eVlYmNjjTHGHDhwwEgyf/rTnxzjGPO/MFLcff/8Mc6vJSYmxqSkpJj33nvP+Pv7m6CgIMc1WNR958JxvLy8TFhYmKMWe3316tVzWi8+Pt78+c9/LnacjIwM07hxY8d9vlmzZiY0NNRxXnl4eJg2bdoYi8VS6N5/4Tl2/r0/OjraSDL//ve/ndYZOnSoGTx4sOnatasZMGCAycnJMSXp16+fGThwoOOaiouLM56enua3335zfOZi12avXr2MJHPbbbcZY/7382/atKnTsWnTpo2ZOHFiseP8+uuvJigoyEgyFovFeHl5me7du5vQ0FATHBzsWP6Xv/zFNGvW7KKB98L7sH1s/jLXfZBtS4dsW3ZkW7Lthci2ZFuybT6yLdkWlYdsWzpk27Ij25JtL0S2JduSbfORbcm2pUWjQgXq3LlzsSdTdHR0oQv8iSeeMG3btq2CysquuAssJyfHDBo0yLRt29YcPnz4ksYu6ThVlpJuGMeOHTMZGRnGmPy5fu67774yjX333XdftJv3Uo0bN85ERUU53fyKk5mZaSSZpUuXFvn+Sy+9ZCwWi/Hw8HB8STJWq9U0bNiwQurt06ePGTNmjOMX/NGjR53eb9CggZk5c2aR6546dcp4eXmZ//znP07L7777btO/f/8KqasoFztul1rX559/7viFev7xtv8Mli9fXuZjVJymTZua6dOnl3rb48ePL/Zc6NWrV5m2HRYWVuK2zp075/jsu+++a7y8vBzXW0ns94kvv/zScZzOvz5LOk67du0yUuE5yHr27Gnuv//+Eu9B2dnZxs/Pr9BfUBTl/LnOShqzrOeYfayhQ4cayXlORmPy5zpr0aKF07LXXnvNREREFLv/ffr0MeHh4eb+++93LGvQoIGjAzQ7O9t4eHiYsWPHOsYxxpiRI0eaG2+8sdj7/vljFFWL/b5j/yruvnPhOA0aNDDdunVzjJOdnW2sVqupXbu207b+8Y9/mG7dul20nvDwcLN//36ze/duY7FYTHR0tOPeb79fXbhecefYnj17jNVqNZKc/uPAGGO6detmwsLCTJ8+fS76H032cb744gvHsgceeMBxfEpzbdrHsFqt5umnnzbGGPPbb785uprPPza33nprif+axj7Wxx9/7Jgj7tZbbzXXX3+9McaYiRMnmiuvvNIYY0y9evVKvMaKct111xmLxVLod/HIkSPNTTfdVGxdcC2ybemQbUuPbEu2LQ2yrTOyLdn2wnrItmRbXBqybemQbUuPbEu2LQ2yrTOyLdn2wnrItmRbq1AhMjMztWvXLoWHhxf5fmxsrBITE52WJSQkOM2/VN2dPXtWt956q1JSUrR8+XLVq1evzGNc7Di5QmBgoIKDg5WSkqINGzbo5ptvLtP6eXl5jvlzKooxRuPHj9fnn3+ub7/9Vo0bN77oOps3b5akYo/tiBEjtHXrVm3evNnxFRERob///e9atmxZhdRtPxadOnWSl5eX0zmfnJysffv2FXvOnz17VmfPnpXV6nxb8vDwcJp/qTx1FeVix+1S6+rTp4+2bdvmdLw7d+6s22+/3fG6rMeotPt3sW0//vjjhc4FSXrppZc0b968Mm3bx8dHf/3rX4vdloeHh+Ozb7/9tm666SYFBweXOOb594levXrJy8tL77//vuP6vNhxaty4scLCwpyO7YkTJ7R27Vp16NChxHuQyW/gK9M1ferUqRLHLMs5dv6+G2MkqdC5V6dOHR09etRp2c6dO9WwYUNJRe9/Tk6O0tPTnY5Z9+7dlZycLEny9vZWp06dtGbNGsc4eXl5Wr58uX777bdi7/vnj1FULfb7TufOnRUfH1/sfefCcbp37649e/Y4xvH29lZoaKhsNlux2yqpnkaNGikyMlJvv/22rFarhg8f7rj32+dtO//nU9I5Nm/ePIWEhMjHx0cZGRmO5fv379fq1asVFBSkRYsWOc2lWRT7ODfccINj2cSJExUVFaWxY8eW6tq0j9G1a1fHfjdq1EgRERFKSUlxOjYXHqvixrrllluUnZ2tM2fOaNmyZY7fiQEBAZKkb7/9Vn/88YeCg4OLvMZKun/Vq1fPaZ28vDwlJia6VRa6nJBtS4dsWzpk2/8h25Z9/8i2ZFuyrfNnyLZkW5Qd2bZ0yLalQ7b9H7Jt2fePbEu2Jds6f4ZsS7bliQqX6OGHHzYrVqwwu3fvNj/++KOJi4sz9evXd3ScjRgxwqlL68cffzSenp7mhRdeMDt27DBTp041Xl5eZtu2ba7ahUJOnjxpNm3aZDZt2mQkOeaT2bt3r8nJyTE33XSTiYqKMps3bzapqamOr+zsbMcYvXv3Nq+88orj+4sdJ1ftjzHGfPLJJ+a7774zu3btMl988YVp2LChGTJkiNMYF/4cp0+fbr755huza9cu8/PPP5sXXnjBeHp6mrfeeqtCa//rX/9qAgMDzYoVK5yO9alTp4wx+Y96eeqpp8yGDRvM7t27zZdffmmuuOIK07NnT6dxmjdvbhYuXFjsdsrzCLGJEyealStXmt27d5utW7eaiRMnGovFYr755htjTP6jzxo0aGC+/fZbs2HDBhMbG1voUUMX1terVy/TunVr891335nffvvNzJs3z/j4+JjXXnutQuq61ONWEXXZxzn/0VplPUaZmZlm0qRJZvXq1WbPnj1mw4YNZtSoUcZmsxXq3rzYti+kIrrXL3XbRW0rJSXFWCwW8/XXXxfa9sMPP2yio6PNnDlzHPeJ2rVrm88//9zs2rXLDBgwwHh4eJgePXqU+lx65plnTJ06dcygQYPM3LlzTd++fU14eLjp3bu34x60a9cuM336dLNhwwazd+9e8+OPP5r4+HhTt25dp0eyXTj2uHHjzFtvvWXmzp1rJJk2bdqYOnXqmG3btpX5HLPfI2NiYkzjxo1Np06dTN26dc3LL79sbDabCQ4ONj169DBr1641v/76q3nhhRccndD//Oc/TUpKimnVqpXx9vY277//vjEm/xoYO3asCQgIMC+//LK56667jCQTFhbm1C3auXNnY7VaHePY57AaM2aM+fnnn80999xjPD09TURERLH3/XXr1hmLxWJuvPFGk5KSYj744APj5eVlJk+eXOy9oaj7zoW1PPXUU0aSGTp0qGNcb29v4+HhYd58802TkpJiXnnlFePh4WG+//57xzgDBw50GufJJ580NpvNzJw506xYscLYbDbj5+dnvvrqK6d7f+PGjZ2uxeDgYBMZGekYd/r06SYqKsq8+uqrJjw83Fx33XXGarUaPz8/8+WXX5pVq1aZoKAg4+XlZX766SenY3V+d7r9556bm2uio6PN1VdffdFrqrhr89///rdp0KCBefTRR83ChQuNl5eX49gMGTLESDJPPfWUSUlJMZMnTzY+Pj5Oj7E7//d1bm6uCQkJMUOHDjW//fab6du3r/Hy8jLNmjUzM2bMMDNmzDBBQUHmhhtuMHXr1jUTJkxwXGNffvml6dq1q2nTpo1p3LixOX36tOM+3K1bNzNp0iTHOfDYY48Zm81m5s+fb37++WczZswYU6dOHZOWlmbgemRbsi3ZlmxLtiXbkm3JtmRbsm1NQbYl25JtybZkW7It2ZZsS7Z1j2xLo8IlGjZsmAkPDzfe3t4mMjLSDBs2zOlE6tWrl7njjjuc1vnkk09Ms2bNjLe3t2ndurVZvHhxFVddsu+++86oYP6X87/uuOMOx6Nyivo6f56vhg0bmqlTpzq+v9hxctX+GGPMyy+/bKKiooyXl5dp0KCBmTx5slN4N6bwz/Hxxx83TZs2NT4+PiYoKMjExsaajz/+uMJrL+5Yz5s3zxiTP5dVz549Td26dY3NZjNNmzY1f//73wvNPXf+OkUpT+C96667TMOGDY23t7cJDg42ffr0cfxCM8aY06dPm/vuu88EBQUZPz8/M3jwYJOamlpifampqebOO+80ERERxsfHxzRv3ty8+OKLJi8vr0LqutTjVhF1GVM4CJb1GJ0+fdoMHjzYREREGG9vbxMeHm5uuukms27dujJv+0JF/VK91G0Xta1JkyaZ6Ohok5ubW+jzw4YNM5KMp6en4z4xZcoUx/UZHR1tOnXqVKZzKS8vz0yZMsXYbDbHI81CQ0Od7kEHDhwwAwcONCEhIcbLy8tERUWZ4cOHm19++aXEsbt27Vrk9Tl16tQyn2Pn3yP9/PyMj4+P8fb2dpxjycnJZsiQISYkJMT4+fmZtm3bmnfffdd89dVX5qqrrjI2m814enqaG2+80TH2XXfdZRo0aGCsVquxWCzGarWaDh06mOTkZKcaGjZsaG677TbHOC1atDB//vOfTYMGDYy3t7djLsiL3feDg4NNSEiIY4zu3buXeG8o6r5TVC3jx493+v7NN980b7/9tuMe3K5dO6fHbxmTf+717t3bsV6DBg1MWFiYsdlspnbt2kaSuf/++wvd+48fP+50LdavX99pXrjHH3/c8SgvSaZ9+/bmo48+MlOmTDGhoaHGy8ur2GO1e/fuQj/3ZcuWGUkmLi7uotdUcdfmww8/bCQ5fq4XHpsRI0aYqKgo4+fnZ2JjY53+w8B+zO2/r+31REVFGW9vbxMSEmLatm1roqKijKenp/Hw8DBWq9U0bdrUce+zX2P2ueMaN27sqMV+H5Zk/Pz8nM6BV155xXGOde3a1axZs8ageiDbkm3JtmRbsi3ZlmxLtiXbkm1rCrIt2ZZsS7Yl25JtybZkW7Kte2RbS8GBAwAAAAAAAAAAAAAAqHTWi38EAAAAAAAAAAAAAACgYtCoAAAAAAAAAAAAAAAAqgyNCgAAAAAAAAAAAAAAoMrQqAAAAAAAAAAAAAAAAKoMjQoAAAAAAAAAAAAAAKDK0KgAAAAAAAAAAAAAAACqDI0KAAAAAAAAAAAAAACgytCoAAAAAAAAAAAAAAAAqgyNCgBwGZo2bZpCQ0NlsVj0xRdflGqdFStWyGKx6NixY5VaW3XSqFEjzZo1y9VlAAAAoARk29Ih2wIAAFR/ZNvSIdsCNQONCgCqhTvvvFMWi0UWi0Xe3t5q2rSpnnrqKZ07d87VpV1UWUJjdbBjxw49+eSTeuONN5SamqqBAwdW2rauvfZaPfjgg5U2PgAAQHVEtq06ZFsAAIDKRbatOmRbAJcbT1cXAAB2AwYM0Lx585Sdna0lS5Zo3Lhx8vLy0qRJk8o8Vm5uriwWi6xW+rEutGvXLknSzTffLIvF4uJqAAAAaiaybdUg2wIAAFQ+sm3VINsCuNzwmwBAtWGz2RQWFqaGDRvqr3/9q+Li4rRo0SJJUnZ2th555BFFRkbK399fMTExWrFihWPd+fPnq06dOlq0aJFatWolm82mffv2KTs7W48++qiio6Nls9nUtGlTvf322471tm/froEDB6pWrVoKDQ3ViBEjdPjwYcf71157re6//3794x//UN26dRUWFqZp06Y53m/UqJEkafDgwbJYLI7vd+3apZtvvlmhoaGqVauWunTpouXLlzvtb2pqqm644Qb5+vqqcePG+vDDDws9surYsWO65557FBwcrICAAPXu3Vtbtmwp8Thu27ZNvXv3lq+vr+rVq6cxY8YoMzNTUv6jw+Lj4yVJVqu1xMC7ZMkSNWvWTL6+vrruuuu0Z88ep/f/+OMP3XbbbYqMjJSfn5/atGmjjz76yPH+nXfeqZUrV+rll192dF3v2bNHubm5uvvuu9W4cWP5+vqqefPmevnll0vcJ/vP93xffPGFU/1btmzRddddp9q1aysgIECdOnXShg0bHO//8MMP6tGjh3x9fRUdHa37779fWVlZjvczMjIUHx/v+Hl88MEHJdYEAABQErIt2bY4ZFsAAOBuyLZk2+KQbQGUB40KAKotX19f5eTkSJLGjx+v1atX6+OPP9bWrVs1dOhQDRgwQCkpKY7Pnzp1Ss8++6z+7//+Tz/99JNCQkI0cuRIffTRR/rXv/6lHTt26I033lCtWrUk5YfJ3r17q0OHDtqwYYOWLl2q9PR03XrrrU51vPPOO/L399fatWv13HPP6amnnlJCQoIkaf369ZKkefPmKTU11fF9Zmamrr/+eiUmJmrTpk0aMGCA4uPjtW/fPse4I0eO1MGDB7VixQp99tlnevPNN5WRkeG07aFDhyojI0Nff/21kpKS1LFjR/Xp00dHjhwp8phlZWWpf//+CgoK0vr16/Xpp59q+fLlGj9+vCTpkUce0bx58yTlB+7U1NQix/n99981ZMgQxcfHa/Pmzbrnnns0ceJEp8+cOXNGnTp10uLFi7V9+3aNGTNGI0aM0Lp16yRJL7/8smJjYzV69GjHtqKjo5WXl6eoqCh9+umn+vnnn/XEE0/oscce0yeffFJkLaV1++23KyoqSuvXr1dSUpImTpwoLy8vSfn/ATJgwADdcsst2rp1qxYsWKAffvjBcVyk/ID++++/67vvvtO///1vvfbaa4V+HgAAAJeKbEu2LQuyLQAAqM7ItmTbsiDbAiiWAYBq4I477jA333yzMcaYvLw8k5CQYGw2m3nkkUfM3r17jYeHhzlw4IDTOn369DGTJk0yxhgzb948I8ls3rzZ8X5ycrKRZBISEorc5tNPP2369evntOz33383kkxycrIxxphevXqZa665xukzXbp0MY8++qjje0nm888/v+g+tm7d2rzyyivGGGN27NhhJJn169c73k9JSTGSzEsvvWSMMeb77783AQEB5syZM07jNGnSxLzxxhtFbuPNN980QUFBJjMz07Fs8eLFxmq1mrS0NGOMMZ9//rm52O1/0qRJplWrVk7LHn30USPJHD16tNj1brjhBvPwww87vu/Vq5d54IEHStyWMcaMGzfO3HLLLcW+P2/ePBMYGOi07ML9qF27tpk/f36R6999991mzJgxTsu+//57Y7VazenTpx3nyrp16xzv239G9p8HAABAaZFtybZkWwAAUFOQbcm2ZFsAlcWz0jshAKCU/vOf/6hWrVo6e/as8vLyNHz4cE2bNk0rVqxQbm6umjVr5vT57Oxs1atXz/G9t7e32rZt6/h+8+bN8vDwUK9evYrc3pYtW/Tdd985OnXPt2vXLsf2zh9TksLDwy/asZmZmalp06Zp8eLFSk1N1blz53T69GlHZ25ycrI8PT3VsWNHxzpNmzZVUFCQU32ZmZlO+yhJp0+fdsxXdqEdO3aoXbt28vf3dyzr3r278vLylJycrNDQ0BLrPn+cmJgYp2WxsbFO3+fm5mr69On65JNPdODAAeXk5Cg7O1t+fn4XHX/27NmaO3eu9u3bp9OnTysnJ0ft27cvVW3FmTBhgu655x699957iouL09ChQ9WkSRNJ+cdy69atTo8FM8YoLy9Pu3fv1s6dO+Xp6alOnTo53m/RokWhx5YBAACUFtmWbFseZFsAAFCdkG3JtuVBtgVQHBoVAFQb1113nV5//XV5e3srIiJCnp75t6jMzEx5eHgoKSlJHh4eTuucH1Z9fX2d5r7y9fUtcXuZmZmKj4/Xs88+W+i98PBwx2v7Y6jsLBaL8vLyShz7kUceUUJCgl544QU1bdpUvr6++tOf/uR4JFppZGZmKjw83GlON7vqEMSef/55vfzyy5o1a5batGkjf39/Pfjggxfdx48//liPPPKIXnzxRcXGxqp27dp6/vnntXbt2mLXsVqtMsY4LTt79qzT99OmTdPw4cO1ePFiff3115o6dao+/vhjDR48WJmZmRo7dqzuv//+QmM3aNBAO3fuLMOeAwAAXBzZtnB9ZNt8ZFsAAOBuyLaF6yPb5iPbAigPGhUAVBv+/v5q2rRpoeUdOnRQbm6uMjIy1KNHj1KP16ZNG+Xl5WnlypWKi4sr9H7Hjh312WefqVGjRo5wfSm8vLyUm5vrtOzHH3/UnXfeqcGDB0vKD6979uxxvN+8eXOdO3dOmzZtcnSD/vrrrzp69KhTfWlpafL09FSjRo1KVUvLli01f/58ZWVlObpzf/zxR1mtVjVv3rzU+9SyZUstWrTIadmaNWsK7ePNN9+sv/zlL5KkvLw87dy5U61atXJ8xtvbu8hj061bN913332OZcV1GtsFBwfr5MmTTvu1efPmQp9r1qyZmjVrpoceeki33Xab5s2bp8GDB6tjx476+eefizy/pPwu3HPnzikpKUldunSRlN89fezYsRLrAgAAKA7ZlmxbHLItAABwN2Rbsm1xyLYAysPq6gIA4GKaNWum22+/XSNHjtTChQu1e/durVu3TjNmzNDixYuLXa9Ro0a64447dNddd+mLL77Q7t27tWLFCn3yySeSpHHjxunIkSO67bbbtH79eu3atUvLli3TqFGjCoW0kjRq1EiJiYlKS0tzBNYrr7xSCxcu1ObNm7VlyxYNHz7cqZu3RYsWiouL05gxY7Ru3Tpt2rRJY8aMceoujouLU2xsrAYNGqRvvvlGe/bs0apVq/T4449rw4YNRdZy++23y8fHR3fccYe2b9+u7777Tn/72980YsSIUj8+TJLuvfdepaSk6O9//7uSk5P14Ycfav78+U6fufLKK5WQkKBVq1Zpx44dGjt2rNLT0wsdm7Vr12rPnj06fPiw8vLydOWVV2rDhg1atmyZdu7cqSlTpmj9+vUl1hMTEyM/Pz899thj2rVrV6F6Tp8+rfHjx2vFihXau3evfvzxR61fv14tW7aUJD366KNatWqVxo8fr82bNyslJUVffvmlxo8fLyn/P0AGDBigsWPHau3atUpKStI999xz0e5uAACAsiLbkm3JtgAAoKYg25JtybYAyoNGBQBuYd68eRo5cqQefvhhNW/eXIMGDdL69evVoEGDEtd7/fXX9ac//Un33XefWrRoodGjRysrK0uSFBERoR9//FG5ubnq16+f2rRpowcffFB16tSR1Vr62+OLL76ohIQERUdHq0OHDpKkmTNnKigoSN26dVN8fLz69+/vNK+ZJL377rsKDQ1Vz549NXjwYI0ePVq1a9eWj4+PpPxHlS1ZskQ9e/bUqFGj1KxZM/35z3/W3r17iw2vfn5+WrZsmY4cOaIuXbroT3/6k/r06aNXX3211Psj5T9W67PPPtMXX3yhdu3aac6cOZo+fbrTZyZPnqyOHTuqf//+uvbaaxUWFqZBgwY5feaRRx6Rh4eHWrVqpeDgYO3bt09jx47VkCFDNGzYMMXExOiPP/5w6tItSt26dfX+++9ryZIlatOmjT766CNNmzbN8b6Hh4f++OMPjRw5Us2aNdOtt96qgQMH6sknn5SUP1/dypUrtXPnTvXo0UMdOnTQE088oYiICMcY8+bNU0REhHr16qUhQ4ZozJgxCgkJKdNxAwAAKA2yLdmWbAsAAGoKsi3ZlmwL4FJZzIWTxwAAXGL//v2Kjo7W8uXL1adPH1eXAwAAAFwysi0AAABqCrItAFQOGhUAwEW+/fZbZWZmqk2bNkpNTdU//vEPHThwQDt37pSXl5erywMAAABKjWwLAACAmoJsCwBVw9PVBQDA5ers2bN67LHH9Ntvv6l27drq1q2bPvjgA8IuAAAA3A7ZFgAAADUF2RYAqgZPVAAAAAAAAAAAAAAAAFXG6uoCAAAAAAAAAAAAAADA5YNGBQAAAAAAAAAAAAAAUGVoVAAAAAAAAAAAAAAAAFWGRgUAAAAAAAAAAAAAAFBlaFQAAAAAAAAAAAAAAABVhkYFAAAAAAAAAAAAAABQZWhUAAAAAAAAAAAAAAAAVYZGBQAAAAAAAAAAAAAAUGVoVAAAAAAAAAAAAAAAAFXm/wOv8VW22A+xYQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce395607",
   "metadata": {
    "papermill": {
     "duration": 0.015547,
     "end_time": "2025-04-05T06:35:21.465393",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.449846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 3\n",
      "Random seed: [14, 61, 33]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6796, Accuracy: 0.776, F1 Micro: 0.8715, F1 Macro: 0.8667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5844, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5397, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5372, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4843, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4798, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4852, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4458, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4178, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Epoch 10/10, Train Loss: 0.4018, Accuracy: 0.7932, F1 Micro: 0.8837, F1 Macro: 0.8819\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6006, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.431, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3108, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2303, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1708, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1375, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1066, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0828, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0732, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.4286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.38      0.50      0.43         4\n",
      "weighted avg       0.56      0.75      0.64         4\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3012\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.74      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       0.67      0.05      0.09        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.46      0.35      0.31       216\n",
      "weighted avg       0.63      0.71      0.60       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 65.1979775428772 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7078, Accuracy: 0.7024, F1 Micro: 0.8155, F1 Macro: 0.7878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6021, Accuracy: 0.7872, F1 Micro: 0.8807, F1 Macro: 0.8791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5455, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5268, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4897, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Epoch 6/10, Train Loss: 0.4716, Accuracy: 0.7879, F1 Micro: 0.8805, F1 Macro: 0.8784\n",
      "Epoch 7/10, Train Loss: 0.4659, Accuracy: 0.7827, F1 Micro: 0.8767, F1 Macro: 0.8734\n",
      "Epoch 8/10, Train Loss: 0.4462, Accuracy: 0.7827, F1 Micro: 0.8759, F1 Macro: 0.872\n",
      "Epoch 9/10, Train Loss: 0.4178, Accuracy: 0.7812, F1 Micro: 0.8752, F1 Macro: 0.8715\n",
      "Epoch 10/10, Train Loss: 0.3894, Accuracy: 0.7812, F1 Micro: 0.8751, F1 Macro: 0.8716\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.72      0.99      0.84       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.814, Accuracy: 0.0, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6518, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5041, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3836, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2981, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2348, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1847, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1552, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1217, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1058, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         6\n",
      "   macro avg       0.50      0.50      0.50         6\n",
      "weighted avg       1.00      1.00      1.00         6\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7924, F1 Micro: 0.7924, F1 Macro: 0.3036\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.83      0.10      0.17        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.52      0.36      0.34       216\n",
      "weighted avg       0.71      0.72      0.63       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 56.81465029716492 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6904, Accuracy: 0.7835, F1 Micro: 0.8752, F1 Macro: 0.8707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5708, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5489, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5207, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4748, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4754, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.48, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4383, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4139, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3882, Accuracy: 0.7939, F1 Micro: 0.8841, F1 Macro: 0.8826\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7939, F1 Micro: 0.8841, F1 Macro: 0.8826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.80      0.98      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.72      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4833, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3188, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1606, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1135, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2591, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2411, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1873, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0508, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0344, Accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.6154, F1 Micro: 0.6154, F1 Macro: 0.381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         5\n",
      "    positive       0.62      1.00      0.76         8\n",
      "\n",
      "    accuracy                           0.62        13\n",
      "   macro avg       0.31      0.50      0.38        13\n",
      "weighted avg       0.38      0.62      0.47        13\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7909, F1 Micro: 0.7909, F1 Macro: 0.3068\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.79      0.99      0.88       167\n",
      "    positive       0.29      0.06      0.10        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.36      0.35      0.33       216\n",
      "weighted avg       0.65      0.77      0.69       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.74      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.40      0.05      0.09        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.37      0.35      0.31       216\n",
      "weighted avg       0.58      0.71      0.60       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 71.96233677864075 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3039\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 8.284765005111694 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6203, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5141, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4769, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4589, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4203, Accuracy: 0.8006, F1 Micro: 0.8876, F1 Macro: 0.886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3998, Accuracy: 0.811, F1 Micro: 0.8917, F1 Macro: 0.8896\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3633, Accuracy: 0.8251, F1 Micro: 0.8991, F1 Macro: 0.8975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3015, Accuracy: 0.8571, F1 Micro: 0.9157, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2709, Accuracy: 0.875, F1 Micro: 0.9249, F1 Macro: 0.9228\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2336, Accuracy: 0.8854, F1 Micro: 0.9309, F1 Macro: 0.9291\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.8854, F1 Micro: 0.9309, F1 Macro: 0.9291\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.89      1.00      0.94       187\n",
      "     machine       0.88      0.99      0.93       175\n",
      "      others       0.85      0.90      0.87       158\n",
      "        part       0.88      0.97      0.92       158\n",
      "       price       0.96      0.98      0.97       192\n",
      "     service       0.87      1.00      0.93       191\n",
      "\n",
      "   micro avg       0.89      0.98      0.93      1061\n",
      "   macro avg       0.89      0.98      0.93      1061\n",
      "weighted avg       0.89      0.98      0.93      1061\n",
      " samples avg       0.89      0.98      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6521, Accuracy: 0.7396, F1 Micro: 0.7396, F1 Macro: 0.4252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5256, Accuracy: 0.7633, F1 Micro: 0.7633, F1 Macro: 0.5144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3934, Accuracy: 0.858, F1 Micro: 0.858, F1 Macro: 0.8068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2412, Accuracy: 0.8994, F1 Micro: 0.8994, F1 Macro: 0.8739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1571, Accuracy: 0.9172, F1 Micro: 0.9172, F1 Macro: 0.8968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0608, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0795, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9048\n",
      "Epoch 8/10, Train Loss: 0.0307, Accuracy: 0.9172, F1 Micro: 0.9172, F1 Macro: 0.8955\n",
      "Epoch 9/10, Train Loss: 0.0273, Accuracy: 0.9172, F1 Micro: 0.9172, F1 Macro: 0.8955\n",
      "Epoch 10/10, Train Loss: 0.0382, Accuracy: 0.8994, F1 Micro: 0.8994, F1 Macro: 0.88\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.93      0.86        44\n",
      "    positive       0.97      0.92      0.95       125\n",
      "\n",
      "    accuracy                           0.92       169\n",
      "   macro avg       0.89      0.93      0.90       169\n",
      "weighted avg       0.93      0.92      0.92       169\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8796, F1 Micro: 0.8796, F1 Macro: 0.686\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.27      0.43        11\n",
      "     neutral       0.90      1.00      0.95       181\n",
      "    positive       1.00      0.46      0.63        24\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.97      0.58      0.67       216\n",
      "weighted avg       0.91      0.90      0.88       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        16\n",
      "     neutral       0.87      0.99      0.93       167\n",
      "    positive       0.89      0.48      0.63        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.92      0.66      0.74       216\n",
      "weighted avg       0.89      0.88      0.86       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.54      0.58      0.56        12\n",
      "     neutral       0.85      0.91      0.88       152\n",
      "    positive       0.73      0.58      0.65        52\n",
      "\n",
      "    accuracy                           0.81       216\n",
      "   macro avg       0.71      0.69      0.69       216\n",
      "weighted avg       0.81      0.81      0.80       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.65      0.75        23\n",
      "     neutral       0.87      0.97      0.92       152\n",
      "    positive       0.90      0.63      0.74        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.88      0.75      0.80       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.62      0.70        13\n",
      "     neutral       0.96      0.98      0.97       186\n",
      "    positive       0.75      0.71      0.73        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.84      0.77      0.80       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.87      1.00      0.93       185\n",
      "    positive       1.00      0.18      0.30        17\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.62      0.39      0.41       216\n",
      "weighted avg       0.82      0.87      0.82       216\n",
      "\n",
      "Total train time: 74.02769255638123 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6412, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5252, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4826, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Epoch 4/10, Train Loss: 0.4693, Accuracy: 0.7879, F1 Micro: 0.8804, F1 Macro: 0.8782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4383, Accuracy: 0.7946, F1 Micro: 0.8834, F1 Macro: 0.881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4246, Accuracy: 0.7984, F1 Micro: 0.8852, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3841, Accuracy: 0.8162, F1 Micro: 0.8942, F1 Macro: 0.8919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3253, Accuracy: 0.8438, F1 Micro: 0.9083, F1 Macro: 0.9066\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2888, Accuracy: 0.8624, F1 Micro: 0.9184, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2566, Accuracy: 0.8854, F1 Micro: 0.9311, F1 Macro: 0.9294\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8854, F1 Micro: 0.9311, F1 Macro: 0.9294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.94      1.00      0.97       187\n",
      "     machine       0.85      1.00      0.92       175\n",
      "      others       0.86      0.91      0.88       158\n",
      "        part       0.84      0.97      0.90       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.89      0.98      0.93      1061\n",
      "   macro avg       0.89      0.98      0.93      1061\n",
      "weighted avg       0.89      0.98      0.93      1061\n",
      " samples avg       0.89      0.98      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.637, Accuracy: 0.7239, F1 Micro: 0.7239, F1 Macro: 0.4199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6035, Accuracy: 0.7239, F1 Micro: 0.7239, F1 Macro: 0.4199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5003, Accuracy: 0.7301, F1 Micro: 0.7301, F1 Macro: 0.4432\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4725, Accuracy: 0.816, F1 Micro: 0.816, F1 Macro: 0.7271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3171, Accuracy: 0.8834, F1 Micro: 0.8834, F1 Macro: 0.8571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1999, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.894\n",
      "Epoch 7/10, Train Loss: 0.1605, Accuracy: 0.8896, F1 Micro: 0.8896, F1 Macro: 0.8509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.8995\n",
      "Epoch 9/10, Train Loss: 0.102, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.8954\n",
      "Epoch 10/10, Train Loss: 0.0887, Accuracy: 0.908, F1 Micro: 0.908, F1 Macro: 0.8857\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.8995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.84      0.85        45\n",
      "    positive       0.94      0.95      0.95       118\n",
      "\n",
      "    accuracy                           0.92       163\n",
      "   macro avg       0.90      0.90      0.90       163\n",
      "weighted avg       0.92      0.92      0.92       163\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8773, F1 Micro: 0.8773, F1 Macro: 0.6782\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.55      0.71        11\n",
      "     neutral       0.95      1.00      0.97       181\n",
      "    positive       1.00      0.79      0.88        24\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.98      0.78      0.85       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.38      0.55        16\n",
      "     neutral       0.85      1.00      0.92       167\n",
      "    positive       0.77      0.30      0.43        33\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.87      0.56      0.63       216\n",
      "weighted avg       0.85      0.85      0.82       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.67      0.73        12\n",
      "     neutral       0.86      0.91      0.89       152\n",
      "    positive       0.70      0.60      0.65        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.79      0.73      0.75       216\n",
      "weighted avg       0.82      0.82      0.82       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.43      0.56        23\n",
      "     neutral       0.84      0.97      0.90       152\n",
      "    positive       0.85      0.56      0.68        41\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.82      0.66      0.71       216\n",
      "weighted avg       0.84      0.84      0.82       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.62      0.73        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.75      0.71      0.73        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.77      0.81       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 69.91694593429565 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5973, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5097, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4776, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4596, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4173, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.394, Accuracy: 0.8147, F1 Micro: 0.8948, F1 Macro: 0.8934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3559, Accuracy: 0.8326, F1 Micro: 0.9041, F1 Macro: 0.903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2991, Accuracy: 0.875, F1 Micro: 0.9258, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2681, Accuracy: 0.8943, F1 Micro: 0.9358, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2282, Accuracy: 0.907, F1 Micro: 0.9433, F1 Macro: 0.9413\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.907, F1 Micro: 0.9433, F1 Macro: 0.9413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.93      1.00      0.96       187\n",
      "     machine       0.89      1.00      0.94       175\n",
      "      others       0.88      0.91      0.89       158\n",
      "        part       0.87      0.98      0.92       158\n",
      "       price       0.97      0.98      0.98       192\n",
      "     service       0.91      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.91      0.98      0.94      1061\n",
      "   macro avg       0.91      0.98      0.94      1061\n",
      "weighted avg       0.91      0.98      0.94      1061\n",
      " samples avg       0.91      0.98      0.94      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5954, Accuracy: 0.6856, F1 Micro: 0.6856, F1 Macro: 0.4067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4759, Accuracy: 0.7474, F1 Micro: 0.7474, F1 Macro: 0.637\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3612, Accuracy: 0.8608, F1 Micro: 0.8608, F1 Macro: 0.8364\n",
      "Epoch 4/10, Train Loss: 0.2627, Accuracy: 0.8351, F1 Micro: 0.8351, F1 Macro: 0.8243\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.184, Accuracy: 0.9021, F1 Micro: 0.9021, F1 Macro: 0.8897\n",
      "Epoch 6/10, Train Loss: 0.0747, Accuracy: 0.8763, F1 Micro: 0.8763, F1 Macro: 0.8659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0722, Accuracy: 0.9021, F1 Micro: 0.9021, F1 Macro: 0.8905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0492, Accuracy: 0.9072, F1 Micro: 0.9072, F1 Macro: 0.8959\n",
      "Epoch 9/10, Train Loss: 0.0657, Accuracy: 0.8814, F1 Micro: 0.8814, F1 Macro: 0.8711\n",
      "Epoch 10/10, Train Loss: 0.0256, Accuracy: 0.866, F1 Micro: 0.866, F1 Macro: 0.8556\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9072, F1 Micro: 0.9072, F1 Macro: 0.8959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.92      0.86        61\n",
      "    positive       0.96      0.90      0.93       133\n",
      "\n",
      "    accuracy                           0.91       194\n",
      "   macro avg       0.89      0.91      0.90       194\n",
      "weighted avg       0.91      0.91      0.91       194\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8974, F1 Micro: 0.8974, F1 Macro: 0.7663\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.55      0.71        11\n",
      "     neutral       0.93      1.00      0.96       181\n",
      "    positive       1.00      0.62      0.77        24\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.98      0.72      0.81       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.69      0.81        16\n",
      "     neutral       0.88      1.00      0.94       167\n",
      "    positive       0.94      0.45      0.61        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.94      0.71      0.79       216\n",
      "weighted avg       0.90      0.89      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.83      0.67        12\n",
      "     neutral       0.88      0.91      0.90       152\n",
      "    positive       0.76      0.62      0.68        52\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.73      0.79      0.75       216\n",
      "weighted avg       0.84      0.83      0.83       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.65      0.73        23\n",
      "     neutral       0.87      0.98      0.92       152\n",
      "    positive       0.93      0.61      0.74        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.88      0.75      0.80       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.98      0.98       186\n",
      "    positive       0.71      0.71      0.71        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.29      0.44        14\n",
      "     neutral       0.91      1.00      0.95       185\n",
      "    positive       0.75      0.35      0.48        17\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.55      0.63       216\n",
      "weighted avg       0.90      0.90      0.88       216\n",
      "\n",
      "Total train time: 72.83237957954407 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8382, F1 Micro: 0.8382, F1 Macro: 0.507\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 13.685245990753174 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5917, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.4738, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 3/10, Train Loss: 0.4774, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4294, Accuracy: 0.8036, F1 Micro: 0.8891, F1 Macro: 0.8876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3944, Accuracy: 0.8199, F1 Micro: 0.897, F1 Macro: 0.8954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3506, Accuracy: 0.8631, F1 Micro: 0.9186, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2757, Accuracy: 0.8996, F1 Micro: 0.9391, F1 Macro: 0.9373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2335, Accuracy: 0.9182, F1 Micro: 0.95, F1 Macro: 0.9483\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2012, Accuracy: 0.933, F1 Micro: 0.9586, F1 Macro: 0.9568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1648, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9606\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.939, F1 Micro: 0.9621, F1 Macro: 0.9606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.91      0.93      0.92       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.96      0.99      0.98       192\n",
      "     service       0.94      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6005, Accuracy: 0.687, F1 Micro: 0.687, F1 Macro: 0.4072\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4767, Accuracy: 0.8652, F1 Micro: 0.8652, F1 Macro: 0.8346\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3291, Accuracy: 0.887, F1 Micro: 0.887, F1 Macro: 0.8768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1923, Accuracy: 0.9217, F1 Micro: 0.9217, F1 Macro: 0.9103\n",
      "Epoch 5/10, Train Loss: 0.1383, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.8997\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1495, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9162\n",
      "Epoch 7/10, Train Loss: 0.113, Accuracy: 0.9217, F1 Micro: 0.9217, F1 Macro: 0.9103\n",
      "Epoch 8/10, Train Loss: 0.1067, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0824, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0985, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9214\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.94      0.89        72\n",
      "    positive       0.97      0.92      0.95       158\n",
      "\n",
      "    accuracy                           0.93       230\n",
      "   macro avg       0.91      0.93      0.92       230\n",
      "weighted avg       0.93      0.93      0.93       230\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.8493\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.64      0.74        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.79      0.83       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.91      0.93      0.92       152\n",
      "    positive       0.82      0.71      0.76        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.78      0.80      0.78       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.73      0.81        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.96      0.99      0.98       186\n",
      "    positive       0.79      0.65      0.71        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.89      0.80      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        14\n",
      "     neutral       0.94      1.00      0.97       185\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.95      0.72      0.80       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Total train time: 84.18980598449707 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6063, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4856, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4882, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.449, Accuracy: 0.7976, F1 Micro: 0.8853, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4168, Accuracy: 0.8043, F1 Micro: 0.8884, F1 Macro: 0.8862\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3706, Accuracy: 0.8415, F1 Micro: 0.9076, F1 Macro: 0.906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3046, Accuracy: 0.8891, F1 Micro: 0.9338, F1 Macro: 0.9324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2573, Accuracy: 0.9189, F1 Micro: 0.95, F1 Macro: 0.9479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2188, Accuracy: 0.936, F1 Micro: 0.9604, F1 Macro: 0.9585\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1787, Accuracy: 0.939, F1 Micro: 0.9623, F1 Macro: 0.9608\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.939, F1 Micro: 0.9623, F1 Macro: 0.9608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.92      0.98      0.95       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.90      0.99      0.94       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.94      1.00      0.97       191\n",
      "\n",
      "   micro avg       0.94      0.99      0.96      1061\n",
      "   macro avg       0.94      0.99      0.96      1061\n",
      "weighted avg       0.94      0.99      0.96      1061\n",
      " samples avg       0.94      0.99      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6551, Accuracy: 0.6818, F1 Micro: 0.6818, F1 Macro: 0.4054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5204, Accuracy: 0.75, F1 Micro: 0.75, F1 Macro: 0.599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3836, Accuracy: 0.8773, F1 Micro: 0.8773, F1 Macro: 0.8533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2918, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1291, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9106\n",
      "Epoch 6/10, Train Loss: 0.1011, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0962, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9084\n",
      "Epoch 8/10, Train Loss: 0.0919, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9077\n",
      "Epoch 9/10, Train Loss: 0.1132, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9001\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9136, F1 Micro: 0.9136, F1 Macro: 0.9042\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.83      0.87        70\n",
      "    positive       0.92      0.97      0.94       150\n",
      "\n",
      "    accuracy                           0.92       220\n",
      "   macro avg       0.92      0.90      0.91       220\n",
      "weighted avg       0.92      0.92      0.92       220\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.8373\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.88      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.62      0.74        16\n",
      "     neutral       0.92      0.98      0.95       167\n",
      "    positive       0.78      0.64      0.70        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.75      0.80       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.61      0.72        23\n",
      "     neutral       0.89      0.99      0.94       152\n",
      "    positive       0.88      0.68      0.77        41\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.76      0.81       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        14\n",
      "     neutral       0.94      1.00      0.97       185\n",
      "    positive       0.75      0.53      0.62        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.68      0.75       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Total train time: 80.66889214515686 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5846, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.473, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4767, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4254, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3876, Accuracy: 0.8177, F1 Micro: 0.896, F1 Macro: 0.8943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3423, Accuracy: 0.8884, F1 Micro: 0.9332, F1 Macro: 0.932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2751, Accuracy: 0.9107, F1 Micro: 0.9459, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2281, Accuracy: 0.9241, F1 Micro: 0.9532, F1 Macro: 0.9511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1973, Accuracy: 0.9308, F1 Micro: 0.9571, F1 Macro: 0.9548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1626, Accuracy: 0.9412, F1 Micro: 0.9635, F1 Macro: 0.9618\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9412, F1 Micro: 0.9635, F1 Macro: 0.9618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.92      0.93      0.92       158\n",
      "        part       0.91      0.98      0.94       158\n",
      "       price       0.96      1.00      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.94      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.94      0.98      0.96      1061\n",
      " samples avg       0.94      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5833, Accuracy: 0.6812, F1 Micro: 0.6812, F1 Macro: 0.4052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4505, Accuracy: 0.8777, F1 Micro: 0.8777, F1 Macro: 0.8639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3044, Accuracy: 0.8821, F1 Micro: 0.8821, F1 Macro: 0.8721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2731, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9142\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9079\n",
      "Epoch 6/10, Train Loss: 0.1489, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9079\n",
      "Epoch 7/10, Train Loss: 0.0834, Accuracy: 0.9039, F1 Micro: 0.9039, F1 Macro: 0.8955\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0918, Accuracy: 0.9345, F1 Micro: 0.9345, F1 Macro: 0.9249\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.9207\n",
      "Epoch 10/10, Train Loss: 0.0206, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.9222\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9345, F1 Micro: 0.9345, F1 Macro: 0.9249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.90      0.90        73\n",
      "    positive       0.95      0.95      0.95       156\n",
      "\n",
      "    accuracy                           0.93       229\n",
      "   macro avg       0.92      0.93      0.92       229\n",
      "weighted avg       0.93      0.93      0.93       229\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.8588\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.80      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.93      0.93       152\n",
      "    positive       0.81      0.75      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.91      0.98      0.94       152\n",
      "    positive       0.94      0.71      0.81        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.82      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.62      0.76        13\n",
      "     neutral       0.96      1.00      0.98       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.77      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.57      0.73        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.76      0.82       216\n",
      "weighted avg       0.95      0.95      0.94       216\n",
      "\n",
      "Total train time: 81.69352769851685 s\n",
      "Averaged - Iteration 208: Accuracy: 0.8689, F1 Micro: 0.8689, F1 Macro: 0.6208\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 14.971516370773315 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5737, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.497, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4489, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.405, Accuracy: 0.8333, F1 Micro: 0.9027, F1 Macro: 0.9006\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3462, Accuracy: 0.8683, F1 Micro: 0.9213, F1 Macro: 0.919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2919, Accuracy: 0.9152, F1 Micro: 0.9476, F1 Macro: 0.9454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2284, Accuracy: 0.9315, F1 Micro: 0.9576, F1 Macro: 0.9557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.197, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9632\n",
      "Epoch 9/10, Train Loss: 0.1526, Accuracy: 0.939, F1 Micro: 0.9619, F1 Macro: 0.9593\n",
      "Epoch 10/10, Train Loss: 0.1284, Accuracy: 0.942, F1 Micro: 0.9638, F1 Macro: 0.9615\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.97      0.95       175\n",
      "      others       0.90      0.94      0.92       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5475, Accuracy: 0.6904, F1 Micro: 0.6904, F1 Macro: 0.4084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.432, Accuracy: 0.8619, F1 Micro: 0.8619, F1 Macro: 0.8213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3274, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9055\n",
      "Epoch 4/10, Train Loss: 0.1973, Accuracy: 0.9079, F1 Micro: 0.9079, F1 Macro: 0.8953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1497, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9021\n",
      "Epoch 6/10, Train Loss: 0.1314, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.901\n",
      "Epoch 7/10, Train Loss: 0.0874, Accuracy: 0.9079, F1 Micro: 0.9079, F1 Macro: 0.8946\n",
      "Epoch 8/10, Train Loss: 0.1282, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0961, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9074\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8916\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.88      0.87        74\n",
      "    positive       0.95      0.94      0.94       165\n",
      "\n",
      "    accuracy                           0.92       239\n",
      "   macro avg       0.91      0.91      0.91       239\n",
      "weighted avg       0.92      0.92      0.92       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.8583\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.82      0.86        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.90      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.94      0.97      0.95       167\n",
      "    positive       0.77      0.70      0.73        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.78      0.81       216\n",
      "weighted avg       0.90      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.74      0.77        23\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.78      0.83        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.78      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Total train time: 82.43664503097534 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5897, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5058, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.464, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4247, Accuracy: 0.8199, F1 Micro: 0.8968, F1 Macro: 0.8953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3651, Accuracy: 0.8728, F1 Micro: 0.9244, F1 Macro: 0.9225\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3028, Accuracy: 0.9234, F1 Micro: 0.9525, F1 Macro: 0.9502\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2397, Accuracy: 0.9368, F1 Micro: 0.9609, F1 Macro: 0.9592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2027, Accuracy: 0.9405, F1 Micro: 0.9629, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1586, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9618\n",
      "Epoch 10/10, Train Loss: 0.1315, Accuracy: 0.9427, F1 Micro: 0.964, F1 Macro: 0.9615\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.92      0.98      0.95       175\n",
      "      others       0.93      0.87      0.90       158\n",
      "        part       0.94      0.97      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.97      0.96      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.96      0.97      0.96      1061\n",
      " samples avg       0.96      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6005, Accuracy: 0.6914, F1 Micro: 0.6914, F1 Macro: 0.4088\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5106, Accuracy: 0.7461, F1 Micro: 0.7461, F1 Macro: 0.58\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3415, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9166\n",
      "Epoch 4/10, Train Loss: 0.1726, Accuracy: 0.9062, F1 Micro: 0.9062, F1 Macro: 0.8937\n",
      "Epoch 5/10, Train Loss: 0.1649, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8807\n",
      "Epoch 6/10, Train Loss: 0.1338, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.8951\n",
      "Epoch 7/10, Train Loss: 0.0929, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0867, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0713, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0561, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9225\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.90      0.89        79\n",
      "    positive       0.95      0.95      0.95       177\n",
      "\n",
      "    accuracy                           0.93       256\n",
      "   macro avg       0.92      0.92      0.92       256\n",
      "weighted avg       0.93      0.93      0.93       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.872\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.92      0.98      0.95       167\n",
      "    positive       0.83      0.61      0.70        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.80      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.87      0.90       152\n",
      "    positive       0.70      0.81      0.75        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.78      0.84      0.81       216\n",
      "weighted avg       0.86      0.85      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.86      0.78      0.82        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.83      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 86.6325352191925 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5687, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4981, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4494, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4009, Accuracy: 0.8333, F1 Micro: 0.9029, F1 Macro: 0.9007\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3409, Accuracy: 0.8914, F1 Micro: 0.9346, F1 Macro: 0.9329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2833, Accuracy: 0.9256, F1 Micro: 0.9539, F1 Macro: 0.9514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2231, Accuracy: 0.936, F1 Micro: 0.96, F1 Macro: 0.9578\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1854, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9662\n",
      "Epoch 9/10, Train Loss: 0.1469, Accuracy: 0.9427, F1 Micro: 0.9641, F1 Macro: 0.962\n",
      "Epoch 10/10, Train Loss: 0.1249, Accuracy: 0.9435, F1 Micro: 0.9645, F1 Macro: 0.9616\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9662\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.93      0.92      0.93       158\n",
      "        part       0.92      0.97      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.97      1061\n",
      "weighted avg       0.95      0.98      0.97      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5516, Accuracy: 0.6831, F1 Micro: 0.6831, F1 Macro: 0.4059\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4074, Accuracy: 0.8807, F1 Micro: 0.8807, F1 Macro: 0.8677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2449, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.91\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.183, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9274\n",
      "Epoch 5/10, Train Loss: 0.1035, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1114, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9434\n",
      "Epoch 7/10, Train Loss: 0.0897, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9316\n",
      "Epoch 8/10, Train Loss: 0.0368, Accuracy: 0.9465, F1 Micro: 0.9465, F1 Macro: 0.9384\n",
      "Epoch 9/10, Train Loss: 0.0903, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0715, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9426\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9426\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.91      0.92        77\n",
      "    positive       0.96      0.97      0.96       166\n",
      "\n",
      "    accuracy                           0.95       243\n",
      "   macro avg       0.95      0.94      0.94       243\n",
      "weighted avg       0.95      0.95      0.95       243\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9406, F1 Micro: 0.9406, F1 Macro: 0.8848\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.85      0.70      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.81      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.74      0.81        23\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.86      0.76      0.81        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.98       185\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.85      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 85.6218569278717 s\n",
      "Averaged - Iteration 274: Accuracy: 0.8857, F1 Micro: 0.8857, F1 Macro: 0.6836\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 14.87334418296814 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5572, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.482, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4267, Accuracy: 0.8214, F1 Micro: 0.8978, F1 Macro: 0.8964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3758, Accuracy: 0.8765, F1 Micro: 0.9256, F1 Macro: 0.9236\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2963, Accuracy: 0.9219, F1 Micro: 0.952, F1 Macro: 0.9506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2339, Accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1702, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1333, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1087, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9691\n",
      "Epoch 10/10, Train Loss: 0.096, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9688\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.92      0.92       158\n",
      "        part       0.94      0.99      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5665, Accuracy: 0.8765, F1 Micro: 0.8765, F1 Macro: 0.852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.331, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.207, Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1492, Accuracy: 0.9383, F1 Micro: 0.9383, F1 Macro: 0.9303\n",
      "Epoch 5/10, Train Loss: 0.1326, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.9015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1242, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9437\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.9424, F1 Micro: 0.9424, F1 Macro: 0.9339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0842, Accuracy: 0.9547, F1 Micro: 0.9547, F1 Macro: 0.9492\n",
      "Epoch 9/10, Train Loss: 0.0819, Accuracy: 0.9465, F1 Micro: 0.9465, F1 Macro: 0.9392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9547, F1 Micro: 0.9547, F1 Macro: 0.9486\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9547, F1 Micro: 0.9547, F1 Macro: 0.9486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        78\n",
      "    positive       0.98      0.96      0.97       165\n",
      "\n",
      "    accuracy                           0.95       243\n",
      "   macro avg       0.94      0.95      0.95       243\n",
      "weighted avg       0.96      0.95      0.95       243\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.8952\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.92      0.93      0.92       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.84      0.82       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.91      0.93        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.80      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 96.69859051704407 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5697, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4841, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4496, Accuracy: 0.8013, F1 Micro: 0.8881, F1 Macro: 0.8866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3975, Accuracy: 0.8638, F1 Micro: 0.9198, F1 Macro: 0.919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3086, Accuracy: 0.9249, F1 Micro: 0.9535, F1 Macro: 0.9513\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2401, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1719, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1364, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1131, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0967, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.94      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6552, Accuracy: 0.6721, F1 Micro: 0.6721, F1 Macro: 0.4019\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5012, Accuracy: 0.8016, F1 Micro: 0.8016, F1 Macro: 0.7189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2969, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9369\n",
      "Epoch 4/10, Train Loss: 0.1986, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.902\n",
      "Epoch 5/10, Train Loss: 0.1643, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9236\n",
      "Epoch 6/10, Train Loss: 0.0999, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9389\n",
      "Epoch 8/10, Train Loss: 0.1074, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9206\n",
      "Epoch 9/10, Train Loss: 0.0619, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0819, Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9459\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        81\n",
      "    positive       0.98      0.95      0.96       166\n",
      "\n",
      "    accuracy                           0.95       247\n",
      "   macro avg       0.94      0.95      0.95       247\n",
      "weighted avg       0.95      0.95      0.95       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9016\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.89      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 95.79143214225769 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5522, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4779, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4266, Accuracy: 0.8289, F1 Micro: 0.9018, F1 Macro: 0.9009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3654, Accuracy: 0.9025, F1 Micro: 0.9407, F1 Macro: 0.9387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2831, Accuracy: 0.9308, F1 Micro: 0.957, F1 Macro: 0.9547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2236, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1611, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Epoch 8/10, Train Loss: 0.1334, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9663\n",
      "Epoch 9/10, Train Loss: 0.1071, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9675\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0912, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9687\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.94      0.97      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5942, Accuracy: 0.6803, F1 Micro: 0.6803, F1 Macro: 0.4049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4629, Accuracy: 0.8402, F1 Micro: 0.8402, F1 Macro: 0.8309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2558, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.14, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9442\n",
      "Epoch 5/10, Train Loss: 0.109, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9107\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1076, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9435\n",
      "Epoch 7/10, Train Loss: 0.0954, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.8814\n",
      "Epoch 8/10, Train Loss: 0.0654, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9331\n",
      "Epoch 9/10, Train Loss: 0.0578, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9405\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9405\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        78\n",
      "    positive       0.96      0.96      0.96       166\n",
      "\n",
      "    accuracy                           0.95       244\n",
      "   macro avg       0.94      0.94      0.94       244\n",
      "weighted avg       0.95      0.95      0.95       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9452, F1 Micro: 0.9452, F1 Macro: 0.8913\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.87      0.89        23\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.84      0.78      0.81        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.80      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 90.0524263381958 s\n",
      "Averaged - Iteration 333: Accuracy: 0.8981, F1 Micro: 0.8981, F1 Macro: 0.726\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 13.312636852264404 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5472, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4683, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.412, Accuracy: 0.8475, F1 Micro: 0.91, F1 Macro: 0.908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3279, Accuracy: 0.9129, F1 Micro: 0.9466, F1 Macro: 0.9448\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2396, Accuracy: 0.942, F1 Micro: 0.9641, F1 Macro: 0.9626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1828, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9683\n",
      "Epoch 7/10, Train Loss: 0.1423, Accuracy: 0.9472, F1 Micro: 0.9667, F1 Macro: 0.9639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.114, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0894, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9714\n",
      "Epoch 10/10, Train Loss: 0.0783, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.9682\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.96       175\n",
      "      others       0.91      0.94      0.92       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5708, Accuracy: 0.7953, F1 Micro: 0.7953, F1 Macro: 0.7022\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3422, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9374\n",
      "Epoch 3/10, Train Loss: 0.181, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9121\n",
      "Epoch 4/10, Train Loss: 0.1462, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9126\n",
      "Epoch 5/10, Train Loss: 0.096, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9209\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.082, Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9547\n",
      "Epoch 8/10, Train Loss: 0.0965, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9172\n",
      "Epoch 9/10, Train Loss: 0.0787, Accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9506\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9463\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9547\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        80\n",
      "    positive       0.98      0.97      0.97       174\n",
      "\n",
      "    accuracy                           0.96       254\n",
      "   macro avg       0.95      0.96      0.95       254\n",
      "weighted avg       0.96      0.96      0.96       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.911\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 93.10038352012634 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5646, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4798, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4318, Accuracy: 0.8296, F1 Micro: 0.9017, F1 Macro: 0.9004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3503, Accuracy: 0.9122, F1 Micro: 0.9463, F1 Macro: 0.9441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2526, Accuracy: 0.9278, F1 Micro: 0.9549, F1 Macro: 0.9512\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1935, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1503, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1212, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.094, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Epoch 10/10, Train Loss: 0.081, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9726\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.96       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6131, Accuracy: 0.6802, F1 Micro: 0.6802, F1 Macro: 0.4387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3802, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9222\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2216, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.924\n",
      "Epoch 4/10, Train Loss: 0.1472, Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0946, Accuracy: 0.9595, F1 Micro: 0.9595, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1015, Accuracy: 0.9595, F1 Micro: 0.9595, F1 Macro: 0.9535\n",
      "Epoch 7/10, Train Loss: 0.1042, Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9449\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9422\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9349\n",
      "Epoch 10/10, Train Loss: 0.0357, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9369\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9595, F1 Micro: 0.9595, F1 Macro: 0.9535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      0.90      0.94        82\n",
      "    positive       0.95      0.99      0.97       165\n",
      "\n",
      "    accuracy                           0.96       247\n",
      "   macro avg       0.96      0.95      0.95       247\n",
      "weighted avg       0.96      0.96      0.96       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9144\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.79      0.79      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.94      0.84      0.88       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.91      0.93        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 103.89765882492065 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5426, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4681, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4045, Accuracy: 0.8609, F1 Micro: 0.918, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3257, Accuracy: 0.9286, F1 Micro: 0.9561, F1 Macro: 0.9542\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2314, Accuracy: 0.9397, F1 Micro: 0.9626, F1 Macro: 0.9605\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1787, Accuracy: 0.9539, F1 Micro: 0.9714, F1 Macro: 0.9699\n",
      "Epoch 7/10, Train Loss: 0.1445, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1161, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "Epoch 9/10, Train Loss: 0.0888, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9706\n",
      "Epoch 10/10, Train Loss: 0.08, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9701\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5676, Accuracy: 0.764, F1 Micro: 0.764, F1 Macro: 0.6393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.306, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1363, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9352\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1385, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9452\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9128\n",
      "Epoch 6/10, Train Loss: 0.1004, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9249\n",
      "Epoch 7/10, Train Loss: 0.097, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9181\n",
      "Epoch 8/10, Train Loss: 0.1076, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1024, Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9499\n",
      "Epoch 10/10, Train Loss: 0.0432, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9412\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9499\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        80\n",
      "    positive       0.98      0.96      0.97       170\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.95      0.95      0.95       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9057\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 95.61713480949402 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9074, F1 Micro: 0.9074, F1 Macro: 0.7568\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 12.631531715393066 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5408, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4611, Accuracy: 0.8006, F1 Micro: 0.8879, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4089, Accuracy: 0.8452, F1 Micro: 0.9078, F1 Macro: 0.905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3211, Accuracy: 0.9263, F1 Micro: 0.9544, F1 Macro: 0.9521\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2515, Accuracy: 0.936, F1 Micro: 0.9601, F1 Macro: 0.958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1844, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9667\n",
      "Epoch 7/10, Train Loss: 0.1444, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9663\n",
      "Epoch 8/10, Train Loss: 0.1164, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0882, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0798, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5924, Accuracy: 0.7976, F1 Micro: 0.7976, F1 Macro: 0.7091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3213, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9458\n",
      "Epoch 3/10, Train Loss: 0.1513, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9164\n",
      "Epoch 4/10, Train Loss: 0.1478, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1227, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9451\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1117, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0628, Accuracy: 0.9643, F1 Micro: 0.9643, F1 Macro: 0.9592\n",
      "Epoch 8/10, Train Loss: 0.0922, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9403\n",
      "Epoch 9/10, Train Loss: 0.0807, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9464\n",
      "Epoch 10/10, Train Loss: 0.0606, Accuracy: 0.9603, F1 Micro: 0.9603, F1 Macro: 0.9551\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9643, F1 Micro: 0.9643, F1 Macro: 0.9592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.96      0.94        80\n",
      "    positive       0.98      0.97      0.97       172\n",
      "\n",
      "    accuracy                           0.96       252\n",
      "   macro avg       0.95      0.96      0.96       252\n",
      "weighted avg       0.96      0.96      0.96       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9119\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.89      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.94      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 104.52027654647827 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.554, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4672, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4131, Accuracy: 0.8356, F1 Micro: 0.9036, F1 Macro: 0.9017\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.327, Accuracy: 0.9308, F1 Micro: 0.9575, F1 Macro: 0.9557\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2507, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1883, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1471, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.97\n",
      "Epoch 8/10, Train Loss: 0.1176, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9667\n",
      "Epoch 9/10, Train Loss: 0.0919, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9692\n",
      "Epoch 10/10, Train Loss: 0.0808, Accuracy: 0.9524, F1 Micro: 0.97, F1 Macro: 0.9675\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6162, Accuracy: 0.7592, F1 Micro: 0.7592, F1 Macro: 0.6424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4106, Accuracy: 0.8898, F1 Micro: 0.8898, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2231, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9219\n",
      "Epoch 4/10, Train Loss: 0.2089, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.089, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1144, Accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.9443\n",
      "Epoch 7/10, Train Loss: 0.0735, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9406\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.941\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9469, F1 Micro: 0.9469, F1 Macro: 0.9402\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.9453\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.951, F1 Micro: 0.951, F1 Macro: 0.9453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        81\n",
      "    positive       0.97      0.95      0.96       164\n",
      "\n",
      "    accuracy                           0.95       245\n",
      "   macro avg       0.94      0.95      0.95       245\n",
      "weighted avg       0.95      0.95      0.95       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.9014\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.97      0.73      0.83        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.94      0.88      0.90       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 99.74195170402527 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5367, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4571, Accuracy: 0.7999, F1 Micro: 0.8875, F1 Macro: 0.886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3965, Accuracy: 0.8668, F1 Micro: 0.9196, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.309, Accuracy: 0.9382, F1 Micro: 0.9619, F1 Macro: 0.9601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2365, Accuracy: 0.939, F1 Micro: 0.9619, F1 Macro: 0.9596\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1835, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1416, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.115, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0897, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0782, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9743\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5868, Accuracy: 0.8441, F1 Micro: 0.8441, F1 Macro: 0.7916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3298, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1611, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1369, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9437\n",
      "Epoch 5/10, Train Loss: 0.1608, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Epoch 6/10, Train Loss: 0.0897, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Epoch 7/10, Train Loss: 0.0639, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.072, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9472\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0406, Accuracy: 0.9582, F1 Micro: 0.9582, F1 Macro: 0.9526\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9582, F1 Micro: 0.9582, F1 Macro: 0.9526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        83\n",
      "    positive       0.99      0.95      0.97       180\n",
      "\n",
      "    accuracy                           0.96       263\n",
      "   macro avg       0.94      0.96      0.95       263\n",
      "weighted avg       0.96      0.96      0.96       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9552, F1 Micro: 0.9552, F1 Macro: 0.9192\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.75      0.83      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.88      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.91      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 110.27051711082458 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9138, F1 Micro: 0.9138, F1 Macro: 0.7788\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 11.261483192443848 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.542, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.8065, F1 Micro: 0.8903, F1 Macro: 0.8886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3948, Accuracy: 0.8713, F1 Micro: 0.922, F1 Macro: 0.919\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.291, Accuracy: 0.9301, F1 Micro: 0.9566, F1 Macro: 0.9543\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.213, Accuracy: 0.9442, F1 Micro: 0.9651, F1 Macro: 0.9626\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1763, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1238, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1063, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.9687\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0897, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9722\n",
      "Epoch 10/10, Train Loss: 0.0719, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.971\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.90      0.91       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5341, Accuracy: 0.8839, F1 Micro: 0.8839, F1 Macro: 0.8569\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2214, Accuracy: 0.9139, F1 Micro: 0.9139, F1 Macro: 0.9049\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1667, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1034, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9441\n",
      "Epoch 5/10, Train Loss: 0.1067, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Epoch 6/10, Train Loss: 0.1172, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.079, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9444\n",
      "Epoch 8/10, Train Loss: 0.0903, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9407\n",
      "Epoch 9/10, Train Loss: 0.0363, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9384\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9326\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        84\n",
      "    positive       0.98      0.95      0.96       183\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.94      0.95      0.94       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9124\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.90      0.92       152\n",
      "    positive       0.75      0.81      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.85      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 111.2466230392456 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5548, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4836, Accuracy: 0.7969, F1 Micro: 0.8859, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4097, Accuracy: 0.8802, F1 Micro: 0.9285, F1 Macro: 0.9273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3032, Accuracy: 0.9249, F1 Micro: 0.9536, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.216, Accuracy: 0.9442, F1 Micro: 0.965, F1 Macro: 0.962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1787, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1282, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9724\n",
      "Epoch 8/10, Train Loss: 0.1051, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0879, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "Epoch 10/10, Train Loss: 0.0746, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.968\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.91      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6231, Accuracy: 0.7115, F1 Micro: 0.7115, F1 Macro: 0.5005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3868, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1755, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1355, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9427\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Epoch 6/10, Train Loss: 0.0976, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1182, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9415\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 9/10, Train Loss: 0.0567, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9399\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0615, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9476\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        83\n",
      "    positive       0.98      0.95      0.97       177\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.95      0.95       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9147\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.77      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.89      0.87       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.84      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 107.77476572990417 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5365, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4688, Accuracy: 0.8095, F1 Micro: 0.8921, F1 Macro: 0.8906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.379, Accuracy: 0.8936, F1 Micro: 0.9346, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2733, Accuracy: 0.9301, F1 Micro: 0.9563, F1 Macro: 0.9536\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2013, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9691\n",
      "Epoch 6/10, Train Loss: 0.1678, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.121, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.971\n",
      "Epoch 8/10, Train Loss: 0.1002, Accuracy: 0.9524, F1 Micro: 0.97, F1 Macro: 0.9674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.086, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.075, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9705\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5234, Accuracy: 0.8837, F1 Micro: 0.8837, F1 Macro: 0.8579\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3062, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9428\n",
      "Epoch 3/10, Train Loss: 0.1761, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1165, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1393, Accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9556\n",
      "Epoch 6/10, Train Loss: 0.0896, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9428\n",
      "Epoch 7/10, Train Loss: 0.0774, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9187\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.947\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9467\n",
      "Epoch 10/10, Train Loss: 0.0351, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9428\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        82\n",
      "    positive       0.98      0.97      0.97       176\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.95      0.96      0.96       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9179\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.93      0.93       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.84      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 102.13502359390259 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9188, F1 Micro: 0.9188, F1 Macro: 0.7958\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 10.4421226978302 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5405, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4664, Accuracy: 0.8095, F1 Micro: 0.8923, F1 Macro: 0.8909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3661, Accuracy: 0.8973, F1 Micro: 0.9369, F1 Macro: 0.9339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2738, Accuracy: 0.9442, F1 Micro: 0.9651, F1 Macro: 0.9627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.195, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1436, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1182, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0977, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.90      0.99      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4732, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8814\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2227, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1397, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1163, Accuracy: 0.9553, F1 Micro: 0.9553, F1 Macro: 0.9504\n",
      "Epoch 5/10, Train Loss: 0.0966, Accuracy: 0.9268, F1 Micro: 0.9268, F1 Macro: 0.9204\n",
      "Epoch 6/10, Train Loss: 0.0826, Accuracy: 0.9512, F1 Micro: 0.9512, F1 Macro: 0.9461\n",
      "Epoch 7/10, Train Loss: 0.0786, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9331\n",
      "Epoch 8/10, Train Loss: 0.0541, Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.9331\n",
      "Epoch 9/10, Train Loss: 0.0505, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.9593, F1 Micro: 0.9593, F1 Macro: 0.9543\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9593, F1 Micro: 0.9593, F1 Macro: 0.9543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94        81\n",
      "    positive       0.98      0.96      0.97       165\n",
      "\n",
      "    accuracy                           0.96       246\n",
      "   macro avg       0.95      0.96      0.95       246\n",
      "weighted avg       0.96      0.96      0.96       246\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9165\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.90      0.99      0.94       152\n",
      "    positive       0.95      0.73      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.82      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 109.13462734222412 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5481, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4722, Accuracy: 0.8028, F1 Micro: 0.8875, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3754, Accuracy: 0.9129, F1 Micro: 0.9466, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2783, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1975, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1492, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1227, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9727\n",
      "Epoch 8/10, Train Loss: 0.0971, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0748, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Epoch 10/10, Train Loss: 0.0668, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9725\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6061, Accuracy: 0.6929, F1 Micro: 0.6929, F1 Macro: 0.4541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3444, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.9008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1708, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.9013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1442, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.145, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1376, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0959, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9427\n",
      "Epoch 8/10, Train Loss: 0.0799, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9242\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9427\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9488, F1 Micro: 0.9488, F1 Macro: 0.9427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92        82\n",
      "    positive       0.98      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       254\n",
      "   macro avg       0.93      0.95      0.94       254\n",
      "weighted avg       0.95      0.95      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9117\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.83      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 121.99662756919861 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5381, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.458, Accuracy: 0.8132, F1 Micro: 0.8939, F1 Macro: 0.8925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3529, Accuracy: 0.9159, F1 Micro: 0.9477, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2629, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1907, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1454, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1178, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0934, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "Epoch 9/10, Train Loss: 0.0764, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9744\n",
      "Epoch 10/10, Train Loss: 0.0637, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.95      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.539, Accuracy: 0.8127, F1 Micro: 0.8127, F1 Macro: 0.7469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2783, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9314\n",
      "Epoch 3/10, Train Loss: 0.163, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9258\n",
      "Epoch 4/10, Train Loss: 0.1184, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9179\n",
      "Epoch 5/10, Train Loss: 0.1201, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9463\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9323\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9417\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9384\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        82\n",
      "    positive       0.98      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.95      0.95       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9149\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.95      0.98      0.96       152\n",
      "    positive       0.94      0.76      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 105.67987775802612 s\n",
      "Averaged - Iteration 517: Accuracy: 0.923, F1 Micro: 0.923, F1 Macro: 0.809\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 9.378554821014404 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5381, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4679, Accuracy: 0.8065, F1 Micro: 0.8908, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3758, Accuracy: 0.9003, F1 Micro: 0.9389, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2649, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1902, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1517, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1138, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0724, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Epoch 10/10, Train Loss: 0.0647, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9704\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5157, Accuracy: 0.8933, F1 Micro: 0.8933, F1 Macro: 0.8762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2815, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.906\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1872, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1426, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1132, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9508\n",
      "Epoch 6/10, Train Loss: 0.094, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1017, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9508\n",
      "Epoch 8/10, Train Loss: 0.0669, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9261\n",
      "Epoch 9/10, Train Loss: 0.0431, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9368\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9334\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        82\n",
      "    positive       0.98      0.96      0.97       171\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.95      0.96      0.95       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.921\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.88      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 112.22511315345764 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5455, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4761, Accuracy: 0.8043, F1 Micro: 0.8893, F1 Macro: 0.8877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.395, Accuracy: 0.9025, F1 Micro: 0.9409, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2809, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.201, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1566, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1187, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0992, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9729\n",
      "Epoch 10/10, Train Loss: 0.0664, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9698\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5699, Accuracy: 0.746, F1 Micro: 0.746, F1 Macro: 0.5947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3133, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.921\n",
      "Epoch 3/10, Train Loss: 0.2182, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8993\n",
      "Epoch 4/10, Train Loss: 0.1711, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1349, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9454\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9371\n",
      "Epoch 8/10, Train Loss: 0.057, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0733, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9504\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        81\n",
      "    positive       0.98      0.96      0.97       171\n",
      "\n",
      "    accuracy                           0.96       252\n",
      "   macro avg       0.95      0.95      0.95       252\n",
      "weighted avg       0.96      0.96      0.96       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9176\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.88      0.83      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 110.11900568008423 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5337, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.456, Accuracy: 0.814, F1 Micro: 0.8945, F1 Macro: 0.8933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3613, Accuracy: 0.8996, F1 Micro: 0.9383, F1 Macro: 0.9341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2542, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1867, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Epoch 6/10, Train Loss: 0.15, Accuracy: 0.9501, F1 Micro: 0.9685, F1 Macro: 0.9648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1123, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0929, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9774\n",
      "Epoch 9/10, Train Loss: 0.0735, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9719\n",
      "Epoch 10/10, Train Loss: 0.0632, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9726\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5272, Accuracy: 0.8924, F1 Micro: 0.8924, F1 Macro: 0.8757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2771, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9327\n",
      "Epoch 3/10, Train Loss: 0.1517, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9179\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1805, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9424\n",
      "Epoch 5/10, Train Loss: 0.0856, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9323\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0849, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.077, Accuracy: 0.9562, F1 Micro: 0.9562, F1 Macro: 0.9512\n",
      "Epoch 9/10, Train Loss: 0.0462, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0531, Accuracy: 0.9562, F1 Micro: 0.9562, F1 Macro: 0.9515\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9562, F1 Micro: 0.9562, F1 Macro: 0.9515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        83\n",
      "    positive       0.99      0.95      0.97       168\n",
      "\n",
      "    accuracy                           0.96       251\n",
      "   macro avg       0.94      0.96      0.95       251\n",
      "weighted avg       0.96      0.96      0.96       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9214\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 111.65893745422363 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.8201\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 8.824331283569336 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5305, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4411, Accuracy: 0.8274, F1 Micro: 0.9005, F1 Macro: 0.8992\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3489, Accuracy: 0.9122, F1 Micro: 0.9465, F1 Macro: 0.9446\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.247, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1765, Accuracy: 0.9509, F1 Micro: 0.9691, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1415, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1087, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0876, Accuracy: 0.9546, F1 Micro: 0.9712, F1 Macro: 0.9682\n",
      "Epoch 9/10, Train Loss: 0.0746, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9741\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5289, Accuracy: 0.8842, F1 Micro: 0.8842, F1 Macro: 0.8644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2506, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9217\n",
      "Epoch 3/10, Train Loss: 0.1594, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9124\n",
      "Epoch 4/10, Train Loss: 0.1366, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9135\n",
      "Epoch 5/10, Train Loss: 0.1331, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.888\n",
      "Epoch 6/10, Train Loss: 0.0777, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9333\n",
      "Epoch 8/10, Train Loss: 0.0609, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9174\n",
      "Epoch 9/10, Train Loss: 0.0809, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9262\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9202\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.90      0.91        83\n",
      "    positive       0.95      0.96      0.96       176\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.93      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9081\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 114.96981191635132 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4506, Accuracy: 0.8155, F1 Micro: 0.8952, F1 Macro: 0.8938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3606, Accuracy: 0.9211, F1 Micro: 0.9515, F1 Macro: 0.9494\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2529, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9698\n",
      "Epoch 5/10, Train Loss: 0.1807, Accuracy: 0.9479, F1 Micro: 0.9671, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1381, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1085, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 8/10, Train Loss: 0.0894, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.073, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9728\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5678, Accuracy: 0.8086, F1 Micro: 0.8086, F1 Macro: 0.7318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2903, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9338\n",
      "Epoch 3/10, Train Loss: 0.1902, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1541, Accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.9514\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1271, Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9562\n",
      "Epoch 6/10, Train Loss: 0.1053, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9394\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9352\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.918, F1 Micro: 0.918, F1 Macro: 0.9107\n",
      "Epoch 10/10, Train Loss: 0.0781, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9436\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94        83\n",
      "    positive       0.99      0.95      0.97       173\n",
      "\n",
      "    accuracy                           0.96       256\n",
      "   macro avg       0.95      0.96      0.96       256\n",
      "weighted avg       0.96      0.96      0.96       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9232\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 115.86147928237915 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5278, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4378, Accuracy: 0.8371, F1 Micro: 0.9056, F1 Macro: 0.9039\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3433, Accuracy: 0.9256, F1 Micro: 0.9542, F1 Macro: 0.9518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2419, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1359, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1113, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 8/10, Train Loss: 0.0887, Accuracy: 0.9554, F1 Micro: 0.9716, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4797, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.248, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1464, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1137, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9304\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1406, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.9345\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9258\n",
      "Epoch 7/10, Train Loss: 0.0971, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9176\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9161\n",
      "Epoch 9/10, Train Loss: 0.057, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0394, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9368\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9368\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.89      0.91        83\n",
      "    positive       0.95      0.97      0.96       173\n",
      "\n",
      "    accuracy                           0.95       256\n",
      "   macro avg       0.94      0.93      0.94       256\n",
      "weighted avg       0.95      0.95      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9105\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.85      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 117.55558800697327 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9293, F1 Micro: 0.9293, F1 Macro: 0.8286\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 8.250432968139648 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.539, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4468, Accuracy: 0.8356, F1 Micro: 0.9036, F1 Macro: 0.9018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.345, Accuracy: 0.9338, F1 Micro: 0.9589, F1 Macro: 0.9566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2353, Accuracy: 0.9568, F1 Micro: 0.9733, F1 Macro: 0.9722\n",
      "Epoch 5/10, Train Loss: 0.1787, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Epoch 7/10, Train Loss: 0.107, Accuracy: 0.9546, F1 Micro: 0.9714, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0671, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0601, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.98      0.97      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5105, Accuracy: 0.8902, F1 Micro: 0.8902, F1 Macro: 0.8667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2269, Accuracy: 0.9059, F1 Micro: 0.9059, F1 Macro: 0.8993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1934, Accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9611\n",
      "Epoch 4/10, Train Loss: 0.1351, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9516\n",
      "Epoch 5/10, Train Loss: 0.1269, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9519\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9371\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9286\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9527\n",
      "Epoch 9/10, Train Loss: 0.0678, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0677, Accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9609\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.97      0.95        86\n",
      "    positive       0.98      0.96      0.97       169\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.96      0.96      0.96       255\n",
      "weighted avg       0.97      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9358\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.98      0.97      0.97       167\n",
      "    positive       0.85      0.88      0.87        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.85      0.89       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 116.50732159614563 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5435, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4548, Accuracy: 0.8304, F1 Micro: 0.9021, F1 Macro: 0.9009\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3485, Accuracy: 0.9375, F1 Micro: 0.9614, F1 Macro: 0.9599\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2414, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1858, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1315, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9583, F1 Micro: 0.9736, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0704, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 10/10, Train Loss: 0.0604, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5738, Accuracy: 0.878, F1 Micro: 0.878, F1 Macro: 0.8528\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3457, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2026, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1833, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1313, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0724, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9218\n",
      "Epoch 7/10, Train Loss: 0.1098, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 8/10, Train Loss: 0.1101, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Epoch 9/10, Train Loss: 0.0544, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9099\n",
      "Epoch 10/10, Train Loss: 0.0725, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.96      0.90        82\n",
      "    positive       0.98      0.91      0.95       172\n",
      "\n",
      "    accuracy                           0.93       254\n",
      "   macro avg       0.91      0.94      0.92       254\n",
      "weighted avg       0.94      0.93      0.93       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.908\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.93      0.77      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 118.0748519897461 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5314, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4418, Accuracy: 0.8445, F1 Micro: 0.909, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3282, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2272, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.176, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1275, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1054, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "Epoch 9/10, Train Loss: 0.0675, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.505, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.8703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2535, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9387\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.204, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9472\n",
      "Epoch 4/10, Train Loss: 0.1166, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9429\n",
      "Epoch 5/10, Train Loss: 0.1305, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9383\n",
      "Epoch 6/10, Train Loss: 0.0896, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8914\n",
      "Epoch 7/10, Train Loss: 0.1191, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9435\n",
      "Epoch 8/10, Train Loss: 0.078, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.948\n",
      "Epoch 10/10, Train Loss: 0.0636, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9287\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.99      0.93        84\n",
      "    positive       0.99      0.93      0.96       169\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.96      0.95       253\n",
      "weighted avg       0.96      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9243\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 115.19445395469666 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9319, F1 Micro: 0.9319, F1 Macro: 0.8365\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 7.482051849365234 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5243, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4412, Accuracy: 0.8549, F1 Micro: 0.9147, F1 Macro: 0.9131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3212, Accuracy: 0.9308, F1 Micro: 0.9574, F1 Macro: 0.9551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2157, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9713\n",
      "Epoch 5/10, Train Loss: 0.1575, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1259, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Epoch 8/10, Train Loss: 0.0814, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5052, Accuracy: 0.8631, F1 Micro: 0.8631, F1 Macro: 0.8264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.314, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 3/10, Train Loss: 0.1969, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9208\n",
      "Epoch 4/10, Train Loss: 0.194, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1315, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1338, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9395\n",
      "Epoch 7/10, Train Loss: 0.1065, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9361\n",
      "Epoch 8/10, Train Loss: 0.0786, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9305\n",
      "Epoch 9/10, Train Loss: 0.0616, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9244\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        85\n",
      "    positive       0.97      0.96      0.96       178\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.94      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9227\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.90      0.88      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.88      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 122.92526984214783 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5302, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4554, Accuracy: 0.8199, F1 Micro: 0.8968, F1 Macro: 0.8954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3436, Accuracy: 0.9196, F1 Micro: 0.9504, F1 Macro: 0.9474\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2358, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1644, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1306, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 7/10, Train Loss: 0.1056, Accuracy: 0.9576, F1 Micro: 0.9732, F1 Macro: 0.9713\n",
      "Epoch 8/10, Train Loss: 0.0832, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 9/10, Train Loss: 0.0705, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.055, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5474, Accuracy: 0.8627, F1 Micro: 0.8627, F1 Macro: 0.8269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2717, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9238\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1769, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1336, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9328\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1385, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Epoch 6/10, Train Loss: 0.1211, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9186\n",
      "Epoch 7/10, Train Loss: 0.1055, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0706, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9379\n",
      "Epoch 9/10, Train Loss: 0.0634, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9179\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.75      0.86        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 124.6712908744812 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5226, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4311, Accuracy: 0.8772, F1 Micro: 0.9268, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3079, Accuracy: 0.933, F1 Micro: 0.9586, F1 Macro: 0.9563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2092, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9714\n",
      "Epoch 5/10, Train Loss: 0.1495, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1258, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0996, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 8/10, Train Loss: 0.0786, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Epoch 9/10, Train Loss: 0.0655, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5022, Accuracy: 0.8783, F1 Micro: 0.8783, F1 Macro: 0.847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.322, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9361\n",
      "Epoch 3/10, Train Loss: 0.1832, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9164\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.196, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.935\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9485\n",
      "Epoch 6/10, Train Loss: 0.1495, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "Epoch 7/10, Train Loss: 0.1252, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "Epoch 8/10, Train Loss: 0.0851, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9392\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9291\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9544, F1 Micro: 0.9544, F1 Macro: 0.9485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        84\n",
      "    positive       0.98      0.95      0.97       179\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.96      0.95       263\n",
      "weighted avg       0.96      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9281\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.88      0.83      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 122.54089021682739 s\n",
      "Averaged - Iteration 630: Accuracy: 0.934, F1 Micro: 0.934, F1 Macro: 0.8431\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 6.8020548820495605 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5307, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4291, Accuracy: 0.8609, F1 Micro: 0.9182, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3066, Accuracy: 0.9435, F1 Micro: 0.965, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.215, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1537, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1217, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 7/10, Train Loss: 0.0947, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0751, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Epoch 10/10, Train Loss: 0.0504, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4639, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9062\n",
      "Epoch 2/10, Train Loss: 0.2424, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.8981\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1734, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "Epoch 4/10, Train Loss: 0.1641, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "Epoch 5/10, Train Loss: 0.1327, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 6/10, Train Loss: 0.0953, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9164\n",
      "Epoch 7/10, Train Loss: 0.1099, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.924\n",
      "Epoch 8/10, Train Loss: 0.1164, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "Epoch 9/10, Train Loss: 0.09, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "Epoch 10/10, Train Loss: 0.092, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.931\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9296\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 120.38781690597534 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5382, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.443, Accuracy: 0.843, F1 Micro: 0.9089, F1 Macro: 0.9074\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3183, Accuracy: 0.9375, F1 Micro: 0.9609, F1 Macro: 0.959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2218, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1629, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.127, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0763, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 10/10, Train Loss: 0.053, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5591, Accuracy: 0.8721, F1 Micro: 0.8721, F1 Macro: 0.8443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.28, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9269\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1843, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9277\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1359, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1312, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9386\n",
      "Epoch 7/10, Train Loss: 0.0934, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0538, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9428\n",
      "Epoch 9/10, Train Loss: 0.0474, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9382\n",
      "Epoch 10/10, Train Loss: 0.0572, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9382\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        84\n",
      "    positive       0.97      0.96      0.96       174\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.94      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9188\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 124.59396433830261 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5264, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4155, Accuracy: 0.8847, F1 Micro: 0.9308, F1 Macro: 0.9286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2924, Accuracy: 0.9345, F1 Micro: 0.9591, F1 Macro: 0.9567\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2059, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1514, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1171, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 7/10, Train Loss: 0.0955, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 8/10, Train Loss: 0.0727, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 9/10, Train Loss: 0.0607, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5279, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2528, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9436\n",
      "Epoch 3/10, Train Loss: 0.2034, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 4/10, Train Loss: 0.1348, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 5/10, Train Loss: 0.1458, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Epoch 6/10, Train Loss: 0.1203, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9349\n",
      "Epoch 7/10, Train Loss: 0.0816, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9157\n",
      "Epoch 9/10, Train Loss: 0.0739, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9345\n",
      "Epoch 10/10, Train Loss: 0.068, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9286\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        84\n",
      "    positive       0.98      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9264\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.88      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 116.29620599746704 s\n",
      "Averaged - Iteration 648: Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.849\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 6.425740957260132 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5278, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4296, Accuracy: 0.8586, F1 Micro: 0.9142, F1 Macro: 0.9103\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3172, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1993, Accuracy: 0.9494, F1 Micro: 0.9683, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1577, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 7/10, Train Loss: 0.0907, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.075, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 9/10, Train Loss: 0.0598, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9769\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4793, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2197, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1479, Accuracy: 0.9531, F1 Micro: 0.9531, F1 Macro: 0.9475\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1475, Accuracy: 0.9648, F1 Micro: 0.9648, F1 Macro: 0.9609\n",
      "Epoch 5/10, Train Loss: 0.1049, Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9562\n",
      "Epoch 6/10, Train Loss: 0.082, Accuracy: 0.957, F1 Micro: 0.957, F1 Macro: 0.9514\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9492, F1 Micro: 0.9492, F1 Macro: 0.9429\n",
      "Epoch 8/10, Train Loss: 0.0554, Accuracy: 0.9453, F1 Micro: 0.9453, F1 Macro: 0.9372\n",
      "Epoch 9/10, Train Loss: 0.0627, Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.932\n",
      "Epoch 10/10, Train Loss: 0.0577, Accuracy: 0.9609, F1 Micro: 0.9609, F1 Macro: 0.9557\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9648, F1 Micro: 0.9648, F1 Macro: 0.9609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95        85\n",
      "    positive       0.99      0.96      0.97       171\n",
      "\n",
      "    accuracy                           0.96       256\n",
      "   macro avg       0.96      0.97      0.96       256\n",
      "weighted avg       0.97      0.96      0.97       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.932\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.82      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 123.3500726222992 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4336, Accuracy: 0.869, F1 Micro: 0.9214, F1 Macro: 0.9184\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3176, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1983, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1582, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1186, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 7/10, Train Loss: 0.0919, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0718, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5439, Accuracy: 0.8701, F1 Micro: 0.8701, F1 Macro: 0.8597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2645, Accuracy: 0.9055, F1 Micro: 0.9055, F1 Macro: 0.8973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1766, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1301, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9388\n",
      "Epoch 5/10, Train Loss: 0.147, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1278, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9385\n",
      "Epoch 7/10, Train Loss: 0.0653, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9226\n",
      "Epoch 8/10, Train Loss: 0.092, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0897, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9381\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9346\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        83\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.93      0.94      0.94       254\n",
      "weighted avg       0.95      0.94      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9151\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.77      0.77        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 133.89396691322327 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5221, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.415, Accuracy: 0.8921, F1 Micro: 0.934, F1 Macro: 0.9307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2991, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9687\n",
      "Epoch 4/10, Train Loss: 0.1883, Accuracy: 0.9524, F1 Micro: 0.9701, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1533, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1106, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "Epoch 8/10, Train Loss: 0.0723, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Epoch 9/10, Train Loss: 0.0593, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5165, Accuracy: 0.8947, F1 Micro: 0.8947, F1 Macro: 0.8854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2378, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9321\n",
      "Epoch 3/10, Train Loss: 0.2062, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.8954\n",
      "Epoch 4/10, Train Loss: 0.1828, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9276\n",
      "Epoch 5/10, Train Loss: 0.1274, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9256\n",
      "Epoch 6/10, Train Loss: 0.1408, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0868, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9415\n",
      "Epoch 8/10, Train Loss: 0.1084, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9325\n",
      "Epoch 9/10, Train Loss: 0.0718, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9335\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9496\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9549, F1 Micro: 0.9549, F1 Macro: 0.9496\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        87\n",
      "    positive       0.98      0.95      0.97       179\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.94      0.96      0.95       266\n",
      "weighted avg       0.96      0.95      0.96       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9301\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 121.727947473526 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9376, F1 Micro: 0.9376, F1 Macro: 0.8541\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.736933946609497 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5378, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4229, Accuracy: 0.8914, F1 Micro: 0.9338, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3035, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1997, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1531, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1142, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0926, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 8/10, Train Loss: 0.0763, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 10/10, Train Loss: 0.0477, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.98      0.98      0.98       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4796, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "Epoch 2/10, Train Loss: 0.2486, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1667, Accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "Epoch 4/10, Train Loss: 0.1687, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "Epoch 5/10, Train Loss: 0.1343, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 6/10, Train Loss: 0.0992, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9223\n",
      "Epoch 7/10, Train Loss: 0.1089, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9212\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.92\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9204\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9468, F1 Micro: 0.9468, F1 Macro: 0.9402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.923\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.98      0.98      0.98       167\n",
      "    positive       0.91      0.88      0.89        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 124.07787585258484 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5421, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4359, Accuracy: 0.8832, F1 Micro: 0.9293, F1 Macro: 0.927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3143, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.9627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2076, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1591, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1173, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 8/10, Train Loss: 0.0748, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0663, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 10/10, Train Loss: 0.0497, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5413, Accuracy: 0.9026, F1 Micro: 0.9026, F1 Macro: 0.8856\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2788, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.933\n",
      "Epoch 3/10, Train Loss: 0.1826, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9258\n",
      "Epoch 4/10, Train Loss: 0.1774, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1487, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.931\n",
      "Epoch 6/10, Train Loss: 0.1023, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9178\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 8/10, Train Loss: 0.0837, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9188\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0555, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9366\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.92        86\n",
      "    positive       0.97      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.94       267\n",
      "   macro avg       0.93      0.94      0.94       267\n",
      "weighted avg       0.95      0.94      0.94       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9185\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 128.15724277496338 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5332, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4175, Accuracy: 0.904, F1 Micro: 0.941, F1 Macro: 0.9384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2946, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1931, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1488, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1097, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0482, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.98\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4893, Accuracy: 0.9049, F1 Micro: 0.9049, F1 Macro: 0.8923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2522, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1579, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9368\n",
      "Epoch 4/10, Train Loss: 0.1454, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.902\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9354\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9089\n",
      "Epoch 8/10, Train Loss: 0.0599, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Epoch 9/10, Train Loss: 0.0515, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 10/10, Train Loss: 0.0524, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.91      0.91        87\n",
      "    positive       0.95      0.96      0.96       176\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.94      0.93      0.94       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.917\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.88      0.90       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 138.8785581588745 s\n",
      "Averaged - Iteration 698: Accuracy: 0.939, F1 Micro: 0.939, F1 Macro: 0.8582\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.485608339309692 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5331, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4404, Accuracy: 0.8943, F1 Micro: 0.9355, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2934, Accuracy: 0.9397, F1 Micro: 0.9623, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2012, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1474, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1159, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 7/10, Train Loss: 0.0888, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0577, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9788\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4693, Accuracy: 0.8922, F1 Micro: 0.8922, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1931, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9361\n",
      "Epoch 3/10, Train Loss: 0.1464, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "Epoch 4/10, Train Loss: 0.1194, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9296\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.127, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9372\n",
      "Epoch 6/10, Train Loss: 0.122, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9288\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9284\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0926, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9419\n",
      "Epoch 10/10, Train Loss: 0.0585, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.948, F1 Micro: 0.948, F1 Macro: 0.9419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        87\n",
      "    positive       0.98      0.94      0.96       182\n",
      "\n",
      "    accuracy                           0.95       269\n",
      "   macro avg       0.93      0.95      0.94       269\n",
      "weighted avg       0.95      0.95      0.95       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9239\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.78      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.14449095726013 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5378, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4501, Accuracy: 0.8921, F1 Micro: 0.9346, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2981, Accuracy: 0.9494, F1 Micro: 0.9687, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2025, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1502, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1143, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.978\n",
      "Epoch 7/10, Train Loss: 0.091, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Epoch 10/10, Train Loss: 0.0499, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.8911, F1 Micro: 0.8911, F1 Macro: 0.8715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2817, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1831, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9271\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1906, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9288\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1157, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0767, Accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.9463\n",
      "Epoch 8/10, Train Loss: 0.0906, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.941\n",
      "Epoch 9/10, Train Loss: 0.0737, Accuracy: 0.9355, F1 Micro: 0.9355, F1 Macro: 0.9292\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.942\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9516, F1 Micro: 0.9516, F1 Macro: 0.9463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.96       166\n",
      "\n",
      "    accuracy                           0.95       248\n",
      "   macro avg       0.94      0.95      0.95       248\n",
      "weighted avg       0.95      0.95      0.95       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9146\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.88      0.73      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.83039712905884 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5282, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4327, Accuracy: 0.9048, F1 Micro: 0.9417, F1 Macro: 0.9392\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2855, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.9649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1976, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1437, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0873, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0512, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.92      0.93      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4842, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9149\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2222, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9374\n",
      "Epoch 3/10, Train Loss: 0.1945, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9199\n",
      "Epoch 4/10, Train Loss: 0.1148, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.141, Accuracy: 0.9478, F1 Micro: 0.9478, F1 Macro: 0.9385\n",
      "Epoch 6/10, Train Loss: 0.1142, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0564, Accuracy: 0.9515, F1 Micro: 0.9515, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0527, Accuracy: 0.9627, F1 Micro: 0.9627, F1 Macro: 0.9577\n",
      "Epoch 9/10, Train Loss: 0.0443, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9239\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9208\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9627, F1 Micro: 0.9627, F1 Macro: 0.9577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.97      0.94        86\n",
      "    positive       0.98      0.96      0.97       182\n",
      "\n",
      "    accuracy                           0.96       268\n",
      "   macro avg       0.95      0.96      0.96       268\n",
      "weighted avg       0.96      0.96      0.96       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9317\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.78      0.81      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.7347068786621 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.862\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.824850559234619 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5292, Accuracy: 0.7939, F1 Micro: 0.8845, F1 Macro: 0.8831\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4197, Accuracy: 0.9077, F1 Micro: 0.9436, F1 Macro: 0.9416\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2789, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1928, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Epoch 5/10, Train Loss: 0.1368, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9734\n",
      "Epoch 6/10, Train Loss: 0.1066, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0842, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 8/10, Train Loss: 0.0688, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 9/10, Train Loss: 0.0564, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9734\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.965, F1 Micro: 0.9778, F1 Macro: 0.9759\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.479, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2105, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.181, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.929\n",
      "Epoch 4/10, Train Loss: 0.1204, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1247, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9379\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1205, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9429\n",
      "Epoch 7/10, Train Loss: 0.0932, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.939\n",
      "Epoch 8/10, Train Loss: 0.0756, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9353\n",
      "Epoch 10/10, Train Loss: 0.0653, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.92      0.92        84\n",
      "    positive       0.96      0.97      0.96       178\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.94      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9174\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.83      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 134.6605830192566 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5342, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4242, Accuracy: 0.9018, F1 Micro: 0.9408, F1 Macro: 0.939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2888, Accuracy: 0.9464, F1 Micro: 0.9666, F1 Macro: 0.965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1944, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1377, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9741\n",
      "Epoch 6/10, Train Loss: 0.1049, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0864, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0586, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9781\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.8841, F1 Micro: 0.8841, F1 Macro: 0.8745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3018, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1876, Accuracy: 0.9348, F1 Micro: 0.9348, F1 Macro: 0.9274\n",
      "Epoch 4/10, Train Loss: 0.1539, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9084\n",
      "Epoch 5/10, Train Loss: 0.1417, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9185\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9185\n",
      "Epoch 7/10, Train Loss: 0.0948, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9219\n",
      "Epoch 8/10, Train Loss: 0.0915, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9459\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9189\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.96      0.97       189\n",
      "\n",
      "    accuracy                           0.95       276\n",
      "   macro avg       0.94      0.95      0.95       276\n",
      "weighted avg       0.95      0.95      0.95       276\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9304\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.76      0.85      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.86      0.87       216\n",
      "weighted avg       0.90      0.89      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.2647979259491 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5234, Accuracy: 0.7984, F1 Micro: 0.8865, F1 Macro: 0.8849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4123, Accuracy: 0.9182, F1 Micro: 0.9494, F1 Macro: 0.9467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2712, Accuracy: 0.9442, F1 Micro: 0.9651, F1 Macro: 0.963\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1867, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0853, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0688, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.981\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.464, Accuracy: 0.8914, F1 Micro: 0.8914, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2832, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2057, Accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9403\n",
      "Epoch 4/10, Train Loss: 0.1618, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9183\n",
      "Epoch 5/10, Train Loss: 0.139, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9166\n",
      "Epoch 6/10, Train Loss: 0.0894, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9132\n",
      "Epoch 7/10, Train Loss: 0.119, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9171\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9171\n",
      "Epoch 9/10, Train Loss: 0.0704, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9363\n",
      "Epoch 10/10, Train Loss: 0.0669, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9166\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9476, F1 Micro: 0.9476, F1 Macro: 0.9403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        87\n",
      "    positive       0.96      0.96      0.96       180\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.94      0.94      0.94       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9282\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 133.97050213813782 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9413, F1 Micro: 0.9413, F1 Macro: 0.8655\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 4.339823246002197 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5218, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4224, Accuracy: 0.8943, F1 Micro: 0.9349, F1 Macro: 0.9318\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2929, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1933, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1429, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 6/10, Train Loss: 0.1079, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 10/10, Train Loss: 0.0493, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4628, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.237, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1813, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9485\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9441\n",
      "Epoch 5/10, Train Loss: 0.1173, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1022, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "Epoch 7/10, Train Loss: 0.0879, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9441\n",
      "Epoch 8/10, Train Loss: 0.0645, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9431\n",
      "Epoch 9/10, Train Loss: 0.0776, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9358\n",
      "Epoch 10/10, Train Loss: 0.0642, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9332\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.95      0.95       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9278\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.96      0.93       152\n",
      "    positive       0.84      0.73      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.81      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 132.62278485298157 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5299, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4282, Accuracy: 0.907, F1 Micro: 0.9433, F1 Macro: 0.9412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2863, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1887, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 6/10, Train Loss: 0.1066, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 7/10, Train Loss: 0.0851, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 8/10, Train Loss: 0.069, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9739\n",
      "Epoch 9/10, Train Loss: 0.0567, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.978\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5393, Accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2572, Accuracy: 0.9137, F1 Micro: 0.9137, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1793, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.109, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9386\n",
      "Epoch 5/10, Train Loss: 0.0955, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.934\n",
      "Epoch 6/10, Train Loss: 0.1081, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9101\n",
      "Epoch 7/10, Train Loss: 0.0958, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9141\n",
      "Epoch 8/10, Train Loss: 0.086, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9386\n",
      "Epoch 10/10, Train Loss: 0.0434, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9256\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        84\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.94      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9188\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       1.00      0.78      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 126.76061153411865 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5153, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4034, Accuracy: 0.9137, F1 Micro: 0.9461, F1 Macro: 0.9427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2702, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1867, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1389, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.9568, F1 Micro: 0.9727, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0846, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 8/10, Train Loss: 0.0697, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9738\n",
      "Epoch 9/10, Train Loss: 0.0563, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4739, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2369, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 3/10, Train Loss: 0.1513, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.134, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9447\n",
      "Epoch 5/10, Train Loss: 0.0906, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 7/10, Train Loss: 0.0922, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9352\n",
      "Epoch 8/10, Train Loss: 0.0552, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 9/10, Train Loss: 0.0455, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        86\n",
      "    positive       0.97      0.96      0.96       175\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.92\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 130.4058928489685 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.8685\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.9851434230804443 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5236, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4077, Accuracy: 0.9107, F1 Micro: 0.9452, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2571, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1723, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1339, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.971, F1 Micro: 0.9819, F1 Macro: 0.981\n",
      "Epoch 7/10, Train Loss: 0.0797, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "Epoch 8/10, Train Loss: 0.0642, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9819, F1 Macro: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4156, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.938\n",
      "Epoch 2/10, Train Loss: 0.2158, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1743, Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9374\n",
      "Epoch 4/10, Train Loss: 0.1392, Accuracy: 0.9315, F1 Micro: 0.9315, F1 Macro: 0.9249\n",
      "Epoch 5/10, Train Loss: 0.109, Accuracy: 0.8952, F1 Micro: 0.8952, F1 Macro: 0.8899\n",
      "Epoch 6/10, Train Loss: 0.1105, Accuracy: 0.9274, F1 Micro: 0.9274, F1 Macro: 0.9199\n",
      "Epoch 7/10, Train Loss: 0.0882, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9177\n",
      "Epoch 8/10, Train Loss: 0.1097, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9177\n",
      "Epoch 9/10, Train Loss: 0.0763, Accuracy: 0.9194, F1 Micro: 0.9194, F1 Macro: 0.9136\n",
      "Epoch 10/10, Train Loss: 0.0632, Accuracy: 0.9395, F1 Micro: 0.9395, F1 Macro: 0.9338\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.9374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        85\n",
      "    positive       0.96      0.96      0.96       163\n",
      "\n",
      "    accuracy                           0.94       248\n",
      "   macro avg       0.94      0.94      0.94       248\n",
      "weighted avg       0.94      0.94      0.94       248\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9206\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.93      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.82      0.84       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 128.07584619522095 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5295, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4218, Accuracy: 0.9129, F1 Micro: 0.9468, F1 Macro: 0.9449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2679, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1771, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 5/10, Train Loss: 0.1382, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9782\n",
      "Epoch 6/10, Train Loss: 0.0988, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 8/10, Train Loss: 0.0648, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9785\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5129, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8989\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2453, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9171\n",
      "Epoch 3/10, Train Loss: 0.148, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1415, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9308\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0912, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9401\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 8/10, Train Loss: 0.0907, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "Epoch 9/10, Train Loss: 0.0738, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9266\n",
      "Epoch 10/10, Train Loss: 0.0721, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9176\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        85\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.93      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9219\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.82      0.83      0.82       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 133.45471024513245 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5224, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4027, Accuracy: 0.9234, F1 Micro: 0.9529, F1 Macro: 0.9509\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2544, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1728, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1376, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1024, Accuracy: 0.9673, F1 Micro: 0.9796, F1 Macro: 0.9787\n",
      "Epoch 7/10, Train Loss: 0.0824, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.475, Accuracy: 0.8897, F1 Micro: 0.8897, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2715, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "Epoch 3/10, Train Loss: 0.1689, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9204\n",
      "Epoch 4/10, Train Loss: 0.167, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Epoch 5/10, Train Loss: 0.1446, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "Epoch 6/10, Train Loss: 0.0975, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.928\n",
      "Epoch 7/10, Train Loss: 0.0781, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "Epoch 8/10, Train Loss: 0.0543, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9089\n",
      "Epoch 9/10, Train Loss: 0.0519, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9309\n",
      "Epoch 10/10, Train Loss: 0.0527, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9089\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.95      0.94       263\n",
      "weighted avg       0.95      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9231\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.87      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      1.00      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 132.69339108467102 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.8712\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.5700583457946777 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5207, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4006, Accuracy: 0.9159, F1 Micro: 0.948, F1 Macro: 0.9452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2542, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.174, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9747\n",
      "Epoch 5/10, Train Loss: 0.1284, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9742\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.076, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0604, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0511, Accuracy: 0.9732, F1 Micro: 0.9831, F1 Macro: 0.982\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9732, F1 Micro: 0.9831, F1 Macro: 0.982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4683, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9246\n",
      "Epoch 2/10, Train Loss: 0.2151, Accuracy: 0.9064, F1 Micro: 0.9064, F1 Macro: 0.8986\n",
      "Epoch 3/10, Train Loss: 0.1538, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.914\n",
      "Epoch 4/10, Train Loss: 0.1231, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0825, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9258\n",
      "Epoch 6/10, Train Loss: 0.091, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.925\n",
      "Epoch 8/10, Train Loss: 0.0774, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9101\n",
      "Epoch 9/10, Train Loss: 0.0759, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9202\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9101, F1 Micro: 0.9101, F1 Macro: 0.9015\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        87\n",
      "    positive       0.97      0.93      0.95       180\n",
      "\n",
      "    accuracy                           0.93       267\n",
      "   macro avg       0.92      0.94      0.92       267\n",
      "weighted avg       0.94      0.93      0.93       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9229\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.95      0.95       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 136.8859782218933 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5286, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4103, Accuracy: 0.9196, F1 Micro: 0.9506, F1 Macro: 0.9485\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.263, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1779, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1324, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0963, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0736, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "Epoch 8/10, Train Loss: 0.0594, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0487, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5041, Accuracy: 0.8927, F1 Micro: 0.8927, F1 Macro: 0.8813\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2543, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.9042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1809, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9241\n",
      "Epoch 4/10, Train Loss: 0.1348, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1015, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1109, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9362\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9277\n",
      "Epoch 9/10, Train Loss: 0.0417, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9241\n",
      "Epoch 10/10, Train Loss: 0.0645, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9224\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        84\n",
      "    positive       0.99      0.93      0.96       177\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.93      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9266\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.84      0.84       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.40548753738403 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5157, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3906, Accuracy: 0.9263, F1 Micro: 0.9539, F1 Macro: 0.9504\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2472, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1707, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1282, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Epoch 6/10, Train Loss: 0.0986, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9751\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9778\n",
      "Epoch 8/10, Train Loss: 0.0591, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0529, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.048, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4591, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2188, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.158, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1338, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9223\n",
      "Epoch 5/10, Train Loss: 0.0927, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9236\n",
      "Epoch 8/10, Train Loss: 0.1056, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9263\n",
      "Epoch 9/10, Train Loss: 0.0533, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Epoch 10/10, Train Loss: 0.0557, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        85\n",
      "    positive       0.98      0.93      0.95       178\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.92      0.95      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9222\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.81      0.86        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 148.33941316604614 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9441, F1 Micro: 0.9441, F1 Macro: 0.8737\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.0606727600097656 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5382, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4111, Accuracy: 0.9085, F1 Micro: 0.9441, F1 Macro: 0.9421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.269, Accuracy: 0.9554, F1 Micro: 0.9723, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1695, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1288, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1025, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9789\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0526, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.981\n",
      "Epoch 10/10, Train Loss: 0.0458, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9799\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.95      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4516, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9241\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1918, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9245\n",
      "Epoch 3/10, Train Loss: 0.1466, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1145, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.94\n",
      "Epoch 5/10, Train Loss: 0.1111, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.921\n",
      "Epoch 6/10, Train Loss: 0.0721, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9306\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0768, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.94\n",
      "Epoch 8/10, Train Loss: 0.0812, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.063, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9403\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9355\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.94      0.95      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9288\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.85      0.85      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.23545718193054 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4164, Accuracy: 0.91, F1 Micro: 0.9446, F1 Macro: 0.9421\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2735, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1741, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1286, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.9788\n",
      "Epoch 9/10, Train Loss: 0.0537, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4902, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9139\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2375, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9292\n",
      "Epoch 3/10, Train Loss: 0.1471, Accuracy: 0.9145, F1 Micro: 0.9145, F1 Macro: 0.9057\n",
      "Epoch 4/10, Train Loss: 0.1299, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.913\n",
      "Epoch 5/10, Train Loss: 0.1041, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9101\n",
      "Epoch 6/10, Train Loss: 0.0825, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9221\n",
      "Epoch 7/10, Train Loss: 0.0673, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9195\n",
      "Epoch 8/10, Train Loss: 0.1024, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9217\n",
      "Epoch 9/10, Train Loss: 0.0588, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0426, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9284\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.92      0.90        87\n",
      "    positive       0.96      0.95      0.95       182\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.92      0.93      0.93       269\n",
      "weighted avg       0.94      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9206\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.86      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.47464799880981 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5268, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3923, Accuracy: 0.9211, F1 Micro: 0.9514, F1 Macro: 0.9488\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2553, Accuracy: 0.9568, F1 Micro: 0.9733, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1626, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1235, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0996, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Epoch 10/10, Train Loss: 0.046, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4971, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9147\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2122, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1745, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9393\n",
      "Epoch 4/10, Train Loss: 0.1366, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9336\n",
      "Epoch 5/10, Train Loss: 0.1099, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9273\n",
      "Epoch 6/10, Train Loss: 0.1038, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9317\n",
      "Epoch 7/10, Train Loss: 0.0617, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0682, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9435\n",
      "Epoch 9/10, Train Loss: 0.0771, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "Epoch 10/10, Train Loss: 0.0574, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9233\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        85\n",
      "    positive       0.97      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9218\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 146.78372192382812 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.876\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.6374733448028564 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5255, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3852, Accuracy: 0.9345, F1 Micro: 0.9596, F1 Macro: 0.9584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2405, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1645, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1182, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.098, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.9744\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.95      0.94      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4392, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.217, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1708, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9322\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1196, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.946\n",
      "Epoch 5/10, Train Loss: 0.1095, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.93\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0723, Accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9494\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9333\n",
      "Epoch 8/10, Train Loss: 0.0707, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9337\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9139, F1 Micro: 0.9139, F1 Macro: 0.9067\n",
      "Epoch 10/10, Train Loss: 0.0639, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9166\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9551, F1 Micro: 0.9551, F1 Macro: 0.9494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        88\n",
      "    positive       0.97      0.96      0.97       179\n",
      "\n",
      "    accuracy                           0.96       267\n",
      "   macro avg       0.95      0.95      0.95       267\n",
      "weighted avg       0.96      0.96      0.96       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9348\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.88      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.89      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.88      0.94        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.96      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.78204321861267 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5279, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3975, Accuracy: 0.9226, F1 Micro: 0.952, F1 Macro: 0.9498\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2505, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1639, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.975\n",
      "Epoch 5/10, Train Loss: 0.1227, Accuracy: 0.9598, F1 Micro: 0.9745, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0956, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9794\n",
      "Epoch 8/10, Train Loss: 0.0608, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9771\n",
      "Epoch 9/10, Train Loss: 0.0512, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9727\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9688, F1 Micro: 0.9802, F1 Macro: 0.979\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.482, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2149, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9324\n",
      "Epoch 3/10, Train Loss: 0.1647, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9144\n",
      "Epoch 4/10, Train Loss: 0.1331, Accuracy: 0.9257, F1 Micro: 0.9257, F1 Macro: 0.916\n",
      "Epoch 5/10, Train Loss: 0.097, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.924\n",
      "Epoch 6/10, Train Loss: 0.1007, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0997, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9369\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9361\n",
      "Epoch 9/10, Train Loss: 0.0824, Accuracy: 0.9182, F1 Micro: 0.9182, F1 Macro: 0.9101\n",
      "Epoch 10/10, Train Loss: 0.0617, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.913\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        86\n",
      "    positive       0.96      0.96      0.96       183\n",
      "\n",
      "    accuracy                           0.94       269\n",
      "   macro avg       0.93      0.94      0.94       269\n",
      "weighted avg       0.94      0.94      0.94       269\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9221\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.80      0.83      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.84      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.43808579444885 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5214, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3832, Accuracy: 0.9353, F1 Micro: 0.96, F1 Macro: 0.9582\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2336, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1565, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1179, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Epoch 6/10, Train Loss: 0.0927, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.974\n",
      "Epoch 7/10, Train Loss: 0.0755, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9736\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0494, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9781\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4561, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9174\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2265, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9318\n",
      "Epoch 3/10, Train Loss: 0.1462, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9265\n",
      "Epoch 4/10, Train Loss: 0.1193, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9233\n",
      "Epoch 5/10, Train Loss: 0.1171, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0686, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.076, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9389\n",
      "Epoch 8/10, Train Loss: 0.0774, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Epoch 9/10, Train Loss: 0.0624, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9359\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9125\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        84\n",
      "    positive       0.97      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.94      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9165\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.57961583137512 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9456, F1 Micro: 0.9456, F1 Macro: 0.8781\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 2.2169532775878906 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5306, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3971, Accuracy: 0.9293, F1 Micro: 0.9567, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2493, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1677, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1226, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9789\n",
      "Epoch 7/10, Train Loss: 0.0773, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9762\n",
      "Epoch 8/10, Train Loss: 0.0605, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9748\n",
      "Epoch 9/10, Train Loss: 0.0539, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0446, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4273, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "Epoch 2/10, Train Loss: 0.2161, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1425, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "Epoch 4/10, Train Loss: 0.1474, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.933\n",
      "Epoch 5/10, Train Loss: 0.1143, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.084, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9484\n",
      "Epoch 8/10, Train Loss: 0.0681, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "Epoch 9/10, Train Loss: 0.0653, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0412, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9531\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.97      0.94        86\n",
      "    positive       0.98      0.95      0.97       176\n",
      "\n",
      "    accuracy                           0.96       262\n",
      "   macro avg       0.95      0.96      0.95       262\n",
      "weighted avg       0.96      0.96      0.96       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.93\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.56425070762634 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5341, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4087, Accuracy: 0.9226, F1 Micro: 0.9525, F1 Macro: 0.9506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2605, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1751, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1259, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0958, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 7/10, Train Loss: 0.077, Accuracy: 0.9613, F1 Micro: 0.9754, F1 Macro: 0.9733\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9735\n",
      "Epoch 9/10, Train Loss: 0.0545, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.9742\n",
      "Epoch 10/10, Train Loss: 0.0461, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.506, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9258\n",
      "Epoch 2/10, Train Loss: 0.2513, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1516, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9249\n",
      "Epoch 4/10, Train Loss: 0.1231, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1393, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "Epoch 6/10, Train Loss: 0.1129, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9271\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9197\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0816, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9345\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9189\n",
      "Epoch 10/10, Train Loss: 0.0554, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.8728\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        84\n",
      "    positive       0.97      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9182\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.84      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.87      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 146.4661123752594 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5233, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3837, Accuracy: 0.933, F1 Micro: 0.9584, F1 Macro: 0.9561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2351, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1628, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1174, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 6/10, Train Loss: 0.0948, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.0757, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9739\n",
      "Epoch 8/10, Train Loss: 0.0601, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9769\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9751\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4592, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2242, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9354\n",
      "Epoch 3/10, Train Loss: 0.1191, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "Epoch 4/10, Train Loss: 0.1423, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9231\n",
      "Epoch 5/10, Train Loss: 0.1003, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Epoch 6/10, Train Loss: 0.0976, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9231\n",
      "Epoch 7/10, Train Loss: 0.071, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9105\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9105\n",
      "Epoch 9/10, Train Loss: 0.063, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9131\n",
      "Epoch 10/10, Train Loss: 0.0431, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9181\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        84\n",
      "    positive       0.98      0.93      0.96       173\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.95      0.94       257\n",
      "weighted avg       0.95      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9231\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 133.3900306224823 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9463, F1 Micro: 0.9463, F1 Macro: 0.88\n",
      "Total runtime: 9114.654509782791 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a7ddd",
   "metadata": {
    "papermill": {
     "duration": 0.015643,
     "end_time": "2025-04-05T06:35:21.534971",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.519328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc1824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 4\n",
      "Random seed: [3, 44, 85]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6645, Accuracy: 0.7865, F1 Micro: 0.8804, F1 Macro: 0.8788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5755, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5509, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5294, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4827, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4817, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 7/10, Train Loss: 0.4898, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 8/10, Train Loss: 0.4555, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4326, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4, Accuracy: 0.7946, F1 Micro: 0.8843, F1 Macro: 0.8825\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7946, F1 Micro: 0.8843, F1 Macro: 0.8825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.73      0.96      0.83       158\n",
      "        part       0.72      1.00      0.84       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.88      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7166, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5281, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4526, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3997, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3523, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3078, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2888, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2379, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1964, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1981, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.4865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.95      1.00      0.97        18\n",
      "\n",
      "    accuracy                           0.95        19\n",
      "   macro avg       0.47      0.50      0.49        19\n",
      "weighted avg       0.90      0.95      0.92        19\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.794, F1 Micro: 0.794, F1 Macro: 0.3165\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.73      0.96      0.83       152\n",
      "    positive       0.60      0.17      0.27        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.44      0.38      0.37       216\n",
      "weighted avg       0.66      0.72      0.65       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      1.00      0.84       152\n",
      "    positive       0.75      0.07      0.13        41\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.49      0.36      0.32       216\n",
      "weighted avg       0.65      0.72      0.61       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 63.76608633995056 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6903, Accuracy: 0.7671, F1 Micro: 0.8669, F1 Macro: 0.8648\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5862, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 3/10, Train Loss: 0.5474, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 4/10, Train Loss: 0.5251, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 5/10, Train Loss: 0.4831, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 6/10, Train Loss: 0.4764, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4815, Accuracy: 0.7887, F1 Micro: 0.8818, F1 Macro: 0.8803\n",
      "Epoch 8/10, Train Loss: 0.4411, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4087, Accuracy: 0.7969, F1 Micro: 0.8856, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3783, Accuracy: 0.8043, F1 Micro: 0.889, F1 Macro: 0.8873\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8043, F1 Micro: 0.889, F1 Macro: 0.8873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.81      0.97      0.88       175\n",
      "      others       0.73      0.99      0.84       158\n",
      "        part       0.74      0.99      0.85       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.81      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.89      1061\n",
      "weighted avg       0.81      0.99      0.89      1061\n",
      " samples avg       0.81      0.99      0.89      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6552, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5933, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5783, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5062, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4427, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3945, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.3966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4686, Accuracy: 0.6857, F1 Micro: 0.6857, F1 Macro: 0.4804\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4872, Accuracy: 0.7429, F1 Micro: 0.7429, F1 Macro: 0.6504\n",
      "Epoch 9/10, Train Loss: 0.3907, Accuracy: 0.6571, F1 Micro: 0.6571, F1 Macro: 0.6196\n",
      "Epoch 10/10, Train Loss: 0.4138, Accuracy: 0.6286, F1 Micro: 0.6286, F1 Macro: 0.5956\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.7429, F1 Micro: 0.7429, F1 Macro: 0.6504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.33      0.47        12\n",
      "    positive       0.73      0.96      0.83        23\n",
      "\n",
      "    accuracy                           0.74        35\n",
      "   macro avg       0.77      0.64      0.65        35\n",
      "weighted avg       0.76      0.74      0.71        35\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7994, F1 Micro: 0.7994, F1 Macro: 0.3498\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.25      0.38        16\n",
      "     neutral       0.80      0.98      0.88       167\n",
      "    positive       0.25      0.06      0.10        33\n",
      "\n",
      "    accuracy                           0.78       216\n",
      "   macro avg       0.62      0.43      0.45       216\n",
      "weighted avg       0.72      0.78      0.72       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.73      0.99      0.84       152\n",
      "    positive       0.70      0.13      0.23        52\n",
      "\n",
      "    accuracy                           0.73       216\n",
      "   macro avg       0.48      0.37      0.35       216\n",
      "weighted avg       0.68      0.73      0.64       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.74      0.99      0.85       152\n",
      "    positive       0.58      0.17      0.26        41\n",
      "\n",
      "    accuracy                           0.73       216\n",
      "   macro avg       0.44      0.39      0.37       216\n",
      "weighted avg       0.63      0.73      0.65       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 54.2345290184021 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6548, Accuracy: 0.7827, F1 Micro: 0.878, F1 Macro: 0.8767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5764, Accuracy: 0.7879, F1 Micro: 0.8814, F1 Macro: 0.8799\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5377, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5305, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4931, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4768, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4845, Accuracy: 0.7879, F1 Micro: 0.8814, F1 Macro: 0.8799\n",
      "Epoch 8/10, Train Loss: 0.4606, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4249, Accuracy: 0.7924, F1 Micro: 0.8834, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4039, Accuracy: 0.7969, F1 Micro: 0.8852, F1 Macro: 0.8834\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7969, F1 Micro: 0.8852, F1 Macro: 0.8834\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.80      0.99      0.88       175\n",
      "      others       0.72      0.97      0.83       158\n",
      "        part       0.72      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.80      0.99      0.89      1061\n",
      "   macro avg       0.80      0.99      0.88      1061\n",
      "weighted avg       0.80      0.99      0.89      1061\n",
      " samples avg       0.80      0.99      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6588, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5725, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4665, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5629, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4718, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3767, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3798, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3906, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.4146\n",
      "Epoch 9/10, Train Loss: 0.4707, Accuracy: 0.6667, F1 Micro: 0.6667, F1 Macro: 0.4\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.326, Accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.5214\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.7083, F1 Micro: 0.7083, F1 Macro: 0.5214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.14      0.22         7\n",
      "    positive       0.73      0.94      0.82        17\n",
      "\n",
      "    accuracy                           0.71        24\n",
      "   macro avg       0.61      0.54      0.52        24\n",
      "weighted avg       0.66      0.71      0.65        24\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.794, F1 Micro: 0.794, F1 Macro: 0.3226\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.06      0.11        16\n",
      "     neutral       0.79      0.99      0.88       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.43      0.35      0.33       216\n",
      "weighted avg       0.65      0.77      0.69       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.72      0.98      0.83       152\n",
      "    positive       0.70      0.13      0.23        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.47      0.37      0.35       216\n",
      "weighted avg       0.68      0.72      0.64       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.72      0.99      0.83       152\n",
      "    positive       0.50      0.10      0.16        41\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.41      0.36      0.33       216\n",
      "weighted avg       0.60      0.71      0.62       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 62.529061794281006 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7958, F1 Micro: 0.7958, F1 Macro: 0.3296\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 9.081574440002441 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6152, Accuracy: 0.7783, F1 Micro: 0.8742, F1 Macro: 0.8705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5095, Accuracy: 0.7879, F1 Micro: 0.88, F1 Macro: 0.8775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5069, Accuracy: 0.7917, F1 Micro: 0.8831, F1 Macro: 0.8815\n",
      "Epoch 4/10, Train Loss: 0.4873, Accuracy: 0.7932, F1 Micro: 0.8828, F1 Macro: 0.8805\n",
      "Epoch 5/10, Train Loss: 0.4578, Accuracy: 0.779, F1 Micro: 0.8717, F1 Macro: 0.8639\n",
      "Epoch 6/10, Train Loss: 0.425, Accuracy: 0.7887, F1 Micro: 0.8763, F1 Macro: 0.8686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3856, Accuracy: 0.8088, F1 Micro: 0.888, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3389, Accuracy: 0.8229, F1 Micro: 0.8951, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3119, Accuracy: 0.8527, F1 Micro: 0.9114, F1 Macro: 0.9073\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2649, Accuracy: 0.8743, F1 Micro: 0.9224, F1 Macro: 0.9165\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.8743, F1 Micro: 0.9224, F1 Macro: 0.9165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.89      1.00      0.94       187\n",
      "     machine       0.88      0.98      0.93       175\n",
      "      others       0.92      0.74      0.82       158\n",
      "        part       0.84      0.92      0.88       158\n",
      "       price       0.94      1.00      0.97       192\n",
      "     service       0.93      1.00      0.96       191\n",
      "\n",
      "   micro avg       0.90      0.95      0.92      1061\n",
      "   macro avg       0.90      0.94      0.92      1061\n",
      "weighted avg       0.90      0.95      0.92      1061\n",
      " samples avg       0.91      0.95      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.621, Accuracy: 0.7512, F1 Micro: 0.7512, F1 Macro: 0.4289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5491, Accuracy: 0.7512, F1 Micro: 0.7512, F1 Macro: 0.4289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.458, Accuracy: 0.7512, F1 Micro: 0.7512, F1 Macro: 0.4289\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3391, Accuracy: 0.788, F1 Micro: 0.788, F1 Macro: 0.589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.317, Accuracy: 0.8249, F1 Micro: 0.8249, F1 Macro: 0.7789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2793, Accuracy: 0.8433, F1 Micro: 0.8433, F1 Macro: 0.8042\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2099, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.8077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1527, Accuracy: 0.871, F1 Micro: 0.871, F1 Macro: 0.8419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1082, Accuracy: 0.9032, F1 Micro: 0.9032, F1 Macro: 0.8714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9078, F1 Micro: 0.9078, F1 Macro: 0.8782\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9078, F1 Micro: 0.9078, F1 Macro: 0.8782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.83      0.82        54\n",
      "    positive       0.94      0.93      0.94       163\n",
      "\n",
      "    accuracy                           0.91       217\n",
      "   macro avg       0.87      0.88      0.88       217\n",
      "weighted avg       0.91      0.91      0.91       217\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8657, F1 Micro: 0.8657, F1 Macro: 0.7155\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.36      0.53        11\n",
      "     neutral       0.89      1.00      0.94       181\n",
      "    positive       1.00      0.38      0.55        24\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.96      0.58      0.67       216\n",
      "weighted avg       0.91      0.90      0.88       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        16\n",
      "     neutral       0.88      0.98      0.93       167\n",
      "    positive       0.71      0.45      0.56        33\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.86      0.65      0.72       216\n",
      "weighted avg       0.86      0.87      0.85       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.75      0.60        12\n",
      "     neutral       0.92      0.74      0.82       152\n",
      "    positive       0.54      0.79      0.64        52\n",
      "\n",
      "    accuracy                           0.75       216\n",
      "   macro avg       0.65      0.76      0.69       216\n",
      "weighted avg       0.80      0.75      0.76       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.43      0.59        23\n",
      "     neutral       0.85      0.93      0.89       152\n",
      "    positive       0.68      0.61      0.64        41\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.81      0.66      0.71       216\n",
      "weighted avg       0.82      0.82      0.81       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.38      0.53        13\n",
      "     neutral       0.93      1.00      0.97       186\n",
      "    positive       0.91      0.59      0.71        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.66      0.74       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        14\n",
      "     neutral       0.93      1.00      0.96       185\n",
      "    positive       1.00      0.41      0.58        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.97      0.68      0.78       216\n",
      "weighted avg       0.94      0.93      0.92       216\n",
      "\n",
      "Total train time: 83.81025862693787 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6383, Accuracy: 0.7894, F1 Micro: 0.881, F1 Macro: 0.8788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.514, Accuracy: 0.7894, F1 Micro: 0.8819, F1 Macro: 0.8802\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4982, Accuracy: 0.7939, F1 Micro: 0.8841, F1 Macro: 0.8823\n",
      "Epoch 4/10, Train Loss: 0.4747, Accuracy: 0.7946, F1 Micro: 0.8833, F1 Macro: 0.881\n",
      "Epoch 5/10, Train Loss: 0.4457, Accuracy: 0.7976, F1 Micro: 0.8815, F1 Macro: 0.8754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.41, Accuracy: 0.8192, F1 Micro: 0.893, F1 Macro: 0.888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3522, Accuracy: 0.8318, F1 Micro: 0.9, F1 Macro: 0.8954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3186, Accuracy: 0.8542, F1 Micro: 0.9119, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.289, Accuracy: 0.8735, F1 Micro: 0.9224, F1 Macro: 0.9173\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2446, Accuracy: 0.8914, F1 Micro: 0.9322, F1 Macro: 0.9258\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8914, F1 Micro: 0.9322, F1 Macro: 0.9258\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.87      1.00      0.93       187\n",
      "     machine       0.91      0.95      0.93       175\n",
      "      others       0.94      0.70      0.80       158\n",
      "        part       0.87      0.99      0.93       158\n",
      "       price       0.95      1.00      0.97       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.92      0.95      0.93      1061\n",
      "   macro avg       0.92      0.94      0.93      1061\n",
      "weighted avg       0.92      0.95      0.93      1061\n",
      " samples avg       0.92      0.95      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5869, Accuracy: 0.7344, F1 Micro: 0.7344, F1 Macro: 0.4234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5804, Accuracy: 0.7344, F1 Micro: 0.7344, F1 Macro: 0.4234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5324, Accuracy: 0.7344, F1 Micro: 0.7344, F1 Macro: 0.4234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3919, Accuracy: 0.7676, F1 Micro: 0.7676, F1 Macro: 0.553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3335, Accuracy: 0.8299, F1 Micro: 0.8299, F1 Macro: 0.7762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2589, Accuracy: 0.8299, F1 Micro: 0.8299, F1 Macro: 0.7891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.21, Accuracy: 0.8506, F1 Micro: 0.8506, F1 Macro: 0.8203\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1694, Accuracy: 0.8631, F1 Micro: 0.8631, F1 Macro: 0.8254\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1291, Accuracy: 0.8672, F1 Micro: 0.8672, F1 Macro: 0.8361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1044, Accuracy: 0.8797, F1 Micro: 0.8797, F1 Macro: 0.8534\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8797, F1 Micro: 0.8797, F1 Macro: 0.8534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.86      0.79        64\n",
      "    positive       0.95      0.89      0.92       177\n",
      "\n",
      "    accuracy                           0.88       241\n",
      "   macro avg       0.84      0.87      0.85       241\n",
      "weighted avg       0.89      0.88      0.88       241\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8812, F1 Micro: 0.8812, F1 Macro: 0.7429\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.09      0.17        11\n",
      "     neutral       0.88      1.00      0.94       181\n",
      "    positive       0.89      0.33      0.48        24\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.92      0.47      0.53       216\n",
      "weighted avg       0.89      0.88      0.85       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        16\n",
      "     neutral       0.91      0.96      0.93       167\n",
      "    positive       0.79      0.58      0.67        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.76      0.78       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.75      0.53        12\n",
      "     neutral       0.94      0.70      0.80       152\n",
      "    positive       0.55      0.85      0.67        52\n",
      "\n",
      "    accuracy                           0.74       216\n",
      "   macro avg       0.63      0.77      0.67       216\n",
      "weighted avg       0.82      0.74      0.76       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.61      0.74        23\n",
      "     neutral       0.87      0.99      0.93       152\n",
      "    positive       0.83      0.59      0.69        41\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.88      0.73      0.78       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.54      0.64        13\n",
      "     neutral       0.94      1.00      0.97       186\n",
      "    positive       0.90      0.53      0.67        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.69      0.76       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 83.07840514183044 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6253, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.517, Accuracy: 0.7879, F1 Micro: 0.8814, F1 Macro: 0.8798\n",
      "Epoch 3/10, Train Loss: 0.5024, Accuracy: 0.7879, F1 Micro: 0.8794, F1 Macro: 0.8763\n",
      "Epoch 4/10, Train Loss: 0.4838, Accuracy: 0.7909, F1 Micro: 0.881, F1 Macro: 0.8781\n",
      "Epoch 5/10, Train Loss: 0.457, Accuracy: 0.7924, F1 Micro: 0.8811, F1 Macro: 0.8776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4301, Accuracy: 0.8013, F1 Micro: 0.8851, F1 Macro: 0.8817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3833, Accuracy: 0.8147, F1 Micro: 0.8919, F1 Macro: 0.8889\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3503, Accuracy: 0.8333, F1 Micro: 0.9007, F1 Macro: 0.8972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3145, Accuracy: 0.8557, F1 Micro: 0.913, F1 Macro: 0.9098\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2717, Accuracy: 0.8795, F1 Micro: 0.9259, F1 Macro: 0.9225\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.8795, F1 Micro: 0.9259, F1 Macro: 0.9225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.87      0.99      0.93       187\n",
      "     machine       0.90      0.98      0.94       175\n",
      "      others       0.92      0.80      0.85       158\n",
      "        part       0.83      0.97      0.89       158\n",
      "       price       0.90      0.97      0.94       192\n",
      "     service       0.97      0.99      0.98       191\n",
      "\n",
      "   micro avg       0.90      0.95      0.93      1061\n",
      "   macro avg       0.90      0.95      0.92      1061\n",
      "weighted avg       0.90      0.95      0.93      1061\n",
      " samples avg       0.90      0.95      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5304, Accuracy: 0.7429, F1 Micro: 0.7429, F1 Macro: 0.4262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5115, Accuracy: 0.7762, F1 Micro: 0.7762, F1 Macro: 0.5723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4303, Accuracy: 0.8048, F1 Micro: 0.8048, F1 Macro: 0.7291\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.303, Accuracy: 0.8333, F1 Micro: 0.8333, F1 Macro: 0.7805\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2605, Accuracy: 0.8476, F1 Micro: 0.8476, F1 Macro: 0.8094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1481, Accuracy: 0.8714, F1 Micro: 0.8714, F1 Macro: 0.8286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2258, Accuracy: 0.8952, F1 Micro: 0.8952, F1 Macro: 0.8661\n",
      "Epoch 8/10, Train Loss: 0.1035, Accuracy: 0.8905, F1 Micro: 0.8905, F1 Macro: 0.8623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0811, Accuracy: 0.9095, F1 Micro: 0.9095, F1 Macro: 0.8837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0747, Accuracy: 0.9143, F1 Micro: 0.9143, F1 Macro: 0.8939\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9143, F1 Micro: 0.9143, F1 Macro: 0.8939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.93      0.85        54\n",
      "    positive       0.97      0.91      0.94       156\n",
      "\n",
      "    accuracy                           0.91       210\n",
      "   macro avg       0.88      0.92      0.89       210\n",
      "weighted avg       0.92      0.91      0.92       210\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8719, F1 Micro: 0.8719, F1 Macro: 0.7032\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.18      0.31        11\n",
      "     neutral       0.88      0.99      0.93       181\n",
      "    positive       0.89      0.33      0.48        24\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.92      0.50      0.58       216\n",
      "weighted avg       0.89      0.88      0.85       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.90      0.98      0.94       167\n",
      "    positive       0.89      0.48      0.63        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.76      0.80       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.92      0.80      0.85       152\n",
      "    positive       0.59      0.79      0.68        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.70      0.78      0.73       216\n",
      "weighted avg       0.82      0.79      0.80       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.57      0.68        23\n",
      "     neutral       0.84      0.97      0.90       152\n",
      "    positive       0.79      0.46      0.58        41\n",
      "\n",
      "    accuracy                           0.83       216\n",
      "   macro avg       0.83      0.67      0.72       216\n",
      "weighted avg       0.83      0.83      0.82       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.25      0.08      0.12        13\n",
      "     neutral       0.90      0.97      0.94       186\n",
      "    positive       0.55      0.35      0.43        17\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.57      0.47      0.49       216\n",
      "weighted avg       0.83      0.87      0.85       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.97      0.99      0.98       185\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 77.44872212409973 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8344, F1 Micro: 0.8344, F1 Macro: 0.5251\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 15.186063051223755 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5843, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.5099, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4802, Accuracy: 0.7946, F1 Micro: 0.8847, F1 Macro: 0.8831\n",
      "Epoch 4/10, Train Loss: 0.4557, Accuracy: 0.7939, F1 Micro: 0.8823, F1 Macro: 0.8793\n",
      "Epoch 5/10, Train Loss: 0.4163, Accuracy: 0.7999, F1 Micro: 0.8836, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3749, Accuracy: 0.8445, F1 Micro: 0.9065, F1 Macro: 0.9032\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.298, Accuracy: 0.8847, F1 Micro: 0.9293, F1 Macro: 0.9256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2591, Accuracy: 0.901, F1 Micro: 0.9389, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2058, Accuracy: 0.9092, F1 Micro: 0.943, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1783, Accuracy: 0.9211, F1 Micro: 0.9511, F1 Macro: 0.9485\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9211, F1 Micro: 0.9511, F1 Macro: 0.9485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.98       187\n",
      "     machine       0.90      0.97      0.93       175\n",
      "      others       0.88      0.88      0.88       158\n",
      "        part       0.92      0.97      0.94       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.95      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.93      0.97      0.95      1061\n",
      "   macro avg       0.93      0.97      0.95      1061\n",
      "weighted avg       0.93      0.97      0.95      1061\n",
      " samples avg       0.93      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6447, Accuracy: 0.6913, F1 Micro: 0.6913, F1 Macro: 0.4087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5287, Accuracy: 0.7348, F1 Micro: 0.7348, F1 Macro: 0.5516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3767, Accuracy: 0.8913, F1 Micro: 0.8913, F1 Macro: 0.875\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2552, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9011\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.116, Accuracy: 0.9217, F1 Micro: 0.9217, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1411, Accuracy: 0.9348, F1 Micro: 0.9348, F1 Macro: 0.9245\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1031, Accuracy: 0.9391, F1 Micro: 0.9391, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1265, Accuracy: 0.9391, F1 Micro: 0.9391, F1 Macro: 0.9303\n",
      "Epoch 9/10, Train Loss: 0.1272, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.8981\n",
      "Epoch 10/10, Train Loss: 0.0888, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9197\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9391, F1 Micro: 0.9391, F1 Macro: 0.9303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        71\n",
      "    positive       0.97      0.94      0.96       159\n",
      "\n",
      "    accuracy                           0.94       230\n",
      "   macro avg       0.92      0.94      0.93       230\n",
      "weighted avg       0.94      0.94      0.94       230\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9144, F1 Micro: 0.9144, F1 Macro: 0.8301\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.97      1.00      0.99       181\n",
      "    positive       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.90      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.90      0.97      0.93       167\n",
      "    positive       0.77      0.52      0.62        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.75      0.78       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.67      0.67        12\n",
      "     neutral       0.88      0.88      0.88       152\n",
      "    positive       0.70      0.71      0.70        52\n",
      "\n",
      "    accuracy                           0.82       216\n",
      "   macro avg       0.75      0.75      0.75       216\n",
      "weighted avg       0.82      0.82      0.82       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.85      0.71      0.77        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       0.75      0.53      0.62        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.87      0.74      0.79       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.71      0.80        14\n",
      "     neutral       0.95      1.00      0.98       185\n",
      "    positive       1.00      0.65      0.79        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.79      0.85       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Total train time: 88.45600128173828 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5995, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5018, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.468, Accuracy: 0.7976, F1 Micro: 0.886, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4334, Accuracy: 0.8251, F1 Micro: 0.8988, F1 Macro: 0.8969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3845, Accuracy: 0.8579, F1 Micro: 0.9156, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.334, Accuracy: 0.8981, F1 Micro: 0.9379, F1 Macro: 0.9359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2586, Accuracy: 0.9234, F1 Micro: 0.9524, F1 Macro: 0.9499\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2244, Accuracy: 0.9308, F1 Micro: 0.9565, F1 Macro: 0.9533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1798, Accuracy: 0.933, F1 Micro: 0.958, F1 Macro: 0.9548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1537, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9631\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.9631\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.92      0.89      0.90       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6027, Accuracy: 0.6892, F1 Micro: 0.6892, F1 Macro: 0.408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4975, Accuracy: 0.7211, F1 Micro: 0.7211, F1 Macro: 0.5089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3544, Accuracy: 0.8805, F1 Micro: 0.8805, F1 Macro: 0.8642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1952, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1638, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9105\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1069, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0796, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9228\n",
      "Epoch 8/10, Train Loss: 0.0616, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9107\n",
      "Epoch 9/10, Train Loss: 0.0285, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.895\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9005\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9228\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        78\n",
      "    positive       0.97      0.93      0.95       173\n",
      "\n",
      "    accuracy                           0.93       251\n",
      "   macro avg       0.91      0.93      0.92       251\n",
      "weighted avg       0.94      0.93      0.93       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9367, F1 Micro: 0.9367, F1 Macro: 0.8787\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.88      0.64      0.74        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.81      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.89      0.90       152\n",
      "    positive       0.74      0.81      0.77        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.80      0.82      0.81       216\n",
      "weighted avg       0.87      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.83      0.84        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.91      0.71      0.79        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 92.80098271369934 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5908, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.52, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4927, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4677, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4312, Accuracy: 0.8185, F1 Micro: 0.8961, F1 Macro: 0.8946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4036, Accuracy: 0.8497, F1 Micro: 0.911, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3169, Accuracy: 0.8966, F1 Micro: 0.9372, F1 Macro: 0.9353\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2647, Accuracy: 0.9226, F1 Micro: 0.9521, F1 Macro: 0.9497\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.214, Accuracy: 0.9308, F1 Micro: 0.9569, F1 Macro: 0.954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.185, Accuracy: 0.9345, F1 Micro: 0.959, F1 Macro: 0.9568\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9345, F1 Micro: 0.959, F1 Macro: 0.9568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.93      0.94       175\n",
      "      others       0.92      0.89      0.90       158\n",
      "        part       0.91      0.99      0.95       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.97      0.96      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.95      0.97      0.96      1061\n",
      " samples avg       0.95      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6461, Accuracy: 0.7045, F1 Micro: 0.7045, F1 Macro: 0.4133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5282, Accuracy: 0.8381, F1 Micro: 0.8381, F1 Macro: 0.8085\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3526, Accuracy: 0.8947, F1 Micro: 0.8947, F1 Macro: 0.8726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2111, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.182, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9268\n",
      "Epoch 6/10, Train Loss: 0.172, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.924\n",
      "Epoch 7/10, Train Loss: 0.0624, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9201\n",
      "Epoch 8/10, Train Loss: 0.0675, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0481, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9319\n",
      "Epoch 10/10, Train Loss: 0.0839, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9119\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.90      0.90        73\n",
      "    positive       0.96      0.96      0.96       174\n",
      "\n",
      "    accuracy                           0.94       247\n",
      "   macro avg       0.93      0.93      0.93       247\n",
      "weighted avg       0.94      0.94      0.94       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9259, F1 Micro: 0.9259, F1 Macro: 0.858\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.94      0.93      0.94       167\n",
      "    positive       0.65      0.73      0.69        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.78      0.79       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.92      0.89      0.91       152\n",
      "    positive       0.69      0.79      0.74        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.83      0.78      0.80       216\n",
      "weighted avg       0.86      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86        23\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.93      0.66      0.77        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.92      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.54      0.64        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.73      0.79       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 89.59980130195618 s\n",
      "Averaged - Iteration 208: Accuracy: 0.8648, F1 Micro: 0.8648, F1 Macro: 0.6353\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 14.77528429031372 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5807, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 2/10, Train Loss: 0.5254, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4811, Accuracy: 0.7976, F1 Micro: 0.8861, F1 Macro: 0.8845\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4344, Accuracy: 0.8051, F1 Micro: 0.888, F1 Macro: 0.8855\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3795, Accuracy: 0.8519, F1 Micro: 0.9111, F1 Macro: 0.9082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3214, Accuracy: 0.8988, F1 Micro: 0.938, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2577, Accuracy: 0.9211, F1 Micro: 0.9509, F1 Macro: 0.9484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1989, Accuracy: 0.9308, F1 Micro: 0.9564, F1 Macro: 0.9539\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1629, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9646\n",
      "Epoch 10/10, Train Loss: 0.1299, Accuracy: 0.9397, F1 Micro: 0.9621, F1 Macro: 0.9591\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.91      0.94      0.92       158\n",
      "        part       0.92      0.99      0.95       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.97      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.97      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5728, Accuracy: 0.6738, F1 Micro: 0.6738, F1 Macro: 0.4026\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4773, Accuracy: 0.7382, F1 Micro: 0.7382, F1 Macro: 0.5903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3333, Accuracy: 0.8884, F1 Micro: 0.8884, F1 Macro: 0.8747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1983, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1443, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1752, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9243\n",
      "Epoch 7/10, Train Loss: 0.1612, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9121\n",
      "Epoch 8/10, Train Loss: 0.1241, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9127\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0615, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9265\n",
      "Epoch 10/10, Train Loss: 0.0529, Accuracy: 0.9227, F1 Micro: 0.9227, F1 Macro: 0.9148\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90        76\n",
      "    positive       0.95      0.96      0.95       157\n",
      "\n",
      "    accuracy                           0.94       233\n",
      "   macro avg       0.93      0.93      0.93       233\n",
      "weighted avg       0.94      0.94      0.94       233\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.8649\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.88      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.78      0.82       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.91      0.93      0.92       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.87      0.80      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.93      0.66      0.77        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.84      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.99      0.83      0.90       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 99.43373489379883 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.586, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5216, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4633, Accuracy: 0.8065, F1 Micro: 0.8896, F1 Macro: 0.8876\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4058, Accuracy: 0.8542, F1 Micro: 0.9141, F1 Macro: 0.9124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3276, Accuracy: 0.9115, F1 Micro: 0.9458, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2788, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9537\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2308, Accuracy: 0.939, F1 Micro: 0.9618, F1 Macro: 0.9598\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1769, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1498, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9672\n",
      "Epoch 10/10, Train Loss: 0.1152, Accuracy: 0.9464, F1 Micro: 0.9661, F1 Macro: 0.9635\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.98      0.96       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.91      0.99      0.95       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6008, Accuracy: 0.6652, F1 Micro: 0.6652, F1 Macro: 0.3995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4976, Accuracy: 0.8652, F1 Micro: 0.8652, F1 Macro: 0.8361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3015, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1921, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9465\n",
      "Epoch 5/10, Train Loss: 0.1554, Accuracy: 0.9217, F1 Micro: 0.9217, F1 Macro: 0.9152\n",
      "Epoch 6/10, Train Loss: 0.152, Accuracy: 0.9217, F1 Micro: 0.9217, F1 Macro: 0.9121\n",
      "Epoch 7/10, Train Loss: 0.1125, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9102\n",
      "Epoch 8/10, Train Loss: 0.0715, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9229\n",
      "Epoch 9/10, Train Loss: 0.0511, Accuracy: 0.9304, F1 Micro: 0.9304, F1 Macro: 0.9238\n",
      "Epoch 10/10, Train Loss: 0.0397, Accuracy: 0.9391, F1 Micro: 0.9391, F1 Macro: 0.9329\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        77\n",
      "    positive       0.97      0.96      0.96       153\n",
      "\n",
      "    accuracy                           0.95       230\n",
      "   macro avg       0.95      0.95      0.95       230\n",
      "weighted avg       0.95      0.95      0.95       230\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9437, F1 Micro: 0.9437, F1 Macro: 0.887\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.96      0.67      0.79        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.80      0.84       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.92      0.75      0.80       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.96      0.63      0.76        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.93      0.85      0.88       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 94.5052604675293 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5877, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 2/10, Train Loss: 0.5247, Accuracy: 0.7894, F1 Micro: 0.8821, F1 Macro: 0.8806\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4824, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4519, Accuracy: 0.8088, F1 Micro: 0.8913, F1 Macro: 0.8897\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3895, Accuracy: 0.8646, F1 Micro: 0.9198, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3308, Accuracy: 0.9182, F1 Micro: 0.95, F1 Macro: 0.9478\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2632, Accuracy: 0.9375, F1 Micro: 0.9612, F1 Macro: 0.9592\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2068, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9613\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1676, Accuracy: 0.942, F1 Micro: 0.964, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1312, Accuracy: 0.9435, F1 Micro: 0.9644, F1 Macro: 0.9618\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9435, F1 Micro: 0.9644, F1 Macro: 0.9618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.95      0.86      0.90       158\n",
      "        part       0.93      0.98      0.95       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.97      0.96      1061\n",
      "   macro avg       0.96      0.97      0.96      1061\n",
      "weighted avg       0.96      0.97      0.96      1061\n",
      " samples avg       0.96      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5777, Accuracy: 0.6988, F1 Micro: 0.6988, F1 Macro: 0.4349\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4855, Accuracy: 0.8726, F1 Micro: 0.8726, F1 Macro: 0.8406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2268, Accuracy: 0.8958, F1 Micro: 0.8958, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1856, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1454, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1815, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9166\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.8971\n",
      "Epoch 8/10, Train Loss: 0.0593, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.8996\n",
      "Epoch 9/10, Train Loss: 0.092, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9005\n",
      "Epoch 10/10, Train Loss: 0.0417, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8949\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.94      0.89        80\n",
      "    positive       0.97      0.92      0.95       179\n",
      "\n",
      "    accuracy                           0.93       259\n",
      "   macro avg       0.91      0.93      0.92       259\n",
      "weighted avg       0.93      0.93      0.93       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.88\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.88      0.64      0.74        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.81      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.96      0.86      0.91       152\n",
      "    positive       0.70      0.88      0.78        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.78      0.83      0.80       216\n",
      "weighted avg       0.88      0.86      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.93      0.98      0.95       152\n",
      "    positive       0.97      0.68      0.80        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.86      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.76      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.99      0.85      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 99.32218265533447 s\n",
      "Averaged - Iteration 274: Accuracy: 0.8832, F1 Micro: 0.8832, F1 Macro: 0.6958\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 13.587244749069214 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5593, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4949, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4689, Accuracy: 0.8058, F1 Micro: 0.8895, F1 Macro: 0.8877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4119, Accuracy: 0.8549, F1 Micro: 0.9128, F1 Macro: 0.9101\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3126, Accuracy: 0.904, F1 Micro: 0.9406, F1 Macro: 0.9381\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2516, Accuracy: 0.936, F1 Micro: 0.96, F1 Macro: 0.9581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1916, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9662\n",
      "Epoch 8/10, Train Loss: 0.1482, Accuracy: 0.9412, F1 Micro: 0.9628, F1 Macro: 0.9596\n",
      "Epoch 9/10, Train Loss: 0.1287, Accuracy: 0.9427, F1 Micro: 0.9637, F1 Macro: 0.9602\n",
      "Epoch 10/10, Train Loss: 0.0966, Accuracy: 0.9479, F1 Micro: 0.967, F1 Macro: 0.9637\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9487, F1 Micro: 0.9678, F1 Macro: 0.9662\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.92      0.92      0.92       158\n",
      "        part       0.93      0.98      0.96       158\n",
      "       price       0.99      0.98      0.99       192\n",
      "     service       0.96      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6069, Accuracy: 0.6892, F1 Micro: 0.6892, F1 Macro: 0.408\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4418, Accuracy: 0.8566, F1 Micro: 0.8566, F1 Macro: 0.8168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2398, Accuracy: 0.8964, F1 Micro: 0.8964, F1 Macro: 0.8764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2094, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.8892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1382, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1524, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1162, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9016\n",
      "Epoch 8/10, Train Loss: 0.1256, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.8934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0649, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9004\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0666, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.904\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.90      0.87        78\n",
      "    positive       0.95      0.92      0.94       173\n",
      "\n",
      "    accuracy                           0.92       251\n",
      "   macro avg       0.90      0.91      0.90       251\n",
      "weighted avg       0.92      0.92      0.92       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.8701\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.75      0.73        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.84      0.80      0.82       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.92      0.92      0.92       152\n",
      "    positive       0.78      0.83      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.86      0.80      0.83       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.93      0.98      0.96       152\n",
      "    positive       0.89      0.76      0.82        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.98      0.99       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.86      0.90      0.88       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.96      1.00      0.98       185\n",
      "    positive       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.99      0.83      0.90       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Total train time: 106.5982358455658 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5686, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4906, Accuracy: 0.7999, F1 Micro: 0.8868, F1 Macro: 0.8852\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4528, Accuracy: 0.8341, F1 Micro: 0.9045, F1 Macro: 0.9034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3692, Accuracy: 0.8996, F1 Micro: 0.9382, F1 Macro: 0.9355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.284, Accuracy: 0.9338, F1 Micro: 0.9588, F1 Macro: 0.9572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2272, Accuracy: 0.9464, F1 Micro: 0.9664, F1 Macro: 0.9652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1773, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9694\n",
      "Epoch 8/10, Train Loss: 0.1406, Accuracy: 0.9412, F1 Micro: 0.9628, F1 Macro: 0.9595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1216, Accuracy: 0.9539, F1 Micro: 0.9709, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0937, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9741\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.97      0.95       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.98      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5757, Accuracy: 0.6811, F1 Micro: 0.6811, F1 Macro: 0.4052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4426, Accuracy: 0.8583, F1 Micro: 0.8583, F1 Macro: 0.8193\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2239, Accuracy: 0.9055, F1 Micro: 0.9055, F1 Macro: 0.8882\n",
      "Epoch 4/10, Train Loss: 0.2079, Accuracy: 0.8898, F1 Micro: 0.8898, F1 Macro: 0.8643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1437, Accuracy: 0.9134, F1 Micro: 0.9134, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1289, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0902, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9069\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0746, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9126\n",
      "Epoch 9/10, Train Loss: 0.0389, Accuracy: 0.9094, F1 Micro: 0.9094, F1 Macro: 0.8947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9118\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9118\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.84      0.88        81\n",
      "    positive       0.93      0.97      0.95       173\n",
      "\n",
      "    accuracy                           0.93       254\n",
      "   macro avg       0.92      0.90      0.91       254\n",
      "weighted avg       0.92      0.93      0.92       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.8804\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.85      0.92      0.88        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.85      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.62      0.71        16\n",
      "     neutral       0.94      0.97      0.95       167\n",
      "    positive       0.74      0.70      0.72        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.76      0.80       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.90      0.83      0.86        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.90      0.90      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.98      0.98       186\n",
      "    positive       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.86      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 112.11086106300354 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5758, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5001, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4822, Accuracy: 0.7999, F1 Micro: 0.8873, F1 Macro: 0.8859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4245, Accuracy: 0.8646, F1 Micro: 0.92, F1 Macro: 0.9189\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3291, Accuracy: 0.9234, F1 Micro: 0.9526, F1 Macro: 0.9506\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2659, Accuracy: 0.933, F1 Micro: 0.9577, F1 Macro: 0.9552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2027, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.9642\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1608, Accuracy: 0.9479, F1 Micro: 0.967, F1 Macro: 0.9641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.13, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0979, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9678\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9678\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.97      0.95       175\n",
      "      others       0.94      0.91      0.93       158\n",
      "        part       0.93      0.99      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5943, Accuracy: 0.6836, F1 Micro: 0.6836, F1 Macro: 0.406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4429, Accuracy: 0.8594, F1 Micro: 0.8594, F1 Macro: 0.8341\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2327, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1902, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9207\n",
      "Epoch 5/10, Train Loss: 0.1211, Accuracy: 0.9219, F1 Micro: 0.9219, F1 Macro: 0.9124\n",
      "Epoch 6/10, Train Loss: 0.1562, Accuracy: 0.9141, F1 Micro: 0.9141, F1 Macro: 0.9058\n",
      "Epoch 7/10, Train Loss: 0.1085, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8862\n",
      "Epoch 8/10, Train Loss: 0.094, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.8835\n",
      "Epoch 9/10, Train Loss: 0.0843, Accuracy: 0.8984, F1 Micro: 0.8984, F1 Macro: 0.8874\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9023, F1 Micro: 0.9023, F1 Macro: 0.8875\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9207\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.94      0.89        81\n",
      "    positive       0.97      0.93      0.95       175\n",
      "\n",
      "    accuracy                           0.93       256\n",
      "   macro avg       0.91      0.93      0.92       256\n",
      "weighted avg       0.93      0.93      0.93       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.9429, F1 Micro: 0.9429, F1 Macro: 0.8937\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.97      0.96       167\n",
      "    positive       0.80      0.73      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.83      0.71        12\n",
      "     neutral       0.95      0.91      0.93       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.85      0.81       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.90      0.68      0.78        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.99      0.91      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 105.72061610221863 s\n",
      "Averaged - Iteration 333: Accuracy: 0.8952, F1 Micro: 0.8952, F1 Macro: 0.7329\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 12.53325867652893 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5632, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4802, Accuracy: 0.7961, F1 Micro: 0.8855, F1 Macro: 0.884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4407, Accuracy: 0.8185, F1 Micro: 0.8948, F1 Macro: 0.8924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.357, Accuracy: 0.8914, F1 Micro: 0.9337, F1 Macro: 0.9311\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2691, Accuracy: 0.936, F1 Micro: 0.9601, F1 Macro: 0.9583\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.202, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.97\n",
      "Epoch 7/10, Train Loss: 0.1514, Accuracy: 0.9479, F1 Micro: 0.9672, F1 Macro: 0.9651\n",
      "Epoch 8/10, Train Loss: 0.1226, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0968, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Epoch 10/10, Train Loss: 0.0803, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.89      0.97      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5454, Accuracy: 0.6653, F1 Micro: 0.6653, F1 Macro: 0.3995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3401, Accuracy: 0.8939, F1 Micro: 0.8939, F1 Macro: 0.8847\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2046, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9323\n",
      "Epoch 4/10, Train Loss: 0.1505, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9241\n",
      "Epoch 5/10, Train Loss: 0.1439, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9241\n",
      "Epoch 6/10, Train Loss: 0.1382, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9271\n",
      "Epoch 7/10, Train Loss: 0.1387, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9287\n",
      "Epoch 8/10, Train Loss: 0.091, Accuracy: 0.9306, F1 Micro: 0.9306, F1 Macro: 0.9236\n",
      "Epoch 9/10, Train Loss: 0.097, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9137\n",
      "Epoch 10/10, Train Loss: 0.0666, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9287\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.91        82\n",
      "    positive       0.97      0.94      0.95       163\n",
      "\n",
      "    accuracy                           0.94       245\n",
      "   macro avg       0.93      0.94      0.93       245\n",
      "weighted avg       0.94      0.94      0.94       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9047\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.84      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.67      0.73        12\n",
      "     neutral       0.89      0.97      0.93       152\n",
      "    positive       0.90      0.67      0.77        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.86      0.77      0.81       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 109.100750207901 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5682, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4715, Accuracy: 0.808, F1 Micro: 0.8914, F1 Macro: 0.89\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3968, Accuracy: 0.8817, F1 Micro: 0.9295, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3082, Accuracy: 0.9382, F1 Micro: 0.9617, F1 Macro: 0.9603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2332, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9689\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1742, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9733\n",
      "Epoch 7/10, Train Loss: 0.1349, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9726\n",
      "Epoch 8/10, Train Loss: 0.1115, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.094, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Epoch 10/10, Train Loss: 0.0829, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.88      0.98      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5184, Accuracy: 0.6787, F1 Micro: 0.6787, F1 Macro: 0.4043\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4323, Accuracy: 0.8916, F1 Micro: 0.8916, F1 Macro: 0.8697\n",
      "Epoch 3/10, Train Loss: 0.261, Accuracy: 0.8795, F1 Micro: 0.8795, F1 Macro: 0.8713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1675, Accuracy: 0.9357, F1 Micro: 0.9357, F1 Macro: 0.9286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1723, Accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9498\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9351\n",
      "Epoch 7/10, Train Loss: 0.1305, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9367\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.9371\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9317, F1 Micro: 0.9317, F1 Macro: 0.9209\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.932\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9558, F1 Micro: 0.9558, F1 Macro: 0.9498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        80\n",
      "    positive       0.98      0.96      0.97       169\n",
      "\n",
      "    accuracy                           0.96       249\n",
      "   macro avg       0.95      0.95      0.95       249\n",
      "weighted avg       0.96      0.96      0.96       249\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9142\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.88      0.87        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.67      0.80        12\n",
      "     neutral       0.88      0.98      0.93       152\n",
      "    positive       0.92      0.69      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.93      0.78      0.84       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 108.47636294364929 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5688, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4874, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4535, Accuracy: 0.814, F1 Micro: 0.8938, F1 Macro: 0.8922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3813, Accuracy: 0.9055, F1 Micro: 0.9428, F1 Macro: 0.9413\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2834, Accuracy: 0.939, F1 Micro: 0.9622, F1 Macro: 0.9607\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2046, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.157, Accuracy: 0.9501, F1 Micro: 0.9686, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1274, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0998, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0831, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5634, Accuracy: 0.6759, F1 Micro: 0.6759, F1 Macro: 0.4033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3953, Accuracy: 0.8775, F1 Micro: 0.8775, F1 Macro: 0.8548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2952, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1508, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9245\n",
      "Epoch 5/10, Train Loss: 0.1536, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9212\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9138\n",
      "Epoch 7/10, Train Loss: 0.1297, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9198\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9177\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0678, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.924\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.90        82\n",
      "    positive       0.96      0.94      0.95       171\n",
      "\n",
      "    accuracy                           0.93       253\n",
      "   macro avg       0.92      0.93      0.92       253\n",
      "weighted avg       0.93      0.93      0.93       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.8967\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.83      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.58      0.74        12\n",
      "     neutral       0.90      0.96      0.93       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.92      0.77      0.82       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 116.23687815666199 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.7616\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 11.702397108078003 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5516, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4938, Accuracy: 0.7939, F1 Micro: 0.8843, F1 Macro: 0.8828\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4366, Accuracy: 0.817, F1 Micro: 0.8953, F1 Macro: 0.8936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3519, Accuracy: 0.9062, F1 Micro: 0.9425, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2607, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9627\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1921, Accuracy: 0.9501, F1 Micro: 0.9688, F1 Macro: 0.967\n",
      "Epoch 7/10, Train Loss: 0.1441, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9668\n",
      "Epoch 8/10, Train Loss: 0.1135, Accuracy: 0.9494, F1 Micro: 0.9681, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0961, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "Epoch 10/10, Train Loss: 0.0788, Accuracy: 0.9516, F1 Micro: 0.9694, F1 Macro: 0.9671\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.91      0.92      0.92       158\n",
      "        part       0.96      0.97      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5409, Accuracy: 0.6834, F1 Micro: 0.6834, F1 Macro: 0.406\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3644, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2301, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.154, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1592, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.142, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9465\n",
      "Epoch 7/10, Train Loss: 0.1037, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9253\n",
      "Epoch 8/10, Train Loss: 0.1073, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.9023\n",
      "Epoch 9/10, Train Loss: 0.0906, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9189\n",
      "Epoch 10/10, Train Loss: 0.0955, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9212\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        82\n",
      "    positive       0.97      0.97      0.97       177\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.95      0.95      0.95       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9055\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.92      0.92       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.84      0.82      0.83       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        23\n",
      "     neutral       0.96      0.97      0.97       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 118.70220136642456 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5639, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4808, Accuracy: 0.7999, F1 Micro: 0.8874, F1 Macro: 0.8859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3938, Accuracy: 0.8787, F1 Micro: 0.9278, F1 Macro: 0.9266\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2986, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2221, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1667, Accuracy: 0.9583, F1 Micro: 0.9741, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1296, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9741\n",
      "Epoch 8/10, Train Loss: 0.1055, Accuracy: 0.9561, F1 Micro: 0.9723, F1 Macro: 0.9707\n",
      "Epoch 9/10, Train Loss: 0.0951, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9722\n",
      "Epoch 10/10, Train Loss: 0.0754, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9708\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.9741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.88      0.99      0.93       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5997, Accuracy: 0.6582, F1 Micro: 0.6582, F1 Macro: 0.3969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4185, Accuracy: 0.8776, F1 Micro: 0.8776, F1 Macro: 0.8559\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2862, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9351\n",
      "Epoch 4/10, Train Loss: 0.1713, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9143\n",
      "Epoch 5/10, Train Loss: 0.1503, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9226\n",
      "Epoch 6/10, Train Loss: 0.139, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.103, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1056, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9351\n",
      "Epoch 9/10, Train Loss: 0.0884, Accuracy: 0.9367, F1 Micro: 0.9367, F1 Macro: 0.9314\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.921\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.94      0.92        81\n",
      "    positive       0.97      0.94      0.95       156\n",
      "\n",
      "    accuracy                           0.94       237\n",
      "   macro avg       0.93      0.94      0.94       237\n",
      "weighted avg       0.94      0.94      0.94       237\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9005\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.67      0.76        12\n",
      "     neutral       0.88      0.99      0.93       152\n",
      "    positive       0.97      0.65      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.91      0.77      0.83       216\n",
      "weighted avg       0.90      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 114.14183497428894 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5526, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4974, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4386, Accuracy: 0.8274, F1 Micro: 0.9011, F1 Macro: 0.8999\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.349, Accuracy: 0.9271, F1 Micro: 0.955, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2576, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1891, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9708\n",
      "Epoch 7/10, Train Loss: 0.1443, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.97\n",
      "Epoch 8/10, Train Loss: 0.1161, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9677\n",
      "Epoch 9/10, Train Loss: 0.0948, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9675\n",
      "Epoch 10/10, Train Loss: 0.0773, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9675\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.91      0.98      0.95       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.95      1.00      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5876, Accuracy: 0.6623, F1 Micro: 0.6623, F1 Macro: 0.3984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.373, Accuracy: 0.9307, F1 Micro: 0.9307, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1782, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9342\n",
      "Epoch 4/10, Train Loss: 0.1092, Accuracy: 0.9177, F1 Micro: 0.9177, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9525\n",
      "Epoch 6/10, Train Loss: 0.0793, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9335\n",
      "Epoch 7/10, Train Loss: 0.0989, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9252\n",
      "Epoch 8/10, Train Loss: 0.0989, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9207\n",
      "Epoch 9/10, Train Loss: 0.1006, Accuracy: 0.9437, F1 Micro: 0.9437, F1 Macro: 0.9373\n",
      "Epoch 10/10, Train Loss: 0.0755, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9289\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9567, F1 Micro: 0.9567, F1 Macro: 0.9525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.94        78\n",
      "    positive       0.99      0.95      0.97       153\n",
      "\n",
      "    accuracy                           0.96       231\n",
      "   macro avg       0.95      0.96      0.95       231\n",
      "weighted avg       0.96      0.96      0.96       231\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9033\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.91      0.99      0.95       167\n",
      "    positive       0.90      0.58      0.70        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.77      0.82       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.93      0.73      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.91      0.82      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.94      1.00      0.97       152\n",
      "    positive       1.00      0.76      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 110.56668186187744 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9113, F1 Micro: 0.9113, F1 Macro: 0.7818\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 10.170047521591187 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5372, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4968, Accuracy: 0.8021, F1 Micro: 0.8878, F1 Macro: 0.8859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4125, Accuracy: 0.8653, F1 Micro: 0.919, F1 Macro: 0.9168\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3112, Accuracy: 0.9189, F1 Micro: 0.9496, F1 Macro: 0.9465\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2356, Accuracy: 0.9464, F1 Micro: 0.9662, F1 Macro: 0.9646\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1701, Accuracy: 0.9472, F1 Micro: 0.9669, F1 Macro: 0.9649\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1336, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1052, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5495, Accuracy: 0.6863, F1 Micro: 0.6863, F1 Macro: 0.4507\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3419, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9431\n",
      "Epoch 3/10, Train Loss: 0.1979, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9186\n",
      "Epoch 4/10, Train Loss: 0.1427, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9393\n",
      "Epoch 5/10, Train Loss: 0.1071, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9268\n",
      "Epoch 6/10, Train Loss: 0.1535, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Epoch 7/10, Train Loss: 0.0784, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1209, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9431\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Epoch 10/10, Train Loss: 0.0455, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9227\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       172\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9107\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.85      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.86      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 124.07119846343994 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5394, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4811, Accuracy: 0.8371, F1 Micro: 0.9056, F1 Macro: 0.9052\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3698, Accuracy: 0.9167, F1 Micro: 0.9494, F1 Macro: 0.9482\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2675, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9639\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2053, Accuracy: 0.9516, F1 Micro: 0.9695, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1548, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Epoch 7/10, Train Loss: 0.1251, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0992, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9763\n",
      "Epoch 9/10, Train Loss: 0.0816, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9759\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5764, Accuracy: 0.696, F1 Micro: 0.696, F1 Macro: 0.4759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3532, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1589, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9465\n",
      "Epoch 4/10, Train Loss: 0.1386, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.8996\n",
      "Epoch 5/10, Train Loss: 0.1543, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9337\n",
      "Epoch 6/10, Train Loss: 0.1364, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9291\n",
      "Epoch 7/10, Train Loss: 0.0957, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9269\n",
      "Epoch 8/10, Train Loss: 0.12, Accuracy: 0.912, F1 Micro: 0.912, F1 Macro: 0.8975\n",
      "Epoch 9/10, Train Loss: 0.1177, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9193\n",
      "Epoch 10/10, Train Loss: 0.0727, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9207\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        81\n",
      "    positive       0.99      0.94      0.96       169\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.94      0.96      0.95       250\n",
      "weighted avg       0.96      0.95      0.95       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9155\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.93      0.73      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.84      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 117.66264462471008 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5488, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5027, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4255, Accuracy: 0.8549, F1 Micro: 0.9151, F1 Macro: 0.9143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3217, Accuracy: 0.9308, F1 Micro: 0.9575, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2363, Accuracy: 0.9516, F1 Micro: 0.9698, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1739, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9708\n",
      "Epoch 7/10, Train Loss: 0.1369, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9694\n",
      "Epoch 8/10, Train Loss: 0.1131, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0876, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "Epoch 10/10, Train Loss: 0.075, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9722\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.97      0.95       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.574, Accuracy: 0.8745, F1 Micro: 0.8745, F1 Macro: 0.851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2776, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.173, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Epoch 4/10, Train Loss: 0.1829, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1146, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Epoch 6/10, Train Loss: 0.13, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9294\n",
      "Epoch 7/10, Train Loss: 0.127, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9369\n",
      "Epoch 8/10, Train Loss: 0.1183, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.9294\n",
      "Epoch 9/10, Train Loss: 0.0803, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.924\n",
      "Epoch 10/10, Train Loss: 0.0734, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.929\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        82\n",
      "    positive       0.97      0.95      0.96       165\n",
      "\n",
      "    accuracy                           0.95       247\n",
      "   macro avg       0.94      0.95      0.94       247\n",
      "weighted avg       0.95      0.95      0.95       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9076\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.94      0.97      0.95       167\n",
      "    positive       0.85      0.70      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.83      0.84       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.83      0.91        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.91      0.78      0.84        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.87      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 118.83396077156067 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.798\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 9.630576372146606 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5502, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4837, Accuracy: 0.8013, F1 Micro: 0.888, F1 Macro: 0.8863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.386, Accuracy: 0.8988, F1 Micro: 0.9388, F1 Macro: 0.937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2715, Accuracy: 0.9427, F1 Micro: 0.9642, F1 Macro: 0.962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2016, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1453, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1131, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9728\n",
      "Epoch 8/10, Train Loss: 0.0896, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9719\n",
      "Epoch 9/10, Train Loss: 0.0753, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0629, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5266, Accuracy: 0.712, F1 Micro: 0.712, F1 Macro: 0.5359\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3048, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9219\n",
      "Epoch 3/10, Train Loss: 0.1985, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9091\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1495, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1171, Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9295\n",
      "Epoch 6/10, Train Loss: 0.1125, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9253\n",
      "Epoch 7/10, Train Loss: 0.1211, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9151\n",
      "Epoch 8/10, Train Loss: 0.0862, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9257\n",
      "Epoch 9/10, Train Loss: 0.0956, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9219\n",
      "Epoch 10/10, Train Loss: 0.0755, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9173\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.9295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        83\n",
      "    positive       0.97      0.93      0.95       167\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.92      0.94      0.93       250\n",
      "weighted avg       0.94      0.94      0.94       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9132\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.87      0.82        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 127.76358532905579 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5575, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4678, Accuracy: 0.843, F1 Micro: 0.9093, F1 Macro: 0.9086\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3454, Accuracy: 0.9263, F1 Micro: 0.9548, F1 Macro: 0.9532\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2407, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9678\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1341, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9755\n",
      "Epoch 7/10, Train Loss: 0.1047, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9729\n",
      "Epoch 8/10, Train Loss: 0.0841, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0734, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9761\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5469, Accuracy: 0.6772, F1 Micro: 0.6772, F1 Macro: 0.4038\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3378, Accuracy: 0.9331, F1 Micro: 0.9331, F1 Macro: 0.9263\n",
      "Epoch 3/10, Train Loss: 0.2085, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1645, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9335\n",
      "Epoch 5/10, Train Loss: 0.1457, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1363, Accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "Epoch 7/10, Train Loss: 0.1216, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Epoch 8/10, Train Loss: 0.1067, Accuracy: 0.937, F1 Micro: 0.937, F1 Macro: 0.9301\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9088\n",
      "Epoch 10/10, Train Loss: 0.0873, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9116\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9409, F1 Micro: 0.9409, F1 Macro: 0.9343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.91        82\n",
      "    positive       0.98      0.93      0.96       172\n",
      "\n",
      "    accuracy                           0.94       254\n",
      "   macro avg       0.92      0.95      0.93       254\n",
      "weighted avg       0.94      0.94      0.94       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9131\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 127.67803406715393 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5529, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4952, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4023, Accuracy: 0.9048, F1 Micro: 0.9426, F1 Macro: 0.9414\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2777, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.9643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1968, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9665\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1473, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9717\n",
      "Epoch 7/10, Train Loss: 0.1108, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9695\n",
      "Epoch 8/10, Train Loss: 0.0857, Accuracy: 0.9516, F1 Micro: 0.9694, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9722\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.97      0.95       175\n",
      "      others       0.93      0.93      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5514, Accuracy: 0.6718, F1 Micro: 0.6718, F1 Macro: 0.4018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3363, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1743, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1747, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9149\n",
      "Epoch 5/10, Train Loss: 0.1429, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9068\n",
      "Epoch 6/10, Train Loss: 0.1242, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9073\n",
      "Epoch 7/10, Train Loss: 0.1114, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9077\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9189\n",
      "Epoch 9/10, Train Loss: 0.0577, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.9238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        85\n",
      "    positive       0.98      0.91      0.95       174\n",
      "\n",
      "    accuracy                           0.93       259\n",
      "   macro avg       0.91      0.94      0.92       259\n",
      "weighted avg       0.94      0.93      0.93       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9491, F1 Micro: 0.9491, F1 Macro: 0.9014\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.97      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.97      0.71      0.82        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 132.49271178245544 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9207, F1 Micro: 0.9207, F1 Macro: 0.8104\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 8.47464656829834 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5421, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4808, Accuracy: 0.8103, F1 Micro: 0.8905, F1 Macro: 0.888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3886, Accuracy: 0.9003, F1 Micro: 0.9386, F1 Macro: 0.9355\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2743, Accuracy: 0.9405, F1 Micro: 0.9631, F1 Macro: 0.9619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1988, Accuracy: 0.9501, F1 Micro: 0.9687, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1484, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1194, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 8/10, Train Loss: 0.0965, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9737\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0651, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5504, Accuracy: 0.8441, F1 Micro: 0.8441, F1 Macro: 0.8014\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2848, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9081\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.189, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1445, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.916\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1532, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "Epoch 6/10, Train Loss: 0.1208, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0981, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1016, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "Epoch 9/10, Train Loss: 0.0756, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.8984\n",
      "Epoch 10/10, Train Loss: 0.0634, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.8991\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9231\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        83\n",
      "    positive       0.98      0.92      0.95       180\n",
      "\n",
      "    accuracy                           0.93       263\n",
      "   macro avg       0.91      0.94      0.92       263\n",
      "weighted avg       0.94      0.93      0.93       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9038\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.82      0.77      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.82      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 140.07462239265442 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5436, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4559, Accuracy: 0.8586, F1 Micro: 0.9162, F1 Macro: 0.9145\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3447, Accuracy: 0.9301, F1 Micro: 0.957, F1 Macro: 0.9556\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2447, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.969\n",
      "Epoch 5/10, Train Loss: 0.186, Accuracy: 0.9501, F1 Micro: 0.9687, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1378, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1136, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9771\n",
      "Epoch 8/10, Train Loss: 0.0943, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9737\n",
      "Epoch 9/10, Train Loss: 0.0731, Accuracy: 0.9561, F1 Micro: 0.9722, F1 Macro: 0.9703\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.98      0.97      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5759, Accuracy: 0.6908, F1 Micro: 0.6908, F1 Macro: 0.4705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3295, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9162\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1976, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1491, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9404\n",
      "Epoch 5/10, Train Loss: 0.1361, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Epoch 6/10, Train Loss: 0.1143, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9175\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9275\n",
      "Epoch 8/10, Train Loss: 0.0973, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.917\n",
      "Epoch 9/10, Train Loss: 0.0658, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9275\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9275\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        84\n",
      "    positive       0.99      0.93      0.96       178\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.93      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9194\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.98      0.97      0.97       167\n",
      "    positive       0.85      0.88      0.87        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.88      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 129.78008675575256 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5471, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4874, Accuracy: 0.8058, F1 Micro: 0.8902, F1 Macro: 0.8886\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4007, Accuracy: 0.9025, F1 Micro: 0.9412, F1 Macro: 0.9397\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2809, Accuracy: 0.9427, F1 Micro: 0.9644, F1 Macro: 0.9629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2005, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.9657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1499, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1169, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0955, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9731\n",
      "Epoch 9/10, Train Loss: 0.0747, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9723\n",
      "Epoch 10/10, Train Loss: 0.0648, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.9721\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5629, Accuracy: 0.8549, F1 Micro: 0.8549, F1 Macro: 0.817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3019, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9219\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1952, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1353, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 5/10, Train Loss: 0.1344, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9174\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9182\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9178\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9128\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.918\n",
      "Epoch 10/10, Train Loss: 0.052, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9059\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.98      0.91        82\n",
      "    positive       0.99      0.92      0.95       173\n",
      "\n",
      "    accuracy                           0.94       255\n",
      "   macro avg       0.92      0.95      0.93       255\n",
      "weighted avg       0.94      0.94      0.94       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.908\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.81      0.79        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.84      0.84       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 130.45589542388916 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.8204\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 8.12069296836853 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.539, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4783, Accuracy: 0.8058, F1 Micro: 0.8899, F1 Macro: 0.8883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3766, Accuracy: 0.9196, F1 Micro: 0.9511, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2518, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9659\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1834, Accuracy: 0.9501, F1 Micro: 0.9687, F1 Macro: 0.9668\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1378, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1073, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Epoch 8/10, Train Loss: 0.0873, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9724\n",
      "Epoch 9/10, Train Loss: 0.0693, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5374, Accuracy: 0.784, F1 Micro: 0.784, F1 Macro: 0.6893\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2884, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.9333\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1628, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Epoch 4/10, Train Loss: 0.1242, Accuracy: 0.94, F1 Micro: 0.94, F1 Macro: 0.933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1093, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1289, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "Epoch 7/10, Train Loss: 0.0738, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.924\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.92, F1 Micro: 0.92, F1 Macro: 0.9093\n",
      "Epoch 9/10, Train Loss: 0.0638, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9198\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.932, F1 Micro: 0.932, F1 Macro: 0.9236\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        81\n",
      "    positive       0.98      0.93      0.96       169\n",
      "\n",
      "    accuracy                           0.94       250\n",
      "   macro avg       0.93      0.95      0.94       250\n",
      "weighted avg       0.95      0.94      0.94       250\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9065\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.88      0.71      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.81      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 137.48830246925354 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5396, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4546, Accuracy: 0.8571, F1 Micro: 0.9168, F1 Macro: 0.9161\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3331, Accuracy: 0.9412, F1 Micro: 0.9636, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2221, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1657, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1281, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9765\n",
      "Epoch 7/10, Train Loss: 0.1058, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0853, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 9/10, Train Loss: 0.0712, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.972\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5402, Accuracy: 0.8627, F1 Micro: 0.8627, F1 Macro: 0.8285\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2979, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1663, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9513\n",
      "Epoch 4/10, Train Loss: 0.1462, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 5/10, Train Loss: 0.1368, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9294\n",
      "Epoch 6/10, Train Loss: 0.143, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9302\n",
      "Epoch 7/10, Train Loss: 0.0885, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9261\n",
      "Epoch 8/10, Train Loss: 0.0941, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Epoch 9/10, Train Loss: 0.0659, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9298\n",
      "Epoch 10/10, Train Loss: 0.0491, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.934\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.97       173\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.95      0.96      0.95       255\n",
      "weighted avg       0.96      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9194\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.83      0.76      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.67      0.73        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.81      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 131.1971094608307 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5402, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4766, Accuracy: 0.8058, F1 Micro: 0.8905, F1 Macro: 0.8891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3789, Accuracy: 0.9249, F1 Micro: 0.9543, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2492, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1849, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9727\n",
      "Epoch 6/10, Train Loss: 0.1365, Accuracy: 0.9568, F1 Micro: 0.9729, F1 Macro: 0.9715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1092, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.0883, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9725\n",
      "Epoch 9/10, Train Loss: 0.0742, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9743\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5331, Accuracy: 0.8849, F1 Micro: 0.8849, F1 Macro: 0.8685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.265, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9382\n",
      "Epoch 3/10, Train Loss: 0.1828, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9071\n",
      "Epoch 4/10, Train Loss: 0.1724, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1253, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9421\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9173\n",
      "Epoch 7/10, Train Loss: 0.1128, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.895\n",
      "Epoch 8/10, Train Loss: 0.0587, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9248\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.929\n",
      "Epoch 10/10, Train Loss: 0.0833, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9099\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        82\n",
      "    positive       0.98      0.95      0.96       170\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.94      0.95      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9141\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.82      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 129.5998342037201 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.8288\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 7.435102462768555 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5481, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.47, Accuracy: 0.8088, F1 Micro: 0.8907, F1 Macro: 0.8887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3579, Accuracy: 0.9182, F1 Micro: 0.9493, F1 Macro: 0.9469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2372, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1669, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1337, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Epoch 7/10, Train Loss: 0.0987, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0856, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9767\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Epoch 10/10, Train Loss: 0.063, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.93      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.548, Accuracy: 0.7547, F1 Micro: 0.7547, F1 Macro: 0.614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2547, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9124\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1734, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1588, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9279\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1218, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9356\n",
      "Epoch 6/10, Train Loss: 0.1011, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9239\n",
      "Epoch 7/10, Train Loss: 0.0852, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9316\n",
      "Epoch 8/10, Train Loss: 0.0772, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9311\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9169\n",
      "Epoch 10/10, Train Loss: 0.0451, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9169\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.95      0.91        83\n",
      "    positive       0.98      0.94      0.96       182\n",
      "\n",
      "    accuracy                           0.94       265\n",
      "   macro avg       0.93      0.95      0.94       265\n",
      "weighted avg       0.95      0.94      0.94       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9122\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.93      0.94       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.84      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.09168982505798 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5493, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4518, Accuracy: 0.8698, F1 Micro: 0.9231, F1 Macro: 0.9224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3247, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9616\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2132, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1527, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1277, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0964, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0826, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0724, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.978\n",
      "Epoch 10/10, Train Loss: 0.0598, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5421, Accuracy: 0.8538, F1 Micro: 0.8538, F1 Macro: 0.822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2682, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Epoch 3/10, Train Loss: 0.1505, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9085\n",
      "Epoch 4/10, Train Loss: 0.1349, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1149, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0865, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0832, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "Epoch 8/10, Train Loss: 0.108, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0649, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "Epoch 10/10, Train Loss: 0.0518, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        83\n",
      "    positive       0.98      0.92      0.95       177\n",
      "\n",
      "    accuracy                           0.93       260\n",
      "   macro avg       0.92      0.94      0.93       260\n",
      "weighted avg       0.94      0.93      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9148\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.96      0.95       152\n",
      "    positive       0.90      0.85      0.87        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.88      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 144.3091757297516 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5447, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4797, Accuracy: 0.8095, F1 Micro: 0.8921, F1 Macro: 0.8905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3768, Accuracy: 0.9293, F1 Micro: 0.9563, F1 Macro: 0.9544\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2494, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1763, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1398, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1021, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0863, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9747\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "Epoch 10/10, Train Loss: 0.0623, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.97      0.96       175\n",
      "      others       0.94      0.93      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5543, Accuracy: 0.8985, F1 Micro: 0.8985, F1 Macro: 0.887\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2397, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9288\n",
      "Epoch 3/10, Train Loss: 0.1789, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9095\n",
      "Epoch 4/10, Train Loss: 0.1555, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9217\n",
      "Epoch 5/10, Train Loss: 0.1107, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9217\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Epoch 7/10, Train Loss: 0.0949, Accuracy: 0.9211, F1 Micro: 0.9211, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9453\n",
      "Epoch 9/10, Train Loss: 0.0572, Accuracy: 0.9398, F1 Micro: 0.9398, F1 Macro: 0.9332\n",
      "Epoch 10/10, Train Loss: 0.0503, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9213\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9511, F1 Micro: 0.9511, F1 Macro: 0.9453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.94      0.95      0.95       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9198\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.97      0.96       167\n",
      "    positive       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.89      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.22716093063354 s\n",
      "Averaged - Iteration 603: Accuracy: 0.9296, F1 Micro: 0.9296, F1 Macro: 0.836\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 6.804748773574829 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5361, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4569, Accuracy: 0.8251, F1 Micro: 0.8988, F1 Macro: 0.8968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3479, Accuracy: 0.9263, F1 Micro: 0.9551, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2359, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1641, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1243, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.975\n",
      "Epoch 7/10, Train Loss: 0.1017, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0659, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.975\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5569, Accuracy: 0.7103, F1 Micro: 0.7103, F1 Macro: 0.534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2837, Accuracy: 0.9127, F1 Micro: 0.9127, F1 Macro: 0.9054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1864, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1682, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.123, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1067, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Epoch 8/10, Train Loss: 0.1028, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0842, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9379\n",
      "Epoch 10/10, Train Loss: 0.0602, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        83\n",
      "    positive       0.97      0.95      0.96       169\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.94      0.94       252\n",
      "weighted avg       0.95      0.94      0.94       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9086\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.95      0.93       152\n",
      "    positive       0.81      0.75      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 153.35894465446472 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5422, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4242, Accuracy: 0.8943, F1 Micro: 0.9364, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3021, Accuracy: 0.9442, F1 Micro: 0.9654, F1 Macro: 0.9641\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.21, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1604, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1177, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.098, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 8/10, Train Loss: 0.0798, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9735\n",
      "Epoch 9/10, Train Loss: 0.0675, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9745\n",
      "Epoch 10/10, Train Loss: 0.055, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9756\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5413, Accuracy: 0.8254, F1 Micro: 0.8254, F1 Macro: 0.7742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2317, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.9013\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1654, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9431\n",
      "Epoch 4/10, Train Loss: 0.1476, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9389\n",
      "Epoch 5/10, Train Loss: 0.1099, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9347\n",
      "Epoch 6/10, Train Loss: 0.1076, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9264\n",
      "Epoch 8/10, Train Loss: 0.1094, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.9343\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9102\n",
      "Epoch 10/10, Train Loss: 0.0778, Accuracy: 0.9405, F1 Micro: 0.9405, F1 Macro: 0.934\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        83\n",
      "    positive       0.99      0.93      0.96       169\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.93      0.96      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9209\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.98      0.95       152\n",
      "    positive       0.93      0.75      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 142.89666390419006 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5404, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4644, Accuracy: 0.8266, F1 Micro: 0.9007, F1 Macro: 0.8995\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3554, Accuracy: 0.9353, F1 Micro: 0.9603, F1 Macro: 0.959\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2337, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1682, Accuracy: 0.9554, F1 Micro: 0.972, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1012, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.972\n",
      "Epoch 10/10, Train Loss: 0.0564, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9753\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6039, Accuracy: 0.8706, F1 Micro: 0.8706, F1 Macro: 0.8368\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2936, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9132\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1805, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1182, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9256\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1214, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9234\n",
      "Epoch 6/10, Train Loss: 0.1064, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9169\n",
      "Epoch 7/10, Train Loss: 0.0766, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9182\n",
      "Epoch 8/10, Train Loss: 0.0912, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9201\n",
      "Epoch 9/10, Train Loss: 0.0647, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9223\n",
      "Epoch 10/10, Train Loss: 0.0558, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9219\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.89      0.90        82\n",
      "    positive       0.95      0.95      0.95       173\n",
      "\n",
      "    accuracy                           0.93       255\n",
      "   macro avg       0.92      0.92      0.92       255\n",
      "weighted avg       0.93      0.93      0.93       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9046\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.69      0.79        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.82      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.93      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 143.97390627861023 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.8418\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 6.124122858047485 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5304, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4575, Accuracy: 0.8311, F1 Micro: 0.9028, F1 Macro: 0.9012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3181, Accuracy: 0.9375, F1 Micro: 0.9613, F1 Macro: 0.9595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2225, Accuracy: 0.9449, F1 Micro: 0.9654, F1 Macro: 0.9622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1588, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1179, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "Epoch 7/10, Train Loss: 0.0938, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Epoch 10/10, Train Loss: 0.0522, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.972\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.96      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.533, Accuracy: 0.8941, F1 Micro: 0.8941, F1 Macro: 0.8739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2747, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9186\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1539, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9324\n",
      "Epoch 4/10, Train Loss: 0.131, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9271\n",
      "Epoch 5/10, Train Loss: 0.1567, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1066, Accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9559\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9425\n",
      "Epoch 8/10, Train Loss: 0.1034, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9519\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "Epoch 10/10, Train Loss: 0.0543, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9382\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        83\n",
      "    positive       0.98      0.96      0.97       172\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.95      0.96      0.96       255\n",
      "weighted avg       0.96      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9253\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.85      0.85      0.85        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.97      0.97       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.94      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 144.4864640235901 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5384, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.439, Accuracy: 0.8795, F1 Micro: 0.9288, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.293, Accuracy: 0.9472, F1 Micro: 0.9672, F1 Macro: 0.9658\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2111, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1463, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1137, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.977\n",
      "Epoch 7/10, Train Loss: 0.0925, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0762, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0616, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0536, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5673, Accuracy: 0.845, F1 Micro: 0.845, F1 Macro: 0.8003\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2729, Accuracy: 0.8837, F1 Micro: 0.8837, F1 Macro: 0.8761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2231, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1365, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.931\n",
      "Epoch 5/10, Train Loss: 0.1244, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9155\n",
      "Epoch 6/10, Train Loss: 0.1044, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9236\n",
      "Epoch 7/10, Train Loss: 0.1038, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.922\n",
      "Epoch 8/10, Train Loss: 0.0721, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9111\n",
      "Epoch 9/10, Train Loss: 0.0895, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "Epoch 10/10, Train Loss: 0.0811, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9269\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        84\n",
      "    positive       0.98      0.93      0.95       174\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.92      0.94      0.93       258\n",
      "weighted avg       0.94      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.92\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.91      0.81      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.90      0.89       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.90      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.76079726219177 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5356, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.467, Accuracy: 0.8289, F1 Micro: 0.9021, F1 Macro: 0.901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3245, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2274, Accuracy: 0.9531, F1 Micro: 0.9709, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1603, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1188, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Epoch 7/10, Train Loss: 0.0976, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9738\n",
      "Epoch 8/10, Train Loss: 0.0747, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0599, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5737, Accuracy: 0.8555, F1 Micro: 0.8555, F1 Macro: 0.8167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2835, Accuracy: 0.8897, F1 Micro: 0.8897, F1 Macro: 0.8818\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2291, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1192, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.9208\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9248\n",
      "Epoch 7/10, Train Loss: 0.1042, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9146\n",
      "Epoch 8/10, Train Loss: 0.071, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9125\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0826, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0803, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.93      0.91        85\n",
      "    positive       0.97      0.94      0.95       178\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.94      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9136\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.62      0.71        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.77      0.73      0.75        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.85      0.78      0.81       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.94      0.94       152\n",
      "    positive       0.81      0.85      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 155.9069104194641 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9335, F1 Micro: 0.9335, F1 Macro: 0.8474\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.680959701538086 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5376, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.45, Accuracy: 0.8318, F1 Micro: 0.9028, F1 Macro: 0.9012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3202, Accuracy: 0.9397, F1 Micro: 0.9625, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2175, Accuracy: 0.9531, F1 Micro: 0.9706, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1553, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1154, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Epoch 8/10, Train Loss: 0.0742, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.0612, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Epoch 10/10, Train Loss: 0.0476, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5059, Accuracy: 0.8849, F1 Micro: 0.8849, F1 Macro: 0.8677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2599, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9084\n",
      "Epoch 3/10, Train Loss: 0.1939, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1521, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1165, Accuracy: 0.9365, F1 Micro: 0.9365, F1 Macro: 0.9302\n",
      "Epoch 6/10, Train Loss: 0.1206, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1119, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Epoch 8/10, Train Loss: 0.079, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9136\n",
      "Epoch 9/10, Train Loss: 0.0509, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9234\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0597, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        82\n",
      "    positive       0.99      0.93      0.96       170\n",
      "\n",
      "    accuracy                           0.94       252\n",
      "   macro avg       0.93      0.95      0.94       252\n",
      "weighted avg       0.95      0.94      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9098\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 149.75375628471375 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5424, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4231, Accuracy: 0.9025, F1 Micro: 0.9412, F1 Macro: 0.9405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2836, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9674\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1948, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.9731\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1472, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "Epoch 6/10, Train Loss: 0.1129, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9791\n",
      "Epoch 7/10, Train Loss: 0.0863, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9782\n",
      "Epoch 8/10, Train Loss: 0.0731, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 9/10, Train Loss: 0.0629, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 10/10, Train Loss: 0.0495, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.509, Accuracy: 0.8803, F1 Micro: 0.8803, F1 Macro: 0.854\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2516, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9262\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1692, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1227, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1205, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "Epoch 6/10, Train Loss: 0.1165, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9436\n",
      "Epoch 8/10, Train Loss: 0.0921, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "Epoch 10/10, Train Loss: 0.0376, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9398\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        84\n",
      "    positive       0.98      0.94      0.96       175\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.94      0.95      0.94       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.923\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 150.17366456985474 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5412, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4566, Accuracy: 0.8452, F1 Micro: 0.9103, F1 Macro: 0.9094\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3205, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9654\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2143, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1574, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9778\n",
      "Epoch 7/10, Train Loss: 0.0869, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Epoch 9/10, Train Loss: 0.0643, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9737\n",
      "Epoch 10/10, Train Loss: 0.0496, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.92      0.99      0.95       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5767, Accuracy: 0.8745, F1 Micro: 0.8745, F1 Macro: 0.8572\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2585, Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1539, Accuracy: 0.919, F1 Micro: 0.919, F1 Macro: 0.9087\n",
      "Epoch 4/10, Train Loss: 0.1398, Accuracy: 0.915, F1 Micro: 0.915, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1068, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.108, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0992, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.934\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.929\n",
      "Epoch 9/10, Train Loss: 0.0686, Accuracy: 0.9312, F1 Micro: 0.9312, F1 Macro: 0.9248\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0679, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.934\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.98      0.92        83\n",
      "    positive       0.99      0.92      0.95       164\n",
      "\n",
      "    accuracy                           0.94       247\n",
      "   macro avg       0.92      0.95      0.93       247\n",
      "weighted avg       0.94      0.94      0.94       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9104\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.91      0.99      0.95       167\n",
      "    positive       0.95      0.55      0.69        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.78      0.83       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       1.00      0.80      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.91084170341492 s\n",
      "Averaged - Iteration 673: Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.8519\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.223785161972046 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5311, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4515, Accuracy: 0.8341, F1 Micro: 0.9042, F1 Macro: 0.9027\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.32, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2107, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.152, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Epoch 6/10, Train Loss: 0.1194, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0906, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 8/10, Train Loss: 0.0765, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9775\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Epoch 10/10, Train Loss: 0.0547, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5099, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9076\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2336, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9393\n",
      "Epoch 3/10, Train Loss: 0.1718, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9154\n",
      "Epoch 4/10, Train Loss: 0.1712, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "Epoch 5/10, Train Loss: 0.1221, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9206\n",
      "Epoch 6/10, Train Loss: 0.1128, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9215\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9255, F1 Micro: 0.9255, F1 Macro: 0.9186\n",
      "Epoch 8/10, Train Loss: 0.099, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9215\n",
      "Epoch 9/10, Train Loss: 0.0941, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9351\n",
      "Epoch 10/10, Train Loss: 0.0812, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9306\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9393\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        84\n",
      "    positive       0.98      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.95      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9233\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.88      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.91      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.38973760604858 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5346, Accuracy: 0.7976, F1 Micro: 0.8864, F1 Macro: 0.885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4091, Accuracy: 0.8996, F1 Micro: 0.9396, F1 Macro: 0.9385\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2785, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1914, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.14, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9783\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9772\n",
      "Epoch 7/10, Train Loss: 0.0877, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0723, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0623, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0556, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5219, Accuracy: 0.8931, F1 Micro: 0.8931, F1 Macro: 0.8821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2332, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2108, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9448\n",
      "Epoch 4/10, Train Loss: 0.1279, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Epoch 5/10, Train Loss: 0.1179, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9271\n",
      "Epoch 6/10, Train Loss: 0.1107, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.923\n",
      "Epoch 7/10, Train Loss: 0.0815, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9088\n",
      "Epoch 8/10, Train Loss: 0.0573, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9005\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "Epoch 10/10, Train Loss: 0.055, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        85\n",
      "    positive       0.99      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.96      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9296\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.89      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.433851480484 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5357, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4513, Accuracy: 0.8348, F1 Micro: 0.905, F1 Macro: 0.904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.316, Accuracy: 0.9442, F1 Micro: 0.9655, F1 Macro: 0.964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2095, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9754\n",
      "Epoch 5/10, Train Loss: 0.1504, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Epoch 6/10, Train Loss: 0.1209, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0905, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0625, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5219, Accuracy: 0.9035, F1 Micro: 0.9035, F1 Macro: 0.8933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2384, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9315\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2046, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9322\n",
      "Epoch 4/10, Train Loss: 0.1433, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 5/10, Train Loss: 0.1213, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1174, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9353\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9197\n",
      "Epoch 8/10, Train Loss: 0.0768, Accuracy: 0.9189, F1 Micro: 0.9189, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0721, Accuracy: 0.9459, F1 Micro: 0.9459, F1 Macro: 0.9391\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0574, Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        85\n",
      "    positive       0.97      0.96      0.97       174\n",
      "\n",
      "    accuracy                           0.95       259\n",
      "   macro avg       0.95      0.95      0.95       259\n",
      "weighted avg       0.95      0.95      0.95       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9227\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.81      0.83      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 155.68904900550842 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9367, F1 Micro: 0.9367, F1 Macro: 0.8564\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.806931495666504 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5283, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4352, Accuracy: 0.8668, F1 Micro: 0.9208, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3142, Accuracy: 0.9427, F1 Micro: 0.9647, F1 Macro: 0.9634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1986, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9743\n",
      "Epoch 5/10, Train Loss: 0.1491, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.117, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 7/10, Train Loss: 0.086, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 8/10, Train Loss: 0.0677, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0594, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "Epoch 10/10, Train Loss: 0.0484, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9781\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4974, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2277, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1871, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9218\n",
      "Epoch 4/10, Train Loss: 0.1535, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9168\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9164\n",
      "Epoch 6/10, Train Loss: 0.0834, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "Epoch 7/10, Train Loss: 0.1043, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0973, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "Epoch 9/10, Train Loss: 0.0853, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.912\n",
      "Epoch 10/10, Train Loss: 0.0684, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        85\n",
      "    positive       0.97      0.94      0.95       178\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.94      0.93       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9197\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.89      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 157.53085851669312 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5343, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4089, Accuracy: 0.9137, F1 Micro: 0.9475, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2828, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1834, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "Epoch 5/10, Train Loss: 0.1379, Accuracy: 0.9546, F1 Micro: 0.9713, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0847, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9796\n",
      "Epoch 8/10, Train Loss: 0.072, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9772\n",
      "Epoch 9/10, Train Loss: 0.0618, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0508, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.982\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.98      0.96       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5474, Accuracy: 0.8867, F1 Micro: 0.8867, F1 Macro: 0.8711\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2577, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1759, Accuracy: 0.9258, F1 Micro: 0.9258, F1 Macro: 0.9192\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1501, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1402, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9318\n",
      "Epoch 6/10, Train Loss: 0.1295, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9274\n",
      "Epoch 7/10, Train Loss: 0.0803, Accuracy: 0.9102, F1 Micro: 0.9102, F1 Macro: 0.8984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0861, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9277\n",
      "Epoch 10/10, Train Loss: 0.0589, Accuracy: 0.9297, F1 Micro: 0.9297, F1 Macro: 0.9233\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        86\n",
      "    positive       0.98      0.93      0.95       170\n",
      "\n",
      "    accuracy                           0.94       256\n",
      "   macro avg       0.92      0.94      0.93       256\n",
      "weighted avg       0.94      0.94      0.94       256\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9248\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.98      0.96       152\n",
      "    positive       0.93      0.81      0.87        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 157.3610394001007 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5314, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4514, Accuracy: 0.8534, F1 Micro: 0.9144, F1 Macro: 0.9138\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3159, Accuracy: 0.9405, F1 Micro: 0.963, F1 Macro: 0.9608\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2021, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Epoch 5/10, Train Loss: 0.1524, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.9723\n",
      "Epoch 6/10, Train Loss: 0.1146, Accuracy: 0.9591, F1 Micro: 0.974, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0901, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0729, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0616, Accuracy: 0.968, F1 Micro: 0.9797, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.94      0.95      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4951, Accuracy: 0.8918, F1 Micro: 0.8918, F1 Macro: 0.8836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2301, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9181\n",
      "Epoch 3/10, Train Loss: 0.1601, Accuracy: 0.9104, F1 Micro: 0.9104, F1 Macro: 0.9031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1294, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9203\n",
      "Epoch 5/10, Train Loss: 0.122, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1048, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9243\n",
      "Epoch 7/10, Train Loss: 0.0976, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0825, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.9056\n",
      "Epoch 10/10, Train Loss: 0.0838, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.892\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.93      0.91        87\n",
      "    positive       0.97      0.94      0.95       181\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.92      0.94      0.93       268\n",
      "weighted avg       0.94      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.921\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.97      0.97      0.97       167\n",
      "    positive       0.82      0.85      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.83      0.86      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 160.26955437660217 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.8603\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.366762399673462 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5303, Accuracy: 0.7902, F1 Micro: 0.8826, F1 Macro: 0.8811\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4323, Accuracy: 0.8668, F1 Micro: 0.9216, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2924, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.9652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1978, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 5/10, Train Loss: 0.1353, Accuracy: 0.9606, F1 Micro: 0.975, F1 Macro: 0.9727\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1116, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Epoch 7/10, Train Loss: 0.0879, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0679, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 9/10, Train Loss: 0.0587, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "Epoch 10/10, Train Loss: 0.0448, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4676, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9155\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2617, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1984, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9305\n",
      "Epoch 4/10, Train Loss: 0.1561, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9305\n",
      "Epoch 6/10, Train Loss: 0.0921, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0915, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.928\n",
      "Epoch 9/10, Train Loss: 0.0596, Accuracy: 0.9316, F1 Micro: 0.9316, F1 Macro: 0.9227\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0578, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.98      0.91        84\n",
      "    positive       0.99      0.92      0.95       179\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.92      0.95      0.93       263\n",
      "weighted avg       0.95      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.915\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.93      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 160.1107313632965 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5297, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3938, Accuracy: 0.9219, F1 Micro: 0.9524, F1 Macro: 0.9511\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2548, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1848, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9752\n",
      "Epoch 5/10, Train Loss: 0.1325, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1075, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 7/10, Train Loss: 0.0841, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Epoch 8/10, Train Loss: 0.0689, Accuracy: 0.9628, F1 Micro: 0.9763, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "Epoch 10/10, Train Loss: 0.048, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9795\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.97      0.97       175\n",
      "      others       0.93      0.98      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.478, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9065\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.1961, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "Epoch 3/10, Train Loss: 0.1447, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9084\n",
      "Epoch 4/10, Train Loss: 0.1604, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Epoch 5/10, Train Loss: 0.1482, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Epoch 6/10, Train Loss: 0.0964, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9301\n",
      "Epoch 7/10, Train Loss: 0.0983, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9213\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "Epoch 9/10, Train Loss: 0.0711, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9227\n",
      "Epoch 10/10, Train Loss: 0.0676, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9232\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.96       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.98      0.96       152\n",
      "    positive       0.93      0.77      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 155.1535665988922 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5367, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4429, Accuracy: 0.8743, F1 Micro: 0.9259, F1 Macro: 0.9249\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2882, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1951, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9713\n",
      "Epoch 5/10, Train Loss: 0.1367, Accuracy: 0.9539, F1 Micro: 0.9708, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1086, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Epoch 7/10, Train Loss: 0.0883, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.9591, F1 Micro: 0.9741, F1 Macro: 0.9724\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9734\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5424, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9172\n",
      "Epoch 2/10, Train Loss: 0.2319, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8931\n",
      "Epoch 3/10, Train Loss: 0.1745, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1384, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1006, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0819, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0563, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "Epoch 8/10, Train Loss: 0.0621, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Epoch 9/10, Train Loss: 0.0662, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Epoch 10/10, Train Loss: 0.0795, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        85\n",
      "    positive       0.98      0.94      0.96       175\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9185\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.96      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.79      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.86      0.81      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 162.81693530082703 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.8635\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.8355958461761475 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5186, Accuracy: 0.7932, F1 Micro: 0.8838, F1 Macro: 0.8821\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4359, Accuracy: 0.8847, F1 Micro: 0.929, F1 Macro: 0.926\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.302, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.9686\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1903, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9755\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1091, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.068, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0494, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4677, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9152\n",
      "Epoch 2/10, Train Loss: 0.1804, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9082\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1582, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "Epoch 4/10, Train Loss: 0.1253, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9116\n",
      "Epoch 5/10, Train Loss: 0.1237, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9285\n",
      "Epoch 6/10, Train Loss: 0.1322, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9237\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0879, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "Epoch 8/10, Train Loss: 0.0914, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9162\n",
      "Epoch 9/10, Train Loss: 0.0723, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9162\n",
      "Epoch 10/10, Train Loss: 0.05, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9201\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        84\n",
      "    positive       0.98      0.93      0.95       180\n",
      "\n",
      "    accuracy                           0.94       264\n",
      "   macro avg       0.92      0.94      0.93       264\n",
      "weighted avg       0.94      0.94      0.94       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9174\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.82      0.84        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.85      0.81        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 159.96122026443481 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5198, Accuracy: 0.8006, F1 Micro: 0.8878, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3984, Accuracy: 0.9226, F1 Micro: 0.952, F1 Macro: 0.9495\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2681, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1773, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1321, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "Epoch 6/10, Train Loss: 0.104, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9752\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 9/10, Train Loss: 0.0532, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9774\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5389, Accuracy: 0.8846, F1 Micro: 0.8846, F1 Macro: 0.8738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2143, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.177, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1532, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9354\n",
      "Epoch 5/10, Train Loss: 0.13, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9231\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9141\n",
      "Epoch 7/10, Train Loss: 0.0881, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9199\n",
      "Epoch 8/10, Train Loss: 0.0636, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Epoch 9/10, Train Loss: 0.0703, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0459, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.96      0.92        85\n",
      "    positive       0.98      0.93      0.96       175\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9174\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.96      0.95       152\n",
      "    positive       0.86      0.83      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.92      0.83        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.87      0.87      0.86       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.22647857666016 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5204, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4445, Accuracy: 0.8936, F1 Micro: 0.9354, F1 Macro: 0.9337\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3076, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9653\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1955, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "Epoch 5/10, Train Loss: 0.1405, Accuracy: 0.9568, F1 Micro: 0.9728, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1104, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.088, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.0693, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9763\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.95      0.91      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4834, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9212\n",
      "Epoch 2/10, Train Loss: 0.2024, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1456, Accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9374\n",
      "Epoch 4/10, Train Loss: 0.1283, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9255\n",
      "Epoch 5/10, Train Loss: 0.1238, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9052\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9194\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9291, F1 Micro: 0.9291, F1 Macro: 0.9212\n",
      "Epoch 8/10, Train Loss: 0.0846, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9291\n",
      "Epoch 9/10, Train Loss: 0.0761, Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.9334\n",
      "Epoch 10/10, Train Loss: 0.0573, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9099\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.944, F1 Micro: 0.944, F1 Macro: 0.9374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.97      0.92        86\n",
      "    positive       0.98      0.93      0.96       182\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.93      0.95      0.94       268\n",
      "weighted avg       0.95      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9228\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.96      0.91      0.94       152\n",
      "    positive       0.79      0.85      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.89      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 158.21833682060242 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9403, F1 Micro: 0.9403, F1 Macro: 0.8665\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.549731969833374 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.533, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4347, Accuracy: 0.8914, F1 Micro: 0.9351, F1 Macro: 0.9336\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2742, Accuracy: 0.9561, F1 Micro: 0.9728, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1895, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1383, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 6/10, Train Loss: 0.1042, Accuracy: 0.9665, F1 Micro: 0.9791, F1 Macro: 0.9782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.059, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.974\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4793, Accuracy: 0.9011, F1 Micro: 0.9011, F1 Macro: 0.8924\n",
      "Epoch 2/10, Train Loss: 0.2417, Accuracy: 0.8517, F1 Micro: 0.8517, F1 Macro: 0.8452\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.187, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9321\n",
      "Epoch 4/10, Train Loss: 0.149, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9288\n",
      "Epoch 5/10, Train Loss: 0.1247, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9284\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.909\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9291\n",
      "Epoch 8/10, Train Loss: 0.0615, Accuracy: 0.924, F1 Micro: 0.924, F1 Macro: 0.9172\n",
      "Epoch 9/10, Train Loss: 0.0772, Accuracy: 0.9011, F1 Micro: 0.9011, F1 Macro: 0.8824\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0624, Accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9447\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9506, F1 Micro: 0.9506, F1 Macro: 0.9447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.95       263\n",
      "   macro avg       0.94      0.95      0.94       263\n",
      "weighted avg       0.95      0.95      0.95       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9274\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.85      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.20967483520508 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5337, Accuracy: 0.7954, F1 Micro: 0.8853, F1 Macro: 0.8838\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3926, Accuracy: 0.9219, F1 Micro: 0.9522, F1 Macro: 0.951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2462, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.178, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1358, Accuracy: 0.9688, F1 Micro: 0.9805, F1 Macro: 0.9796\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9749\n",
      "Epoch 7/10, Train Loss: 0.0823, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 8/10, Train Loss: 0.0688, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9785\n",
      "Epoch 9/10, Train Loss: 0.0591, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9739\n",
      "Epoch 10/10, Train Loss: 0.0516, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.9738\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9805, F1 Macro: 0.9796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.97      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5139, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2646, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1637, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.0992, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9372\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1343, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9422\n",
      "Epoch 6/10, Train Loss: 0.0994, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9333\n",
      "Epoch 7/10, Train Loss: 0.068, Accuracy: 0.9352, F1 Micro: 0.9352, F1 Macro: 0.925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0684, Accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9502\n",
      "Epoch 9/10, Train Loss: 0.077, Accuracy: 0.9393, F1 Micro: 0.9393, F1 Macro: 0.9295\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0593, Accuracy: 0.9595, F1 Micro: 0.9595, F1 Macro: 0.9541\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9595, F1 Micro: 0.9595, F1 Macro: 0.9541\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.93      0.94        82\n",
      "    positive       0.96      0.98      0.97       165\n",
      "\n",
      "    accuracy                           0.96       247\n",
      "   macro avg       0.96      0.95      0.95       247\n",
      "weighted avg       0.96      0.96      0.96       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9228\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.81      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.88      0.83      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.94      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.86      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 162.84844779968262 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5372, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4427, Accuracy: 0.8854, F1 Micro: 0.932, F1 Macro: 0.9307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2796, Accuracy: 0.9501, F1 Micro: 0.9691, F1 Macro: 0.968\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1913, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9739\n",
      "Epoch 5/10, Train Loss: 0.1411, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1054, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0835, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Epoch 10/10, Train Loss: 0.0478, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9767\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4453, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2308, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9287\n",
      "Epoch 3/10, Train Loss: 0.1446, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9177\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1229, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1384, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1097, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9368\n",
      "Epoch 7/10, Train Loss: 0.1001, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9163\n",
      "Epoch 9/10, Train Loss: 0.0848, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.902\n",
      "Epoch 10/10, Train Loss: 0.1001, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9368\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.98      0.92        86\n",
      "    positive       0.99      0.93      0.96       174\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.921\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.79      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.87      0.86      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 170.94497966766357 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9414, F1 Micro: 0.9414, F1 Macro: 0.8693\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.960965394973755 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5233, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4165, Accuracy: 0.8943, F1 Micro: 0.9339, F1 Macro: 0.9305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2712, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9681\n",
      "Epoch 4/10, Train Loss: 0.1844, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9672\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1346, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0973, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 7/10, Train Loss: 0.072, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9787\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0504, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Epoch 10/10, Train Loss: 0.044, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4939, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9215\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2447, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1554, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1539, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1075, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9438\n",
      "Epoch 6/10, Train Loss: 0.1171, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.931\n",
      "Epoch 7/10, Train Loss: 0.0679, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9354\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9348\n",
      "Epoch 9/10, Train Loss: 0.0797, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.931\n",
      "Epoch 10/10, Train Loss: 0.0605, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.915\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        84\n",
      "    positive       0.99      0.94      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.93      0.96      0.94       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9232\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.82      0.83       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 164.57481622695923 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5263, Accuracy: 0.8036, F1 Micro: 0.8894, F1 Macro: 0.888\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3839, Accuracy: 0.9353, F1 Micro: 0.9598, F1 Macro: 0.9584\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2482, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1727, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1308, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1009, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9814\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9793\n",
      "Epoch 8/10, Train Loss: 0.0647, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Epoch 9/10, Train Loss: 0.0528, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 10/10, Train Loss: 0.0473, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      1.00      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4989, Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.8939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.231, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9198\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1269, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.935\n",
      "Epoch 4/10, Train Loss: 0.1405, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Epoch 5/10, Train Loss: 0.1233, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0841, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Epoch 8/10, Train Loss: 0.0764, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "Epoch 9/10, Train Loss: 0.0434, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.8938\n",
      "Epoch 10/10, Train Loss: 0.0463, Accuracy: 0.9144, F1 Micro: 0.9144, F1 Macro: 0.9044\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.94      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9213\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.84      0.86       216\n",
      "weighted avg       0.91      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      1.00      0.99       152\n",
      "    positive       1.00      0.85      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 164.85398149490356 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5276, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4373, Accuracy: 0.91, F1 Micro: 0.9448, F1 Macro: 0.943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2752, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1822, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1329, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9775\n",
      "Epoch 6/10, Train Loss: 0.0987, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.073, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.0656, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9765\n",
      "Epoch 9/10, Train Loss: 0.0513, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 10/10, Train Loss: 0.0456, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5068, Accuracy: 0.9011, F1 Micro: 0.9011, F1 Macro: 0.8832\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2165, Accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9358\n",
      "Epoch 3/10, Train Loss: 0.1488, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8944\n",
      "Epoch 4/10, Train Loss: 0.1363, Accuracy: 0.9354, F1 Micro: 0.9354, F1 Macro: 0.9276\n",
      "Epoch 5/10, Train Loss: 0.1208, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9324\n",
      "Epoch 6/10, Train Loss: 0.1094, Accuracy: 0.9163, F1 Micro: 0.9163, F1 Macro: 0.9094\n",
      "Epoch 7/10, Train Loss: 0.0978, Accuracy: 0.9278, F1 Micro: 0.9278, F1 Macro: 0.92\n",
      "Epoch 8/10, Train Loss: 0.0765, Accuracy: 0.9392, F1 Micro: 0.9392, F1 Macro: 0.9313\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "Epoch 10/10, Train Loss: 0.0429, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9129\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.943, F1 Micro: 0.943, F1 Macro: 0.9358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        86\n",
      "    positive       0.97      0.95      0.96       177\n",
      "\n",
      "    accuracy                           0.94       263\n",
      "   macro avg       0.93      0.94      0.94       263\n",
      "weighted avg       0.94      0.94      0.94       263\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9218\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 166.56272625923157 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.8719\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.353145122528076 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5297, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4199, Accuracy: 0.9003, F1 Micro: 0.9384, F1 Macro: 0.9356\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2704, Accuracy: 0.9561, F1 Micro: 0.9728, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1766, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1282, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0999, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9785\n",
      "Epoch 7/10, Train Loss: 0.0746, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Epoch 8/10, Train Loss: 0.0649, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Epoch 10/10, Train Loss: 0.0467, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.94      0.98      0.96       175\n",
      "      others       0.95      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.465, Accuracy: 0.9135, F1 Micro: 0.9135, F1 Macro: 0.9061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2059, Accuracy: 0.9173, F1 Micro: 0.9173, F1 Macro: 0.9108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1525, Accuracy: 0.9361, F1 Micro: 0.9361, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1343, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1286, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9415\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Epoch 7/10, Train Loss: 0.0967, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9252\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.9231\n",
      "Epoch 9/10, Train Loss: 0.067, Accuracy: 0.9248, F1 Micro: 0.9248, F1 Macro: 0.9151\n",
      "Epoch 10/10, Train Loss: 0.0416, Accuracy: 0.9286, F1 Micro: 0.9286, F1 Macro: 0.9171\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        87\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       266\n",
      "   macro avg       0.93      0.95      0.94       266\n",
      "weighted avg       0.95      0.95      0.95       266\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9255\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.92      0.81        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.89      0.86       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 173.82967948913574 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5247, Accuracy: 0.7961, F1 Micro: 0.8855, F1 Macro: 0.8839\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3785, Accuracy: 0.9263, F1 Micro: 0.954, F1 Macro: 0.9518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2443, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9698\n",
      "Epoch 4/10, Train Loss: 0.165, Accuracy: 0.9509, F1 Micro: 0.9692, F1 Macro: 0.9666\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1265, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9765\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1002, Accuracy: 0.9695, F1 Micro: 0.9809, F1 Macro: 0.9799\n",
      "Epoch 7/10, Train Loss: 0.0742, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9785\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 9/10, Train Loss: 0.056, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9809\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.97      0.96       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.52, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2466, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Epoch 3/10, Train Loss: 0.1907, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9088\n",
      "Epoch 4/10, Train Loss: 0.1353, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Epoch 5/10, Train Loss: 0.1164, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9268\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9243\n",
      "Epoch 8/10, Train Loss: 0.0732, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Epoch 9/10, Train Loss: 0.0585, Accuracy: 0.9038, F1 Micro: 0.9038, F1 Macro: 0.8956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0609, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        86\n",
      "    positive       0.97      0.93      0.95       174\n",
      "\n",
      "    accuracy                           0.93       260\n",
      "   macro avg       0.92      0.94      0.93       260\n",
      "weighted avg       0.94      0.93      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9174\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.86      0.76      0.81        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.97      0.96       152\n",
      "    positive       0.89      0.81      0.85        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.87      0.88       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 170.6259036064148 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5312, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4286, Accuracy: 0.9159, F1 Micro: 0.9484, F1 Macro: 0.9467\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2731, Accuracy: 0.9501, F1 Micro: 0.9692, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1778, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9719\n",
      "Epoch 5/10, Train Loss: 0.135, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9719\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1005, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "Epoch 7/10, Train Loss: 0.0782, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0673, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5297, Accuracy: 0.913, F1 Micro: 0.913, F1 Macro: 0.9061\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2621, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.134, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9187\n",
      "Epoch 4/10, Train Loss: 0.1297, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1301, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9257\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0849, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9265\n",
      "Epoch 7/10, Train Loss: 0.1146, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.922\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.902\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9289, F1 Micro: 0.9289, F1 Macro: 0.9212\n",
      "Epoch 10/10, Train Loss: 0.0523, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9101\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.95      0.91        85\n",
      "    positive       0.97      0.92      0.95       168\n",
      "\n",
      "    accuracy                           0.93       253\n",
      "   macro avg       0.92      0.94      0.93       253\n",
      "weighted avg       0.94      0.93      0.93       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9132\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.91      0.75      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 168.17043352127075 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9431, F1 Micro: 0.9431, F1 Macro: 0.874\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 1.837378978729248 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5268, Accuracy: 0.7924, F1 Micro: 0.8837, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4117, Accuracy: 0.8862, F1 Micro: 0.9314, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2637, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1657, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.119, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 6/10, Train Loss: 0.086, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Epoch 7/10, Train Loss: 0.069, Accuracy: 0.9658, F1 Micro: 0.9783, F1 Macro: 0.9767\n",
      "Epoch 8/10, Train Loss: 0.0614, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Epoch 9/10, Train Loss: 0.0506, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0466, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4616, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2204, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.9237\n",
      "Epoch 3/10, Train Loss: 0.1769, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9187\n",
      "Epoch 4/10, Train Loss: 0.1473, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.9115\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1287, Accuracy: 0.9338, F1 Micro: 0.9338, F1 Macro: 0.9261\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0946, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.9039\n",
      "Epoch 8/10, Train Loss: 0.0652, Accuracy: 0.9044, F1 Micro: 0.9044, F1 Macro: 0.8963\n",
      "Epoch 9/10, Train Loss: 0.0642, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9157\n",
      "Epoch 10/10, Train Loss: 0.0742, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.911\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.93\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.91        88\n",
      "    positive       0.97      0.93      0.95       184\n",
      "\n",
      "    accuracy                           0.94       272\n",
      "   macro avg       0.92      0.94      0.93       272\n",
      "weighted avg       0.94      0.94      0.94       272\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9185\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.95      0.93      0.94       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.88      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 173.3046588897705 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5249, Accuracy: 0.8103, F1 Micro: 0.8921, F1 Macro: 0.8905\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3766, Accuracy: 0.9301, F1 Micro: 0.9569, F1 Macro: 0.9553\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2376, Accuracy: 0.9561, F1 Micro: 0.9727, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.158, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1169, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0883, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 7/10, Train Loss: 0.0689, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0638, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 9/10, Train Loss: 0.0496, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.95      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4744, Accuracy: 0.9254, F1 Micro: 0.9254, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2266, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9259\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.173, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9247\n",
      "Epoch 4/10, Train Loss: 0.1533, Accuracy: 0.9179, F1 Micro: 0.9179, F1 Macro: 0.9099\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1126, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0867, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0779, Accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "Epoch 8/10, Train Loss: 0.0644, Accuracy: 0.9067, F1 Micro: 0.9067, F1 Macro: 0.8992\n",
      "Epoch 9/10, Train Loss: 0.0557, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9234\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.9142, F1 Micro: 0.9142, F1 Macro: 0.906\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9366, F1 Micro: 0.9366, F1 Macro: 0.9287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.93      0.91        87\n",
      "    positive       0.97      0.94      0.95       181\n",
      "\n",
      "    accuracy                           0.94       268\n",
      "   macro avg       0.92      0.94      0.93       268\n",
      "weighted avg       0.94      0.94      0.94       268\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.914\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.88      0.92        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 181.5791163444519 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5288, Accuracy: 0.7879, F1 Micro: 0.8813, F1 Macro: 0.8797\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4237, Accuracy: 0.9085, F1 Micro: 0.9446, F1 Macro: 0.9427\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2611, Accuracy: 0.9546, F1 Micro: 0.9718, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1642, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1198, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 6/10, Train Loss: 0.085, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9754\n",
      "Epoch 7/10, Train Loss: 0.0705, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0618, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "Epoch 9/10, Train Loss: 0.0504, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9741\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.976\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.495, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9351\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2015, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1767, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.9403\n",
      "Epoch 4/10, Train Loss: 0.1235, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9329\n",
      "Epoch 5/10, Train Loss: 0.0929, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9318\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9282\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0651, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.94\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0634, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0492, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9441\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9245\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.92        86\n",
      "    positive       0.97      0.96      0.96       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.94      0.95      0.94       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9234\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.89      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.81      0.81      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 176.44561648368835 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.8759\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 1.265897512435913 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5205, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.405, Accuracy: 0.9144, F1 Micro: 0.947, F1 Macro: 0.9449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2479, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1681, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.976\n",
      "Epoch 5/10, Train Loss: 0.1138, Accuracy: 0.9583, F1 Micro: 0.9737, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 7/10, Train Loss: 0.0681, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Epoch 8/10, Train Loss: 0.059, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0487, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 10/10, Train Loss: 0.0408, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9777\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5234, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8927\n",
      "Epoch 2/10, Train Loss: 0.2587, Accuracy: 0.8931, F1 Micro: 0.8931, F1 Macro: 0.8863\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1783, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9221\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1314, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.925\n",
      "Epoch 5/10, Train Loss: 0.1353, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.917\n",
      "Epoch 6/10, Train Loss: 0.0862, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1151, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 8/10, Train Loss: 0.066, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9167\n",
      "Epoch 9/10, Train Loss: 0.0864, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9044\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9242\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9242\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.94      0.90        87\n",
      "    positive       0.97      0.93      0.95       175\n",
      "\n",
      "    accuracy                           0.93       262\n",
      "   macro avg       0.92      0.93      0.92       262\n",
      "weighted avg       0.93      0.93      0.93       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9159\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 175.18827652931213 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5196, Accuracy: 0.7954, F1 Micro: 0.8852, F1 Macro: 0.8837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3751, Accuracy: 0.9315, F1 Micro: 0.9571, F1 Macro: 0.9554\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2309, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1622, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9769\n",
      "Epoch 5/10, Train Loss: 0.1101, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0932, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9788\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0718, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0581, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "Epoch 9/10, Train Loss: 0.05, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9789\n",
      "Epoch 10/10, Train Loss: 0.0442, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.98      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5268, Accuracy: 0.8626, F1 Micro: 0.8626, F1 Macro: 0.8305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2229, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1725, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9367\n",
      "Epoch 4/10, Train Loss: 0.1442, Accuracy: 0.9122, F1 Micro: 0.9122, F1 Macro: 0.9057\n",
      "Epoch 5/10, Train Loss: 0.1069, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.923\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9221\n",
      "Epoch 7/10, Train Loss: 0.0892, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9206\n",
      "Epoch 8/10, Train Loss: 0.0673, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9323\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0681, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "Epoch 10/10, Train Loss: 0.071, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9127\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92        86\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.94      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9276\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      1.00       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.95      0.96      0.95       152\n",
      "    positive       0.86      0.85      0.85        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.88      0.89       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 180.23534417152405 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5246, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4232, Accuracy: 0.9278, F1 Micro: 0.9557, F1 Macro: 0.9541\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2558, Accuracy: 0.9539, F1 Micro: 0.9712, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1715, Accuracy: 0.9635, F1 Micro: 0.9773, F1 Macro: 0.9764\n",
      "Epoch 5/10, Train Loss: 0.118, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0977, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "Epoch 7/10, Train Loss: 0.0744, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9759\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9725\n",
      "Epoch 9/10, Train Loss: 0.0507, Accuracy: 0.9635, F1 Micro: 0.9769, F1 Macro: 0.9751\n",
      "Epoch 10/10, Train Loss: 0.0445, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.9746\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4382, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9206\n",
      "Epoch 2/10, Train Loss: 0.1969, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1751, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1606, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1312, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Epoch 6/10, Train Loss: 0.1234, Accuracy: 0.9046, F1 Micro: 0.9046, F1 Macro: 0.897\n",
      "Epoch 7/10, Train Loss: 0.0812, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0735, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "Epoch 9/10, Train Loss: 0.0799, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9118\n",
      "Epoch 10/10, Train Loss: 0.0622, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9238\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.94      0.92        86\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9232\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 172.61927199363708 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9445, F1 Micro: 0.9445, F1 Macro: 0.8778\n",
      "Total runtime: 10585.322904109955 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376b9ed",
   "metadata": {
    "papermill": {
     "duration": 0.015512,
     "end_time": "2025-04-05T06:35:21.607367",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.591855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RUN 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f993ffb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T06:35:21.642101Z",
     "iopub.status.busy": "2025-04-05T06:35:21.641771Z",
     "iopub.status.idle": "2025-04-05T09:20:47.161092Z",
     "shell.execute_reply": "2025-04-05T09:20:47.160205Z"
    },
    "papermill": {
     "duration": 9925.538207,
     "end_time": "2025-04-05T09:20:47.162586",
     "exception": false,
     "start_time": "2025-04-05T06:35:21.624379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "TRIAL 5\n",
      "Random seed: [94, 21, 5]\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6559, Accuracy: 0.7463, F1 Micro: 0.8496, F1 Macro: 0.8267\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5772, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5458, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5239, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.476, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4654, Accuracy: 0.7902, F1 Micro: 0.8825, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.4704, Accuracy: 0.7879, F1 Micro: 0.8808, F1 Macro: 0.8789\n",
      "Epoch 8/10, Train Loss: 0.4263, Accuracy: 0.7827, F1 Micro: 0.8771, F1 Macro: 0.8742\n",
      "Epoch 9/10, Train Loss: 0.4009, Accuracy: 0.7812, F1 Micro: 0.8761, F1 Macro: 0.8731\n",
      "Epoch 10/10, Train Loss: 0.3867, Accuracy: 0.7798, F1 Micro: 0.8749, F1 Macro: 0.8721\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.7902, F1 Micro: 0.8825, F1 Macro: 0.8809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      0.99      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.88      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.8458, Accuracy: 0.4, F1 Micro: 0.4, F1 Macro: 0.2857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6455, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4744, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3486, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2564, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2002, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1521, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1192, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0909, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0727, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         5\n",
      "   macro avg       0.50      0.50      0.50         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 54: Accuracy: 0.7901, F1 Micro: 0.7901, F1 Macro: 0.2994\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      0.99      0.83       152\n",
      "    positive       0.60      0.06      0.11        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.44      0.35      0.31       216\n",
      "weighted avg       0.64      0.71      0.61       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 66.37845206260681 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6744, Accuracy: 0.7552, F1 Micro: 0.8555, F1 Macro: 0.8479\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5918, Accuracy: 0.785, F1 Micro: 0.8794, F1 Macro: 0.8779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5453, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5309, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4879, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4745, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4859, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.4599, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.4311, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "Epoch 10/10, Train Loss: 0.3935, Accuracy: 0.7909, F1 Micro: 0.8827, F1 Macro: 0.8811\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.7917, F1 Micro: 0.8833, F1 Macro: 0.8818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      0.99      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.89      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6444, Accuracy: 0.8, F1 Micro: 0.8, F1 Macro: 0.4444\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5577, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.531, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4857, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4167, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3914, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3635, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2985, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3418, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2611, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         5\n",
      "   macro avg       0.50      0.50      0.50         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 54: Accuracy: 0.7917, F1 Micro: 0.7917, F1 Macro: 0.3025\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.06      0.11        52\n",
      "\n",
      "    accuracy                           0.72       216\n",
      "   macro avg       0.57      0.35      0.31       216\n",
      "weighted avg       0.74      0.72      0.61       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.71      0.99      0.83       152\n",
      "    positive       0.50      0.02      0.05        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.40      0.34      0.29       216\n",
      "weighted avg       0.59      0.70      0.59       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 73.21565580368042 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7408, Accuracy: 0.5982, F1 Micro: 0.7065, F1 Macro: 0.6096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6394, Accuracy: 0.7723, F1 Micro: 0.8672, F1 Macro: 0.8576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.6003, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 4/10, Train Loss: 0.5587, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 5/10, Train Loss: 0.5047, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 6/10, Train Loss: 0.5075, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 7/10, Train Loss: 0.5115, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 8/10, Train Loss: 0.4764, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Epoch 9/10, Train Loss: 0.4545, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.4405, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.83      1.00      0.91       187\n",
      "     machine       0.78      1.00      0.88       175\n",
      "      others       0.71      1.00      0.83       158\n",
      "        part       0.71      1.00      0.83       158\n",
      "       price       0.86      1.00      0.92       192\n",
      "     service       0.85      1.00      0.92       191\n",
      "\n",
      "   micro avg       0.79      1.00      0.88      1061\n",
      "   macro avg       0.79      1.00      0.88      1061\n",
      "weighted avg       0.80      1.00      0.88      1061\n",
      " samples avg       0.79      1.00      0.88      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.7828, Accuracy: 0.0, F1 Micro: 0.0, F1 Macro: 0.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6312, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4728, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3577, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3441, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.307, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2576, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1603, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2043, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1563, Accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 1.0, F1 Micro: 1.0, F1 Macro: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "    positive       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         1\n",
      "   macro avg       0.50      0.50      0.50         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 54: Accuracy: 0.7901, F1 Micro: 0.7901, F1 Macro: 0.2958\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.84      1.00      0.91       181\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.84       216\n",
      "   macro avg       0.28      0.33      0.30       216\n",
      "weighted avg       0.70      0.84      0.76       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.77      1.00      0.87       167\n",
      "    positive       0.00      0.00      0.00        33\n",
      "\n",
      "    accuracy                           0.77       216\n",
      "   macro avg       0.26      0.33      0.29       216\n",
      "weighted avg       0.60      0.77      0.67       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        12\n",
      "     neutral       0.71      1.00      0.83       152\n",
      "    positive       1.00      0.02      0.04        52\n",
      "\n",
      "    accuracy                           0.71       216\n",
      "   macro avg       0.57      0.34      0.29       216\n",
      "weighted avg       0.74      0.71      0.59       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        23\n",
      "     neutral       0.70      1.00      0.83       152\n",
      "    positive       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.70       216\n",
      "   macro avg       0.23      0.33      0.28       216\n",
      "weighted avg       0.50      0.70      0.58       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        13\n",
      "     neutral       0.86      1.00      0.93       186\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.74      0.86      0.80       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        14\n",
      "     neutral       0.86      1.00      0.92       185\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.29      0.33      0.31       216\n",
      "weighted avg       0.73      0.86      0.79       216\n",
      "\n",
      "Total train time: 64.06351137161255 s\n",
      "Averaged - Iteration 54: Accuracy: 0.7906, F1 Micro: 0.7906, F1 Macro: 0.2992\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 81\n",
      "Sampling duration: 7.54665470123291 seconds\n",
      "New train size: 135\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6071, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5272, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.49, Accuracy: 0.7902, F1 Micro: 0.8824, F1 Macro: 0.8808\n",
      "Epoch 4/10, Train Loss: 0.4677, Accuracy: 0.7894, F1 Micro: 0.8805, F1 Macro: 0.8779\n",
      "Epoch 5/10, Train Loss: 0.4441, Accuracy: 0.7924, F1 Micro: 0.8815, F1 Macro: 0.8784\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4249, Accuracy: 0.8028, F1 Micro: 0.8867, F1 Macro: 0.8837\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.4034, Accuracy: 0.8222, F1 Micro: 0.8971, F1 Macro: 0.8952\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3395, Accuracy: 0.8438, F1 Micro: 0.9081, F1 Macro: 0.9064\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2869, Accuracy: 0.8624, F1 Micro: 0.9168, F1 Macro: 0.9141\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2624, Accuracy: 0.878, F1 Micro: 0.9257, F1 Macro: 0.9225\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.878, F1 Micro: 0.9257, F1 Macro: 0.9225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.87      1.00      0.93       187\n",
      "     machine       0.88      0.97      0.92       175\n",
      "      others       0.86      0.84      0.85       158\n",
      "        part       0.87      0.96      0.91       158\n",
      "       price       0.95      0.99      0.97       192\n",
      "     service       0.90      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.89      0.96      0.93      1061\n",
      "   macro avg       0.89      0.96      0.92      1061\n",
      "weighted avg       0.89      0.96      0.93      1061\n",
      " samples avg       0.90      0.96      0.92      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6366, Accuracy: 0.734, F1 Micro: 0.734, F1 Macro: 0.4233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5464, Accuracy: 0.734, F1 Micro: 0.734, F1 Macro: 0.4233\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4264, Accuracy: 0.8191, F1 Micro: 0.8191, F1 Macro: 0.7196\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3039, Accuracy: 0.8191, F1 Micro: 0.8191, F1 Macro: 0.7437\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2049, Accuracy: 0.8617, F1 Micro: 0.8617, F1 Macro: 0.8409\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.145, Accuracy: 0.8617, F1 Micro: 0.8617, F1 Macro: 0.8394\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0629, Accuracy: 0.8723, F1 Micro: 0.8723, F1 Macro: 0.8518\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.046, Accuracy: 0.8883, F1 Micro: 0.8883, F1 Macro: 0.8685\n",
      "Epoch 9/10, Train Loss: 0.0882, Accuracy: 0.867, F1 Micro: 0.867, F1 Macro: 0.8477\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.8617, F1 Micro: 0.8617, F1 Macro: 0.8423\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.8883, F1 Micro: 0.8883, F1 Macro: 0.8685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.94      0.82        50\n",
      "    positive       0.98      0.87      0.92       138\n",
      "\n",
      "    accuracy                           0.89       188\n",
      "   macro avg       0.85      0.90      0.87       188\n",
      "weighted avg       0.91      0.89      0.89       188\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 135: Accuracy: 0.8696, F1 Micro: 0.8696, F1 Macro: 0.6852\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.87      1.00      0.93       181\n",
      "    positive       1.00      0.38      0.55        24\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.62      0.46      0.49       216\n",
      "weighted avg       0.84      0.88      0.84       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.88      0.97      0.92       167\n",
      "    positive       0.76      0.39      0.52        33\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.84      0.73      0.76       216\n",
      "weighted avg       0.86      0.87      0.86       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.43      0.75      0.55        12\n",
      "     neutral       0.86      0.84      0.85       152\n",
      "    positive       0.62      0.56      0.59        52\n",
      "\n",
      "    accuracy                           0.76       216\n",
      "   macro avg       0.63      0.71      0.66       216\n",
      "weighted avg       0.78      0.76      0.77       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.57      0.67        23\n",
      "     neutral       0.88      0.97      0.92       152\n",
      "    positive       0.81      0.63      0.71        41\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.83      0.72      0.77       216\n",
      "weighted avg       0.86      0.86      0.85       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.95      0.99      0.97       186\n",
      "    positive       0.85      0.65      0.73        17\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.73      0.79       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.36      0.53        14\n",
      "     neutral       0.90      1.00      0.95       185\n",
      "    positive       1.00      0.29      0.45        17\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.97      0.55      0.64       216\n",
      "weighted avg       0.91      0.90      0.88       216\n",
      "\n",
      "Total train time: 82.74847555160522 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6484, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.539, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4916, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Epoch 4/10, Train Loss: 0.478, Accuracy: 0.7894, F1 Micro: 0.8822, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4482, Accuracy: 0.7984, F1 Micro: 0.885, F1 Macro: 0.8826\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.424, Accuracy: 0.8021, F1 Micro: 0.8858, F1 Macro: 0.8822\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3993, Accuracy: 0.8296, F1 Micro: 0.9001, F1 Macro: 0.8971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3397, Accuracy: 0.8646, F1 Micro: 0.9192, F1 Macro: 0.9175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2883, Accuracy: 0.8854, F1 Micro: 0.9301, F1 Macro: 0.9275\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2624, Accuracy: 0.8936, F1 Micro: 0.9351, F1 Macro: 0.9329\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.8936, F1 Micro: 0.9351, F1 Macro: 0.9329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.89      0.99      0.94       187\n",
      "     machine       0.93      0.95      0.94       175\n",
      "      others       0.86      0.87      0.87       158\n",
      "        part       0.89      0.99      0.94       158\n",
      "       price       0.94      0.99      0.96       192\n",
      "     service       0.91      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.90      0.97      0.94      1061\n",
      "   macro avg       0.90      0.97      0.93      1061\n",
      "weighted avg       0.90      0.97      0.93      1061\n",
      " samples avg       0.91      0.97      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5813, Accuracy: 0.7245, F1 Micro: 0.7245, F1 Macro: 0.4201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.6057, Accuracy: 0.7245, F1 Micro: 0.7245, F1 Macro: 0.4201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.442, Accuracy: 0.8163, F1 Micro: 0.8163, F1 Macro: 0.77\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3016, Accuracy: 0.852, F1 Micro: 0.852, F1 Macro: 0.8157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1737, Accuracy: 0.852, F1 Micro: 0.852, F1 Macro: 0.8297\n",
      "Epoch 6/10, Train Loss: 0.1286, Accuracy: 0.8469, F1 Micro: 0.8469, F1 Macro: 0.8274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0788, Accuracy: 0.8776, F1 Micro: 0.8776, F1 Macro: 0.8559\n",
      "Epoch 8/10, Train Loss: 0.0712, Accuracy: 0.852, F1 Micro: 0.852, F1 Macro: 0.8338\n",
      "Epoch 9/10, Train Loss: 0.067, Accuracy: 0.8571, F1 Micro: 0.8571, F1 Macro: 0.8389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0687, Accuracy: 0.8827, F1 Micro: 0.8827, F1 Macro: 0.8585\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.8827, F1 Micro: 0.8827, F1 Macro: 0.8585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.85      0.80        54\n",
      "    positive       0.94      0.89      0.92       142\n",
      "\n",
      "    accuracy                           0.88       196\n",
      "   macro avg       0.85      0.87      0.86       196\n",
      "weighted avg       0.89      0.88      0.88       196\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 135: Accuracy: 0.8827, F1 Micro: 0.8827, F1 Macro: 0.7015\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        11\n",
      "     neutral       0.90      0.99      0.94       181\n",
      "    positive       0.87      0.54      0.67        24\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.59      0.51      0.54       216\n",
      "weighted avg       0.85      0.89      0.86       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.92      0.96      0.94       167\n",
      "    positive       0.78      0.64      0.70        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.47      0.58      0.52        12\n",
      "     neutral       0.86      0.87      0.86       152\n",
      "    positive       0.68      0.62      0.65        52\n",
      "\n",
      "    accuracy                           0.79       216\n",
      "   macro avg       0.67      0.69      0.68       216\n",
      "weighted avg       0.79      0.79      0.79       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.52      0.62        23\n",
      "     neutral       0.89      0.99      0.94       152\n",
      "    positive       0.87      0.63      0.73        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.72      0.76       216\n",
      "weighted avg       0.87      0.88      0.86       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.94      0.99      0.97       186\n",
      "    positive       0.82      0.53      0.64        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.69      0.76       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.43      0.60        14\n",
      "     neutral       0.90      1.00      0.95       185\n",
      "    positive       0.80      0.24      0.36        17\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.55      0.64       216\n",
      "weighted avg       0.90      0.90      0.88       216\n",
      "\n",
      "Total train time: 81.32735109329224 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6745, Accuracy: 0.7537, F1 Micro: 0.8532, F1 Macro: 0.8278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5552, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.508, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.5003, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4702, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.4546, Accuracy: 0.8051, F1 Micro: 0.8896, F1 Macro: 0.8882\n",
      "Epoch 7/10, Train Loss: 0.4485, Accuracy: 0.8065, F1 Micro: 0.8886, F1 Macro: 0.8857\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.3818, Accuracy: 0.8423, F1 Micro: 0.9074, F1 Macro: 0.9058\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.3363, Accuracy: 0.8713, F1 Micro: 0.9216, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.3002, Accuracy: 0.8891, F1 Micro: 0.932, F1 Macro: 0.9295\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.8891, F1 Micro: 0.932, F1 Macro: 0.9295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.89      1.00      0.94       187\n",
      "     machine       0.92      0.93      0.92       175\n",
      "      others       0.88      0.85      0.87       158\n",
      "        part       0.89      0.97      0.93       158\n",
      "       price       0.93      0.99      0.96       192\n",
      "     service       0.91      1.00      0.95       191\n",
      "\n",
      "   micro avg       0.90      0.96      0.93      1061\n",
      "   macro avg       0.90      0.96      0.93      1061\n",
      "weighted avg       0.90      0.96      0.93      1061\n",
      " samples avg       0.91      0.96      0.93      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5916, Accuracy: 0.7198, F1 Micro: 0.7198, F1 Macro: 0.4185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4176, Accuracy: 0.7295, F1 Micro: 0.7295, F1 Macro: 0.4953\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3993, Accuracy: 0.8116, F1 Micro: 0.8116, F1 Macro: 0.7652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2715, Accuracy: 0.8551, F1 Micro: 0.8551, F1 Macro: 0.8143\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1754, Accuracy: 0.8841, F1 Micro: 0.8841, F1 Macro: 0.8643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1016, Accuracy: 0.8889, F1 Micro: 0.8889, F1 Macro: 0.8716\n",
      "Epoch 7/10, Train Loss: 0.0595, Accuracy: 0.8792, F1 Micro: 0.8792, F1 Macro: 0.8626\n",
      "Epoch 8/10, Train Loss: 0.0738, Accuracy: 0.8841, F1 Micro: 0.8841, F1 Macro: 0.8676\n",
      "Epoch 9/10, Train Loss: 0.1128, Accuracy: 0.8744, F1 Micro: 0.8744, F1 Macro: 0.8576\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0739, Accuracy: 0.8937, F1 Micro: 0.8937, F1 Macro: 0.8767\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.8937, F1 Micro: 0.8937, F1 Macro: 0.8767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.93      0.83        58\n",
      "    positive       0.97      0.88      0.92       149\n",
      "\n",
      "    accuracy                           0.89       207\n",
      "   macro avg       0.86      0.91      0.88       207\n",
      "weighted avg       0.91      0.89      0.90       207\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 135: Accuracy: 0.8819, F1 Micro: 0.8819, F1 Macro: 0.7258\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.18      0.31        11\n",
      "     neutral       0.89      1.00      0.94       181\n",
      "    positive       1.00      0.46      0.63        24\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.96      0.55      0.63       216\n",
      "weighted avg       0.91      0.90      0.88       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.81      0.76        16\n",
      "     neutral       0.91      0.93      0.92       167\n",
      "    positive       0.74      0.61      0.67        33\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.78      0.78       216\n",
      "weighted avg       0.87      0.88      0.87       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.92      0.71        12\n",
      "     neutral       0.88      0.86      0.87       152\n",
      "    positive       0.66      0.63      0.65        52\n",
      "\n",
      "    accuracy                           0.81       216\n",
      "   macro avg       0.71      0.80      0.74       216\n",
      "weighted avg       0.81      0.81      0.81       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.65      0.71        23\n",
      "     neutral       0.89      0.97      0.93       152\n",
      "    positive       0.87      0.66      0.75        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.85      0.76      0.80       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.54      0.67        13\n",
      "     neutral       0.93      0.99      0.96       186\n",
      "    positive       0.80      0.47      0.59        17\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.87      0.67      0.74       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.43      0.60        14\n",
      "     neutral       0.91      1.00      0.95       185\n",
      "    positive       0.83      0.29      0.43        17\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.57      0.66       216\n",
      "weighted avg       0.91      0.91      0.89       216\n",
      "\n",
      "Total train time: 82.22972393035889 s\n",
      "Averaged - Iteration 135: Accuracy: 0.8344, F1 Micro: 0.8344, F1 Macro: 0.5017\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 73\n",
      "Sampling duration: 11.672796726226807 seconds\n",
      "New train size: 208\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5851, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5174, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.505, Accuracy: 0.7969, F1 Micro: 0.8856, F1 Macro: 0.884\n",
      "Epoch 4/10, Train Loss: 0.4492, Accuracy: 0.7924, F1 Micro: 0.8834, F1 Macro: 0.8817\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4321, Accuracy: 0.8155, F1 Micro: 0.894, F1 Macro: 0.8921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3652, Accuracy: 0.8438, F1 Micro: 0.909, F1 Macro: 0.9078\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3197, Accuracy: 0.8817, F1 Micro: 0.929, F1 Macro: 0.9274\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2617, Accuracy: 0.9234, F1 Micro: 0.9528, F1 Macro: 0.9505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2041, Accuracy: 0.9293, F1 Micro: 0.9561, F1 Macro: 0.9538\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1857, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9535\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9301, F1 Micro: 0.9564, F1 Macro: 0.9535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      1.00      0.99       187\n",
      "     machine       0.91      0.97      0.94       175\n",
      "      others       0.91      0.87      0.89       158\n",
      "        part       0.89      0.98      0.93       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.94      0.97      0.96      1061\n",
      "   macro avg       0.94      0.97      0.95      1061\n",
      "weighted avg       0.94      0.97      0.96      1061\n",
      " samples avg       0.94      0.97      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.622, Accuracy: 0.6987, F1 Micro: 0.6987, F1 Macro: 0.4113\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4475, Accuracy: 0.795, F1 Micro: 0.795, F1 Macro: 0.705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2931, Accuracy: 0.8828, F1 Micro: 0.8828, F1 Macro: 0.8574\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.189, Accuracy: 0.8954, F1 Micro: 0.8954, F1 Macro: 0.8829\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.113, Accuracy: 0.9121, F1 Micro: 0.9121, F1 Macro: 0.9004\n",
      "Epoch 6/10, Train Loss: 0.1291, Accuracy: 0.8912, F1 Micro: 0.8912, F1 Macro: 0.8793\n",
      "Epoch 7/10, Train Loss: 0.0991, Accuracy: 0.8828, F1 Micro: 0.8828, F1 Macro: 0.8715\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0795, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9052\n",
      "Epoch 9/10, Train Loss: 0.0865, Accuracy: 0.8996, F1 Micro: 0.8996, F1 Macro: 0.8873\n",
      "Epoch 10/10, Train Loss: 0.0613, Accuracy: 0.8912, F1 Micro: 0.8912, F1 Macro: 0.8793\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.86      0.87        72\n",
      "    positive       0.94      0.95      0.94       167\n",
      "\n",
      "    accuracy                           0.92       239\n",
      "   macro avg       0.91      0.90      0.91       239\n",
      "weighted avg       0.92      0.92      0.92       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 208: Accuracy: 0.9221, F1 Micro: 0.9221, F1 Macro: 0.8438\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.64      0.78        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.91      0.97      0.94       167\n",
      "    positive       0.80      0.61      0.69        33\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.75      0.80       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.91      0.88      0.89       152\n",
      "    positive       0.73      0.77      0.75        52\n",
      "\n",
      "    accuracy                           0.85       216\n",
      "   macro avg       0.77      0.83      0.79       216\n",
      "weighted avg       0.85      0.85      0.85       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.57      0.70        23\n",
      "     neutral       0.89      0.99      0.93       152\n",
      "    positive       0.85      0.68      0.76        41\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.89      0.74      0.80       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.69      0.78        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.81      0.76      0.79        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.90      0.82      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 89.69281148910522 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.604, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5138, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5052, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4541, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4295, Accuracy: 0.808, F1 Micro: 0.8895, F1 Macro: 0.8866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3623, Accuracy: 0.8661, F1 Micro: 0.9212, F1 Macro: 0.92\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3186, Accuracy: 0.8906, F1 Micro: 0.9339, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.265, Accuracy: 0.9144, F1 Micro: 0.9478, F1 Macro: 0.9461\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2037, Accuracy: 0.9249, F1 Micro: 0.9536, F1 Macro: 0.9516\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1913, Accuracy: 0.9293, F1 Micro: 0.9565, F1 Macro: 0.9549\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9293, F1 Micro: 0.9565, F1 Macro: 0.9549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.97      0.99      0.98       187\n",
      "     machine       0.92      0.96      0.94       175\n",
      "      others       0.88      0.96      0.92       158\n",
      "        part       0.90      0.99      0.94       158\n",
      "       price       0.96      1.00      0.98       192\n",
      "     service       0.95      1.00      0.98       191\n",
      "\n",
      "   micro avg       0.93      0.98      0.96      1061\n",
      "   macro avg       0.93      0.98      0.95      1061\n",
      "weighted avg       0.93      0.98      0.96      1061\n",
      " samples avg       0.93      0.98      0.95      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6139, Accuracy: 0.6901, F1 Micro: 0.6901, F1 Macro: 0.4083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4866, Accuracy: 0.7559, F1 Micro: 0.7559, F1 Macro: 0.6212\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3265, Accuracy: 0.8967, F1 Micro: 0.8967, F1 Macro: 0.8782\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1679, Accuracy: 0.9108, F1 Micro: 0.9108, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0993, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9092\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1224, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9038\n",
      "Epoch 7/10, Train Loss: 0.1141, Accuracy: 0.8873, F1 Micro: 0.8873, F1 Macro: 0.878\n",
      "Epoch 8/10, Train Loss: 0.0696, Accuracy: 0.9155, F1 Micro: 0.9155, F1 Macro: 0.9035\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0448, Accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9063\n",
      "Epoch 10/10, Train Loss: 0.0385, Accuracy: 0.9108, F1 Micro: 0.9108, F1 Macro: 0.8986\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9202, F1 Micro: 0.9202, F1 Macro: 0.9063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.86      0.87        66\n",
      "    positive       0.94      0.95      0.94       147\n",
      "\n",
      "    accuracy                           0.92       213\n",
      "   macro avg       0.91      0.90      0.91       213\n",
      "weighted avg       0.92      0.92      0.92       213\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 208: Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.8228\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.45      0.62        11\n",
      "     neutral       0.97      0.99      0.98       181\n",
      "    positive       0.81      0.92      0.86        24\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.79      0.82       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.75      0.77        16\n",
      "     neutral       0.91      0.96      0.94       167\n",
      "    positive       0.84      0.64      0.72        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.78      0.81       216\n",
      "weighted avg       0.89      0.90      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.75      0.72        12\n",
      "     neutral       0.88      0.96      0.92       152\n",
      "    positive       0.86      0.62      0.72        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.81      0.78      0.79       216\n",
      "weighted avg       0.87      0.87      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.61      0.74        23\n",
      "     neutral       0.90      0.99      0.94       152\n",
      "    positive       0.88      0.71      0.78        41\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.90      0.77      0.82       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.54      0.70        13\n",
      "     neutral       0.95      1.00      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.96      0.77      0.84       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.95      1.00      0.98       185\n",
      "    positive       0.92      0.65      0.76        17\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.96      0.79      0.86       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Total train time: 90.03168773651123 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6466, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5279, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.5166, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4746, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4489, Accuracy: 0.8073, F1 Micro: 0.891, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3929, Accuracy: 0.843, F1 Micro: 0.9084, F1 Macro: 0.9068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.3399, Accuracy: 0.8832, F1 Micro: 0.9296, F1 Macro: 0.9272\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.2821, Accuracy: 0.9196, F1 Micro: 0.9505, F1 Macro: 0.9484\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.2221, Accuracy: 0.9315, F1 Micro: 0.9572, F1 Macro: 0.9547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.2055, Accuracy: 0.9353, F1 Micro: 0.9596, F1 Macro: 0.9575\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9353, F1 Micro: 0.9596, F1 Macro: 0.9575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.98       187\n",
      "     machine       0.92      0.96      0.94       175\n",
      "      others       0.91      0.92      0.91       158\n",
      "        part       0.90      0.97      0.94       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.97      0.96      1061\n",
      "   macro avg       0.94      0.97      0.96      1061\n",
      "weighted avg       0.95      0.97      0.96      1061\n",
      " samples avg       0.95      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5695, Accuracy: 0.6929, F1 Micro: 0.6929, F1 Macro: 0.4093\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4186, Accuracy: 0.8423, F1 Micro: 0.8423, F1 Macro: 0.8036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3502, Accuracy: 0.8921, F1 Micro: 0.8921, F1 Macro: 0.8691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1551, Accuracy: 0.9129, F1 Micro: 0.9129, F1 Macro: 0.9008\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0794, Accuracy: 0.9378, F1 Micro: 0.9378, F1 Macro: 0.9277\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9212, F1 Micro: 0.9212, F1 Macro: 0.9114\n",
      "Epoch 7/10, Train Loss: 0.1059, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8983\n",
      "Epoch 8/10, Train Loss: 0.0529, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.907\n",
      "Epoch 9/10, Train Loss: 0.0707, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9237\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.922\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9378, F1 Micro: 0.9378, F1 Macro: 0.9277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.92      0.90        74\n",
      "    positive       0.96      0.95      0.95       167\n",
      "\n",
      "    accuracy                           0.94       241\n",
      "   macro avg       0.92      0.93      0.93       241\n",
      "weighted avg       0.94      0.94      0.94       241\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 208: Accuracy: 0.9282, F1 Micro: 0.9282, F1 Macro: 0.8616\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.91      0.96      0.94       167\n",
      "    positive       0.81      0.64      0.71        33\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.78      0.82       216\n",
      "weighted avg       0.89      0.90      0.89       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.91      0.91      0.91       152\n",
      "    positive       0.78      0.77      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.65      0.75        23\n",
      "     neutral       0.90      0.98      0.94       152\n",
      "    positive       0.82      0.68      0.75        41\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.77      0.81       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.92      0.71      0.80        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 86.76627826690674 s\n",
      "Averaged - Iteration 208: Accuracy: 0.8641, F1 Micro: 0.8641, F1 Macro: 0.6154\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 66\n",
      "Sampling duration: 11.75727128982544 seconds\n",
      "New train size: 274\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5892, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5212, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4668, Accuracy: 0.8013, F1 Micro: 0.8881, F1 Macro: 0.8866\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4232, Accuracy: 0.8222, F1 Micro: 0.8977, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3752, Accuracy: 0.8638, F1 Micro: 0.9196, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2988, Accuracy: 0.9182, F1 Micro: 0.95, F1 Macro: 0.9487\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2355, Accuracy: 0.933, F1 Micro: 0.9583, F1 Macro: 0.9556\n",
      "Epoch 8/10, Train Loss: 0.1874, Accuracy: 0.9293, F1 Micro: 0.9557, F1 Macro: 0.9525\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1489, Accuracy: 0.9368, F1 Micro: 0.9607, F1 Macro: 0.9581\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1297, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9629\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.92      0.97      0.95       175\n",
      "      others       0.89      0.94      0.91       158\n",
      "        part       0.93      0.97      0.95       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.95      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6186, Accuracy: 0.6709, F1 Micro: 0.6709, F1 Macro: 0.4015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4061, Accuracy: 0.8608, F1 Micro: 0.8608, F1 Macro: 0.8188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2296, Accuracy: 0.9241, F1 Micro: 0.9241, F1 Macro: 0.9146\n",
      "Epoch 4/10, Train Loss: 0.1336, Accuracy: 0.9156, F1 Micro: 0.9156, F1 Macro: 0.9087\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1179, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9377\n",
      "Epoch 6/10, Train Loss: 0.1173, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.92\n",
      "Epoch 7/10, Train Loss: 0.1196, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9241\n",
      "Epoch 8/10, Train Loss: 0.0644, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9112\n",
      "Epoch 9/10, Train Loss: 0.0614, Accuracy: 0.9156, F1 Micro: 0.9156, F1 Macro: 0.9051\n",
      "Epoch 10/10, Train Loss: 0.0347, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9117\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.91      0.92        78\n",
      "    positive       0.96      0.96      0.96       159\n",
      "\n",
      "    accuracy                           0.95       237\n",
      "   macro avg       0.94      0.94      0.94       237\n",
      "weighted avg       0.95      0.95      0.95       237\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 274: Accuracy: 0.9367, F1 Micro: 0.9367, F1 Macro: 0.8816\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.92      0.98      0.95       167\n",
      "    positive       0.83      0.61      0.70        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.80      0.83       216\n",
      "weighted avg       0.90      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.89      0.93      0.91       152\n",
      "    positive       0.82      0.69      0.75        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.82      0.79      0.80       216\n",
      "weighted avg       0.86      0.87      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.74      0.83        23\n",
      "     neutral       0.93      0.98      0.96       152\n",
      "    positive       0.84      0.78      0.81        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.83      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.89      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 92.24170064926147 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5983, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.524, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4703, Accuracy: 0.7932, F1 Micro: 0.8841, F1 Macro: 0.8825\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4166, Accuracy: 0.8356, F1 Micro: 0.9049, F1 Macro: 0.9033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3644, Accuracy: 0.8772, F1 Micro: 0.926, F1 Macro: 0.9232\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2895, Accuracy: 0.9115, F1 Micro: 0.9458, F1 Macro: 0.9442\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.233, Accuracy: 0.9271, F1 Micro: 0.955, F1 Macro: 0.9534\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1853, Accuracy: 0.933, F1 Micro: 0.9584, F1 Macro: 0.9563\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1584, Accuracy: 0.9382, F1 Micro: 0.9616, F1 Macro: 0.9595\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1311, Accuracy: 0.9397, F1 Micro: 0.9625, F1 Macro: 0.9606\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9397, F1 Micro: 0.9625, F1 Macro: 0.9606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.92      0.98      0.95       175\n",
      "      others       0.90      0.93      0.91       158\n",
      "        part       0.92      0.97      0.95       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.98      0.96      1061\n",
      "   macro avg       0.94      0.98      0.96      1061\n",
      "weighted avg       0.95      0.98      0.96      1061\n",
      " samples avg       0.95      0.98      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6032, Accuracy: 0.6864, F1 Micro: 0.6864, F1 Macro: 0.407\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4339, Accuracy: 0.7797, F1 Micro: 0.7797, F1 Macro: 0.6793\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2428, Accuracy: 0.9025, F1 Micro: 0.9025, F1 Macro: 0.8888\n",
      "Epoch 4/10, Train Loss: 0.1424, Accuracy: 0.8814, F1 Micro: 0.8814, F1 Macro: 0.8726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1341, Accuracy: 0.9068, F1 Micro: 0.9068, F1 Macro: 0.8874\n",
      "Epoch 6/10, Train Loss: 0.1551, Accuracy: 0.8983, F1 Micro: 0.8983, F1 Macro: 0.8885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.126, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9183\n",
      "Epoch 8/10, Train Loss: 0.1142, Accuracy: 0.911, F1 Micro: 0.911, F1 Macro: 0.901\n",
      "Epoch 9/10, Train Loss: 0.045, Accuracy: 0.9153, F1 Micro: 0.9153, F1 Macro: 0.9054\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9144\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.93      0.89        74\n",
      "    positive       0.97      0.93      0.95       162\n",
      "\n",
      "    accuracy                           0.93       236\n",
      "   macro avg       0.91      0.93      0.92       236\n",
      "weighted avg       0.93      0.93      0.93       236\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 274: Accuracy: 0.9329, F1 Micro: 0.9329, F1 Macro: 0.8667\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.92      0.98      0.95       167\n",
      "    positive       0.91      0.64      0.75        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.79      0.83       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.75      0.64        12\n",
      "     neutral       0.90      0.93      0.91       152\n",
      "    positive       0.84      0.69      0.76        52\n",
      "\n",
      "    accuracy                           0.86       216\n",
      "   macro avg       0.77      0.79      0.77       216\n",
      "weighted avg       0.86      0.86      0.86       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82        23\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.91      0.73      0.81        41\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.84      0.89       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 95.70647740364075 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6425, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.532, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4854, Accuracy: 0.7932, F1 Micro: 0.8842, F1 Macro: 0.8827\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4454, Accuracy: 0.814, F1 Micro: 0.8941, F1 Macro: 0.8925\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.4003, Accuracy: 0.8631, F1 Micro: 0.9187, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.3229, Accuracy: 0.9137, F1 Micro: 0.9467, F1 Macro: 0.9441\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2598, Accuracy: 0.9353, F1 Micro: 0.9599, F1 Macro: 0.9583\n",
      "Epoch 8/10, Train Loss: 0.2015, Accuracy: 0.9338, F1 Micro: 0.9586, F1 Macro: 0.9566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1727, Accuracy: 0.9382, F1 Micro: 0.9615, F1 Macro: 0.9589\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1433, Accuracy: 0.9457, F1 Micro: 0.9659, F1 Macro: 0.9639\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9457, F1 Micro: 0.9659, F1 Macro: 0.9639\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.93      0.97      0.95       175\n",
      "      others       0.92      0.91      0.91       158\n",
      "        part       0.93      0.98      0.96       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.98      0.97      1061\n",
      "   macro avg       0.95      0.97      0.96      1061\n",
      "weighted avg       0.96      0.98      0.97      1061\n",
      " samples avg       0.96      0.97      0.96      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5916, Accuracy: 0.6865, F1 Micro: 0.6865, F1 Macro: 0.4071\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4415, Accuracy: 0.869, F1 Micro: 0.869, F1 Macro: 0.844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2008, Accuracy: 0.8929, F1 Micro: 0.8929, F1 Macro: 0.883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1486, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9096\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.8968, F1 Micro: 0.8968, F1 Macro: 0.8859\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1082, Accuracy: 0.9206, F1 Micro: 0.9206, F1 Macro: 0.9084\n",
      "Epoch 7/10, Train Loss: 0.0594, Accuracy: 0.9127, F1 Micro: 0.9127, F1 Macro: 0.9012\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9133\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0568, Accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9127\n",
      "Epoch 10/10, Train Loss: 0.0596, Accuracy: 0.9048, F1 Micro: 0.9048, F1 Macro: 0.8947\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9246, F1 Micro: 0.9246, F1 Macro: 0.9127\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.89      0.88        79\n",
      "    positive       0.95      0.94      0.94       173\n",
      "\n",
      "    accuracy                           0.92       252\n",
      "   macro avg       0.91      0.91      0.91       252\n",
      "weighted avg       0.92      0.92      0.92       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 274: Accuracy: 0.9367, F1 Micro: 0.9367, F1 Macro: 0.8768\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.69      0.73        16\n",
      "     neutral       0.93      0.98      0.95       167\n",
      "    positive       0.81      0.67      0.73        33\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.84      0.78      0.81       216\n",
      "weighted avg       0.90      0.91      0.90       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.92      0.90      0.91       152\n",
      "    positive       0.79      0.79      0.79        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.79      0.84      0.81       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.74      0.81        23\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.86      0.76      0.81        41\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 98.54648399353027 s\n",
      "Averaged - Iteration 274: Accuracy: 0.8819, F1 Micro: 0.8819, F1 Macro: 0.6803\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 59\n",
      "Sampling duration: 11.11862564086914 seconds\n",
      "New train size: 333\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5659, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4971, Accuracy: 0.7984, F1 Micro: 0.8866, F1 Macro: 0.8851\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4537, Accuracy: 0.8199, F1 Micro: 0.8973, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3879, Accuracy: 0.8646, F1 Micro: 0.9198, F1 Macro: 0.9191\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3153, Accuracy: 0.9219, F1 Micro: 0.9523, F1 Macro: 0.9505\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2349, Accuracy: 0.9427, F1 Micro: 0.9643, F1 Macro: 0.9625\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1805, Accuracy: 0.9449, F1 Micro: 0.966, F1 Macro: 0.9645\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1401, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9669\n",
      "Epoch 9/10, Train Loss: 0.115, Accuracy: 0.9464, F1 Micro: 0.9665, F1 Macro: 0.9643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0931, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9704\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.90      0.95      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5687, Accuracy: 0.6792, F1 Micro: 0.6792, F1 Macro: 0.4166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4242, Accuracy: 0.9083, F1 Micro: 0.9083, F1 Macro: 0.8993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1868, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.9156\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1338, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.9166\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1358, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.9161\n",
      "Epoch 6/10, Train Loss: 0.0905, Accuracy: 0.9208, F1 Micro: 0.9208, F1 Macro: 0.9117\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.075, Accuracy: 0.9292, F1 Micro: 0.9292, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0511, Accuracy: 0.9292, F1 Micro: 0.9292, F1 Macro: 0.9206\n",
      "Epoch 9/10, Train Loss: 0.0477, Accuracy: 0.925, F1 Micro: 0.925, F1 Macro: 0.914\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0695, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9255\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.94      0.90        78\n",
      "    positive       0.97      0.93      0.95       162\n",
      "\n",
      "    accuracy                           0.93       240\n",
      "   macro avg       0.92      0.93      0.93       240\n",
      "weighted avg       0.94      0.93      0.93       240\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 333: Accuracy: 0.9452, F1 Micro: 0.9452, F1 Macro: 0.889\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.69      0.76        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.67      0.76        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.78      0.82       216\n",
      "weighted avg       0.91      0.92      0.91       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.91      0.95      0.93       152\n",
      "    positive       0.86      0.71      0.78        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.80      0.80      0.80       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.95      0.99      0.97       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 110.83946204185486 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5763, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4959, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4529, Accuracy: 0.8073, F1 Micro: 0.891, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3901, Accuracy: 0.8757, F1 Micro: 0.9258, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.3207, Accuracy: 0.9129, F1 Micro: 0.9467, F1 Macro: 0.9449\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.24, Accuracy: 0.9353, F1 Micro: 0.96, F1 Macro: 0.9587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1896, Accuracy: 0.9449, F1 Micro: 0.9658, F1 Macro: 0.9646\n",
      "Epoch 8/10, Train Loss: 0.145, Accuracy: 0.9449, F1 Micro: 0.9657, F1 Macro: 0.9638\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1193, Accuracy: 0.9479, F1 Micro: 0.9676, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.1001, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9691\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.9691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.95      0.99      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.95      0.99      0.97      1061\n",
      "   macro avg       0.95      0.99      0.97      1061\n",
      "weighted avg       0.95      0.99      0.97      1061\n",
      " samples avg       0.95      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5792, Accuracy: 0.6709, F1 Micro: 0.6709, F1 Macro: 0.4015\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4341, Accuracy: 0.8692, F1 Micro: 0.8692, F1 Macro: 0.8615\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2088, Accuracy: 0.9114, F1 Micro: 0.9114, F1 Macro: 0.9029\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1485, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9095\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.101, Accuracy: 0.9241, F1 Micro: 0.9241, F1 Macro: 0.9166\n",
      "Epoch 6/10, Train Loss: 0.0791, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9106\n",
      "Epoch 7/10, Train Loss: 0.0713, Accuracy: 0.9114, F1 Micro: 0.9114, F1 Macro: 0.8987\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.087, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.921\n",
      "Epoch 9/10, Train Loss: 0.0566, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9089\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0611, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.921\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.921\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.95      0.90        78\n",
      "    positive       0.97      0.92      0.94       159\n",
      "\n",
      "    accuracy                           0.93       237\n",
      "   macro avg       0.91      0.93      0.92       237\n",
      "weighted avg       0.93      0.93      0.93       237\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 333: Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.8831\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.70      0.78        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.91      0.81      0.86       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.75      0.67        12\n",
      "     neutral       0.90      0.95      0.93       152\n",
      "    positive       0.88      0.67      0.76        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.79      0.79      0.78       216\n",
      "weighted avg       0.88      0.88      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.91      0.86        23\n",
      "     neutral       0.94      0.99      0.96       152\n",
      "    positive       0.94      0.71      0.81        41\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.93      0.93      0.92       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.84      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.87      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 105.30902004241943 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6111, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5085, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4713, Accuracy: 0.8021, F1 Micro: 0.8886, F1 Macro: 0.8872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.4117, Accuracy: 0.8571, F1 Micro: 0.9164, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.343, Accuracy: 0.9226, F1 Micro: 0.9525, F1 Macro: 0.951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2619, Accuracy: 0.9397, F1 Micro: 0.9624, F1 Macro: 0.9603\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.2019, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1536, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1293, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9711\n",
      "Epoch 10/10, Train Loss: 0.1043, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9711\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.93      0.99      0.96       175\n",
      "      others       0.92      0.93      0.92       158\n",
      "        part       0.95      1.00      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.98      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.577, Accuracy: 0.6694, F1 Micro: 0.6694, F1 Macro: 0.401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4027, Accuracy: 0.905, F1 Micro: 0.905, F1 Macro: 0.8943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.203, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9078\n",
      "Epoch 4/10, Train Loss: 0.1275, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9018\n",
      "Epoch 5/10, Train Loss: 0.0585, Accuracy: 0.9132, F1 Micro: 0.9132, F1 Macro: 0.901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0903, Accuracy: 0.9215, F1 Micro: 0.9215, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0777, Accuracy: 0.9256, F1 Micro: 0.9256, F1 Macro: 0.9175\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0697, Accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.9218\n",
      "Epoch 9/10, Train Loss: 0.054, Accuracy: 0.9174, F1 Micro: 0.9174, F1 Macro: 0.9093\n",
      "Epoch 10/10, Train Loss: 0.0802, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.8973\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.9218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.93      0.90        80\n",
      "    positive       0.96      0.93      0.95       162\n",
      "\n",
      "    accuracy                           0.93       242\n",
      "   macro avg       0.92      0.93      0.92       242\n",
      "weighted avg       0.93      0.93      0.93       242\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 333: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8892\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.88      0.64      0.74        33\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.91      0.79      0.84       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.92      0.93      0.92       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.84      0.82       216\n",
      "weighted avg       0.89      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.94      1.00      0.97       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.98      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.89      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 101.63640832901001 s\n",
      "Averaged - Iteration 333: Accuracy: 0.8944, F1 Micro: 0.8944, F1 Macro: 0.7217\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 54\n",
      "Sampling duration: 10.278568983078003 seconds\n",
      "New train size: 387\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5649, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4773, Accuracy: 0.8036, F1 Micro: 0.8888, F1 Macro: 0.8872\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4331, Accuracy: 0.8363, F1 Micro: 0.9053, F1 Macro: 0.9045\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3483, Accuracy: 0.8973, F1 Micro: 0.9383, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2671, Accuracy: 0.9435, F1 Micro: 0.9649, F1 Macro: 0.9632\n",
      "Epoch 6/10, Train Loss: 0.1974, Accuracy: 0.9412, F1 Micro: 0.9633, F1 Macro: 0.9611\n",
      "Epoch 7/10, Train Loss: 0.1453, Accuracy: 0.936, F1 Micro: 0.9598, F1 Macro: 0.9566\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1168, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9709\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0979, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Epoch 10/10, Train Loss: 0.0844, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.972\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.98      0.94       158\n",
      "        part       0.96      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5751, Accuracy: 0.7008, F1 Micro: 0.7008, F1 Macro: 0.4984\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3728, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.927\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.162, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9265\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1167, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9365\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0841, Accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9408\n",
      "Epoch 6/10, Train Loss: 0.0803, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9278\n",
      "Epoch 7/10, Train Loss: 0.0818, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9365\n",
      "Epoch 8/10, Train Loss: 0.0909, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9357\n",
      "Epoch 9/10, Train Loss: 0.0578, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9365\n",
      "Epoch 10/10, Train Loss: 0.0616, Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.9365\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9467, F1 Micro: 0.9467, F1 Macro: 0.9408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        81\n",
      "    positive       0.97      0.94      0.96       163\n",
      "\n",
      "    accuracy                           0.95       244\n",
      "   macro avg       0.94      0.95      0.94       244\n",
      "weighted avg       0.95      0.95      0.95       244\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 387: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9076\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.90      0.98      0.94       152\n",
      "    positive       0.92      0.69      0.79        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.81      0.84       216\n",
      "weighted avg       0.90      0.90      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.96      0.97      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 104.7574028968811 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5699, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4866, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4326, Accuracy: 0.8341, F1 Micro: 0.9045, F1 Macro: 0.9034\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3534, Accuracy: 0.901, F1 Micro: 0.9397, F1 Macro: 0.9382\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2723, Accuracy: 0.9382, F1 Micro: 0.9617, F1 Macro: 0.9602\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2119, Accuracy: 0.9472, F1 Micro: 0.967, F1 Macro: 0.9652\n",
      "Epoch 7/10, Train Loss: 0.1572, Accuracy: 0.9457, F1 Micro: 0.966, F1 Macro: 0.9635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1261, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.1052, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0903, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.97      0.93       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6114, Accuracy: 0.6612, F1 Micro: 0.6612, F1 Macro: 0.398\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3841, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8943\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1788, Accuracy: 0.9132, F1 Micro: 0.9132, F1 Macro: 0.9055\n",
      "Epoch 4/10, Train Loss: 0.1363, Accuracy: 0.9091, F1 Micro: 0.9091, F1 Macro: 0.9018\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1054, Accuracy: 0.9298, F1 Micro: 0.9298, F1 Macro: 0.9227\n",
      "Epoch 6/10, Train Loss: 0.11, Accuracy: 0.9008, F1 Micro: 0.9008, F1 Macro: 0.8939\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1131, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 8/10, Train Loss: 0.0951, Accuracy: 0.9256, F1 Micro: 0.9256, F1 Macro: 0.9188\n",
      "Epoch 9/10, Train Loss: 0.0447, Accuracy: 0.9256, F1 Micro: 0.9256, F1 Macro: 0.9184\n",
      "Epoch 10/10, Train Loss: 0.0483, Accuracy: 0.9256, F1 Micro: 0.9256, F1 Macro: 0.9184\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        82\n",
      "    positive       0.97      0.93      0.95       160\n",
      "\n",
      "    accuracy                           0.93       242\n",
      "   macro avg       0.92      0.94      0.93       242\n",
      "weighted avg       0.94      0.93      0.93       242\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 387: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.8996\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.75      0.69        12\n",
      "     neutral       0.90      0.97      0.93       152\n",
      "    positive       0.95      0.67      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.80      0.80       216\n",
      "weighted avg       0.89      0.89      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Total train time: 112.30541515350342 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6012, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4937, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4653, Accuracy: 0.8147, F1 Micro: 0.8947, F1 Macro: 0.8933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3851, Accuracy: 0.8929, F1 Micro: 0.9355, F1 Macro: 0.9344\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.298, Accuracy: 0.9375, F1 Micro: 0.9608, F1 Macro: 0.9591\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2226, Accuracy: 0.9501, F1 Micro: 0.9689, F1 Macro: 0.9673\n",
      "Epoch 7/10, Train Loss: 0.1659, Accuracy: 0.9464, F1 Micro: 0.9664, F1 Macro: 0.9636\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1324, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.11, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9778\n",
      "Epoch 10/10, Train Loss: 0.0949, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9762\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9786, F1 Macro: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5695, Accuracy: 0.6776, F1 Micro: 0.6776, F1 Macro: 0.4577\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3716, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2301, Accuracy: 0.9184, F1 Micro: 0.9184, F1 Macro: 0.9099\n",
      "Epoch 4/10, Train Loss: 0.1233, Accuracy: 0.9061, F1 Micro: 0.9061, F1 Macro: 0.9001\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0985, Accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9319\n",
      "Epoch 6/10, Train Loss: 0.0772, Accuracy: 0.9347, F1 Micro: 0.9347, F1 Macro: 0.9275\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9189\n",
      "Epoch 8/10, Train Loss: 0.0709, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.916\n",
      "Epoch 9/10, Train Loss: 0.0524, Accuracy: 0.9224, F1 Micro: 0.9224, F1 Macro: 0.9121\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.9265, F1 Micro: 0.9265, F1 Macro: 0.9194\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9388, F1 Micro: 0.9388, F1 Macro: 0.9319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        83\n",
      "    positive       0.96      0.95      0.95       162\n",
      "\n",
      "    accuracy                           0.94       245\n",
      "   macro avg       0.93      0.93      0.93       245\n",
      "weighted avg       0.94      0.94      0.94       245\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 387: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9096\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.95      0.73      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 106.77348971366882 s\n",
      "Averaged - Iteration 387: Accuracy: 0.9043, F1 Micro: 0.9043, F1 Macro: 0.7523\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 48\n",
      "Sampling duration: 9.908668041229248 seconds\n",
      "New train size: 435\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.558, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4758, Accuracy: 0.8043, F1 Micro: 0.8897, F1 Macro: 0.8884\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4272, Accuracy: 0.8571, F1 Micro: 0.9159, F1 Macro: 0.9154\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3221, Accuracy: 0.9405, F1 Micro: 0.9634, F1 Macro: 0.962\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2398, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9634\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1765, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.9643\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1329, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1135, Accuracy: 0.9546, F1 Micro: 0.9715, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0861, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0741, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.97\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.91      0.91       158\n",
      "        part       0.97      0.96      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.97      1061\n",
      " samples avg       0.96      0.97      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5851, Accuracy: 0.8308, F1 Micro: 0.8308, F1 Macro: 0.7744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2873, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.909\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1663, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.9136\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.0934, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9209\n",
      "Epoch 5/10, Train Loss: 0.0987, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.099, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9264\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0573, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9309\n",
      "Epoch 8/10, Train Loss: 0.0722, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9259\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9167\n",
      "Epoch 10/10, Train Loss: 0.057, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9213\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.96      0.91        82\n",
      "    positive       0.98      0.93      0.95       178\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.92      0.95      0.93       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 435: Accuracy: 0.9475, F1 Micro: 0.9475, F1 Macro: 0.9046\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.84      0.88       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.83      0.74        12\n",
      "     neutral       0.91      0.91      0.91       152\n",
      "    positive       0.78      0.73      0.75        52\n",
      "\n",
      "    accuracy                           0.87       216\n",
      "   macro avg       0.79      0.83      0.80       216\n",
      "weighted avg       0.87      0.87      0.87       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.97      0.96      0.97       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.92      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      0.99       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       1.00      0.96      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 115.31187796592712 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5662, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4802, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4329, Accuracy: 0.8527, F1 Micro: 0.9137, F1 Macro: 0.9126\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3389, Accuracy: 0.9115, F1 Micro: 0.9456, F1 Macro: 0.9434\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2585, Accuracy: 0.9397, F1 Micro: 0.9628, F1 Macro: 0.9614\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1896, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9673\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1454, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9706\n",
      "Epoch 8/10, Train Loss: 0.122, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9682\n",
      "Epoch 9/10, Train Loss: 0.0947, Accuracy: 0.9516, F1 Micro: 0.9697, F1 Macro: 0.9681\n",
      "Epoch 10/10, Train Loss: 0.0776, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9694\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.97      1.00      0.98       192\n",
      "     service       0.97      1.00      0.99       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.99      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5515, Accuracy: 0.7792, F1 Micro: 0.7792, F1 Macro: 0.697\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3206, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9094\n",
      "Epoch 3/10, Train Loss: 0.1621, Accuracy: 0.9125, F1 Micro: 0.9125, F1 Macro: 0.9036\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1363, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9299\n",
      "Epoch 5/10, Train Loss: 0.092, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9307\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9263\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.064, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9303\n",
      "Epoch 10/10, Train Loss: 0.0254, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9255\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9375, F1 Micro: 0.9375, F1 Macro: 0.9303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        79\n",
      "    positive       0.97      0.94      0.95       161\n",
      "\n",
      "    accuracy                           0.94       240\n",
      "   macro avg       0.92      0.94      0.93       240\n",
      "weighted avg       0.94      0.94      0.94       240\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 435: Accuracy: 0.946, F1 Micro: 0.946, F1 Macro: 0.8893\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.93      0.99      0.96       167\n",
      "    positive       0.92      0.67      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.93      0.82      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.90      0.96      0.93       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.85      0.81      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.96      0.86        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.89      0.80      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.88      0.91      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.77      0.87        13\n",
      "     neutral       0.97      1.00      0.98       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.97      0.84      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.71      0.83        14\n",
      "     neutral       0.97      1.00      0.99       185\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.85      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Total train time: 112.39187836647034 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5945, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4885, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4564, Accuracy: 0.8341, F1 Micro: 0.9041, F1 Macro: 0.9031\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3593, Accuracy: 0.9174, F1 Micro: 0.949, F1 Macro: 0.9469\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2757, Accuracy: 0.942, F1 Micro: 0.9638, F1 Macro: 0.9623\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.2034, Accuracy: 0.9561, F1 Micro: 0.9725, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.157, Accuracy: 0.9643, F1 Micro: 0.9778, F1 Macro: 0.9769\n",
      "Epoch 8/10, Train Loss: 0.1298, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9705\n",
      "Epoch 9/10, Train Loss: 0.1002, Accuracy: 0.9576, F1 Micro: 0.9733, F1 Macro: 0.9714\n",
      "Epoch 10/10, Train Loss: 0.0827, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9732\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9778, F1 Macro: 0.9769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.90      0.98      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5895, Accuracy: 0.7155, F1 Micro: 0.7155, F1 Macro: 0.5878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3578, Accuracy: 0.9372, F1 Micro: 0.9372, F1 Macro: 0.9317\n",
      "Epoch 3/10, Train Loss: 0.1856, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9148\n",
      "Epoch 4/10, Train Loss: 0.162, Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.9148\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1031, Accuracy: 0.9456, F1 Micro: 0.9456, F1 Macro: 0.9383\n",
      "Epoch 6/10, Train Loss: 0.0933, Accuracy: 0.9372, F1 Micro: 0.9372, F1 Macro: 0.9317\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0973, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0715, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9491\n",
      "Epoch 9/10, Train Loss: 0.057, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9446\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.9498, F1 Micro: 0.9498, F1 Macro: 0.9452\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9491\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.93        81\n",
      "    positive       0.97      0.96      0.96       158\n",
      "\n",
      "    accuracy                           0.95       239\n",
      "   macro avg       0.95      0.95      0.95       239\n",
      "weighted avg       0.95      0.95      0.95       239\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 435: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.912\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.98      1.00      0.99       181\n",
      "    positive       1.00      0.92      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.99      0.94      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.90      0.98      0.94       152\n",
      "    positive       0.93      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.88      0.81      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 110.62642407417297 s\n",
      "Averaged - Iteration 435: Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.7737\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 43\n",
      "Sampling duration: 8.946921586990356 seconds\n",
      "New train size: 478\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4821, Accuracy: 0.8065, F1 Micro: 0.8908, F1 Macro: 0.8895\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4238, Accuracy: 0.8661, F1 Micro: 0.9211, F1 Macro: 0.9209\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3158, Accuracy: 0.942, F1 Micro: 0.9642, F1 Macro: 0.9629\n",
      "Epoch 5/10, Train Loss: 0.2226, Accuracy: 0.942, F1 Micro: 0.9639, F1 Macro: 0.9621\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1677, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1347, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1063, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9713\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0934, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9735\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.92      0.92       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.599, Accuracy: 0.8538, F1 Micro: 0.8538, F1 Macro: 0.809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3061, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1365, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1037, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9193\n",
      "Epoch 5/10, Train Loss: 0.0887, Accuracy: 0.9231, F1 Micro: 0.9231, F1 Macro: 0.915\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9195\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0985, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "Epoch 9/10, Train Loss: 0.0676, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.924\n",
      "Epoch 10/10, Train Loss: 0.0715, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9313\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       177\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 478: Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9118\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.91      0.92       152\n",
      "    positive       0.79      0.81      0.80        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.81      0.85      0.83       216\n",
      "weighted avg       0.89      0.88      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 116.90577268600464 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5456, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4828, Accuracy: 0.7984, F1 Micro: 0.8868, F1 Macro: 0.8853\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4118, Accuracy: 0.8802, F1 Micro: 0.9284, F1 Macro: 0.9273\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3132, Accuracy: 0.9315, F1 Micro: 0.9576, F1 Macro: 0.9561\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2238, Accuracy: 0.942, F1 Micro: 0.9637, F1 Macro: 0.9619\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1724, Accuracy: 0.9494, F1 Micro: 0.9685, F1 Macro: 0.967\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1382, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1105, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9728\n",
      "Epoch 9/10, Train Loss: 0.0915, Accuracy: 0.9546, F1 Micro: 0.9716, F1 Macro: 0.9702\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0724, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.96       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.97      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.96      0.99      0.97      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5908, Accuracy: 0.6932, F1 Micro: 0.6932, F1 Macro: 0.4547\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3054, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9292\n",
      "Epoch 3/10, Train Loss: 0.1936, Accuracy: 0.9283, F1 Micro: 0.9283, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1458, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9292\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1126, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9331\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1099, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9466\n",
      "Epoch 7/10, Train Loss: 0.0626, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.942\n",
      "Epoch 8/10, Train Loss: 0.085, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9331\n",
      "Epoch 9/10, Train Loss: 0.081, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.942\n",
      "Epoch 10/10, Train Loss: 0.0658, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9171\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9466\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93        81\n",
      "    positive       0.99      0.94      0.96       170\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.96      0.95       251\n",
      "weighted avg       0.96      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 478: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9089\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.82      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.91      0.94      0.93       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.88       216\n",
      "   macro avg       0.83      0.81      0.82       216\n",
      "weighted avg       0.88      0.88      0.88       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 117.23811960220337 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5682, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4942, Accuracy: 0.7961, F1 Micro: 0.8856, F1 Macro: 0.8842\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4372, Accuracy: 0.8616, F1 Micro: 0.9186, F1 Macro: 0.9182\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3301, Accuracy: 0.9278, F1 Micro: 0.955, F1 Macro: 0.9529\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.239, Accuracy: 0.9464, F1 Micro: 0.9664, F1 Macro: 0.9644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1814, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1462, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "Epoch 8/10, Train Loss: 0.1136, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.097, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Epoch 10/10, Train Loss: 0.0763, Accuracy: 0.9598, F1 Micro: 0.9747, F1 Macro: 0.973\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.96      0.99      0.98      1061\n",
      "   macro avg       0.96      0.99      0.98      1061\n",
      "weighted avg       0.96      0.99      0.98      1061\n",
      " samples avg       0.96      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5632, Accuracy: 0.8907, F1 Micro: 0.8907, F1 Macro: 0.8748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3293, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9357\n",
      "Epoch 3/10, Train Loss: 0.1447, Accuracy: 0.9271, F1 Micro: 0.9271, F1 Macro: 0.9178\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1268, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9401\n",
      "Epoch 5/10, Train Loss: 0.0858, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9376\n",
      "Epoch 6/10, Train Loss: 0.0823, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9365\n",
      "Epoch 7/10, Train Loss: 0.0946, Accuracy: 0.9433, F1 Micro: 0.9433, F1 Macro: 0.9361\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9474, F1 Micro: 0.9474, F1 Macro: 0.9412\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0487, Accuracy: 0.9514, F1 Micro: 0.9514, F1 Macro: 0.9456\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0587, Accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9508\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9555, F1 Micro: 0.9555, F1 Macro: 0.9508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.98      0.94        82\n",
      "    positive       0.99      0.95      0.97       165\n",
      "\n",
      "    accuracy                           0.96       247\n",
      "   macro avg       0.94      0.96      0.95       247\n",
      "weighted avg       0.96      0.96      0.96       247\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 478: Accuracy: 0.956, F1 Micro: 0.956, F1 Macro: 0.9168\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.96      0.70      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.83      0.87       216\n",
      "weighted avg       0.94      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.91      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 113.8265471458435 s\n",
      "Averaged - Iteration 478: Accuracy: 0.9162, F1 Micro: 0.9162, F1 Macro: 0.791\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 39\n",
      "Sampling duration: 8.348449230194092 seconds\n",
      "New train size: 517\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.477, Accuracy: 0.8162, F1 Micro: 0.8957, F1 Macro: 0.8946\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3816, Accuracy: 0.8981, F1 Micro: 0.9385, F1 Macro: 0.9376\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2753, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.9635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1864, Accuracy: 0.9524, F1 Micro: 0.9704, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1488, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1171, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.973\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0908, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9742\n",
      "Epoch 9/10, Train Loss: 0.074, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9717\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0618, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.97      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5615, Accuracy: 0.8833, F1 Micro: 0.8833, F1 Macro: 0.8718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2644, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9145\n",
      "Epoch 3/10, Train Loss: 0.1965, Accuracy: 0.8988, F1 Micro: 0.8988, F1 Macro: 0.891\n",
      "Epoch 4/10, Train Loss: 0.154, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1056, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9301\n",
      "Epoch 6/10, Train Loss: 0.0715, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9141\n",
      "Epoch 7/10, Train Loss: 0.0843, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9181\n",
      "Epoch 8/10, Train Loss: 0.0949, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9268\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0548, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9305\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0656, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.98      0.91        83\n",
      "    positive       0.99      0.92      0.95       174\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.92      0.95      0.93       257\n",
      "weighted avg       0.94      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 517: Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9078\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.99      0.96       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.92      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.84      0.88       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 120.79951286315918 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5433, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4799, Accuracy: 0.8103, F1 Micro: 0.8925, F1 Macro: 0.8912\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3678, Accuracy: 0.9062, F1 Micro: 0.9427, F1 Macro: 0.9411\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2833, Accuracy: 0.936, F1 Micro: 0.9598, F1 Macro: 0.9573\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1941, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9676\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1531, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9705\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1231, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0979, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0781, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Epoch 10/10, Train Loss: 0.0627, Accuracy: 0.9591, F1 Micro: 0.9743, F1 Macro: 0.9728\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.96      0.98      0.97      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.96      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.6189, Accuracy: 0.6759, F1 Micro: 0.6759, F1 Macro: 0.4033\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3132, Accuracy: 0.8854, F1 Micro: 0.8854, F1 Macro: 0.877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2145, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9295\n",
      "Epoch 4/10, Train Loss: 0.1716, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.9097\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1202, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9419\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "Epoch 7/10, Train Loss: 0.1147, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9415\n",
      "Epoch 8/10, Train Loss: 0.0787, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9415\n",
      "Epoch 9/10, Train Loss: 0.0894, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.933\n",
      "Epoch 10/10, Train Loss: 0.0751, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9334\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        82\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.96      0.95       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 517: Accuracy: 0.9537, F1 Micro: 0.9537, F1 Macro: 0.9108\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.82      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.91      0.94      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.88      0.85      0.87       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.80      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.84      0.87       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 123.36607837677002 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5698, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4951, Accuracy: 0.7976, F1 Micro: 0.8864, F1 Macro: 0.8849\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3996, Accuracy: 0.9003, F1 Micro: 0.9396, F1 Macro: 0.9384\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3007, Accuracy: 0.933, F1 Micro: 0.9581, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2102, Accuracy: 0.9568, F1 Micro: 0.973, F1 Macro: 0.9716\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1655, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.1344, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1059, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "Epoch 9/10, Train Loss: 0.0857, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.97      0.97       158\n",
      "       price       0.97      0.99      0.98       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5552, Accuracy: 0.811, F1 Micro: 0.811, F1 Macro: 0.7519\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2912, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Epoch 3/10, Train Loss: 0.1878, Accuracy: 0.9055, F1 Micro: 0.9055, F1 Macro: 0.8982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1595, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1448, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9385\n",
      "Epoch 6/10, Train Loss: 0.114, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9176\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1128, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9374\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0836, Accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9473\n",
      "Epoch 9/10, Train Loss: 0.043, Accuracy: 0.9449, F1 Micro: 0.9449, F1 Macro: 0.9381\n",
      "Epoch 10/10, Train Loss: 0.0665, Accuracy: 0.9252, F1 Micro: 0.9252, F1 Macro: 0.9181\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9528, F1 Micro: 0.9528, F1 Macro: 0.9473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        83\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       254\n",
      "   macro avg       0.94      0.96      0.95       254\n",
      "weighted avg       0.95      0.95      0.95       254\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 517: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9181\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.85      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.89      0.83      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.77      0.80        13\n",
      "     neutral       0.97      0.99      0.98       186\n",
      "    positive       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 120.37279796600342 s\n",
      "Averaged - Iteration 517: Accuracy: 0.9205, F1 Micro: 0.9205, F1 Macro: 0.8045\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 540\n",
      "Acquired samples: 23\n",
      "Sampling duration: 7.377516031265259 seconds\n",
      "New train size: 540\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5471, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4894, Accuracy: 0.817, F1 Micro: 0.8959, F1 Macro: 0.8947\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3988, Accuracy: 0.8914, F1 Micro: 0.9348, F1 Macro: 0.9338\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2816, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9682\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1995, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9697\n",
      "Epoch 6/10, Train Loss: 0.1511, Accuracy: 0.9509, F1 Micro: 0.9694, F1 Macro: 0.9677\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1151, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0937, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9745\n",
      "Epoch 9/10, Train Loss: 0.079, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0654, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.94      0.94       158\n",
      "        part       0.96      0.98      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5257, Accuracy: 0.8919, F1 Micro: 0.8919, F1 Macro: 0.8808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2779, Accuracy: 0.9112, F1 Micro: 0.9112, F1 Macro: 0.9028\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1775, Accuracy: 0.9305, F1 Micro: 0.9305, F1 Macro: 0.923\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1677, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9356\n",
      "Epoch 5/10, Train Loss: 0.0983, Accuracy: 0.9344, F1 Micro: 0.9344, F1 Macro: 0.9258\n",
      "Epoch 6/10, Train Loss: 0.1118, Accuracy: 0.9073, F1 Micro: 0.9073, F1 Macro: 0.8993\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0842, Accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9341\n",
      "Epoch 8/10, Train Loss: 0.1107, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9312\n",
      "Epoch 9/10, Train Loss: 0.0674, Accuracy: 0.9266, F1 Micro: 0.9266, F1 Macro: 0.9171\n",
      "Epoch 10/10, Train Loss: 0.0389, Accuracy: 0.9382, F1 Micro: 0.9382, F1 Macro: 0.9304\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9421, F1 Micro: 0.9421, F1 Macro: 0.9341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        83\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.94       259\n",
      "   macro avg       0.93      0.94      0.93       259\n",
      "weighted avg       0.94      0.94      0.94       259\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 540: Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.912\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.75      0.80        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.82      0.85       216\n",
      "weighted avg       0.92      0.93      0.92       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.94      0.94       152\n",
      "    positive       0.84      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.87      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.96      0.98      0.97       152\n",
      "    positive       0.92      0.80      0.86        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.90      0.91       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 121.93527913093567 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5574, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4907, Accuracy: 0.8006, F1 Micro: 0.8879, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3888, Accuracy: 0.9107, F1 Micro: 0.9457, F1 Macro: 0.9443\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2789, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2004, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9687\n",
      "Epoch 6/10, Train Loss: 0.154, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1203, Accuracy: 0.9546, F1 Micro: 0.9717, F1 Macro: 0.9701\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0989, Accuracy: 0.9621, F1 Micro: 0.9763, F1 Macro: 0.9751\n",
      "Epoch 9/10, Train Loss: 0.0823, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0691, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.90      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5725, Accuracy: 0.7381, F1 Micro: 0.7381, F1 Macro: 0.5883\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3188, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9386\n",
      "Epoch 3/10, Train Loss: 0.1425, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9264\n",
      "Epoch 4/10, Train Loss: 0.1323, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9264\n",
      "Epoch 5/10, Train Loss: 0.1338, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9099\n",
      "Epoch 6/10, Train Loss: 0.1216, Accuracy: 0.9087, F1 Micro: 0.9087, F1 Macro: 0.8904\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1064, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9363\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0747, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "Epoch 9/10, Train Loss: 0.1008, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9264\n",
      "Epoch 10/10, Train Loss: 0.0414, Accuracy: 0.9325, F1 Micro: 0.9325, F1 Macro: 0.9243\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.92        83\n",
      "    positive       0.98      0.94      0.96       169\n",
      "\n",
      "    accuracy                           0.95       252\n",
      "   macro avg       0.94      0.95      0.94       252\n",
      "weighted avg       0.95      0.95      0.95       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 540: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9208\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.85      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.96      0.93       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.82      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      1.00      0.87        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.91      0.90       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 120.90196371078491 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5782, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.5048, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.4205, Accuracy: 0.9048, F1 Micro: 0.9424, F1 Macro: 0.941\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.3096, Accuracy: 0.9412, F1 Micro: 0.9634, F1 Macro: 0.9622\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2234, Accuracy: 0.9516, F1 Micro: 0.9699, F1 Macro: 0.9683\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1683, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9741\n",
      "Epoch 7/10, Train Loss: 0.1267, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9725\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1083, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0904, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 10/10, Train Loss: 0.0711, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9755\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5731, Accuracy: 0.8651, F1 Micro: 0.8651, F1 Macro: 0.8386\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2871, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9516\n",
      "Epoch 3/10, Train Loss: 0.167, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9473\n",
      "Epoch 4/10, Train Loss: 0.1365, Accuracy: 0.9524, F1 Micro: 0.9524, F1 Macro: 0.9467\n",
      "Epoch 5/10, Train Loss: 0.1407, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9425\n",
      "Epoch 6/10, Train Loss: 0.0879, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9434\n",
      "Epoch 7/10, Train Loss: 0.1141, Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.9379\n",
      "Epoch 8/10, Train Loss: 0.0876, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9516\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9484, F1 Micro: 0.9484, F1 Macro: 0.9431\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9563, F1 Micro: 0.9563, F1 Macro: 0.9516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        84\n",
      "    positive       0.98      0.95      0.97       168\n",
      "\n",
      "    accuracy                           0.96       252\n",
      "   macro avg       0.95      0.96      0.95       252\n",
      "weighted avg       0.96      0.96      0.96       252\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 540: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9234\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.87      0.79      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.91      0.87        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.94      0.78      0.85        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 118.89657139778137 s\n",
      "Averaged - Iteration 540: Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.8159\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 33\n",
      "Sampling duration: 7.136613368988037 seconds\n",
      "New train size: 573\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5375, Accuracy: 0.7976, F1 Micro: 0.8858, F1 Macro: 0.8841\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4785, Accuracy: 0.8237, F1 Micro: 0.8993, F1 Macro: 0.8982\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3825, Accuracy: 0.9271, F1 Micro: 0.9553, F1 Macro: 0.954\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2587, Accuracy: 0.9435, F1 Micro: 0.9647, F1 Macro: 0.9629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1856, Accuracy: 0.9494, F1 Micro: 0.9684, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1358, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9721\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1084, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Epoch 8/10, Train Loss: 0.0874, Accuracy: 0.9598, F1 Micro: 0.9748, F1 Macro: 0.9736\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0727, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 10/10, Train Loss: 0.0606, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9754\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.95      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5426, Accuracy: 0.7778, F1 Micro: 0.7778, F1 Macro: 0.6745\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2866, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1805, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9483\n",
      "Epoch 4/10, Train Loss: 0.1702, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9344\n",
      "Epoch 5/10, Train Loss: 0.0948, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9435\n",
      "Epoch 6/10, Train Loss: 0.0967, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9441\n",
      "Epoch 7/10, Train Loss: 0.0758, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9215\n",
      "Epoch 8/10, Train Loss: 0.0908, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9348\n",
      "Epoch 9/10, Train Loss: 0.091, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "Epoch 10/10, Train Loss: 0.0828, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9281\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.96      0.93        84\n",
      "    positive       0.98      0.95      0.97       177\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.96      0.95       261\n",
      "weighted avg       0.96      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 573: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9232\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.88      0.85      0.86       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 124.36649417877197 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5433, Accuracy: 0.7909, F1 Micro: 0.8826, F1 Macro: 0.8808\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4829, Accuracy: 0.8118, F1 Micro: 0.8934, F1 Macro: 0.892\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3715, Accuracy: 0.9271, F1 Micro: 0.9551, F1 Macro: 0.9533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.259, Accuracy: 0.9457, F1 Micro: 0.9662, F1 Macro: 0.9644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1914, Accuracy: 0.9487, F1 Micro: 0.968, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1407, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9749\n",
      "Epoch 7/10, Train Loss: 0.1146, Accuracy: 0.9539, F1 Micro: 0.971, F1 Macro: 0.9691\n",
      "Epoch 8/10, Train Loss: 0.0895, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9724\n",
      "Epoch 9/10, Train Loss: 0.0766, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9728\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.94      0.93       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5455, Accuracy: 0.8679, F1 Micro: 0.8679, F1 Macro: 0.8366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2733, Accuracy: 0.8679, F1 Micro: 0.8679, F1 Macro: 0.8597\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2124, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1329, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9397\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9434, F1 Micro: 0.9434, F1 Macro: 0.9352\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9252\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0918, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9445\n",
      "Epoch 8/10, Train Loss: 0.0965, Accuracy: 0.9472, F1 Micro: 0.9472, F1 Macro: 0.9405\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9205\n",
      "Epoch 10/10, Train Loss: 0.0559, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9149\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.96      0.93        84\n",
      "    positive       0.98      0.94      0.96       181\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.94      0.95      0.94       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 573: Accuracy: 0.9576, F1 Micro: 0.9576, F1 Macro: 0.9239\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.97      0.98      0.97       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.90      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.83      0.77      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.96      0.88        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.92      0.91       216\n",
      "weighted avg       0.95      0.94      0.94       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 128.64481687545776 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5623, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4879, Accuracy: 0.8147, F1 Micro: 0.8948, F1 Macro: 0.8936\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3988, Accuracy: 0.907, F1 Micro: 0.9436, F1 Macro: 0.9424\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.277, Accuracy: 0.9457, F1 Micro: 0.9661, F1 Macro: 0.9644\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.2056, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1486, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9737\n",
      "Epoch 7/10, Train Loss: 0.1211, Accuracy: 0.9531, F1 Micro: 0.9704, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0972, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9765\n",
      "Epoch 9/10, Train Loss: 0.0839, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0702, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.96      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5588, Accuracy: 0.8706, F1 Micro: 0.8706, F1 Macro: 0.8423\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.28, Accuracy: 0.898, F1 Micro: 0.898, F1 Macro: 0.891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1919, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9313\n",
      "Epoch 4/10, Train Loss: 0.1591, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9109\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1158, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9336\n",
      "Epoch 6/10, Train Loss: 0.1135, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0898, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9477\n",
      "Epoch 8/10, Train Loss: 0.0755, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9396\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0646, Accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9519\n",
      "Epoch 10/10, Train Loss: 0.0865, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9252\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9569, F1 Micro: 0.9569, F1 Macro: 0.9519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        84\n",
      "    positive       0.98      0.95      0.97       171\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.95      0.96      0.95       255\n",
      "weighted avg       0.96      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 573: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9263\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.06853604316711 s\n",
      "Averaged - Iteration 573: Accuracy: 0.9273, F1 Micro: 0.9273, F1 Macro: 0.8258\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 30\n",
      "Sampling duration: 6.660437107086182 seconds\n",
      "New train size: 603\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5421, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4829, Accuracy: 0.8311, F1 Micro: 0.9033, F1 Macro: 0.9024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3668, Accuracy: 0.9293, F1 Micro: 0.9569, F1 Macro: 0.956\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2455, Accuracy: 0.9479, F1 Micro: 0.9674, F1 Macro: 0.966\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1688, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9732\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1342, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Epoch 7/10, Train Loss: 0.1053, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Epoch 8/10, Train Loss: 0.0807, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9752\n",
      "Epoch 9/10, Train Loss: 0.0694, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.062, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.97      0.98      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5057, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9157\n",
      "Epoch 2/10, Train Loss: 0.2253, Accuracy: 0.8911, F1 Micro: 0.8911, F1 Macro: 0.8836\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1738, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1294, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Epoch 5/10, Train Loss: 0.0888, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9246\n",
      "Epoch 6/10, Train Loss: 0.0963, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9339\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0763, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.944\n",
      "Epoch 8/10, Train Loss: 0.0362, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9398\n",
      "Epoch 9/10, Train Loss: 0.0529, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9398\n",
      "Epoch 10/10, Train Loss: 0.0549, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9234\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        84\n",
      "    positive       0.99      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.93      0.96      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 603: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9194\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.81      0.81        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.84      0.86       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.87      0.85      0.86       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.97       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      0.98      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 128.53247928619385 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5536, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4847, Accuracy: 0.8155, F1 Micro: 0.8951, F1 Macro: 0.8937\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3588, Accuracy: 0.9204, F1 Micro: 0.9511, F1 Macro: 0.9496\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2465, Accuracy: 0.9479, F1 Micro: 0.9677, F1 Macro: 0.9664\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1783, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1391, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9747\n",
      "Epoch 7/10, Train Loss: 0.1083, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0826, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0716, Accuracy: 0.9621, F1 Micro: 0.9761, F1 Macro: 0.9747\n",
      "Epoch 10/10, Train Loss: 0.0608, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5252, Accuracy: 0.8588, F1 Micro: 0.8588, F1 Macro: 0.8251\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2498, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Epoch 3/10, Train Loss: 0.188, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9124\n",
      "Epoch 4/10, Train Loss: 0.1407, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9131\n",
      "Epoch 5/10, Train Loss: 0.1436, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9238\n",
      "Epoch 6/10, Train Loss: 0.1023, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9092\n",
      "Epoch 7/10, Train Loss: 0.0891, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9018\n",
      "Epoch 8/10, Train Loss: 0.1284, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0996, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "Epoch 10/10, Train Loss: 0.0718, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       176\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.93      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 603: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9257\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.85      0.79      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 130.000638961792 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5685, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4969, Accuracy: 0.8103, F1 Micro: 0.8925, F1 Macro: 0.891\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3754, Accuracy: 0.9263, F1 Micro: 0.9545, F1 Macro: 0.9533\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2611, Accuracy: 0.9449, F1 Micro: 0.9655, F1 Macro: 0.964\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1858, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9755\n",
      "Epoch 6/10, Train Loss: 0.1485, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1142, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "Epoch 8/10, Train Loss: 0.0933, Accuracy: 0.9628, F1 Micro: 0.9764, F1 Macro: 0.9744\n",
      "Epoch 9/10, Train Loss: 0.0794, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0675, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5225, Accuracy: 0.9, F1 Micro: 0.9, F1 Macro: 0.8857\n",
      "Epoch 2/10, Train Loss: 0.221, Accuracy: 0.8808, F1 Micro: 0.8808, F1 Macro: 0.8738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1898, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1432, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1104, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9358\n",
      "Epoch 6/10, Train Loss: 0.1184, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9223\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0871, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9449\n",
      "Epoch 8/10, Train Loss: 0.0459, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9327\n",
      "Epoch 9/10, Train Loss: 0.049, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9338\n",
      "Epoch 10/10, Train Loss: 0.0544, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9272\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.98      0.93        86\n",
      "    positive       0.99      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.96      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 603: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9266\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 131.10902214050293 s\n",
      "Averaged - Iteration 603: Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.834\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 27\n",
      "Sampling duration: 6.152645111083984 seconds\n",
      "New train size: 630\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5333, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.458, Accuracy: 0.84, F1 Micro: 0.9074, F1 Macro: 0.9067\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3481, Accuracy: 0.9315, F1 Micro: 0.9574, F1 Macro: 0.9551\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2315, Accuracy: 0.9531, F1 Micro: 0.9708, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1575, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9733\n",
      "Epoch 6/10, Train Loss: 0.1207, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1047, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Epoch 8/10, Train Loss: 0.0806, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "Epoch 10/10, Train Loss: 0.0552, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9812, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.97      0.92      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5121, Accuracy: 0.9077, F1 Micro: 0.9077, F1 Macro: 0.8911\n",
      "Epoch 2/10, Train Loss: 0.2251, Accuracy: 0.9041, F1 Micro: 0.9041, F1 Macro: 0.8951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1761, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9251\n",
      "Epoch 4/10, Train Loss: 0.1057, Accuracy: 0.9151, F1 Micro: 0.9151, F1 Macro: 0.9075\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1279, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.9291\n",
      "Epoch 6/10, Train Loss: 0.0993, Accuracy: 0.9336, F1 Micro: 0.9336, F1 Macro: 0.9267\n",
      "Epoch 7/10, Train Loss: 0.0965, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9151\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0492, Accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9448\n",
      "Epoch 9/10, Train Loss: 0.076, Accuracy: 0.941, F1 Micro: 0.941, F1 Macro: 0.9338\n",
      "Epoch 10/10, Train Loss: 0.0814, Accuracy: 0.9483, F1 Micro: 0.9483, F1 Macro: 0.9414\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.952, F1 Micro: 0.952, F1 Macro: 0.9448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.92      0.92        87\n",
      "    positive       0.96      0.97      0.96       184\n",
      "\n",
      "    accuracy                           0.95       271\n",
      "   macro avg       0.95      0.94      0.94       271\n",
      "weighted avg       0.95      0.95      0.95       271\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 630: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9261\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        12\n",
      "     neutral       0.97      0.92      0.94       152\n",
      "    positive       0.80      0.90      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 133.04726338386536 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5392, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4551, Accuracy: 0.8557, F1 Micro: 0.9148, F1 Macro: 0.9135\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3359, Accuracy: 0.9323, F1 Micro: 0.958, F1 Macro: 0.9562\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2268, Accuracy: 0.9509, F1 Micro: 0.9696, F1 Macro: 0.9684\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1627, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1263, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1036, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0796, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9763\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.96      0.99      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5463, Accuracy: 0.7093, F1 Micro: 0.7093, F1 Macro: 0.5313\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3161, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.924\n",
      "Epoch 3/10, Train Loss: 0.1861, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9119\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.13, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9348\n",
      "Epoch 5/10, Train Loss: 0.1351, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0937, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9393\n",
      "Epoch 7/10, Train Loss: 0.0698, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0666, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "Epoch 9/10, Train Loss: 0.0708, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.908\n",
      "Epoch 10/10, Train Loss: 0.0777, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9169\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.94      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 630: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9215\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.96      0.99      0.97       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.92      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.90      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 138.53961443901062 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5568, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4735, Accuracy: 0.8244, F1 Micro: 0.8987, F1 Macro: 0.8974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3713, Accuracy: 0.9308, F1 Micro: 0.9569, F1 Macro: 0.9552\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2471, Accuracy: 0.9561, F1 Micro: 0.9726, F1 Macro: 0.9712\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1755, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1352, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9743\n",
      "Epoch 7/10, Train Loss: 0.1126, Accuracy: 0.9576, F1 Micro: 0.9731, F1 Macro: 0.9706\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "Epoch 9/10, Train Loss: 0.0726, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0621, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.91      0.95      0.93       158\n",
      "        part       0.96      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5539, Accuracy: 0.8482, F1 Micro: 0.8482, F1 Macro: 0.8171\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2757, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.919\n",
      "Epoch 3/10, Train Loss: 0.1564, Accuracy: 0.8872, F1 Micro: 0.8872, F1 Macro: 0.8807\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1305, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1258, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9354\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9074\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9309\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0849, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.936\n",
      "Epoch 9/10, Train Loss: 0.1005, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9242\n",
      "Epoch 10/10, Train Loss: 0.0953, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9194\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.97      0.92        86\n",
      "    positive       0.98      0.93      0.95       171\n",
      "\n",
      "    accuracy                           0.94       257\n",
      "   macro avg       0.93      0.95      0.94       257\n",
      "weighted avg       0.95      0.94      0.94       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 630: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9184\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.89      0.75      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.96      0.99      0.98       152\n",
      "    positive       0.97      0.76      0.85        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 136.88374614715576 s\n",
      "Averaged - Iteration 630: Accuracy: 0.9323, F1 Micro: 0.9323, F1 Macro: 0.8407\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 648\n",
      "Acquired samples: 18\n",
      "Sampling duration: 5.56518816947937 seconds\n",
      "New train size: 648\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5457, Accuracy: 0.8036, F1 Micro: 0.8892, F1 Macro: 0.8878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4632, Accuracy: 0.8371, F1 Micro: 0.9064, F1 Macro: 0.9055\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3392, Accuracy: 0.9457, F1 Micro: 0.9664, F1 Macro: 0.9651\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2357, Accuracy: 0.9539, F1 Micro: 0.9713, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1593, Accuracy: 0.9583, F1 Micro: 0.974, F1 Macro: 0.9725\n",
      "Epoch 6/10, Train Loss: 0.1228, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.972\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0951, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9758\n",
      "Epoch 8/10, Train Loss: 0.076, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9749\n",
      "Epoch 9/10, Train Loss: 0.0631, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0542, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.94      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.97      0.97      0.97       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5271, Accuracy: 0.8775, F1 Micro: 0.8775, F1 Macro: 0.8548\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2608, Accuracy: 0.8933, F1 Micro: 0.8933, F1 Macro: 0.8878\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.176, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9307\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1561, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9435\n",
      "Epoch 5/10, Train Loss: 0.1291, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9341\n",
      "Epoch 6/10, Train Loss: 0.1104, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.939\n",
      "Epoch 7/10, Train Loss: 0.0965, Accuracy: 0.9249, F1 Micro: 0.9249, F1 Macro: 0.9194\n",
      "Epoch 8/10, Train Loss: 0.104, Accuracy: 0.917, F1 Micro: 0.917, F1 Macro: 0.903\n",
      "Epoch 9/10, Train Loss: 0.0837, Accuracy: 0.9328, F1 Micro: 0.9328, F1 Macro: 0.9253\n",
      "Epoch 10/10, Train Loss: 0.08, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9383\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.93        86\n",
      "    positive       0.98      0.95      0.96       167\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.95      0.94       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 648: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9207\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.83      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.92      0.85        12\n",
      "     neutral       0.92      0.97      0.95       152\n",
      "    positive       0.90      0.71      0.80        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.87      0.87      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.97      0.97       152\n",
      "    positive       0.92      0.83      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.5549373626709 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5488, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4599, Accuracy: 0.8497, F1 Micro: 0.9129, F1 Macro: 0.9118\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.34, Accuracy: 0.9345, F1 Micro: 0.9593, F1 Macro: 0.9571\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.237, Accuracy: 0.9546, F1 Micro: 0.9719, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1648, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1254, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9756\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0976, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 8/10, Train Loss: 0.0773, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Epoch 9/10, Train Loss: 0.0645, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0541, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5015, Accuracy: 0.9098, F1 Micro: 0.9098, F1 Macro: 0.901\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2441, Accuracy: 0.9216, F1 Micro: 0.9216, F1 Macro: 0.9112\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1542, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9393\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1434, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9428\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1233, Accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "Epoch 6/10, Train Loss: 0.1122, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9109\n",
      "Epoch 7/10, Train Loss: 0.0966, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9336\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.949, F1 Micro: 0.949, F1 Macro: 0.9435\n",
      "Epoch 9/10, Train Loss: 0.0508, Accuracy: 0.9333, F1 Micro: 0.9333, F1 Macro: 0.9234\n",
      "Epoch 10/10, Train Loss: 0.0703, Accuracy: 0.9176, F1 Micro: 0.9176, F1 Macro: 0.9109\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9529, F1 Micro: 0.9529, F1 Macro: 0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        84\n",
      "    positive       0.98      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       255\n",
      "   macro avg       0.94      0.95      0.95       255\n",
      "weighted avg       0.95      0.95      0.95       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 648: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9268\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.86      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.90      0.73      0.81        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 140.51761507987976 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5669, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4682, Accuracy: 0.8326, F1 Micro: 0.9039, F1 Macro: 0.903\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3485, Accuracy: 0.9353, F1 Micro: 0.9599, F1 Macro: 0.958\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2475, Accuracy: 0.9576, F1 Micro: 0.9735, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1717, Accuracy: 0.9591, F1 Micro: 0.9744, F1 Macro: 0.9729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1333, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9791\n",
      "Epoch 7/10, Train Loss: 0.1028, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0839, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9773\n",
      "Epoch 9/10, Train Loss: 0.0714, Accuracy: 0.9591, F1 Micro: 0.974, F1 Macro: 0.9714\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0614, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.528, Accuracy: 0.8794, F1 Micro: 0.8794, F1 Macro: 0.8568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2583, Accuracy: 0.8794, F1 Micro: 0.8794, F1 Macro: 0.8729\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1938, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1568, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9357\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1395, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1141, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Epoch 7/10, Train Loss: 0.0886, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9364\n",
      "Epoch 8/10, Train Loss: 0.0872, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9251\n",
      "Epoch 9/10, Train Loss: 0.0838, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9357\n",
      "Epoch 10/10, Train Loss: 0.0649, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        86\n",
      "    positive       0.97      0.95      0.96       171\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.94      0.95      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 648: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9198\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.70      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.87      0.75      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.83      0.85      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.79      0.88        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 140.60131931304932 s\n",
      "Averaged - Iteration 648: Accuracy: 0.9343, F1 Micro: 0.9343, F1 Macro: 0.8466\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 5.014821290969849 seconds\n",
      "New train size: 673\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5323, Accuracy: 0.7984, F1 Micro: 0.8861, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4596, Accuracy: 0.8534, F1 Micro: 0.9148, F1 Macro: 0.9142\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3156, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9652\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2053, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9707\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1549, Accuracy: 0.9568, F1 Micro: 0.9731, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1169, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0916, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0741, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9759\n",
      "Epoch 10/10, Train Loss: 0.0514, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5246, Accuracy: 0.8846, F1 Micro: 0.8846, F1 Macro: 0.8738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2705, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9079\n",
      "Epoch 3/10, Train Loss: 0.1688, Accuracy: 0.9115, F1 Micro: 0.9115, F1 Macro: 0.9048\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1507, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9287\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1428, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9264\n",
      "Epoch 7/10, Train Loss: 0.1112, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.932\n",
      "Epoch 8/10, Train Loss: 0.0745, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9227\n",
      "Epoch 9/10, Train Loss: 0.0777, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9101\n",
      "Epoch 10/10, Train Loss: 0.082, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.928\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.95      0.92        86\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 673: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9263\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.84      0.83       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 140.55552315711975 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5379, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4464, Accuracy: 0.8884, F1 Micro: 0.9331, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3027, Accuracy: 0.9435, F1 Micro: 0.9648, F1 Macro: 0.9629\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2052, Accuracy: 0.9524, F1 Micro: 0.9702, F1 Macro: 0.9685\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1547, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9762\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1169, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0755, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9787\n",
      "Epoch 9/10, Train Loss: 0.0602, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9768\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0528, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5079, Accuracy: 0.9004, F1 Micro: 0.9004, F1 Macro: 0.8879\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2672, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.931\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1951, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9366\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1717, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "Epoch 5/10, Train Loss: 0.1232, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1294, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9435\n",
      "Epoch 7/10, Train Loss: 0.0888, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9403\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0661, Accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "Epoch 9/10, Train Loss: 0.064, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9233\n",
      "Epoch 10/10, Train Loss: 0.0568, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9215\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9502, F1 Micro: 0.9502, F1 Macro: 0.9438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.92        85\n",
      "    positive       0.97      0.95      0.96       176\n",
      "\n",
      "    accuracy                           0.95       261\n",
      "   macro avg       0.94      0.95      0.94       261\n",
      "weighted avg       0.95      0.95      0.95       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 673: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9211\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.84      0.83      0.83        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.86      0.84      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 144.86812615394592 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5575, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.468, Accuracy: 0.8586, F1 Micro: 0.9175, F1 Macro: 0.9163\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.324, Accuracy: 0.9375, F1 Micro: 0.9611, F1 Macro: 0.9587\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2176, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9738\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1675, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1261, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 7/10, Train Loss: 0.0996, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0803, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0659, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "Epoch 10/10, Train Loss: 0.0584, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5403, Accuracy: 0.876, F1 Micro: 0.876, F1 Macro: 0.8655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.289, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9358\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2004, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 4/10, Train Loss: 0.1641, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9324\n",
      "Epoch 5/10, Train Loss: 0.1346, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9163\n",
      "Epoch 6/10, Train Loss: 0.0663, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9324\n",
      "Epoch 7/10, Train Loss: 0.1214, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9324\n",
      "Epoch 8/10, Train Loss: 0.0778, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9243\n",
      "Epoch 9/10, Train Loss: 0.0835, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.931\n",
      "Epoch 10/10, Train Loss: 0.0762, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9232\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.93      0.91        86\n",
      "    positive       0.96      0.95      0.96       172\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.93      0.94      0.94       258\n",
      "weighted avg       0.94      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 673: Accuracy: 0.9606, F1 Micro: 0.9606, F1 Macro: 0.9184\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.93      0.76      0.83        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.88      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.87      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.92      0.85      0.89        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.93      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.86      0.89        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 138.308336019516 s\n",
      "Averaged - Iteration 673: Accuracy: 0.936, F1 Micro: 0.936, F1 Macro: 0.8516\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.980448484420776 seconds\n",
      "New train size: 698\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.8013, F1 Micro: 0.8879, F1 Macro: 0.8864\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4518, Accuracy: 0.8683, F1 Micro: 0.9227, F1 Macro: 0.9224\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.31, Accuracy: 0.9509, F1 Micro: 0.9695, F1 Macro: 0.9681\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2017, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1421, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1115, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0825, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9778\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0584, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0495, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.95      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5168, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9096\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2539, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Epoch 3/10, Train Loss: 0.175, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9188\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1467, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1239, Accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9571\n",
      "Epoch 6/10, Train Loss: 0.0904, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9338\n",
      "Epoch 7/10, Train Loss: 0.0974, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Epoch 8/10, Train Loss: 0.0705, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "Epoch 9/10, Train Loss: 0.0719, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9364\n",
      "Epoch 10/10, Train Loss: 0.0755, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9443\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9615, F1 Micro: 0.9615, F1 Macro: 0.9571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94        85\n",
      "    positive       0.99      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.96       260\n",
      "   macro avg       0.95      0.97      0.96       260\n",
      "weighted avg       0.96      0.96      0.96       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 698: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9342\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.95      0.94       152\n",
      "    positive       0.83      0.75      0.79        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.84      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 141.38719964027405 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5407, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4528, Accuracy: 0.869, F1 Micro: 0.923, F1 Macro: 0.9218\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3188, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9656\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2065, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9759\n",
      "Epoch 5/10, Train Loss: 0.1507, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Epoch 6/10, Train Loss: 0.1161, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0892, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.074, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0603, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.979\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0507, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9816\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.468, Accuracy: 0.8992, F1 Micro: 0.8992, F1 Macro: 0.8896\n",
      "Epoch 2/10, Train Loss: 0.2615, Accuracy: 0.8837, F1 Micro: 0.8837, F1 Macro: 0.8766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2041, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9444\n",
      "Epoch 4/10, Train Loss: 0.138, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Epoch 5/10, Train Loss: 0.1445, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9185\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1061, Accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9561\n",
      "Epoch 7/10, Train Loss: 0.1208, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9444\n",
      "Epoch 8/10, Train Loss: 0.1132, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9519\n",
      "Epoch 9/10, Train Loss: 0.1009, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.928\n",
      "Epoch 10/10, Train Loss: 0.0679, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.928\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9612, F1 Micro: 0.9612, F1 Macro: 0.9561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.94      0.94        85\n",
      "    positive       0.97      0.97      0.97       173\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.96      0.96      0.96       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 698: Accuracy: 0.966, F1 Micro: 0.966, F1 Macro: 0.9341\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.99      0.98       167\n",
      "    positive       0.93      0.82      0.87        33\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.90      0.92       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.90      0.83      0.86       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 137.50750374794006 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5622, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.47, Accuracy: 0.8482, F1 Micro: 0.9121, F1 Macro: 0.9108\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3329, Accuracy: 0.939, F1 Micro: 0.9618, F1 Macro: 0.9601\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2223, Accuracy: 0.9621, F1 Micro: 0.9764, F1 Macro: 0.9753\n",
      "Epoch 5/10, Train Loss: 0.1568, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9749\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1227, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0788, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9794\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0664, Accuracy: 0.9702, F1 Micro: 0.9814, F1 Macro: 0.9804\n",
      "Epoch 10/10, Train Loss: 0.0559, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9814, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.89      0.99      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.486, Accuracy: 0.9124, F1 Micro: 0.9124, F1 Macro: 0.9004\n",
      "Epoch 2/10, Train Loss: 0.2291, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9024\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1783, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9397\n",
      "Epoch 4/10, Train Loss: 0.1595, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9319\n",
      "Epoch 5/10, Train Loss: 0.1233, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1151, Accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9469\n",
      "Epoch 7/10, Train Loss: 0.0743, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9187\n",
      "Epoch 8/10, Train Loss: 0.0701, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9187\n",
      "Epoch 9/10, Train Loss: 0.0628, Accuracy: 0.9243, F1 Micro: 0.9243, F1 Macro: 0.9187\n",
      "Epoch 10/10, Train Loss: 0.0424, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9105\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9522, F1 Micro: 0.9522, F1 Macro: 0.9469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        86\n",
      "    positive       0.96      0.96      0.96       165\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.95      0.95      0.95       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 698: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.89      0.99      0.94       152\n",
      "    positive       0.95      0.67      0.79        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.92      0.83      0.86       216\n",
      "weighted avg       0.91      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 135.78484654426575 s\n",
      "Averaged - Iteration 698: Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.8565\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 4.141299724578857 seconds\n",
      "New train size: 723\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5414, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4387, Accuracy: 0.8869, F1 Micro: 0.9323, F1 Macro: 0.9316\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3027, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9693\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1948, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9733\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1348, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1103, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.9763\n",
      "Epoch 8/10, Train Loss: 0.0639, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 9/10, Train Loss: 0.0573, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.91      0.97      0.94       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4971, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.942\n",
      "Epoch 2/10, Train Loss: 0.2579, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9388\n",
      "Epoch 3/10, Train Loss: 0.1607, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9349\n",
      "Epoch 4/10, Train Loss: 0.1483, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9391\n",
      "Epoch 5/10, Train Loss: 0.1114, Accuracy: 0.9402, F1 Micro: 0.9402, F1 Macro: 0.9342\n",
      "Epoch 6/10, Train Loss: 0.1031, Accuracy: 0.9442, F1 Micro: 0.9442, F1 Macro: 0.9388\n",
      "Epoch 7/10, Train Loss: 0.0942, Accuracy: 0.9203, F1 Micro: 0.9203, F1 Macro: 0.9146\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0881, Accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9427\n",
      "Epoch 9/10, Train Loss: 0.0619, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9284\n",
      "Epoch 10/10, Train Loss: 0.0436, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.9288\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9482, F1 Micro: 0.9482, F1 Macro: 0.9427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.95      0.92        84\n",
      "    positive       0.98      0.95      0.96       167\n",
      "\n",
      "    accuracy                           0.95       251\n",
      "   macro avg       0.94      0.95      0.94       251\n",
      "weighted avg       0.95      0.95      0.95       251\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 723: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9183\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.94      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.84      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.97      0.94       152\n",
      "    positive       0.91      0.75      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.88      0.82      0.85       216\n",
      "weighted avg       0.91      0.91      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.96      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 132.47908878326416 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5492, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4405, Accuracy: 0.8921, F1 Micro: 0.9346, F1 Macro: 0.9334\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3133, Accuracy: 0.9457, F1 Micro: 0.9663, F1 Macro: 0.965\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1992, Accuracy: 0.9598, F1 Micro: 0.975, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1397, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9748\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1111, Accuracy: 0.9673, F1 Micro: 0.9796, F1 Macro: 0.9787\n",
      "Epoch 7/10, Train Loss: 0.0867, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9773\n",
      "Epoch 8/10, Train Loss: 0.0664, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9767\n",
      "Epoch 9/10, Train Loss: 0.0601, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0486, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4902, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9157\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2437, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9388\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1923, Accuracy: 0.965, F1 Micro: 0.965, F1 Macro: 0.9608\n",
      "Epoch 4/10, Train Loss: 0.1376, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9437\n",
      "Epoch 5/10, Train Loss: 0.1435, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9611, F1 Micro: 0.9611, F1 Macro: 0.9563\n",
      "Epoch 7/10, Train Loss: 0.0732, Accuracy: 0.9222, F1 Micro: 0.9222, F1 Macro: 0.9153\n",
      "Epoch 8/10, Train Loss: 0.0482, Accuracy: 0.93, F1 Micro: 0.93, F1 Macro: 0.9222\n",
      "Epoch 9/10, Train Loss: 0.0639, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9271\n",
      "Epoch 10/10, Train Loss: 0.0411, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9147\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.965, F1 Micro: 0.965, F1 Macro: 0.9608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95        84\n",
      "    positive       0.99      0.96      0.97       173\n",
      "\n",
      "    accuracy                           0.96       257\n",
      "   macro avg       0.95      0.97      0.96       257\n",
      "weighted avg       0.97      0.96      0.97       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 723: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9337\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.75      0.75        12\n",
      "     neutral       0.92      0.96      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.85      0.83      0.84       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 138.73840618133545 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.563, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4546, Accuracy: 0.8743, F1 Micro: 0.9256, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3264, Accuracy: 0.9412, F1 Micro: 0.9636, F1 Macro: 0.9624\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2135, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9748\n",
      "Epoch 5/10, Train Loss: 0.1507, Accuracy: 0.9606, F1 Micro: 0.9751, F1 Macro: 0.9734\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1244, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9784\n",
      "Epoch 7/10, Train Loss: 0.0909, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9772\n",
      "Epoch 8/10, Train Loss: 0.0725, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0644, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "Epoch 10/10, Train Loss: 0.0519, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.92      0.94      0.93       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.558, Accuracy: 0.8885, F1 Micro: 0.8885, F1 Macro: 0.8657\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2486, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9068\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1987, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9487\n",
      "Epoch 4/10, Train Loss: 0.1608, Accuracy: 0.95, F1 Micro: 0.95, F1 Macro: 0.9443\n",
      "Epoch 5/10, Train Loss: 0.1388, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9203\n",
      "Epoch 6/10, Train Loss: 0.1313, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9084\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1035, Accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "Epoch 8/10, Train Loss: 0.0869, Accuracy: 0.9192, F1 Micro: 0.9192, F1 Macro: 0.9123\n",
      "Epoch 9/10, Train Loss: 0.0656, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9239\n",
      "Epoch 10/10, Train Loss: 0.0745, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9243\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9538, F1 Micro: 0.9538, F1 Macro: 0.9482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.94      0.95      0.95       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 723: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9236\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.92      0.94      0.93       152\n",
      "    positive       0.81      0.75      0.78        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.83      0.84      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 139.1713035106659 s\n",
      "Averaged - Iteration 723: Accuracy: 0.9391, F1 Micro: 0.9391, F1 Macro: 0.8606\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.805455207824707 seconds\n",
      "New train size: 748\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5249, Accuracy: 0.7946, F1 Micro: 0.8849, F1 Macro: 0.8834\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.438, Accuracy: 0.8988, F1 Micro: 0.9387, F1 Macro: 0.9373\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2848, Accuracy: 0.9516, F1 Micro: 0.9701, F1 Macro: 0.9691\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1822, Accuracy: 0.9606, F1 Micro: 0.9754, F1 Macro: 0.9742\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1271, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0951, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9793\n",
      "Epoch 7/10, Train Loss: 0.0785, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9773\n",
      "Epoch 8/10, Train Loss: 0.0655, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9786\n",
      "Epoch 9/10, Train Loss: 0.0554, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0465, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.98      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5267, Accuracy: 0.902, F1 Micro: 0.902, F1 Macro: 0.894\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2613, Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.9389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1766, Accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9568\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.142, Accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9611\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1038, Accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9609\n",
      "Epoch 6/10, Train Loss: 0.1083, Accuracy: 0.9608, F1 Micro: 0.9608, F1 Macro: 0.9568\n",
      "Epoch 7/10, Train Loss: 0.0953, Accuracy: 0.9373, F1 Micro: 0.9373, F1 Macro: 0.932\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9238\n",
      "Epoch 9/10, Train Loss: 0.07, Accuracy: 0.9294, F1 Micro: 0.9294, F1 Macro: 0.9238\n",
      "Epoch 10/10, Train Loss: 0.0712, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.934\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9647, F1 Micro: 0.9647, F1 Macro: 0.9609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.97      0.95        86\n",
      "    positive       0.98      0.96      0.97       169\n",
      "\n",
      "    accuracy                           0.96       255\n",
      "   macro avg       0.96      0.96      0.96       255\n",
      "weighted avg       0.97      0.96      0.96       255\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 748: Accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9397\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.98      0.95       152\n",
      "    positive       0.93      0.77      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 145.1092231273651 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5308, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4351, Accuracy: 0.8958, F1 Micro: 0.9366, F1 Macro: 0.9347\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2933, Accuracy: 0.9479, F1 Micro: 0.9678, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1903, Accuracy: 0.9591, F1 Micro: 0.9745, F1 Macro: 0.9736\n",
      "Epoch 5/10, Train Loss: 0.1317, Accuracy: 0.9583, F1 Micro: 0.9738, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0978, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0814, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0699, Accuracy: 0.965, F1 Micro: 0.9782, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0506, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.96      1.00      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4508, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9111\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2289, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9324\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1722, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9485\n",
      "Epoch 4/10, Train Loss: 0.1762, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9167\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1419, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9483\n",
      "Epoch 6/10, Train Loss: 0.1117, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.1081, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9527\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0669, Accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9527\n",
      "Epoch 9/10, Train Loss: 0.0736, Accuracy: 0.9496, F1 Micro: 0.9496, F1 Macro: 0.9438\n",
      "Epoch 10/10, Train Loss: 0.0565, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.934\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9574, F1 Micro: 0.9574, F1 Macro: 0.9527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.95      0.94        87\n",
      "    positive       0.98      0.96      0.97       171\n",
      "\n",
      "    accuracy                           0.96       258\n",
      "   macro avg       0.95      0.96      0.95       258\n",
      "weighted avg       0.96      0.96      0.96       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 748: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9312\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.88      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.88      0.83      0.85        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.96      1.00      0.98       152\n",
      "    positive       0.97      0.78      0.86        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.82      0.82      0.82        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 153.0316195487976 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.552, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4628, Accuracy: 0.8705, F1 Micro: 0.9228, F1 Macro: 0.9214\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3207, Accuracy: 0.9464, F1 Micro: 0.9668, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.209, Accuracy: 0.9628, F1 Micro: 0.9768, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1442, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1069, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9789\n",
      "Epoch 7/10, Train Loss: 0.0902, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0763, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9797\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Epoch 10/10, Train Loss: 0.0533, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9784\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5619, Accuracy: 0.8951, F1 Micro: 0.8951, F1 Macro: 0.8833\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2844, Accuracy: 0.9213, F1 Micro: 0.9213, F1 Macro: 0.9144\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1976, Accuracy: 0.9288, F1 Micro: 0.9288, F1 Macro: 0.9222\n",
      "Epoch 4/10, Train Loss: 0.1553, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9142\n",
      "Epoch 5/10, Train Loss: 0.1235, Accuracy: 0.9251, F1 Micro: 0.9251, F1 Macro: 0.9183\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1, Accuracy: 0.9326, F1 Micro: 0.9326, F1 Macro: 0.9258\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0927, Accuracy: 0.9401, F1 Micro: 0.9401, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.1028, Accuracy: 0.9438, F1 Micro: 0.9438, F1 Macro: 0.937\n",
      "Epoch 9/10, Train Loss: 0.0657, Accuracy: 0.9363, F1 Micro: 0.9363, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0652, Accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9457\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9513, F1 Micro: 0.9513, F1 Macro: 0.9457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        87\n",
      "    positive       0.98      0.94      0.96       180\n",
      "\n",
      "    accuracy                           0.95       267\n",
      "   macro avg       0.94      0.95      0.95       267\n",
      "weighted avg       0.95      0.95      0.95       267\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 748: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9318\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.85      0.77      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.85      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 150.52940344810486 s\n",
      "Averaged - Iteration 748: Accuracy: 0.9404, F1 Micro: 0.9404, F1 Macro: 0.8647\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 756\n",
      "Acquired samples: 8\n",
      "Sampling duration: 3.802220106124878 seconds\n",
      "New train size: 756\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5318, Accuracy: 0.7924, F1 Micro: 0.8838, F1 Macro: 0.8823\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4229, Accuracy: 0.8802, F1 Micro: 0.9287, F1 Macro: 0.9281\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2819, Accuracy: 0.9494, F1 Micro: 0.9686, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1728, Accuracy: 0.9598, F1 Micro: 0.9749, F1 Macro: 0.9735\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1307, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9781\n",
      "Epoch 6/10, Train Loss: 0.0985, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9759\n",
      "Epoch 7/10, Train Loss: 0.0811, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9768\n",
      "Epoch 8/10, Train Loss: 0.0657, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0468, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4871, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9241\n",
      "Epoch 2/10, Train Loss: 0.2262, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9201\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1744, Accuracy: 0.9349, F1 Micro: 0.9349, F1 Macro: 0.9242\n",
      "Epoch 4/10, Train Loss: 0.1387, Accuracy: 0.9119, F1 Micro: 0.9119, F1 Macro: 0.9046\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.954, F1 Micro: 0.954, F1 Macro: 0.9473\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0878, Accuracy: 0.9693, F1 Micro: 0.9693, F1 Macro: 0.9655\n",
      "Epoch 7/10, Train Loss: 0.0854, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9325\n",
      "Epoch 8/10, Train Loss: 0.0996, Accuracy: 0.9157, F1 Micro: 0.9157, F1 Macro: 0.9086\n",
      "Epoch 9/10, Train Loss: 0.092, Accuracy: 0.9464, F1 Micro: 0.9464, F1 Macro: 0.9397\n",
      "Epoch 10/10, Train Loss: 0.0666, Accuracy: 0.9425, F1 Micro: 0.9425, F1 Macro: 0.9352\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9693, F1 Micro: 0.9693, F1 Macro: 0.9655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.98      0.95        85\n",
      "    positive       0.99      0.97      0.98       176\n",
      "\n",
      "    accuracy                           0.97       261\n",
      "   macro avg       0.96      0.97      0.97       261\n",
      "weighted avg       0.97      0.97      0.97       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 756: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9343\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.80      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.84      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.95      0.95      0.95       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.93      0.95       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 146.0182867050171 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5364, Accuracy: 0.7902, F1 Micro: 0.8827, F1 Macro: 0.8812\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4163, Accuracy: 0.8876, F1 Micro: 0.9321, F1 Macro: 0.9303\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2888, Accuracy: 0.9487, F1 Micro: 0.9681, F1 Macro: 0.9667\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1839, Accuracy: 0.9554, F1 Micro: 0.9721, F1 Macro: 0.9708\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1362, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9777\n",
      "Epoch 6/10, Train Loss: 0.1035, Accuracy: 0.9613, F1 Micro: 0.9755, F1 Macro: 0.9732\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9739\n",
      "Epoch 8/10, Train Loss: 0.0662, Accuracy: 0.965, F1 Micro: 0.9779, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0549, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "Epoch 10/10, Train Loss: 0.0479, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9804\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.99      0.98       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4997, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9312\n",
      "Epoch 2/10, Train Loss: 0.2663, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9131\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2095, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9299\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1359, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9436\n",
      "Epoch 5/10, Train Loss: 0.1153, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9364\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1121, Accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9531\n",
      "Epoch 7/10, Train Loss: 0.0908, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 8/10, Train Loss: 0.0627, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9327\n",
      "Epoch 10/10, Train Loss: 0.0374, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.9154\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.958, F1 Micro: 0.958, F1 Macro: 0.9531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.97      0.94        86\n",
      "    positive       0.98      0.95      0.97       176\n",
      "\n",
      "    accuracy                           0.96       262\n",
      "   macro avg       0.95      0.96      0.95       262\n",
      "weighted avg       0.96      0.96      0.96       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 756: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9333\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      1.00      0.96        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.97      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.82      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.95      0.94       152\n",
      "    positive       0.84      0.79      0.81        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.93      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 143.63005137443542 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5563, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4572, Accuracy: 0.8564, F1 Micro: 0.9162, F1 Macro: 0.915\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.3249, Accuracy: 0.9442, F1 Micro: 0.9653, F1 Macro: 0.9635\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.2048, Accuracy: 0.9583, F1 Micro: 0.9739, F1 Macro: 0.9722\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1493, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9774\n",
      "Epoch 6/10, Train Loss: 0.1124, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0896, Accuracy: 0.9688, F1 Micro: 0.9803, F1 Macro: 0.9792\n",
      "Epoch 8/10, Train Loss: 0.0768, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9785\n",
      "Epoch 9/10, Train Loss: 0.0611, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0522, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.92      0.97      0.94       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4939, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.8996\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2181, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.924\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1566, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1473, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1337, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0989, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.9488\n",
      "Epoch 7/10, Train Loss: 0.0783, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.934\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0939, Accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "Epoch 9/10, Train Loss: 0.0669, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9406\n",
      "Epoch 10/10, Train Loss: 0.0637, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.9403\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9535, F1 Micro: 0.9535, F1 Macro: 0.948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.94      0.93        86\n",
      "    positive       0.97      0.96      0.96       172\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.95      0.95      0.95       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 756: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9255\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.86      0.73      0.79        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.84      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.92      0.97      0.94       152\n",
      "    positive       0.89      0.77      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.91      0.86      0.88       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.89      0.91      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.64033222198486 s\n",
      "Averaged - Iteration 756: Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.8682\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.203681707382202 seconds\n",
      "New train size: 781\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5267, Accuracy: 0.7976, F1 Micro: 0.8864, F1 Macro: 0.885\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.414, Accuracy: 0.9055, F1 Micro: 0.9429, F1 Macro: 0.9418\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2698, Accuracy: 0.9531, F1 Micro: 0.971, F1 Macro: 0.9698\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1674, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9739\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1219, Accuracy: 0.9606, F1 Micro: 0.9753, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0938, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9769\n",
      "Epoch 7/10, Train Loss: 0.0804, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.062, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0531, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.99      0.97       175\n",
      "      others       0.94      0.96      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4777, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9202\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2436, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.933\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1798, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9303\n",
      "Epoch 4/10, Train Loss: 0.1557, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 5/10, Train Loss: 0.1189, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9198\n",
      "Epoch 6/10, Train Loss: 0.1037, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9123\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.083, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Epoch 8/10, Train Loss: 0.0897, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9083\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0856, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "Epoch 10/10, Train Loss: 0.064, Accuracy: 0.916, F1 Micro: 0.916, F1 Macro: 0.9088\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.9319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.94      0.91        86\n",
      "    positive       0.97      0.94      0.95       176\n",
      "\n",
      "    accuracy                           0.94       262\n",
      "   macro avg       0.93      0.94      0.93       262\n",
      "weighted avg       0.94      0.94      0.94       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 781: Accuracy: 0.9591, F1 Micro: 0.9591, F1 Macro: 0.9154\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.91      0.88      0.89        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.92      0.86      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.95      0.95      0.95       152\n",
      "    positive       0.84      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.86      0.86       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.77      0.83        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.93      0.88      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.86687755584717 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.537, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4169, Accuracy: 0.9062, F1 Micro: 0.9433, F1 Macro: 0.942\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2806, Accuracy: 0.9487, F1 Micro: 0.9682, F1 Macro: 0.9671\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1779, Accuracy: 0.9576, F1 Micro: 0.9734, F1 Macro: 0.9718\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1256, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9796\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Epoch 7/10, Train Loss: 0.0831, Accuracy: 0.9598, F1 Micro: 0.9746, F1 Macro: 0.9724\n",
      "Epoch 8/10, Train Loss: 0.0641, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0538, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9794\n",
      "Epoch 10/10, Train Loss: 0.047, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9776\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.94      0.94       158\n",
      "        part       0.97      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4986, Accuracy: 0.9084, F1 Micro: 0.9084, F1 Macro: 0.9005\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.241, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1726, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9401\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1617, Accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "Epoch 5/10, Train Loss: 0.1222, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9357\n",
      "Epoch 6/10, Train Loss: 0.1093, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.937\n",
      "Epoch 7/10, Train Loss: 0.0856, Accuracy: 0.9313, F1 Micro: 0.9313, F1 Macro: 0.9246\n",
      "Epoch 8/10, Train Loss: 0.09, Accuracy: 0.9466, F1 Micro: 0.9466, F1 Macro: 0.9411\n",
      "Epoch 9/10, Train Loss: 0.0779, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "Epoch 10/10, Train Loss: 0.0706, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9286\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9504, F1 Micro: 0.9504, F1 Macro: 0.9445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.94      0.93        87\n",
      "    positive       0.97      0.95      0.96       175\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.95      0.94       262\n",
      "weighted avg       0.95      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 781: Accuracy: 0.9614, F1 Micro: 0.9614, F1 Macro: 0.9312\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.97       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.88      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.94      0.94       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.84      0.85      0.85       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.96      0.90        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.92      0.96        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.95      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 147.08559274673462 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.553, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4399, Accuracy: 0.8996, F1 Micro: 0.9392, F1 Macro: 0.9378\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.298, Accuracy: 0.9516, F1 Micro: 0.97, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1921, Accuracy: 0.9606, F1 Micro: 0.9752, F1 Macro: 0.9737\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1356, Accuracy: 0.9635, F1 Micro: 0.9772, F1 Macro: 0.9761\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1062, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9773\n",
      "Epoch 7/10, Train Loss: 0.0866, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9753\n",
      "Epoch 8/10, Train Loss: 0.067, Accuracy: 0.9621, F1 Micro: 0.9759, F1 Macro: 0.974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0604, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0501, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.97       175\n",
      "      others       0.93      0.96      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5385, Accuracy: 0.8977, F1 Micro: 0.8977, F1 Macro: 0.8877\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2708, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9206\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1987, Accuracy: 0.9432, F1 Micro: 0.9432, F1 Macro: 0.9359\n",
      "Epoch 4/10, Train Loss: 0.1078, Accuracy: 0.9356, F1 Micro: 0.9356, F1 Macro: 0.9293\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1375, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9491\n",
      "Epoch 6/10, Train Loss: 0.1001, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9333\n",
      "Epoch 7/10, Train Loss: 0.0822, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.917\n",
      "Epoch 8/10, Train Loss: 0.0834, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9224\n",
      "Epoch 9/10, Train Loss: 0.0516, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "Epoch 10/10, Train Loss: 0.0414, Accuracy: 0.9318, F1 Micro: 0.9318, F1 Macro: 0.9253\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9491\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        86\n",
      "    positive       0.98      0.95      0.97       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.94      0.96      0.95       264\n",
      "weighted avg       0.96      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 781: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9329\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.99      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.93      0.87      0.90       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.94      0.96      0.95       152\n",
      "    positive       0.89      0.79      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.85      0.86      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 149.54944491386414 s\n",
      "Averaged - Iteration 781: Accuracy: 0.9426, F1 Micro: 0.9426, F1 Macro: 0.8711\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.7640278339385986 seconds\n",
      "New train size: 806\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5179, Accuracy: 0.8021, F1 Micro: 0.8886, F1 Macro: 0.8873\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4157, Accuracy: 0.9033, F1 Micro: 0.941, F1 Macro: 0.9389\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2584, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.9688\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1648, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9746\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1261, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9764\n",
      "Epoch 6/10, Train Loss: 0.0914, Accuracy: 0.9628, F1 Micro: 0.9765, F1 Macro: 0.9753\n",
      "Epoch 7/10, Train Loss: 0.0704, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9755\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0589, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0535, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0439, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.94      0.97      0.96       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4856, Accuracy: 0.9198, F1 Micro: 0.9198, F1 Macro: 0.9114\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2034, Accuracy: 0.9389, F1 Micro: 0.9389, F1 Macro: 0.933\n",
      "Epoch 3/10, Train Loss: 0.1527, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.918\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1724, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.9357\n",
      "Epoch 5/10, Train Loss: 0.1065, Accuracy: 0.9237, F1 Micro: 0.9237, F1 Macro: 0.917\n",
      "Epoch 6/10, Train Loss: 0.0864, Accuracy: 0.9275, F1 Micro: 0.9275, F1 Macro: 0.9206\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0473, Accuracy: 0.9427, F1 Micro: 0.9427, F1 Macro: 0.936\n",
      "Epoch 9/10, Train Loss: 0.0682, Accuracy: 0.9351, F1 Micro: 0.9351, F1 Macro: 0.929\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0511, Accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9492\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9542, F1 Micro: 0.9542, F1 Macro: 0.9492\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.97      0.93        87\n",
      "    positive       0.98      0.95      0.97       175\n",
      "\n",
      "    accuracy                           0.95       262\n",
      "   macro avg       0.94      0.96      0.95       262\n",
      "weighted avg       0.96      0.95      0.95       262\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 806: Accuracy: 0.9637, F1 Micro: 0.9637, F1 Macro: 0.9322\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.95      0.98      0.96       167\n",
      "    positive       0.83      0.73      0.77        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.86      0.88       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.94      0.97      0.96       152\n",
      "    positive       0.90      0.83      0.86        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.92      0.88      0.90       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.93      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.05556440353394 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5246, Accuracy: 0.7969, F1 Micro: 0.8859, F1 Macro: 0.8844\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4086, Accuracy: 0.907, F1 Micro: 0.9425, F1 Macro: 0.9405\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2664, Accuracy: 0.9472, F1 Micro: 0.9671, F1 Macro: 0.9655\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1721, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.975\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1284, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.977\n",
      "Epoch 6/10, Train Loss: 0.0952, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9763\n",
      "Epoch 7/10, Train Loss: 0.0741, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9752\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0617, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Epoch 9/10, Train Loss: 0.0561, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 10/10, Train Loss: 0.0457, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9804\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9817, F1 Macro: 0.9804\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.98      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.98      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4927, Accuracy: 0.9015, F1 Micro: 0.9015, F1 Macro: 0.8921\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2046, Accuracy: 0.9394, F1 Micro: 0.9394, F1 Macro: 0.9329\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1711, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9491\n",
      "Epoch 4/10, Train Loss: 0.1819, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9451\n",
      "Epoch 5/10, Train Loss: 0.1247, Accuracy: 0.947, F1 Micro: 0.947, F1 Macro: 0.941\n",
      "Epoch 6/10, Train Loss: 0.1229, Accuracy: 0.9242, F1 Micro: 0.9242, F1 Macro: 0.9174\n",
      "Epoch 7/10, Train Loss: 0.0971, Accuracy: 0.9508, F1 Micro: 0.9508, F1 Macro: 0.9454\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9479\n",
      "Epoch 9/10, Train Loss: 0.0871, Accuracy: 0.9167, F1 Micro: 0.9167, F1 Macro: 0.9096\n",
      "Epoch 10/10, Train Loss: 0.0374, Accuracy: 0.928, F1 Micro: 0.928, F1 Macro: 0.9206\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9545, F1 Micro: 0.9545, F1 Macro: 0.9479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.92      0.93        86\n",
      "    positive       0.96      0.97      0.97       178\n",
      "\n",
      "    accuracy                           0.95       264\n",
      "   macro avg       0.95      0.95      0.95       264\n",
      "weighted avg       0.95      0.95      0.95       264\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 806: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9284\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.90      0.79      0.84        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.86      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.94      0.93       152\n",
      "    positive       0.82      0.81      0.82        52\n",
      "\n",
      "    accuracy                           0.90       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.90      0.90      0.90       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.93      0.90      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.95      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.93      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.75554156303406 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5441, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4303, Accuracy: 0.9033, F1 Micro: 0.9404, F1 Macro: 0.9377\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2763, Accuracy: 0.9524, F1 Micro: 0.9703, F1 Macro: 0.969\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1832, Accuracy: 0.9643, F1 Micro: 0.9777, F1 Macro: 0.9767\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1385, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9764\n",
      "Epoch 6/10, Train Loss: 0.0983, Accuracy: 0.9635, F1 Micro: 0.977, F1 Macro: 0.9756\n",
      "Epoch 7/10, Train Loss: 0.08, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0651, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.058, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "Epoch 10/10, Train Loss: 0.0509, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9782\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.94      0.93       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.98      0.98      1061\n",
      "   macro avg       0.97      0.98      0.98      1061\n",
      "weighted avg       0.97      0.98      0.98      1061\n",
      " samples avg       0.97      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5369, Accuracy: 0.8906, F1 Micro: 0.8906, F1 Macro: 0.8796\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2234, Accuracy: 0.9132, F1 Micro: 0.9132, F1 Macro: 0.9054\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1736, Accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9452\n",
      "Epoch 4/10, Train Loss: 0.1648, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9252\n",
      "Epoch 5/10, Train Loss: 0.1272, Accuracy: 0.9321, F1 Micro: 0.9321, F1 Macro: 0.9247\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9294\n",
      "Epoch 7/10, Train Loss: 0.0963, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9334\n",
      "Epoch 8/10, Train Loss: 0.0697, Accuracy: 0.9245, F1 Micro: 0.9245, F1 Macro: 0.9176\n",
      "Epoch 9/10, Train Loss: 0.0534, Accuracy: 0.9396, F1 Micro: 0.9396, F1 Macro: 0.9319\n",
      "Epoch 10/10, Train Loss: 0.0481, Accuracy: 0.9358, F1 Micro: 0.9358, F1 Macro: 0.9275\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9509, F1 Micro: 0.9509, F1 Macro: 0.9452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.97      0.93        86\n",
      "    positive       0.98      0.94      0.96       179\n",
      "\n",
      "    accuracy                           0.95       265\n",
      "   macro avg       0.94      0.95      0.95       265\n",
      "weighted avg       0.95      0.95      0.95       265\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 806: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9255\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.93      0.93       152\n",
      "    positive       0.82      0.79      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.82      0.85      0.84       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.96      0.94        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       0.95      0.88      0.91        41\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.92      0.89        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 149.58067727088928 s\n",
      "Averaged - Iteration 806: Accuracy: 0.9435, F1 Micro: 0.9435, F1 Macro: 0.8738\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 3.194718837738037 seconds\n",
      "New train size: 831\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5321, Accuracy: 0.7969, F1 Micro: 0.886, F1 Macro: 0.8846\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4198, Accuracy: 0.91, F1 Micro: 0.9456, F1 Macro: 0.945\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2605, Accuracy: 0.9524, F1 Micro: 0.9706, F1 Macro: 0.9696\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1671, Accuracy: 0.9561, F1 Micro: 0.9724, F1 Macro: 0.9704\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1234, Accuracy: 0.9621, F1 Micro: 0.9762, F1 Macro: 0.9747\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0906, Accuracy: 0.965, F1 Micro: 0.9781, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0717, Accuracy: 0.9658, F1 Micro: 0.9785, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0625, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0551, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "Epoch 10/10, Train Loss: 0.0453, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9772\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.97      0.97       175\n",
      "      others       0.92      0.99      0.95       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4552, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2299, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 3/10, Train Loss: 0.1959, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.928\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1429, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9348\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1159, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9344\n",
      "Epoch 6/10, Train Loss: 0.1154, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9224\n",
      "Epoch 7/10, Train Loss: 0.0998, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9196\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9277\n",
      "Epoch 9/10, Train Loss: 0.0774, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9169\n",
      "Epoch 10/10, Train Loss: 0.0612, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.928\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91        85\n",
      "    positive       0.96      0.95      0.96       173\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.93      0.94      0.93       258\n",
      "weighted avg       0.94      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 831: Accuracy: 0.9599, F1 Micro: 0.9599, F1 Macro: 0.9196\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.81      0.87        16\n",
      "     neutral       0.96      0.97      0.97       167\n",
      "    positive       0.79      0.82      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.83      0.80        12\n",
      "     neutral       0.93      0.99      0.96       152\n",
      "    positive       0.95      0.75      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.86      0.86       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.91      0.89        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.90      0.85      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.92      0.92       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 157.43712449073792 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5353, Accuracy: 0.7917, F1 Micro: 0.8834, F1 Macro: 0.882\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4051, Accuracy: 0.9085, F1 Micro: 0.9442, F1 Macro: 0.9435\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2613, Accuracy: 0.9554, F1 Micro: 0.9724, F1 Macro: 0.9714\n",
      "Epoch 4/10, Train Loss: 0.1713, Accuracy: 0.9516, F1 Micro: 0.9696, F1 Macro: 0.9669\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1292, Accuracy: 0.9643, F1 Micro: 0.9775, F1 Macro: 0.976\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0935, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9797\n",
      "Epoch 7/10, Train Loss: 0.0764, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Epoch 8/10, Train Loss: 0.0624, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0595, Accuracy: 0.9732, F1 Micro: 0.9832, F1 Macro: 0.9824\n",
      "Epoch 10/10, Train Loss: 0.0464, Accuracy: 0.9665, F1 Micro: 0.9788, F1 Macro: 0.9769\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9732, F1 Micro: 0.9832, F1 Macro: 0.9824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.91      0.99      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      1.00      1.00       192\n",
      "     service       0.99      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.547, Accuracy: 0.8893, F1 Micro: 0.8893, F1 Macro: 0.881\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.242, Accuracy: 0.8972, F1 Micro: 0.8972, F1 Macro: 0.8913\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1745, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "Epoch 4/10, Train Loss: 0.1454, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9314\n",
      "Epoch 5/10, Train Loss: 0.1408, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9314\n",
      "Epoch 6/10, Train Loss: 0.0962, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9383\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0838, Accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "Epoch 8/10, Train Loss: 0.0711, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.9352\n",
      "Epoch 9/10, Train Loss: 0.0745, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.939\n",
      "Epoch 10/10, Train Loss: 0.0492, Accuracy: 0.9209, F1 Micro: 0.9209, F1 Macro: 0.9103\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9526, F1 Micro: 0.9526, F1 Macro: 0.9474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93        85\n",
      "    positive       0.98      0.95      0.96       168\n",
      "\n",
      "    accuracy                           0.95       253\n",
      "   macro avg       0.94      0.95      0.95       253\n",
      "weighted avg       0.95      0.95      0.95       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 831: Accuracy: 0.9653, F1 Micro: 0.9653, F1 Macro: 0.9285\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.92      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.94      0.95       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.93      0.90      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.99      0.95       152\n",
      "    positive       0.97      0.73      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.82      0.86       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.98      0.99      0.98       152\n",
      "    positive       0.97      0.85      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.95      0.94       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      1.00       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.95      0.94      0.94       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.93      0.96        14\n",
      "     neutral       0.99      1.00      1.00       185\n",
      "    positive       0.94      0.94      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.96      0.97       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 152.44347190856934 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5538, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4322, Accuracy: 0.9107, F1 Micro: 0.9453, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2817, Accuracy: 0.9524, F1 Micro: 0.9705, F1 Macro: 0.9695\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1811, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1356, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.9779\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0961, Accuracy: 0.9673, F1 Micro: 0.9795, F1 Macro: 0.9786\n",
      "Epoch 7/10, Train Loss: 0.082, Accuracy: 0.9621, F1 Micro: 0.976, F1 Macro: 0.9744\n",
      "Epoch 8/10, Train Loss: 0.0671, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0632, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "Epoch 10/10, Train Loss: 0.0504, Accuracy: 0.9643, F1 Micro: 0.9774, F1 Macro: 0.9757\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9725, F1 Micro: 0.9827, F1 Macro: 0.9818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.98       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.99      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4751, Accuracy: 0.907, F1 Micro: 0.907, F1 Macro: 0.9\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2208, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1705, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9362\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1572, Accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.94\n",
      "Epoch 5/10, Train Loss: 0.1053, Accuracy: 0.9419, F1 Micro: 0.9419, F1 Macro: 0.9351\n",
      "Epoch 6/10, Train Loss: 0.0857, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.928\n",
      "Epoch 7/10, Train Loss: 0.1032, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9314\n",
      "Epoch 8/10, Train Loss: 0.0559, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9277\n",
      "Epoch 9/10, Train Loss: 0.0447, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9169\n",
      "Epoch 10/10, Train Loss: 0.0537, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9119\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9457, F1 Micro: 0.9457, F1 Macro: 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        85\n",
      "    positive       0.98      0.94      0.96       173\n",
      "\n",
      "    accuracy                           0.95       258\n",
      "   macro avg       0.93      0.95      0.94       258\n",
      "weighted avg       0.95      0.95      0.95       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 831: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.9287\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.97      0.98      0.98       167\n",
      "    positive       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.83      0.77        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.93      0.75      0.83        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.86      0.85      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      1.00      0.94        23\n",
      "     neutral       0.97      0.99      0.98       152\n",
      "    positive       0.97      0.83      0.89        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.94      0.94      0.94       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.92      0.91      0.92       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 154.0224950313568 s\n",
      "Averaged - Iteration 831: Accuracy: 0.9444, F1 Micro: 0.9444, F1 Macro: 0.8762\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 25\n",
      "Sampling duration: 2.164961814880371 seconds\n",
      "New train size: 856\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.515, Accuracy: 0.7909, F1 Micro: 0.8831, F1 Macro: 0.8816\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.402, Accuracy: 0.9226, F1 Micro: 0.9528, F1 Macro: 0.951\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.251, Accuracy: 0.9531, F1 Micro: 0.9707, F1 Macro: 0.9692\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1619, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.116, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9758\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0901, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9783\n",
      "Epoch 7/10, Train Loss: 0.0706, Accuracy: 0.9665, F1 Micro: 0.979, F1 Macro: 0.978\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0619, Accuracy: 0.9688, F1 Micro: 0.9804, F1 Macro: 0.9795\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0502, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "Epoch 10/10, Train Loss: 0.0438, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9795\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.971, F1 Micro: 0.9818, F1 Macro: 0.9808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.93      0.96      0.94       158\n",
      "        part       0.98      0.99      0.99       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4445, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9283\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.246, Accuracy: 0.9346, F1 Micro: 0.9346, F1 Macro: 0.9287\n",
      "Epoch 3/10, Train Loss: 0.1695, Accuracy: 0.9308, F1 Micro: 0.9308, F1 Macro: 0.9247\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1235, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9354\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.0914, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9408\n",
      "Epoch 6/10, Train Loss: 0.0917, Accuracy: 0.9385, F1 Micro: 0.9385, F1 Macro: 0.9301\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0749, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9402\n",
      "Epoch 8/10, Train Loss: 0.073, Accuracy: 0.9269, F1 Micro: 0.9269, F1 Macro: 0.9207\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0617, Accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "Epoch 10/10, Train Loss: 0.0368, Accuracy: 0.9423, F1 Micro: 0.9423, F1 Macro: 0.9368\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.9462, F1 Micro: 0.9462, F1 Macro: 0.9405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.97      0.92        86\n",
      "    positive       0.98      0.94      0.96       174\n",
      "\n",
      "    accuracy                           0.95       260\n",
      "   macro avg       0.93      0.95      0.94       260\n",
      "weighted avg       0.95      0.95      0.95       260\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 856: Accuracy: 0.963, F1 Micro: 0.963, F1 Macro: 0.9277\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.97      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.91      0.87      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.83      0.87        12\n",
      "     neutral       0.93      0.96      0.94       152\n",
      "    positive       0.88      0.81      0.84        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.90      0.87      0.88       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.99      0.99       152\n",
      "    positive       1.00      0.83      0.91        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.91      0.91      0.91       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      1.00      0.97        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      0.94      0.97        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 161.44864130020142 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5173, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.3964, Accuracy: 0.9122, F1 Micro: 0.9461, F1 Macro: 0.9445\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2636, Accuracy: 0.9554, F1 Micro: 0.9722, F1 Macro: 0.971\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1677, Accuracy: 0.9628, F1 Micro: 0.9767, F1 Macro: 0.9754\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1264, Accuracy: 0.9635, F1 Micro: 0.9771, F1 Macro: 0.9757\n",
      "Epoch 6/10, Train Loss: 0.0941, Accuracy: 0.9628, F1 Micro: 0.9766, F1 Macro: 0.9751\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0754, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.977\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0659, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9781\n",
      "Epoch 10/10, Train Loss: 0.0439, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9766\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9798, F1 Macro: 0.9781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.97      0.98      0.97       175\n",
      "      others       0.94      0.92      0.93       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.99      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1061\n",
      "   macro avg       0.98      0.98      0.98      1061\n",
      "weighted avg       0.98      0.98      0.98      1061\n",
      " samples avg       0.98      0.98      0.97      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4436, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9328\n",
      "Epoch 2/10, Train Loss: 0.2454, Accuracy: 0.9154, F1 Micro: 0.9154, F1 Macro: 0.9072\n",
      "Epoch 3/10, Train Loss: 0.1701, Accuracy: 0.9118, F1 Micro: 0.9118, F1 Macro: 0.9034\n",
      "Epoch 4/10, Train Loss: 0.1604, Accuracy: 0.9191, F1 Micro: 0.9191, F1 Macro: 0.911\n",
      "Epoch 5/10, Train Loss: 0.1182, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.92\n",
      "Epoch 6/10, Train Loss: 0.0829, Accuracy: 0.9228, F1 Micro: 0.9228, F1 Macro: 0.9144\n",
      "Epoch 7/10, Train Loss: 0.0684, Accuracy: 0.9301, F1 Micro: 0.9301, F1 Macro: 0.9213\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.0805, Accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9324\n",
      "Epoch 9/10, Train Loss: 0.0657, Accuracy: 0.9338, F1 Micro: 0.9338, F1 Macro: 0.9257\n",
      "Epoch 10/10, Train Loss: 0.0377, Accuracy: 0.9338, F1 Micro: 0.9338, F1 Macro: 0.9257\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9412, F1 Micro: 0.9412, F1 Macro: 0.9324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.92      0.91        86\n",
      "    positive       0.96      0.95      0.96       186\n",
      "\n",
      "    accuracy                           0.94       272\n",
      "   macro avg       0.93      0.94      0.93       272\n",
      "weighted avg       0.94      0.94      0.94       272\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 856: Accuracy: 0.9568, F1 Micro: 0.9568, F1 Macro: 0.9169\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.96      0.92        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.92      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.75      0.83        16\n",
      "     neutral       0.96      0.98      0.97       167\n",
      "    positive       0.82      0.82      0.82        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.85      0.87       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.75      0.82        12\n",
      "     neutral       0.94      0.91      0.93       152\n",
      "    positive       0.76      0.85      0.80        52\n",
      "\n",
      "    accuracy                           0.89       216\n",
      "   macro avg       0.87      0.84      0.85       216\n",
      "weighted avg       0.89      0.89      0.89       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      1.00      0.88        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.80      0.87        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.91      0.93      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.92      0.92        13\n",
      "     neutral       0.99      1.00      0.99       186\n",
      "    positive       0.93      0.82      0.87        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.92      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 153.46222400665283 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.535, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4216, Accuracy: 0.9115, F1 Micro: 0.9453, F1 Macro: 0.9431\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2727, Accuracy: 0.9539, F1 Micro: 0.9711, F1 Macro: 0.9694\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1756, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.9791\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.13, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.979\n",
      "Epoch 6/10, Train Loss: 0.0998, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 7/10, Train Loss: 0.0803, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9776\n",
      "Epoch 8/10, Train Loss: 0.0691, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0576, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 10/10, Train Loss: 0.0484, Accuracy: 0.9695, F1 Micro: 0.9808, F1 Macro: 0.9798\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.92      0.97      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5123, Accuracy: 0.9066, F1 Micro: 0.9066, F1 Macro: 0.8974\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2056, Accuracy: 0.9416, F1 Micro: 0.9416, F1 Macro: 0.9357\n",
      "Epoch 3/10, Train Loss: 0.1627, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9251\n",
      "Epoch 4/10, Train Loss: 0.1471, Accuracy: 0.9183, F1 Micro: 0.9183, F1 Macro: 0.9121\n",
      "Epoch 5/10, Train Loss: 0.1065, Accuracy: 0.9261, F1 Micro: 0.9261, F1 Macro: 0.9186\n",
      "Epoch 6/10, Train Loss: 0.092, Accuracy: 0.9377, F1 Micro: 0.9377, F1 Macro: 0.9319\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.9455, F1 Micro: 0.9455, F1 Macro: 0.9395\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 8/10, Train Loss: 0.065, Accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9446\n",
      "Epoch 9/10, Train Loss: 0.0569, Accuracy: 0.9339, F1 Micro: 0.9339, F1 Macro: 0.9275\n",
      "Epoch 10/10, Train Loss: 0.0696, Accuracy: 0.9105, F1 Micro: 0.9105, F1 Macro: 0.9024\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9494, F1 Micro: 0.9494, F1 Macro: 0.9446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.99      0.93        85\n",
      "    positive       0.99      0.93      0.96       172\n",
      "\n",
      "    accuracy                           0.95       257\n",
      "   macro avg       0.93      0.96      0.94       257\n",
      "weighted avg       0.95      0.95      0.95       257\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 856: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9263\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.98      0.98       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.94      0.88        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.92      0.73      0.81        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.90      0.88      0.89       216\n",
      "weighted avg       0.94      0.94      0.94       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.93      0.97      0.95       152\n",
      "    positive       0.91      0.77      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.92      0.92      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.92       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.92      0.87      0.89       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 152.42059111595154 s\n",
      "Averaged - Iteration 856: Accuracy: 0.9451, F1 Micro: 0.9451, F1 Macro: 0.8782\n",
      "Launching training on 2 GPUs.\n",
      "Nearest checkpoint: 864\n",
      "Acquired samples: 8\n",
      "Sampling duration: 1.5460147857666016 seconds\n",
      "New train size: 864\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5195, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4032, Accuracy: 0.9196, F1 Micro: 0.9508, F1 Macro: 0.9493\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2445, Accuracy: 0.9576, F1 Micro: 0.9736, F1 Macro: 0.9726\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1611, Accuracy: 0.9613, F1 Micro: 0.9758, F1 Macro: 0.9745\n",
      "Epoch 5/10, Train Loss: 0.1204, Accuracy: 0.9613, F1 Micro: 0.9757, F1 Macro: 0.9744\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0945, Accuracy: 0.9643, F1 Micro: 0.9776, F1 Macro: 0.9766\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0759, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9767\n",
      "Epoch 8/10, Train Loss: 0.0606, Accuracy: 0.9613, F1 Micro: 0.9756, F1 Macro: 0.9741\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0499, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.979\n",
      "Epoch 10/10, Train Loss: 0.043, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9775\n",
      "\n",
      "Model 1 of aspect detection, Accuracy: 0.968, F1 Micro: 0.9799, F1 Macro: 0.979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      0.99      0.99       187\n",
      "     machine       0.95      0.98      0.96       175\n",
      "      others       0.93      0.97      0.95       158\n",
      "        part       0.98      0.98      0.98       158\n",
      "       price       0.98      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4525, Accuracy: 0.9031, F1 Micro: 0.9031, F1 Macro: 0.8961\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2208, Accuracy: 0.9147, F1 Micro: 0.9147, F1 Macro: 0.908\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1379, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9321\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1374, Accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "Epoch 5/10, Train Loss: 0.139, Accuracy: 0.9264, F1 Micro: 0.9264, F1 Macro: 0.9199\n",
      "Epoch 6/10, Train Loss: 0.119, Accuracy: 0.9225, F1 Micro: 0.9225, F1 Macro: 0.9133\n",
      "Epoch 7/10, Train Loss: 0.0873, Accuracy: 0.9302, F1 Micro: 0.9302, F1 Macro: 0.9236\n",
      "Epoch 8/10, Train Loss: 0.0855, Accuracy: 0.9186, F1 Micro: 0.9186, F1 Macro: 0.9111\n",
      "Epoch 9/10, Train Loss: 0.0608, Accuracy: 0.9341, F1 Micro: 0.9341, F1 Macro: 0.9257\n",
      "Epoch 10/10, Train Loss: 0.0407, Accuracy: 0.9109, F1 Micro: 0.9109, F1 Macro: 0.9035\n",
      "\n",
      "Model 1 of sentiment analysis, accuracy: 0.938, F1 Micro: 0.938, F1 Macro: 0.9298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91        85\n",
      "    positive       0.95      0.95      0.95       173\n",
      "\n",
      "    accuracy                           0.94       258\n",
      "   macro avg       0.93      0.93      0.93       258\n",
      "weighted avg       0.94      0.94      0.94       258\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 1 - Iteration 864: Accuracy: 0.9583, F1 Micro: 0.9583, F1 Macro: 0.9137\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.82      0.90        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.88      0.92      0.90        24\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.96      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.81      0.84        16\n",
      "     neutral       0.94      0.98      0.96       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.90      0.84      0.87       216\n",
      "weighted avg       0.93      0.93      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.94      0.97      0.95       152\n",
      "    positive       0.87      0.79      0.83        52\n",
      "\n",
      "    accuracy                           0.92       216\n",
      "   macro avg       0.88      0.86      0.87       216\n",
      "weighted avg       0.92      0.92      0.92       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.96      0.92        23\n",
      "     neutral       0.98      0.98      0.98       152\n",
      "    positive       0.92      0.88      0.90        41\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.93      0.94      0.93       216\n",
      "weighted avg       0.96      0.96      0.96       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.85      0.88        13\n",
      "     neutral       0.98      0.99      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.95      0.91      0.93       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.86      0.92        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.96      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Total train time: 154.8830053806305 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5298, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4095, Accuracy: 0.9025, F1 Micro: 0.9402, F1 Macro: 0.938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2556, Accuracy: 0.9501, F1 Micro: 0.969, F1 Macro: 0.9679\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.1718, Accuracy: 0.9554, F1 Micro: 0.9719, F1 Macro: 0.9699\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1236, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9774\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0954, Accuracy: 0.968, F1 Micro: 0.98, F1 Macro: 0.9789\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 7/10, Train Loss: 0.0791, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "Epoch 8/10, Train Loss: 0.0621, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9779\n",
      "Epoch 9/10, Train Loss: 0.0518, Accuracy: 0.9658, F1 Micro: 0.9784, F1 Macro: 0.9769\n",
      "Epoch 10/10, Train Loss: 0.0449, Accuracy: 0.9673, F1 Micro: 0.9793, F1 Macro: 0.9778\n",
      "\n",
      "Model 2 of aspect detection, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.99      0.99      0.99       187\n",
      "     machine       0.96      0.99      0.98       175\n",
      "      others       0.91      0.98      0.94       158\n",
      "        part       0.98      0.97      0.98       158\n",
      "       price       0.98      1.00      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.97      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.97      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4711, Accuracy: 0.9407, F1 Micro: 0.9407, F1 Macro: 0.933\n",
      "Epoch 2/10, Train Loss: 0.2649, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9278\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.1516, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.938\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.152, Accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9517\n",
      "Epoch 5/10, Train Loss: 0.1108, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.938\n",
      "Epoch 6/10, Train Loss: 0.1219, Accuracy: 0.9447, F1 Micro: 0.9447, F1 Macro: 0.9393\n",
      "Epoch 7/10, Train Loss: 0.0793, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9432\n",
      "Epoch 8/10, Train Loss: 0.0769, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9419\n",
      "Epoch 9/10, Train Loss: 0.0903, Accuracy: 0.9368, F1 Micro: 0.9368, F1 Macro: 0.9307\n",
      "Epoch 10/10, Train Loss: 0.0472, Accuracy: 0.9486, F1 Micro: 0.9486, F1 Macro: 0.9435\n",
      "\n",
      "Model 2 of sentiment analysis, accuracy: 0.9565, F1 Micro: 0.9565, F1 Macro: 0.9517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.96      0.94        84\n",
      "    positive       0.98      0.95      0.97       169\n",
      "\n",
      "    accuracy                           0.96       253\n",
      "   macro avg       0.95      0.96      0.95       253\n",
      "weighted avg       0.96      0.96      0.96       253\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 2 - Iteration 864: Accuracy: 0.9645, F1 Micro: 0.9645, F1 Macro: 0.932\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      0.99      0.99       181\n",
      "    positive       0.92      0.96      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.97      0.95      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.90        16\n",
      "     neutral       0.96      0.99      0.98       167\n",
      "    positive       0.93      0.79      0.85        33\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.94      0.89      0.91       216\n",
      "weighted avg       0.95      0.95      0.95       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.75      0.78        12\n",
      "     neutral       0.91      0.98      0.94       152\n",
      "    positive       0.95      0.75      0.84        52\n",
      "\n",
      "    accuracy                           0.91       216\n",
      "   macro avg       0.89      0.83      0.85       216\n",
      "weighted avg       0.91      0.91      0.91       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      1.00      0.90        23\n",
      "     neutral       0.98      0.97      0.98       152\n",
      "    positive       0.95      0.85      0.90        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92        13\n",
      "     neutral       0.98      1.00      0.99       186\n",
      "    positive       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.98       216\n",
      "   macro avg       0.97      0.91      0.94       216\n",
      "weighted avg       0.98      0.98      0.98       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 151.112318277359 s\n",
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASPECT DETECTION\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.5459, Accuracy: 0.7894, F1 Micro: 0.8823, F1 Macro: 0.8809\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.4147, Accuracy: 0.9129, F1 Micro: 0.9461, F1 Macro: 0.9439\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 3/10, Train Loss: 0.2586, Accuracy: 0.9613, F1 Micro: 0.9759, F1 Macro: 0.9751\n",
      "Epoch 4/10, Train Loss: 0.1759, Accuracy: 0.9591, F1 Micro: 0.9742, F1 Macro: 0.9723\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1281, Accuracy: 0.965, F1 Micro: 0.978, F1 Macro: 0.9771\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.1019, Accuracy: 0.9702, F1 Micro: 0.9813, F1 Macro: 0.9805\n",
      "Epoch 7/10, Train Loss: 0.0842, Accuracy: 0.9665, F1 Micro: 0.9789, F1 Macro: 0.9778\n",
      "Epoch 8/10, Train Loss: 0.0663, Accuracy: 0.9673, F1 Micro: 0.9794, F1 Macro: 0.9781\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 9/10, Train Loss: 0.0547, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "Epoch 10/10, Train Loss: 0.0469, Accuracy: 0.9695, F1 Micro: 0.9807, F1 Macro: 0.9796\n",
      "\n",
      "Model 3 of aspect detection, Accuracy: 0.9717, F1 Micro: 0.9822, F1 Macro: 0.9812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fuel       0.98      1.00      0.99       187\n",
      "     machine       0.96      0.98      0.97       175\n",
      "      others       0.94      0.97      0.96       158\n",
      "        part       0.97      0.98      0.98       158\n",
      "       price       0.99      0.99      0.99       192\n",
      "     service       1.00      1.00      1.00       191\n",
      "\n",
      "   micro avg       0.98      0.99      0.98      1061\n",
      "   macro avg       0.97      0.99      0.98      1061\n",
      "weighted avg       0.98      0.99      0.98      1061\n",
      " samples avg       0.97      0.99      0.98      1061\n",
      "\n",
      "--------------------------------------------------\n",
      "SENTIMENT ANALYSIS\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 1/10, Train Loss: 0.4243, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9169\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 2/10, Train Loss: 0.2104, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9212\n",
      "Epoch 3/10, Train Loss: 0.1506, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9172\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 4/10, Train Loss: 0.114, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.922\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 5/10, Train Loss: 0.1229, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9314\n",
      "Higher F1 achieved, saving model\n",
      "Epoch 6/10, Train Loss: 0.0919, Accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9325\n",
      "Epoch 7/10, Train Loss: 0.0821, Accuracy: 0.931, F1 Micro: 0.931, F1 Macro: 0.9248\n",
      "Epoch 8/10, Train Loss: 0.0725, Accuracy: 0.9234, F1 Micro: 0.9234, F1 Macro: 0.9156\n",
      "Epoch 9/10, Train Loss: 0.0626, Accuracy: 0.9272, F1 Micro: 0.9272, F1 Macro: 0.9201\n",
      "Epoch 10/10, Train Loss: 0.0475, Accuracy: 0.9195, F1 Micro: 0.9195, F1 Macro: 0.9121\n",
      "\n",
      "Model 3 of sentiment analysis, accuracy: 0.9387, F1 Micro: 0.9387, F1 Macro: 0.9325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.95      0.91        87\n",
      "    positive       0.98      0.93      0.95       174\n",
      "\n",
      "    accuracy                           0.94       261\n",
      "   macro avg       0.92      0.94      0.93       261\n",
      "weighted avg       0.94      0.94      0.94       261\n",
      "\n",
      "--------------------------------------------------\n",
      "Model 3 - Iteration 864: Accuracy: 0.9622, F1 Micro: 0.9622, F1 Macro: 0.9228\n",
      "--------------------------------------------------\n",
      "Aspect fuel report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.91      0.95        11\n",
      "     neutral       0.99      1.00      0.99       181\n",
      "    positive       0.96      0.92      0.94        24\n",
      "\n",
      "    accuracy                           0.99       216\n",
      "   macro avg       0.98      0.94      0.96       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "Aspect machine report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85        16\n",
      "     neutral       0.95      0.98      0.97       167\n",
      "    positive       0.89      0.73      0.80        33\n",
      "\n",
      "    accuracy                           0.94       216\n",
      "   macro avg       0.89      0.86      0.87       216\n",
      "weighted avg       0.93      0.94      0.93       216\n",
      "\n",
      "Aspect others report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83        12\n",
      "     neutral       0.95      0.97      0.96       152\n",
      "    positive       0.88      0.83      0.85        52\n",
      "\n",
      "    accuracy                           0.93       216\n",
      "   macro avg       0.89      0.88      0.88       216\n",
      "weighted avg       0.92      0.93      0.93       216\n",
      "\n",
      "Aspect part report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      1.00      0.92        23\n",
      "     neutral       0.97      0.98      0.98       152\n",
      "    positive       0.94      0.83      0.88        41\n",
      "\n",
      "    accuracy                           0.95       216\n",
      "   macro avg       0.92      0.94      0.93       216\n",
      "weighted avg       0.96      0.95      0.95       216\n",
      "\n",
      "Aspect price report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.92      0.86        13\n",
      "     neutral       0.99      0.99      0.99       186\n",
      "    positive       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.97       216\n",
      "   macro avg       0.91      0.89      0.90       216\n",
      "weighted avg       0.97      0.97      0.97       216\n",
      "\n",
      "Aspect service report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        14\n",
      "     neutral       1.00      1.00      1.00       185\n",
      "    positive       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00       216\n",
      "   macro avg       1.00      1.00      1.00       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "Total train time: 153.8981683254242 s\n",
      "Averaged - Iteration 864: Accuracy: 0.9458, F1 Micro: 0.9458, F1 Macro: 0.8801\n",
      "Total runtime: 9924.357300281525 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "active_learning(seeds[4], 4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6734022,
     "sourceId": 10843162,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9961.764037,
   "end_time": "2025-04-05T09:20:50.446642",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-05T06:34:48.682605",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03529ea70b49449e9632ce1390b29db7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0526a2cf3c0040a1af12bf54ec69f55f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ccca0f5e313d403391ba45a8cc1b2efd",
       "placeholder": "",
       "style": "IPY_MODEL_75fb4add3f7a4fbe98ba160761b678a8",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:100%"
      }
     },
     "061c17281dee46089a6d000b988d7cc9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "081266df42564fd988417c9ae445b4df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ff18c50103b048babc62613fa5d16937",
       "placeholder": "",
       "style": "IPY_MODEL_9cf8ced7feab4ceeb8885faf2619bb0f",
       "tabbable": null,
       "tooltip": null,
       "value": "1.53k/1.53k[00:00&lt;00:00,145kB/s]"
      }
     },
     "1021bc823af246998d54c088ac4f2226": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0526a2cf3c0040a1af12bf54ec69f55f",
        "IPY_MODEL_3ceeb39f070340a6ac9b52e654bd6142",
        "IPY_MODEL_081266df42564fd988417c9ae445b4df"
       ],
       "layout": "IPY_MODEL_3ea29d3c875145d5adf504de3376f8ff",
       "tabbable": null,
       "tooltip": null
      }
     },
     "18ea5b392ac84ab6aaef8848ca1657df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "20a85283299642c697fb8deec10e8b13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f1a1385324ab44bbbba6ef86384f9367",
       "placeholder": "",
       "style": "IPY_MODEL_061c17281dee46089a6d000b988d7cc9",
       "tabbable": null,
       "tooltip": null,
       "value": "112/112[00:00&lt;00:00,10.4kB/s]"
      }
     },
     "314669a0a4c24127b8efc47c031b2e04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ff71fd20b4c84c47902615969912e4a6",
       "max": 229167,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3cba7fc3261b4ccca84326fd8e1f2ce4",
       "tabbable": null,
       "tooltip": null,
       "value": 229167
      }
     },
     "3cba7fc3261b4ccca84326fd8e1f2ce4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3ceeb39f070340a6ac9b52e654bd6142": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bcfcdbe850684295bc840a26a07f6957",
       "max": 1534,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_56f238ea67c04aa09152af85b8b26f55",
       "tabbable": null,
       "tooltip": null,
       "value": 1534
      }
     },
     "3ea29d3c875145d5adf504de3376f8ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40ae142cacc942d8b7007177442ec8c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "424c98b725b849beb4b76e6158c37b27": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_03529ea70b49449e9632ce1390b29db7",
       "placeholder": "",
       "style": "IPY_MODEL_8b2cde3ae5504579934675df88cb8ae8",
       "tabbable": null,
       "tooltip": null,
       "value": "229k/229k[00:00&lt;00:00,5.49MB/s]"
      }
     },
     "433dfda64a3841b8872fbe074af9c01b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "460baffa9940478ab98712e4b982646b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c5bf3132d2f34f41ac808cf3fb89cd99",
        "IPY_MODEL_5ae93b66dfb1444099136ccf06abcb94",
        "IPY_MODEL_ad40a3c708ad41ba983b5107daf63051"
       ],
       "layout": "IPY_MODEL_4d79f03173fe4cbf8e1422d2e32741d0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4ccf46d309d94c90b8c1c886f0f9062a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b568398e04944285ba88d472d5b9e7a4",
        "IPY_MODEL_b10ef5c8f0b846fb90a98547ea1ab8f0",
        "IPY_MODEL_20a85283299642c697fb8deec10e8b13"
       ],
       "layout": "IPY_MODEL_feefd63e28ee4300972bf8bf83175192",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4d79f03173fe4cbf8e1422d2e32741d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "56f238ea67c04aa09152af85b8b26f55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5a913668ab6443ffa9bdf6bd6514231b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5ae93b66dfb1444099136ccf06abcb94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_433dfda64a3841b8872fbe074af9c01b",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b7b23aed103a485ba65f1fb83b403850",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "75fb4add3f7a4fbe98ba160761b678a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7e4606dbbd174af49f12434fbede8d41": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "830b2daa532645908ac39c8d073125ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8472951fb9da48a39a180fc3e64f5475": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8b2cde3ae5504579934675df88cb8ae8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9af83042b2d840a5a54d34b2a9b8a8dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fda9785e9af541fbbf9acbc826bc877d",
       "placeholder": "",
       "style": "IPY_MODEL_d8ee5c84adfd4aec8764f7d782edb3fe",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt:100%"
      }
     },
     "9cf8ced7feab4ceeb8885faf2619bb0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ad40a3c708ad41ba983b5107daf63051": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7e4606dbbd174af49f12434fbede8d41",
       "placeholder": "",
       "style": "IPY_MODEL_cfa6eb4388364783a86043d827d5f975",
       "tabbable": null,
       "tooltip": null,
       "value": "2.00/2.00[00:00&lt;00:00,148B/s]"
      }
     },
     "b10ef5c8f0b846fb90a98547ea1ab8f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f0abfa780337419c8d8ea03564b561fe",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_40ae142cacc942d8b7007177442ec8c1",
       "tabbable": null,
       "tooltip": null,
       "value": 112
      }
     },
     "b568398e04944285ba88d472d5b9e7a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_830b2daa532645908ac39c8d073125ca",
       "placeholder": "",
       "style": "IPY_MODEL_18ea5b392ac84ab6aaef8848ca1657df",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json:100%"
      }
     },
     "b7b23aed103a485ba65f1fb83b403850": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bcfcdbe850684295bc840a26a07f6957": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c5bf3132d2f34f41ac808cf3fb89cd99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5a913668ab6443ffa9bdf6bd6514231b",
       "placeholder": "",
       "style": "IPY_MODEL_8472951fb9da48a39a180fc3e64f5475",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json:100%"
      }
     },
     "ccca0f5e313d403391ba45a8cc1b2efd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cfa6eb4388364783a86043d827d5f975": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d8875954e9374aa2808edf3918e244f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8ee5c84adfd4aec8764f7d782edb3fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f0abfa780337419c8d8ea03564b561fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1a1385324ab44bbbba6ef86384f9367": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f7476e149a0b4f2092d95c45e74df7f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9af83042b2d840a5a54d34b2a9b8a8dc",
        "IPY_MODEL_314669a0a4c24127b8efc47c031b2e04",
        "IPY_MODEL_424c98b725b849beb4b76e6158c37b27"
       ],
       "layout": "IPY_MODEL_d8875954e9374aa2808edf3918e244f9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fda9785e9af541fbbf9acbc826bc877d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "feefd63e28ee4300972bf8bf83175192": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ff18c50103b048babc62613fa5d16937": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ff71fd20b4c84c47902615969912e4a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
